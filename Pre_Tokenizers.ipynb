{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f681588e",
   "metadata": {},
   "source": [
    "## work with pre-tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17db76bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isolated: [('Hello', (0, 5)), ('    ', (5, 9)), ('world!\\U0001fae8', (9, 16))]\n",
      "contiguous: [('Hello', (0, 5)), ('    ', (5, 9)), ('world!\\U0001fae8', (9, 16))]\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.pre_tokenizers import Split\n",
    "from tokenizers import Regex\n",
    "\n",
    "# create regex for \\s\n",
    "ws_re = Regex(r'\\s+')\n",
    "\n",
    "test_str = \"Hello    world!🫨\"\n",
    "isolated_pre_tokenizer = Split(ws_re, behavior='isolated')\n",
    "print(\"isolated:\", isolated_pre_tokenizer.pre_tokenize_str(test_str))\n",
    "contiguous_pre_tokenizer = Split(ws_re, behavior='contiguous')\n",
    "print(\"contiguous:\", contiguous_pre_tokenizer.pre_tokenize_str(test_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31c49473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just byte: [('HelloĠĠĠĠ1000Ġworld!ðŁ«¨', (0, 21))]\n",
      "sequence: [('Hello', (0, 5)), ('ĠĠĠ', (5, 8)), ('Ġ', (8, 9)), ('100', (9, 12)), ('0', (12, 13)), ('Ġworld', (13, 19)), ('!ðŁ«¨', (19, 21))]\n"
     ]
    }
   ],
   "source": [
    "# Define regex pattern\n",
    "regex_pattern = (\n",
    "    r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}|\"\n",
    "    r\" ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\n",
    ")\n",
    "\n",
    "# Define pre-tokenizers\n",
    "split = Split(pattern=Regex(regex_pattern), behavior=\"merged_with_next\", invert=False)\n",
    "byte = ByteLevel(add_prefix_space=False, use_regex=False)\n",
    "pre_tokenizer = Sequence([split, byte])\n",
    "\n",
    "test_str = \"Hello    1000 world!🫨\"\n",
    "print(\"just byte:\", byte.pre_tokenize_str(test_str))\n",
    "print(\"sequence:\", pre_tokenizer.pre_tokenize_str(test_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2779399b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isolated: [('Hello', (0, 5)), ('ĠĠĠ', (5, 8)), ('Ġ1', (8, 10)), ('00', (10, 12)), ('000', (12, 15)), ('Ġworld', (15, 21)), ('!ðŁ«¨', (21, 23))]\n"
     ]
    }
   ],
   "source": [
    "# Define regex pattern\n",
    "regex_pattern =  (\n",
    "    r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\"\n",
    "    r\"\\d(?=(\\d{2})+(\\D|$))| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\"\n",
    "    r\"\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\n",
    ")\n",
    "\n",
    "# Define pre-tokenizers\n",
    "split = Split(pattern=Regex(regex_pattern), behavior=\"merged_with_next\", invert=False)\n",
    "byte = ByteLevel(add_prefix_space=False, use_regex=False)\n",
    "pre_tokenizer = Sequence([split, byte])\n",
    "\n",
    "test_str = \"Hello    100000 world!🫨\"\n",
    "print(\"isolated:\", pre_tokenizer.pre_tokenize_str(test_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f696872f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecdef650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 000'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def split_number(number):\n",
    "    # Regular expression to find the right spot for inserting a space\n",
    "    # This looks for digit sequences that are followed by three more digits, from the right\n",
    "    # Ensure the pattern matches only in the right-to-left order\n",
    "    regex = r'(?<=\\d)(?=(\\d{3})+$)'\n",
    "\n",
    "    # Use re.sub to insert spaces at matched positions\n",
    "    result = re.sub(regex, ' ', number)\n",
    "\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "number = \"1000\"\n",
    "formatted_number = split_number(number)\n",
    "formatted_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96c8e241",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, Regex\n",
    "from tokenizers.models import BPE\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96ed3b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(f\"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cb050ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "fitting_corpus_path = \"/shared/3/projects/hiatus/TOKENIZER_wegmann/data/fitting-corpora/wikipedia\"\n",
    "corpus_name = os.path.basename(fitting_corpus_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b6ae0a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikipedia'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78cca0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
