{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d0dcd78",
   "metadata": {},
   "source": [
    "### on SADIRI 1%: Do Candidate/Query Overlap: No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd2f1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the overlap between candidate and query files\n",
    "from datasets import load_from_disk\n",
    "directory_path = '/shared/3/projects/hiatus/aggregated_trainset_v2/content_masking_research/down_1_shuffle/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07ffe9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['query_id', 'query_authorID', 'query_text', 'candidate_id', 'candidate_authorID', 'candidate_text'],\n",
      "    num_rows: 40796\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_from_disk(directory_path)\n",
    "\n",
    "# Display the first few rows of the train split\n",
    "print(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "017e96bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no overlapping IDs between query_id and candidate_id.\n"
     ]
    }
   ],
   "source": [
    "# Extract the query_id and candidate_id columns from the train split\n",
    "query_ids = set(dataset['train']['query_id'])\n",
    "candidate_ids = set(dataset['train']['candidate_id'])\n",
    "\n",
    "# Find the intersection of query_id and candidate_id\n",
    "overlap = query_ids.intersection(candidate_ids)\n",
    "\n",
    "# Check if there is any overlap\n",
    "if overlap:\n",
    "    print(\"There are overlapping IDs between query_id and candidate_id:\")\n",
    "    print(overlap)\n",
    "else:\n",
    "    print(\"There are no overlapping IDs between query_id and candidate_id.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2b7799",
   "metadata": {},
   "source": [
    "## On SADIRI 100%, amazon: do candidate/query overlap?: No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77c29f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Keys in the train_candidates.jsonl DataFrame:\n",
      "Index(['documentID', 'authorIDs', 'fullText', 'spanAttribution', 'isNeedle',\n",
      "       'collectionNum', 'source', 'dateCollected', 'publiclyAvailable',\n",
      "       'deidentified', 'languages', 'lengthWords', 'dateCreated',\n",
      "       'timeCreated', 'sourceSpecific'],\n",
      "      dtype='object')\n",
      "                             documentID                 authorIDs  \\\n",
      "0  7041c0a5-6ef6-4e41-8630-5a8409583d89  ['A0103849GBVWICKXD4T6']   \n",
      "\n",
      "                                            fullText  \\\n",
      "0  Bought this as a possible EDC item, but found ...   \n",
      "\n",
      "                                     spanAttribution  isNeedle collectionNum  \\\n",
      "0  [{'authorID': 'A0103849GBVWICKXD4T6', 'start':...     False        amazon   \n",
      "\n",
      "   source dateCollected  publiclyAvailable  deidentified languages  \\\n",
      "0  amazon    2022-12-31               True          True      [en]   \n",
      "\n",
      "   lengthWords  dateCreated  timeCreated  \\\n",
      "0           72  09 15, 2014  09 15, 2014   \n",
      "\n",
      "                                      sourceSpecific  \n",
      "0  {'product': 'B006M9NIEI', 'sentiment': 4.0, 'p...  \n",
      "There are no overlapping IDs between query_id and candidate_id.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "amazon_base = '/shared/3/projects/hiatus/aggregated_trainset_v2/content_masking_research/amazon/'\n",
    "# Load the train_queries.jsonl file\n",
    "queries_path = amazon_base + 'train_queries.jsonl'\n",
    "queries_df = pd.read_json(queries_path, lines=True)\n",
    "\n",
    "# Load the train_candidates.jsonl file\n",
    "candidates_path = amazon_base + 'train_candidates.jsonl'\n",
    "candidates_df = pd.read_json(candidates_path, lines=True)\n",
    "\n",
    "# Print the keys of the train_candidates.jsonl DataFrame\n",
    "print(\"\\nKeys in the train_candidates.jsonl DataFrame:\")\n",
    "print(candidates_df.columns)\n",
    "print(candidates_df[:1])\n",
    "\n",
    "# Extract query_id and candidate_id from the jsonl datasets\n",
    "query_ids_jsonl = set(queries_df['documentID'])\n",
    "candidate_ids_jsonl = set(candidates_df['documentID'])\n",
    "\n",
    "# Check for overlap between query_id and candidate_id\n",
    "overlap = query_ids_jsonl.intersection(candidate_ids_jsonl)\n",
    "\n",
    "if overlap:\n",
    "    print(\"There are overlapping IDs between query_id and candidate_id:\")\n",
    "    print(overlap)\n",
    "else:\n",
    "    print(\"There are no overlapping IDs between query_id and candidate_id.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9759b12c",
   "metadata": {},
   "source": [
    "## Get the size of full (not with held-out) SADIRI TRAIN dataset in terms of #words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2932b489",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_base = \"/shared/3/projects/hiatus/aggregated_trainset_v2/content_masking_research/\"\n",
    "data_folders = [\"amazon\", \"ao3\", \"bookcorpus\", \"gmane\", \"nytimes-articles-and-comments\", \"pubmed\", \"realnews\", \"reddit\", \"stackexchange\", \"wiki_articles\"]\n",
    "data_folders = [project_base + folder_name for folder_name in data_folders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "516df9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def count_words_in_fullText(file_path):\n",
    "    # Load the JSONL file into a DataFrame\n",
    "    df = pd.read_json(file_path, lines=True)\n",
    "    \n",
    "    # Check if the 'fullText' column exists\n",
    "    if 'fullText' in df.columns:\n",
    "        # Count the number of words in the 'fullText' column\n",
    "        word_count = df['fullText'].str.split().str.len().sum()\n",
    "        return word_count\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def process_datasets(dataset_paths):\n",
    "    total_word_count = 0\n",
    "    word_counts = {}\n",
    "    \n",
    "    for dataset_path in dataset_paths:\n",
    "        dataset_name = os.path.basename(dataset_path)\n",
    "        \n",
    "        # Initialize word count for the dataset\n",
    "        dataset_word_count = 0\n",
    "        \n",
    "        # Process only the specified JSONL files in the dataset folder\n",
    "        for file_name in ['train_queries.jsonl', 'train_candidates.jsonl']:\n",
    "            file_path = os.path.join(dataset_path, file_name)\n",
    "            if os.path.exists(file_path):\n",
    "                file_word_count = count_words_in_fullText(file_path)\n",
    "                dataset_word_count += file_word_count\n",
    "        \n",
    "        # Store the word count for the dataset\n",
    "        word_counts[dataset_name] = dataset_word_count\n",
    "        total_word_count += dataset_word_count\n",
    "    \n",
    "    return word_counts, total_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "082c20a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in amazon: 31650279\n",
      "Total number of words in ao3: 573926907\n",
      "Total number of words in bookcorpus: 57367225\n",
      "Total number of words in gmane: 141837101\n",
      "Total number of words in nytimes-articles-and-comments: 24131163\n",
      "Total number of words in pubmed: 9748317\n",
      "Total number of words in realnews: 272933709\n",
      "Total number of words in reddit: 446769021\n",
      "Total number of words in stackexchange: 153991860\n",
      "Total number of words in wiki_articles: 34779747\n",
      "Overall total number of words: 1747135329\n"
     ]
    }
   ],
   "source": [
    "# Process the datasets and get the word counts\n",
    "word_counts, total_word_count = process_datasets(data_folders)\n",
    "\n",
    "# Print the word counts for each dataset\n",
    "for dataset_name, word_count in word_counts.items():\n",
    "    print(f\"Total number of words in {dataset_name}: {word_count}\")\n",
    "\n",
    "# Print the overall total word count\n",
    "print(f\"Overall total number of words: {total_word_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19cb8a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'': 34779747}, 1747135329)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts, total_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbe28f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
