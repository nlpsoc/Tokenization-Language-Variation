{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f9a32a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "cache_dir = \"/shared/3/projects/hiatus/EVAL_wegmann/cache/huggingface\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = cache_dir\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = cache_dir\n",
    "output_folder = \"/shared/3/projects/hiatus/EVAL_wegmann/tiny-BERT/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bc9d629",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mannawegmann\u001b[0m (\u001b[33msadiri-michigan\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/annaweg/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mannawegmann\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/annaweg/StyleTokenizer/src/wandb/run-20240514_135741-la860s36</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/annawegmann/bert-tiny-pretraining/runs/la860s36' target=\"_blank\">exalted-pyramid-21</a></strong> to <a href='https://wandb.ai/annawegmann/bert-tiny-pretraining' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/annawegmann/bert-tiny-pretraining' target=\"_blank\">https://wandb.ai/annawegmann/bert-tiny-pretraining</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/annawegmann/bert-tiny-pretraining/runs/la860s36' target=\"_blank\">https://wandb.ai/annawegmann/bert-tiny-pretraining/runs/la860s36</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/annawegmann/bert-tiny-pretraining/runs/la860s36?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7faf2305aa60>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# Login to WandB account (this might prompt for an API key if not logged in already)\n",
    "wandb.login(key=\"c042d6be624a66d40b7f2a82a76e343896608cf0\")\n",
    "# Initialize a new run with a project name\n",
    "wandb.init(project=\"bert-tiny-pretraining\", entity=\"annawegmann\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdfa04c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.9/site-packages/datasets/load.py:1486: FutureWarning: The repository for wikipedia contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wikipedia\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9ba82d0946b456da3d29b98c54b4125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loading dataset, following https://huggingface.co/blog/pretraining-bert#4-pre-train-bert-on-habana-gaudi\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "\n",
    "bookcorpus = load_dataset(\"bookcorpus\", split=\"train\")\n",
    "wiki = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\")\n",
    "wiki = wiki.remove_columns([col for col in wiki.column_names if col != \"text\"])  # only keep the 'text' column\n",
    "\n",
    "assert bookcorpus.features.type == wiki.features.type\n",
    "raw_datasets = concatenate_datasets([bookcorpus, wiki])\n",
    "raw_datasets = raw_datasets.shuffle(seed=42)  # You can set a seed for reproducibility\n",
    "\n",
    "# full_dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")  # (before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe85f0f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80462898"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f02d195",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "735f3fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "\n",
    "# Create a new DatasetDict to store the truncated datasets\n",
    "dataset = DatasetDict()\n",
    "\n",
    "# Truncate each split to the first 10 entries and store it in the new DatasetDict\n",
    "# for split, ds in raw_datasets.items():\n",
    "#    dataset[split] = ds.select(range(512))\n",
    "dataset = raw_datasets.select(range(512))  # [\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5516b4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Column train not in the dataset. Current columns in the dataset: ['text']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m trainer \u001b[38;5;241m=\u001b[39m BpeTrainer(vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30000\u001b[39m, special_tokens\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[UNK]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[CLS]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[SEP]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[PAD]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[MASK]\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      8\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpre_tokenizer \u001b[38;5;241m=\u001b[39m Whitespace()\n\u001b[0;32m----> 9\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mtrain_from_iterator(\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m], trainer\u001b[38;5;241m=\u001b[39mtrainer)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedTokenizerFast\n\u001b[1;32m     11\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m PreTrainedTokenizerFast(tokenizer_object\u001b[38;5;241m=\u001b[39mtokenizer)\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.9/site-packages/datasets/arrow_dataset.py:2861\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2859\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2860\u001b[0m     \u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2861\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.9/site-packages/datasets/arrow_dataset.py:2845\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2843\u001b[0m format_kwargs \u001b[38;5;241m=\u001b[39m format_kwargs \u001b[38;5;28;01mif\u001b[39;00m format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m   2844\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m-> 2845\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m \u001b[43mquery_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2846\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m format_table(\n\u001b[1;32m   2847\u001b[0m     pa_subtable, key, formatter\u001b[38;5;241m=\u001b[39mformatter, format_columns\u001b[38;5;241m=\u001b[39mformat_columns, output_all_columns\u001b[38;5;241m=\u001b[39moutput_all_columns\n\u001b[1;32m   2848\u001b[0m )\n\u001b[1;32m   2849\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.9/site-packages/datasets/formatting/formatting.py:584\u001b[0m, in \u001b[0;36mquery_table\u001b[0;34m(table, key, indices)\u001b[0m\n\u001b[1;32m    582\u001b[0m         _raise_bad_key_type(key)\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 584\u001b[0m     \u001b[43m_check_valid_column_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    586\u001b[0m     size \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mnum_rows \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m table\u001b[38;5;241m.\u001b[39mnum_rows\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.9/site-packages/datasets/formatting/formatting.py:521\u001b[0m, in \u001b[0;36m_check_valid_column_key\u001b[0;34m(key, columns)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_valid_column_key\u001b[39m(key: \u001b[38;5;28mstr\u001b[39m, columns: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[0;32m--> 521\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in the dataset. Current columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Column train not in the dataset. Current columns in the dataset: ['text']\""
     ]
    }
   ],
   "source": [
    "# train a tokenizer\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\", sep_token=\"[SEP]\", pad_token=\"[PAD]\", cls_token=\"[CLS]\", mask_token=\"[MASK]\"))\n",
    "trainer = BpeTrainer(vocab_size=30000, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.train_from_iterator(dataset[\"train\"][\"text\"], trainer=trainer)\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]', 'mask_token': '[MASK]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19b3159a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.9/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50267, 128)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM, RobertaTokenizer, BertConfig, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]', 'mask_token': '[MASK]'})\n",
    "\n",
    "\n",
    "# Define a lightweight BERT configuration\n",
    "config = BertConfig(\n",
    "    vocab_size=tokenizer.vocab_size,  # Standard BERT vocabulary size\n",
    "    hidden_size=128,   # Reduced hidden layer size\n",
    "    num_hidden_layers=2,  # Fewer layers\n",
    "    num_attention_heads=2,  # Fewer attention heads, H/64\n",
    "    intermediate_size=512  # Size of the intermediate (feed-forward) layer, 4H\n",
    ")\n",
    "# model = BertForMaskedLM(config=config)\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained('prajjwal1/bert-tiny') # sequence len still 512\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4dc1f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and prepare the dataset\n",
    "def tokenize_and_encode(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=\"max_length\", max_length=512)\n",
    "# Apply tokenization and encoding\n",
    "tokenized_datasets = dataset.map(tokenize_and_encode, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21b80d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d259f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:11, Epoch 6/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=9.577440185546875, metrics={'train_runtime': 12.3172, 'train_samples_per_second': 259.8, 'train_steps_per_second': 8.119, 'total_flos': 4559683584000.0, 'train_loss': 9.577440185546875, 'epoch': 6.25})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_folder + \"bert-tiny-pretrained\",\n",
    "    overwrite_output_dir=True,\n",
    "    max_steps=100,\n",
    "    per_device_train_batch_size=32,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=output_folder + 'logs',\n",
    "    logging_steps=500,\n",
    "    report_to=\"wandb\",  # Enables WandB integration\n",
    ")\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Start pretraining\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a4da79a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/shared/3/projects/hiatus/EVAL_wegmann/tiny-BERT/bert-tiny-pretrained/roberta-base/tokenizer_config.json',\n",
       " '/shared/3/projects/hiatus/EVAL_wegmann/tiny-BERT/bert-tiny-pretrained/roberta-base/special_tokens_map.json',\n",
       " '/shared/3/projects/hiatus/EVAL_wegmann/tiny-BERT/bert-tiny-pretrained/roberta-base/vocab.json',\n",
       " '/shared/3/projects/hiatus/EVAL_wegmann/tiny-BERT/bert-tiny-pretrained/roberta-base/merges.txt',\n",
       " '/shared/3/projects/hiatus/EVAL_wegmann/tiny-BERT/bert-tiny-pretrained/roberta-base/added_tokens.json',\n",
       " '/shared/3/projects/hiatus/EVAL_wegmann/tiny-BERT/bert-tiny-pretrained/roberta-base/tokenizer.json')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained model\n",
    "trainer.save_model(output_folder + \"bert-tiny-pretrained/roberta-base\")\n",
    "tokenizer.save_pretrained(output_folder + \"bert-tiny-pretrained/roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0cfaae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# add STEL folder to path\n",
    "sys.path.append('../../STEL/src/')\n",
    "import torch\n",
    "\n",
    "from STEL import STEL\n",
    "from STEL.similarity import Similarity, cosine_sim\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class SBERTSimilarity(Similarity):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = SentenceTransformer(output_folder + \"bert-tiny-pretrained\")\n",
    "        self.model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def similarities(self, sentences_1, sentences_2):\n",
    "        with torch.no_grad():\n",
    "            sentence_emb_1 = self.model.encode(sentences_1, show_progress_bar=False)\n",
    "            sentence_emb_2 = self.model.encode(sentences_2, show_progress_bar=False)\n",
    "        return [cosine_sim(sentence_emb_1[i], sentence_emb_2[i]) for i in range(len(sentences_1))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cae3c94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 17:53:22,865 : INFO : Load pretrained SentenceTransformer: /shared/3/projects/hiatus/EVAL_wegmann/tiny-BERT/bert-tiny-pretrained\n",
      "2024-05-10 17:53:22,868 : WARNING : No sentence-transformers model found with name /shared/3/projects/hiatus/EVAL_wegmann/tiny-BERT/bert-tiny-pretrained. Creating a new one with MEAN pooling.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /shared/3/projects/hiatus/EVAL_wegmann/tiny-BERT/bert-tiny-pretrained and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2024-05-10 17:53:22,971 : INFO : Use pytorch device: cuda\n",
      "2024-05-10 17:53:23,015 : INFO : Running STEL framework \n",
      "2024-05-10 17:53:23,030 : INFO : Filtering out tasks with low agreement ... \n",
      "2024-05-10 17:53:23,033 : INFO :       on dimensions ['formality', 'simplicity'] using files ['/home/annaweg/STEL/src/../Data/STEL/dimensions/_quad_stel-dimensions_formal-815_complex-815.tsv']...\n",
      "2024-05-10 17:53:23,050 : INFO :       on characteristics ['nbr_substitution', 'contraction', 'emotives', 'em-subj-pronoun'] using file ['/home/annaweg/STEL/src/../Data/STEL/characteristics/quad_questions_char_substitution.tsv', '/home/annaweg/STEL/src/../Data/STEL/characteristics/quad_questions_char_contraction.tsv', '/home/annaweg/STEL/src/../Data/STEL/characteristics/quad_questions_char_emotives.tsv', '/home/annaweg/STEL/src/../Data/STEL/characteristics/quad_questions_em-subj-pronoun_100.tsv']\n",
      "2024-05-10 17:53:23,052 : INFO : Evaluating on 1630 style dim and 500 style char tasks ... \n",
      "2024-05-10 17:53:23,057 : INFO : Evaluation for method SBERTSimilarity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on original STEL tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 17:53:29,599 : INFO : random assignments: 0\n",
      "2024-05-10 17:53:29,604 : INFO :   Accuracy at 0.5596244131455399, without random 0.5596244131455399 with 0 questions\n",
      "2024-05-10 17:53:29,606 : INFO :   Accuracy formality at 0.5214723926380368 for 815 task instances, without random 0.5214723926380368 with 815 left questions\n",
      "2024-05-10 17:53:29,609 : INFO :   Accuracy simplicity at 0.5361963190184049 for 815 task instances, without random 0.5361963190184049 with 815 left questions\n",
      "2024-05-10 17:53:29,611 : INFO :   Accuracy nbr_substitution at 0.6 for 100 task instances, without random 0.6 with 100 left questions\n",
      "2024-05-10 17:53:29,612 : INFO :   Accuracy contraction at 0.65 for 100 task instances, without random 0.65 with 100 left questions\n",
      "2024-05-10 17:53:29,614 : INFO :   Accuracy emotives at 0.7 for 200 task instances, without random 0.7 with 200 left questions\n",
      "2024-05-10 17:53:29,616 : INFO :   Accuracy em-subj-pronoun at 0.65 for 100 task instances, without random 0.65 with 100 left questions\n",
      "2024-05-10 17:53:29,625 : INFO : Saved results to output/STEL-quadruple_SBERTSimilarity.tsv\n",
      "2024-05-10 17:53:29,635 : INFO : Saved single predictions to output/STEL_single-pred-quadruple_SBERTSimilarity.tsv\n",
      "2024-05-10 17:53:29,770 : INFO : Running STEL framework \n",
      "2024-05-10 17:53:29,774 : INFO : Evaluation for method SBERTSimilarity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Model Name  Accuracy  Accuracy formality  Accuracy simplicity  \\\n",
      "0  SBERTSimilarity  0.559624            0.521472             0.536196   \n",
      "\n",
      "   Accuracy nbr_substitution  Accuracy contraction  Accuracy emotives  \\\n",
      "0                        0.6                  0.65                0.7   \n",
      "\n",
      "   Accuracy em-subj-pronoun  \n",
      "0                      0.65  \n",
      "Performance on STEL-Or-Content tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 17:53:33,094 : INFO : random assignments: 0\n",
      "2024-05-10 17:53:33,101 : INFO :   Accuracy at 0.09342723004694836, without random 0.09342723004694836 with 0 questions\n",
      "2024-05-10 17:53:33,104 : INFO :   Accuracy formality at 0.22085889570552147 for 815 task instances, without random 0.22085889570552147 with 815 left questions\n",
      "2024-05-10 17:53:33,107 : INFO :   Accuracy simplicity at 0.011042944785276074 for 815 task instances, without random 0.011042944785276074 with 815 left questions\n",
      "2024-05-10 17:53:33,109 : INFO :   Accuracy nbr_substitution at 0.05 for 100 task instances, without random 0.05 with 100 left questions\n",
      "2024-05-10 17:53:33,111 : INFO :   Accuracy contraction at 0.0 for 100 task instances, without random 0.0 with 100 left questions\n",
      "2024-05-10 17:53:33,114 : INFO :   Accuracy emotives at 0.02 for 200 task instances, without random 0.02 with 200 left questions\n",
      "2024-05-10 17:53:33,116 : INFO :   Accuracy em-subj-pronoun at 0.01 for 100 task instances, without random 0.01 with 100 left questions\n",
      "2024-05-10 17:53:33,126 : INFO : Saved results to output/STEL-triple_SBERTSimilarity.tsv\n",
      "2024-05-10 17:53:33,136 : INFO : Saved single predictions to output/STEL_single-pred-triple_SBERTSimilarity.tsv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Model Name  Accuracy  Accuracy formality  Accuracy simplicity  \\\n",
      "0  SBERTSimilarity  0.093427            0.220859             0.011043   \n",
      "\n",
      "   Accuracy nbr_substitution  Accuracy contraction  Accuracy emotives  \\\n",
      "0                       0.05                   0.0               0.02   \n",
      "\n",
      "   Accuracy em-subj-pronoun  \n",
      "0                      0.01  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "STEL.eval_on_STEL(style_objects=[SBERTSimilarity()])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d95977",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
