{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-12T13:00:32.285540Z",
     "start_time": "2024-12-12T13:00:23.712919Z"
    }
   },
   "outputs": [],
   "source": [
    "# import SNLI dataset from h\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"snli\")\n",
    "# Access the splits\n",
    "train_data = dataset['train']\n",
    "validation_data = dataset['validation']\n",
    "test_data = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "{'premise': 'A person on a horse jumps over a broken down airplane.',\n 'hypothesis': 'A person is training his horse for a competition.',\n 'label': 1}"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-11T10:51:08.902086Z",
     "start_time": "2024-12-11T10:51:08.887468Z"
    }
   },
   "id": "1111efc1a74c64b2"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "# Define the punctuation set we care about\n",
    "PUNCT = {'.', '!', '?'}\n",
    "common_contractions = {\n",
    "    \"do not\": \"don't\",\n",
    "    \"is not\": \"isn't\",\n",
    "    \"are not\": \"aren't\",\n",
    "    \"it is\": \"it's\",\n",
    "    \"that is\": \"that's\",\n",
    "    \"we are\": \"we're\",\n",
    "    \"you are\": \"you're\",\n",
    "    \"I am\": \"I'm\",\n",
    "    \"I will\": \"I'll\",\n",
    "    \"I would\": \"I'd\",\n",
    "    \"they are\": \"they're\",\n",
    "    \"will not\": \"won't\",\n",
    "    \"can not\": \"can't\",\n",
    "    \"there is\": \"there's\"\n",
    "}\n",
    "\n",
    "def encased_with_apostrophes(text):\n",
    "    # Check if the text is encased with standard quotes (artificat in SNLI)\n",
    "    return text.startswith('\"') and text.endswith('\"')\n",
    "\n",
    "def starts_with_uppercase_word(text):\n",
    "    # Strip leading whitespace and check if the first character is uppercase\n",
    "    text = text.lstrip()\n",
    "    if not text:\n",
    "        return False\n",
    "    return text[0].isupper()\n",
    "\n",
    "def ends_with_punctuation(text):\n",
    "    # Check if the last non-whitespace character is punctuation\n",
    "    text = text.rstrip()\n",
    "    return len(text) > 0 and text[-1] in PUNCT\n",
    "\n",
    "def contains_punctuation(text):\n",
    "    # Check if there's any punctuation in the text\n",
    "    # return any(ch in string.punctuation for ch in text)\n",
    "    return any(ch in PUNCT for ch in text)\n",
    "\n",
    "def whitespace_encoding(text):\n",
    "    # Identify all distinct whitespace code points used in the text.\n",
    "    # This will differentiate between e.g. U+0020 (normal space) and U+00A0 (no-break space).\n",
    "    whitespaces = set()\n",
    "    for ch in text:\n",
    "        if ch.isspace():\n",
    "            whitespaces.add(ord(ch))  # store the code point\n",
    "    return whitespaces\n",
    "\n",
    "def apostrophe_encoding(text):\n",
    "    # Extract all apostrophe-like characters: common are `'` and `’`\n",
    "    # Return a set of apostrophe chars used\n",
    "    # If you want to be more comprehensive, include other variants.\n",
    "    # Here we include backtick and right single quotation mark as well.\n",
    "    possible_apostrophes = {\"'\", \"’\", \"`\"}\n",
    "    apostrophes = {ch for ch in text if ch in possible_apostrophes}\n",
    "    return apostrophes\n",
    "\n",
    "def extract_number_patterns(text):\n",
    "    # Find all numbers and their surrounding formatting.\n",
    "    # We'll capture substrings around each digit sequence that may include punctuation and spacing.\n",
    "    number_patterns = []\n",
    "    for match in re.finditer(r\"\\d+\", text):\n",
    "        start, end = match.span()\n",
    "        # Extend outwards to include punctuation/whitespace directly adjacent to the digits\n",
    "        left = start\n",
    "        while left > 0 and (text[left-1] in string.punctuation or text[left-1].isspace()):\n",
    "            left -= 1\n",
    "        right = end\n",
    "        while right < len(text) and (text[right] in string.punctuation or text[right].isspace()):\n",
    "            right += 1\n",
    "        substring = text[left:right].strip()\n",
    "        number_patterns.append(substring)\n",
    "    return number_patterns\n",
    "\n",
    "def compare_number_formats(patterns1, patterns2):\n",
    "    # Check if both lists have the same number of numeric patterns\n",
    "    if len(patterns1) != len(patterns2):\n",
    "        return False\n",
    "    # Compare each pair of patterns\n",
    "    for p1, p2 in zip(patterns1, patterns2):\n",
    "        # Compare digits sequence\n",
    "        digits1 = re.sub(r\"\\D\", \"\", p1)\n",
    "        digits2 = re.sub(r\"\\D\", \"\", p2)\n",
    "        if digits1 != digits2:\n",
    "            return False\n",
    "        # Compare non-digit formatting\n",
    "        non_digits1 = re.sub(r\"\\d\", \"\", p1)\n",
    "        non_digits2 = re.sub(r\"\\d\", \"\", p2)\n",
    "        if non_digits1 != non_digits2:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def contains_newline(text):\n",
    "    return \"\\n\" in text\n",
    "\n",
    "def contains_contractions(text):\n",
    "    # Check if text contains any of the known contracted forms\n",
    "    pattern = r'\\b(?:' + '|'.join(map(re.escape, common_contractions.values())) + r')\\b'\n",
    "    return bool(re.search(pattern, text, flags=re.IGNORECASE))\n",
    "\n",
    "def can_form_contractions(text):\n",
    "    # Check if text contains any expansions that could be turned into known contractions\n",
    "    # If we find at least one expansion pattern in the text, return True\n",
    "    for expansion in common_contractions.keys():\n",
    "        # Create a regex pattern for the expansion\n",
    "        exp_words = expansion.split()\n",
    "        pattern = r'\\b' + r'\\s+'.join(exp_words) + r'\\b'\n",
    "        if re.search(pattern, text, flags=re.IGNORECASE):\n",
    "            return True\n",
    "    return False\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-12T13:00:32.312207Z",
     "start_time": "2024-12-12T13:00:32.296918Z"
    }
   },
   "id": "1fee0ccf675459f4"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def compare_texts(text1, text2):\n",
    "    conditions = []\n",
    "    conditions.append(encased_with_apostrophes(text1) == encased_with_apostrophes(text2))\n",
    "    conditions.append(starts_with_uppercase_word(text1) == starts_with_uppercase_word(text2))\n",
    "    conditions.append(ends_with_punctuation(text1) == ends_with_punctuation(text2))\n",
    "    conditions.append(contains_punctuation(text1) == contains_punctuation(text2))\n",
    "    conditions.append(whitespace_encoding(text1) == whitespace_encoding(text2))\n",
    "    conditions.append(apostrophe_encoding(text1) == apostrophe_encoding(text2))\n",
    "    patterns1 = extract_number_patterns(text1)\n",
    "    patterns2 = extract_number_patterns(text2)\n",
    "    conditions.append(compare_number_formats(patterns1, patterns2))\n",
    "    conditions.append(contains_contractions(text1) == contains_contractions(text2))\n",
    "    similarity = sum(conditions) / len(conditions)\n",
    "    return similarity\n",
    "\n",
    "def make_texts_similar(text1, text2):\n",
    "    # Adjust Quotes\n",
    "    if encased_with_apostrophes(text1) != encased_with_apostrophes(text2):\n",
    "        if encased_with_apostrophes(text1) and not encased_with_apostrophes(text2):\n",
    "            text2 = '\"' + text2 + '\"'\n",
    "        elif not encased_with_apostrophes(text1) and encased_with_apostrophes(text2):\n",
    "            text2 = text2[1:-1]\n",
    "    \n",
    "    # Adjust capitalization at the start\n",
    "    if starts_with_uppercase_word(text1) != starts_with_uppercase_word(text2):\n",
    "        if starts_with_uppercase_word(text1) and not starts_with_uppercase_word(text2):\n",
    "            stripped = text2.lstrip()\n",
    "            if stripped:\n",
    "                start_idx = len(text2) - len(stripped)\n",
    "                text2 = text2[:start_idx] + stripped[0].upper() + stripped[1:]\n",
    "        elif not starts_with_uppercase_word(text1) and starts_with_uppercase_word(text2):\n",
    "            stripped = text2.lstrip()\n",
    "            if stripped:\n",
    "                start_idx = len(text2) - len(stripped)\n",
    "                text2 = text2[:start_idx] + stripped[0].lower() + stripped[1:]\n",
    "\n",
    "    # Adjust punctuation at the end\n",
    "    if ends_with_punctuation(text1) != ends_with_punctuation(text2):\n",
    "        if ends_with_punctuation(text1) and not ends_with_punctuation(text2):\n",
    "            t1_end_punct = text1.rstrip()[-1]\n",
    "            text2 = text2.rstrip() + t1_end_punct\n",
    "        elif not ends_with_punctuation(text1) and ends_with_punctuation(text2):\n",
    "            text2 = text2.rstrip()\n",
    "            while text2 and text2[-1] in PUNCT:\n",
    "                text2 = text2[:-1]\n",
    "\n",
    "    # Now text1 and text2 should be similar in capitalization and end punctuation.\n",
    "    # Apostrophe and whitespace encoding is the same initially.\n",
    "    # Randomly decide if we want to change them for BOTH texts simultaneously.\n",
    "    \n",
    "    # Random chance to change whitespace encoding for both\n",
    "    # For example, replace all regular spaces with non-breaking spaces in both texts\n",
    "    if random.random() < 0.5:\n",
    "        # Check if we have spaces\n",
    "        if \" \" in text1 or \" \" in text2:\n",
    "            # Replace all spaces with non-breaking spaces\n",
    "            text1 = text1.replace(\" \", \"\\u00A0\")\n",
    "            text2 = text2.replace(\" \", \"\\u00A0\")\n",
    "\n",
    "    # Random chance to change the dialect for both texts\n",
    "    if random.random() < 0.5:\n",
    "        # Randomly select a dialect from DIALECTS\n",
    "        attempts = 5  # limit attempts to avoid infinite loops\n",
    "        changed = False\n",
    "        while attempts > 0 and not changed:\n",
    "            try:\n",
    "                dialect = random.choice(DIALECTS)\n",
    "                text1 = dialect.transform(text1)\n",
    "                text2 = dialect.transform(text2)\n",
    "                changed = True\n",
    "            except:  # if the dialect transformation fails\n",
    "                print(f\"Failed to transform {text1} or {text2}. Retrying {attempts} more times ...\")\n",
    "            attempts -= 1\n",
    "\n",
    "    # Random chance to toggle apostrophe encoding for both\n",
    "    # If we have apostrophes, switch them from `'` to `’` or vice versa\n",
    "    apos1 = apostrophe_encoding(text1)\n",
    "    apos2 = apostrophe_encoding(text2)\n",
    "    # Since they are initially the same, we can just pick a toggle.\n",
    "    if random.random() < 0.5 and (apos1 and apos2):\n",
    "        # If we have at least one type of apostrophe in the texts\n",
    "        # If we find `'` in texts, replace it with `’`, else if `’` then replace with `'`\n",
    "        if \"'\" in text1 or \"'\" in text2:\n",
    "            # Replace `'` with `’`\n",
    "            text1 = text1.replace(\"'\", \"’\")\n",
    "            text2 = text2.replace(\"'\", \"’\")\n",
    "        elif \"’\" in text1 or \"’\" in text2:\n",
    "            # Replace `’` with `'`\n",
    "            text1 = text1.replace(\"’\", \"'\")\n",
    "            text2 = text2.replace(\"’\", \"'\")\n",
    "\n",
    "    return text1, text2\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-12T13:05:04.865918Z",
     "start_time": "2024-12-12T13:05:04.862203Z"
    }
   },
   "id": "3dd4afabfbbafb88"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.8/12.8 MB\u001B[0m \u001B[31m28.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/anna/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/anna/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/anna/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/anna/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/anna/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/anna/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/anna/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/anna/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/anna/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/anna/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/anna/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/anna/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/anna/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/anna/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/anna/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/anna/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in /Users/anna/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /Users/anna/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/anna/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/anna/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/anna/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/anna/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/anna/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /Users/anna/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/anna/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/anna/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/anna/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/anna/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/anna/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.7.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/anna/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/anna/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/anna/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.1.1)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/anna/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/anna/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /Users/anna/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.0)\n",
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001B[38;5;3m⚠ Restart to reload dependencies\u001B[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to /Users/anna/nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/anna/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from multivalue import Dialects\n",
    "DIALECTS = [Dialects.ColloquialSingaporeDialect(), Dialects.AfricanAmericanVernacular(), Dialects.ChicanoDialect(), Dialects.IndianDialect(), Dialects.AppalachianDialect(), \n",
    "            Dialects.NorthEnglandDialect(), Dialects.MalaysianDialect(), Dialects.AustralianDialect(), Dialects.HongKongDialect(), Dialects.NewZealandDialect(),\n",
    "            Dialects.NigerianDialect(), Dialects.PakistaniDialect(), Dialects.PhilippineDialect(), Dialects.SoutheastAmericanEnclaveDialect()]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-12T13:01:26.581212Z",
     "start_time": "2024-12-12T13:00:32.339993Z"
    }
   },
   "id": "c3018574e04c99ac"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def flip_quotes(t):\n",
    "    if encased_with_apostrophes(t):\n",
    "        return t[1:-1], True\n",
    "    else:\n",
    "        return '\"' + t + '\"', True\n",
    "    \n",
    "def flip_capitalization(t):\n",
    "    stripped = t.lstrip()\n",
    "    if not stripped:\n",
    "        return t, False\n",
    "    start_idx = len(t) - len(stripped)\n",
    "    first_char = stripped[0]\n",
    "    if first_char.isalpha():\n",
    "        flipped = first_char.lower() if first_char.isupper() else first_char.upper()\n",
    "        new_t = t[:start_idx] + flipped + stripped[1:]\n",
    "        changed = (new_t != t)\n",
    "        return new_t, changed\n",
    "    else:\n",
    "        return t, False\n",
    "\n",
    "def toggle_end_punctuation(t):\n",
    "    if ends_with_punctuation(t):\n",
    "        original = t\n",
    "        t = t.rstrip()\n",
    "        while t and t[-1] in PUNCT:\n",
    "            t = t[:-1]\n",
    "        changed = (t != original)\n",
    "        return t, changed\n",
    "    else:\n",
    "        return t + \".\", True\n",
    "\n",
    "# def toggle_punctuation_presence(t):\n",
    "#     if contains_punctuation(t):\n",
    "#         original = t\n",
    "#         t = \"\".join(ch for ch in t if ch not in PUNCT).rstrip()\n",
    "#         changed = (t != original)\n",
    "#         return t, changed\n",
    "#     else:\n",
    "#         return t, False\n",
    "\n",
    "def toggle_whitespace_encoding(t):\n",
    "    # Assume it only includes \" \" whitespaces. Change those to non-breaking spaces (\\u00A0)\n",
    "    original = t\n",
    "    if \" \" in t:\n",
    "        # Replace all spaces with non-breaking spaces\n",
    "        t = t.replace(\" \", \"\\u00A0\")\n",
    "        changed = (t != original)\n",
    "        return t, changed\n",
    "    else:\n",
    "        # No spaces to change\n",
    "        return t, False\n",
    "\n",
    "def toggle_apostrophe_encoding(t):\n",
    "    original = t\n",
    "    apos = apostrophe_encoding(t)\n",
    "    if apos:\n",
    "        if \"'\" in apos and \"’\" in apos:\n",
    "            t = t.replace(\"'\", \"\\uFFFF\")\n",
    "            t = t.replace(\"’\", \"'\")\n",
    "            t = t.replace(\"\\uFFFF\", \"’\")\n",
    "        elif \"'\" in apos:\n",
    "            t = t.replace(\"'\", \"’\")\n",
    "        elif \"’\" in apos:\n",
    "            t = t.replace(\"’\", \"'\")\n",
    "        changed = (t != original)\n",
    "        return t, changed\n",
    "    else:\n",
    "        return t, False\n",
    "\n",
    "def toggle_number_format(t):\n",
    "    patterns = extract_number_patterns(t)\n",
    "    changed = False\n",
    "    if patterns:\n",
    "        for p in patterns:\n",
    "            if ',' in p:\n",
    "                new_p = re.sub(r\",\", \"\", p)\n",
    "                if new_p != p:\n",
    "                    idx = t.find(p)\n",
    "                    if idx != -1:\n",
    "                        t = t[:idx] + new_p + t[idx+len(p):]\n",
    "                        changed = True\n",
    "                        break\n",
    "    return t, changed\n",
    "\n",
    "def dialect_transform(text2):\n",
    "    # randomly select a dialect from DIALECTS\n",
    "    changed = False\n",
    "    attempts = 5  # limit attempts to avoid infinite loops\n",
    "    # transform\n",
    "    while not changed and attempts > 0:\n",
    "        dialect = random.choice(DIALECTS)\n",
    "        transformed_text = text2\n",
    "        try:\n",
    "            transformed_text = dialect.transform(text2)\n",
    "        except:  # if the dialect transformation fails\n",
    "            print(f\"Failed to transform with {text2}. Retrying {attempts} more times...\")\n",
    "        if transformed_text != text2:\n",
    "            return transformed_text, True\n",
    "        attempts -= 1\n",
    "    return text2, False\n",
    "        \n",
    "\n",
    "def maybe_add_contraction(text1, text2):\n",
    "    # Only add a contraction if:\n",
    "    # - text1 can form contractions\n",
    "    # - text1 has no contractions\n",
    "    # - text2 has no contractions\n",
    "    original = text2\n",
    "    if not can_form_contractions(text1):\n",
    "        return text2, False\n",
    "    if contains_contractions(text1) or contains_contractions(text2):\n",
    "        return text2, False\n",
    "\n",
    "    expansions = list(common_contractions.keys())\n",
    "    random.shuffle(expansions)\n",
    "\n",
    "    for expansion in expansions:\n",
    "        exp_words = expansion.split()\n",
    "        pattern = r'\\b' + r'\\s+'.join(exp_words) + r'\\b'\n",
    "        match = re.search(pattern, text2, flags=re.IGNORECASE)\n",
    "        if match:\n",
    "            contraction = common_contractions[expansion]\n",
    "            matched_text = match.group(0)\n",
    "            if matched_text[0].isupper():\n",
    "                contraction = contraction[0].upper() + contraction[1:]\n",
    "            text2 = text2[:match.start()] + contraction + text2[match.end():]\n",
    "            return text2, (text2 != original)\n",
    "\n",
    "    return text2, False\n",
    "\n",
    "\n",
    "\n",
    "def make_texts_distinct(text1, text2):\n",
    "    \"\"\"\n",
    "        Assumes to be called on SNLI text pairs\n",
    "    :param text1: \n",
    "    :param text2: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    transformations = [\n",
    "        flip_quotes,\n",
    "        flip_capitalization,\n",
    "        toggle_end_punctuation,\n",
    "        toggle_whitespace_encoding,\n",
    "        toggle_apostrophe_encoding,\n",
    "        toggle_number_format,\n",
    "        lambda t: maybe_add_contraction(text1, t),\n",
    "    ]\n",
    "    \n",
    "    # flip coin to to dialect_transform as this is a transformation that needs to run before all other transformations\n",
    "    text_modified = text2\n",
    "    if random.random() < 0.5:\n",
    "        text_modified, changed = dialect_transform(text2)\n",
    "\n",
    "    attempts = 20  # limit attempts to avoid infinite loops\n",
    "    while attempts > 0:\n",
    "        # Attempt two further random transformation\n",
    "        three_trans = random.sample(transformations, 3)\n",
    "        for transform in three_trans:\n",
    "            new_text, changed = transform(text_modified)\n",
    "            if changed:\n",
    "                text_modified = new_text\n",
    "        attempts -= 1\n",
    "        if text_modified != text2:\n",
    "            return text_modified\n",
    "\n",
    "    # If we exit the loop, we failed to reduce similarity\n",
    "    return text_modified\n",
    "\n",
    "\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-12T13:08:18.720523Z",
     "start_time": "2024-12-12T13:08:18.718526Z"
    }
   },
   "id": "15964367d2573682"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I have talked with them yesterday.', True)\n"
     ]
    }
   ],
   "source": [
    "text_a = \"I talked with them yesterday.\"\n",
    "text_b = \"Hello,\\u00a0world!\"\n",
    "print(dialect_transform(text_a))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-12T09:37:35.031724Z",
     "start_time": "2024-12-12T09:37:28.386386Z"
    }
   },
   "id": "5de578f272ccd4c5"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score: 0.875\n"
     ]
    }
   ],
   "source": [
    "text_a = \"Hello, world!\"\n",
    "text_b = \"Hello,\\u00a0world!\"\n",
    "score = compare_texts(text_a, text_b)\n",
    "print(\"Similarity score:\", score)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-12T09:37:11.015339Z",
     "start_time": "2024-12-12T09:37:11.011Z"
    }
   },
   "id": "7ed8d149c3e0256"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score: 0.25\n"
     ]
    }
   ],
   "source": [
    "text_a = \"Hello, world!\\nThe price is 1,000 dollars. It’s great.\"\n",
    "text_b = \"hello world. The price is 1000 dollars It's great\"\n",
    "score = compare_texts(text_a, text_b)\n",
    "print(\"Similarity score:\", score)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-11T10:51:08.938841Z",
     "start_time": "2024-12-11T10:51:08.913535Z"
    }
   },
   "id": "53603be18299f695"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score: 0.75\n",
      "Synthesized text: Men are working on John Deere equipment.\n",
      "Similarity score: 0.875\n"
     ]
    }
   ],
   "source": [
    "text_a = \"The two farmers are working on a piece of John Deere equipment.\"\n",
    "text_b = \"Men are working on John Deere equipment\"\n",
    "score = compare_texts(text_a, text_b)\n",
    "print(\"Similarity score:\", score)\n",
    "text_b_synth = make_texts_similar(text_a, text_b)[1]\n",
    "print(\"Synthesized text:\", text_b_synth)\n",
    "score = compare_texts(text_a, text_b_synth)\n",
    "print(\"Similarity score:\", score)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-11T10:51:08.938980Z",
     "start_time": "2024-12-11T10:51:08.915891Z"
    }
   },
   "id": "60acb039abe67d71"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score: 1.0\n",
      "Synthesized text: there is a party\n",
      "Similarity score: 0.875\n"
     ]
    }
   ],
   "source": [
    "text_a = \"There is a party\"\n",
    "text_b = \"There is a party\"\n",
    "score = compare_texts(text_a, text_b)\n",
    "print(\"Similarity score:\", score)\n",
    "text_b_synth = make_texts_distinct(text_a, text_b)\n",
    "print(\"Synthesized text:\", text_b_synth)\n",
    "score = compare_texts(text_a, text_b_synth)\n",
    "print(\"Similarity score:\", score)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-11T10:51:08.939140Z",
     "start_time": "2024-12-11T10:51:08.918056Z"
    }
   },
   "id": "95956ea770054e1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Augmentation for SNLI"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d54e1e41b64ba7d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing splits:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing examples in test:   0%|          | 0/10000 [00:00<?, ?it/s]\u001B[A\n",
      "Processing examples in test:   0%|          | 4/10000 [00:02<1:23:36,  1.99it/s]\u001B[A\n",
      "Processing examples in test:   0%|          | 9/10000 [00:02<50:51,  3.27it/s]  \u001B[A\n",
      "Processing examples in test:   0%|          | 11/10000 [00:04<1:00:48,  2.74it/s]\u001B[A\n",
      "Processing examples in test:   0%|          | 12/10000 [00:05<1:41:52,  1.63it/s]\u001B[A\n",
      "Processing examples in test:   0%|          | 15/10000 [00:07<1:24:38,  1.97it/s]\u001B[A\n",
      "Processing examples in test:   0%|          | 16/10000 [00:08<1:58:24,  1.41it/s]\u001B[A\n",
      "Processing examples in test:   0%|          | 17/10000 [00:10<2:32:22,  1.09it/s]\u001B[A\n",
      "Processing examples in test:   0%|          | 18/10000 [00:12<3:01:24,  1.09s/it]\u001B[A\n",
      "Processing examples in test:   0%|          | 19/10000 [00:14<3:28:27,  1.25s/it]\u001B[A\n",
      "Processing examples in test:   0%|          | 20/10000 [00:14<3:07:43,  1.13s/it]\u001B[A\n",
      "Processing examples in test:   0%|          | 22/10000 [00:15<2:18:43,  1.20it/s]\u001B[A\n",
      "Processing examples in test:   0%|          | 23/10000 [00:17<2:53:47,  1.05s/it]\u001B[A\n",
      "Processing examples in test:   0%|          | 25/10000 [00:18<2:07:51,  1.30it/s]\u001B[A\n",
      "Processing examples in test:   0%|          | 26/10000 [00:19<2:29:49,  1.11it/s]\u001B[A\n",
      "Processing examples in test:   0%|          | 27/10000 [00:20<2:48:48,  1.02s/it]\u001B[A\n",
      "Processing examples in test:   0%|          | 29/10000 [00:22<2:38:49,  1.05it/s]\u001B[A\n",
      "Processing examples in test:   0%|          | 32/10000 [00:24<2:00:28,  1.38it/s]\u001B[A\n",
      "Processing examples in test:   0%|          | 33/10000 [00:25<2:31:38,  1.10it/s]\u001B[A\n",
      "Processing examples in test:   0%|          | 34/10000 [00:27<2:45:19,  1.00it/s]\u001B[A\n",
      "Processing examples in test:   0%|          | 35/10000 [00:30<4:28:18,  1.62s/it]\u001B[A\n",
      "Processing examples in test:   0%|          | 37/10000 [00:31<3:04:29,  1.11s/it]\u001B[A\n",
      "Processing examples in test:   0%|          | 39/10000 [00:33<2:44:10,  1.01it/s]\u001B[A\n",
      "Processing examples in test:   0%|          | 41/10000 [00:34<2:34:09,  1.08it/s]\u001B[A\n",
      "Processing examples in test:   0%|          | 42/10000 [00:36<3:16:10,  1.18s/it]\u001B[A\n",
      "Processing examples in test:   0%|          | 44/10000 [00:37<2:28:44,  1.12it/s]\u001B[A\n",
      "Processing examples in test:   0%|          | 45/10000 [00:39<3:02:59,  1.10s/it]\u001B[A\n",
      "Processing examples in test:   0%|          | 48/10000 [00:41<2:29:38,  1.11it/s]\u001B[A\n",
      "Processing examples in test:   0%|          | 49/10000 [00:43<3:04:02,  1.11s/it]\u001B[A\n",
      "Processing examples in test:   1%|          | 53/10000 [00:45<2:13:53,  1.24it/s]\u001B[A\n",
      "Processing examples in test:   1%|          | 54/10000 [00:47<2:47:47,  1.01s/it]\u001B[A\n",
      "Processing examples in test:   1%|          | 56/10000 [00:49<2:51:45,  1.04s/it]\u001B[A\n",
      "Processing examples in test:   1%|          | 60/10000 [00:51<1:59:41,  1.38it/s]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to transform with A snow field with a snowboarder on it. Retrying 5 more times...\n",
      "Failed to transform with A snow field with a snowboarder on it. Retrying 4 more times...\n",
      "Failed to transform with A snow field with a snowboarder on it. Retrying 3 more times...\n",
      "Failed to transform with A snow field with a snowboarder on it. Retrying 2 more times...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing examples in test:   1%|          | 67/10000 [00:55<1:52:24,  1.47it/s]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to transform with A snow field with a snowboarder on it. Retrying 1 more times...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing examples in test:   1%|          | 68/10000 [00:57<2:14:28,  1.23it/s]\u001B[A\n",
      "Processing examples in test:   1%|          | 69/10000 [00:59<2:34:24,  1.07it/s]\u001B[A\n",
      "Processing examples in test:   1%|          | 76/10000 [01:01<1:31:54,  1.80it/s]\u001B[A\n",
      "Processing examples in test:   1%|          | 78/10000 [01:02<1:35:30,  1.73it/s]\u001B[A\n",
      "Processing examples in test:   1%|          | 79/10000 [01:04<1:51:09,  1.49it/s]\u001B[A\n",
      "Processing examples in test:   1%|          | 80/10000 [01:06<2:17:13,  1.20it/s]\u001B[A\n",
      "Processing examples in test:   1%|          | 81/10000 [01:07<2:31:11,  1.09it/s]\u001B[A\n",
      "Processing examples in test:   1%|          | 83/10000 [01:09<2:26:35,  1.13it/s]\u001B[A\n",
      "Processing examples in test:   1%|          | 90/10000 [01:10<1:20:37,  2.05it/s]\u001B[A\n",
      "Processing examples in test:   1%|          | 91/10000 [01:11<1:26:58,  1.90it/s]\u001B[A\n",
      "Processing examples in test:   1%|          | 94/10000 [01:12<1:23:31,  1.98it/s]\u001B[A\n",
      "Processing examples in test:   1%|          | 95/10000 [01:13<1:27:19,  1.89it/s]\u001B[A\n",
      "Processing examples in test:   1%|          | 96/10000 [01:14<1:33:11,  1.77it/s]\u001B[A\n",
      "Processing examples in test:   1%|          | 97/10000 [01:15<2:00:53,  1.37it/s]\u001B[A\n",
      "Processing examples in test:   1%|          | 99/10000 [01:17<2:07:21,  1.30it/s]\u001B[A\n",
      "Processing examples in test:   1%|          | 102/10000 [01:19<1:54:52,  1.44it/s]\u001B[A\n",
      "Processing examples in test:   1%|          | 105/10000 [01:20<1:45:54,  1.56it/s]\u001B[A\n",
      "Processing examples in test:   1%|          | 106/10000 [01:22<2:18:09,  1.19it/s]\u001B[A\n",
      "Processing examples in test:   1%|          | 109/10000 [01:23<1:47:18,  1.54it/s]\u001B[A\n",
      "Processing examples in test:   1%|          | 110/10000 [01:25<2:20:19,  1.17it/s]\u001B[A\n",
      "Processing examples in test:   1%|          | 112/10000 [01:26<1:55:11,  1.43it/s]\u001B[A\n",
      "Processing examples in test:   1%|          | 114/10000 [01:27<1:46:00,  1.55it/s]\u001B[A\n",
      "Processing examples in test:   1%|          | 116/10000 [01:28<1:41:20,  1.63it/s]\u001B[A\n",
      "Processing examples in test:   1%|          | 117/10000 [01:29<1:49:58,  1.50it/s]\u001B[A\n",
      "Processing examples in test:   1%|          | 121/10000 [01:31<1:35:08,  1.73it/s]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to transform Two teenage girls conversing next to lockers. or Girls talking about their problems next to lockers.. Retrying 5 more times ...\n",
      "Failed to transform Two teenage girls conversing next to lockers. or Girls talking about their problems next to lockers.. Retrying 4 more times ...\n",
      "Failed to transform Two teenage girls a-conversing next to lockers. or Girls talking about their problems next to lockers.. Retrying 3 more times ...\n",
      "Failed to transform Two teenage girl a-a conversing next to lockers. or Girls talking about their problems next to lockers.. Retrying 2 more times ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing examples in test:   1%|          | 123/10000 [01:40<4:24:20,  1.61s/it]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to transform Two teenage girl a-a conversing next to lockers. or Girls talking about their problems next to lockers.. Retrying 1 more times ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing examples in test:   1%|          | 124/10000 [01:42<4:34:03,  1.66s/it]\u001B[A\n",
      "Processing examples in test:   1%|▏         | 128/10000 [01:44<3:00:22,  1.10s/it]\u001B[A\n",
      "Processing examples in test:   1%|▏         | 134/10000 [01:45<1:39:49,  1.65it/s]\u001B[A\n",
      "Processing examples in test:   1%|▏         | 140/10000 [01:47<1:20:32,  2.04it/s]\u001B[A\n",
      "Processing examples in test:   1%|▏         | 141/10000 [01:48<1:40:08,  1.64it/s]\u001B[A\n",
      "Processing examples in test:   1%|▏         | 144/10000 [01:50<1:29:22,  1.84it/s]\u001B[A\n",
      "Processing examples in test:   1%|▏         | 148/10000 [01:51<1:19:52,  2.06it/s]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 154/10000 [01:53<1:08:56,  2.38it/s]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 155/10000 [01:54<1:23:45,  1.96it/s]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 159/10000 [01:55<1:06:32,  2.47it/s]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 160/10000 [01:57<1:21:10,  2.02it/s]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 161/10000 [01:58<1:45:12,  1.56it/s]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 162/10000 [02:00<2:19:54,  1.17it/s]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 163/10000 [02:02<2:49:54,  1.04s/it]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 164/10000 [02:03<2:44:00,  1.00s/it]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 165/10000 [02:04<3:01:41,  1.11s/it]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 168/10000 [02:06<2:09:48,  1.26it/s]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 171/10000 [02:07<1:39:56,  1.64it/s]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 172/10000 [02:08<2:00:59,  1.35it/s]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 173/10000 [02:09<2:00:17,  1.36it/s]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 176/10000 [02:10<1:36:00,  1.71it/s]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 180/10000 [02:12<1:20:57,  2.02it/s]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 181/10000 [02:12<1:28:44,  1.84it/s]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 183/10000 [02:13<1:28:53,  1.84it/s]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 184/10000 [02:14<1:35:00,  1.72it/s]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 186/10000 [02:17<2:08:49,  1.27it/s]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 187/10000 [02:18<2:29:35,  1.09it/s]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 188/10000 [02:19<2:42:42,  1.01it/s]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 189/10000 [02:21<2:49:45,  1.04s/it]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 192/10000 [02:23<2:18:23,  1.18it/s]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 193/10000 [02:24<2:25:56,  1.12it/s]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 195/10000 [02:25<2:25:47,  1.12it/s]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 198/10000 [02:26<1:40:56,  1.62it/s]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 199/10000 [02:28<2:05:50,  1.30it/s]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 201/10000 [02:29<1:55:57,  1.41it/s]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 203/10000 [02:30<1:53:47,  1.43it/s]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 207/10000 [02:32<1:25:52,  1.90it/s]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 208/10000 [02:32<1:32:18,  1.77it/s]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 209/10000 [02:34<1:53:40,  1.44it/s]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 210/10000 [02:35<2:18:54,  1.17it/s]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 211/10000 [02:37<3:03:20,  1.12s/it]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 213/10000 [02:39<2:48:35,  1.03s/it]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 215/10000 [02:40<2:15:18,  1.21it/s]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 216/10000 [02:41<2:25:47,  1.12it/s]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 219/10000 [02:43<2:17:33,  1.19it/s]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 220/10000 [02:44<2:16:29,  1.19it/s]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 222/10000 [02:46<2:06:19,  1.29it/s]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 223/10000 [02:48<2:53:38,  1.07s/it]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 225/10000 [02:49<2:31:27,  1.08it/s]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 227/10000 [02:51<2:41:59,  1.01it/s]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 229/10000 [02:53<2:20:19,  1.16it/s]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 232/10000 [02:54<1:50:58,  1.47it/s]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 233/10000 [02:55<2:06:18,  1.29it/s]\u001B[A\n",
      "Processing examples in test:   2%|▏         | 234/10000 [02:57<2:41:04,  1.01it/s]\u001B[A"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset = load_dataset(\"snli\")\n",
    "os.makedirs(\"snli_modified\", exist_ok=True)\n",
    "\n",
    "for split in tqdm(dataset.keys(), desc=\"Processing splits\"):\n",
    "    print(f\"Processing {split}\")\n",
    "    data = dataset[split]\n",
    "\n",
    "    rows = []\n",
    "    for example in tqdm(data, desc=f\"Processing examples in {split}\"):\n",
    "        premise = example[\"premise\"]\n",
    "        hypothesis = example[\"hypothesis\"]\n",
    "        label = example[\"label\"]\n",
    "        \n",
    "        # make sure that text is not empty\n",
    "        if not premise or not hypothesis:\n",
    "            continue\n",
    "\n",
    "        # Skip if label is not in {0, 1, 2}\n",
    "        if label not in {0, 1, 2}:\n",
    "            continue\n",
    "\n",
    "        # Flip a coin for similar/distinct\n",
    "        want_similar = random.choice([True, False])\n",
    "\n",
    "        # Check current similarity\n",
    "        initial_sim = compare_texts(premise, hypothesis)\n",
    "        # currently_similar = (initial_sim == 1.0)\n",
    "\n",
    "        if want_similar:\n",
    "            # Make them similar\n",
    "            premise, hypothesis = make_texts_similar(premise, hypothesis)\n",
    "        else:\n",
    "            # Make them distinct\n",
    "            hypothesis = make_texts_distinct(premise, hypothesis)\n",
    "\n",
    "        # Re-check similarity after transformations\n",
    "        final_sim = compare_texts(premise, hypothesis)\n",
    "        style = 1 if final_sim == 1.0 else 0 # 1 for similar, 0 for distinct\n",
    "\n",
    "        rows.append({\n",
    "            \"premise\": premise,\n",
    "            \"hypothesis\": hypothesis,\n",
    "            \"premise_original\": example[\"premise\"],\n",
    "            \"hypothesis_original\": example[\"hypothesis\"],\n",
    "            \"nli\": label, # 0 entailment, 1 neutral, 2 contradiction\n",
    "            \"style\": style # 0 distinct, 1 similar\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"premise\", \"hypothesis\", \"premise_original\", \"hypothesis_original\", \"nli\", \"style\"])\n",
    "    output_file = f\"snli_modified/{split}_modified.tsv\"\n",
    "    df.to_csv(output_file, index=False, encoding='utf-8', sep=\"\\t\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-12-12T13:08:20.866873Z"
    }
   },
   "id": "498d9a270007d867"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5e5b745063e02459"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "92e73a3a143eb934"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "801e82992da35a7c"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34;1mTextFlint\u001B[0m: Downloading http://textflint.oss-cn-beijing.aliyuncs.com/download/NLTK_DATA/wordnet/wordnet.zip.\n",
      "100%|██████████| 10.8M/10.8M [00:01<00:00, 5.97MB/s]\n",
      "\u001B[34;1mTextFlint\u001B[0m: Unzipping file /Users/anna/.cache/textflint/tmp3a_4wsdp to /Users/anna/.cache/textflint/NLTK_DATA/wordnet.\n",
      "\u001B[34;1mTextFlint\u001B[0m: Successfully saved NLTK_DATA/wordnet/wordnet.zip to cache.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textflint.transformation'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtextflint\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtransformation\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01muniversal\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mchar_typos\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CharTypos\n\u001B[1;32m      3\u001B[0m original_text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis is a sample sentence.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      4\u001B[0m transformer \u001B[38;5;241m=\u001B[39m CharTypos()\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'textflint.transformation'"
     ]
    }
   ],
   "source": [
    "from textflint.transformation.universal.char_typos import CharTypos\n",
    "\n",
    "original_text = \"This is a sample sentence.\"\n",
    "transformer = CharTypos()\n",
    "\n",
    "# The transform() method expects a list of strings and returns a list of variants.\n",
    "perturbed_variants = transformer.transform([original_text])\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(original_text)\n",
    "print(\"\\nPerturbed Variants:\")\n",
    "for i, variant in enumerate(perturbed_variants, start=1):\n",
    "    print(f\"{i}. {variant}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-10T14:55:58.415312Z",
     "start_time": "2024-12-10T14:55:51.601173Z"
    }
   },
   "id": "1c4b2554e575481e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9844f73f784247d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
