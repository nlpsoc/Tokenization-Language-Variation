{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6db7cddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TOKENIZER = [\"ws\", \"gpt2\", \"llama3\"]\n",
    "VOCAB_SIZE = [500, 1000, 2000, 4000, 8000, 16000, 32000, 64000, 128000, 256000, 512000]\n",
    "FITTING_CORPORA = [\"wikipedia\", \"mixed\"]\n",
    "OUT_PATH = \"/shared/3/projects/hiatus/TOKENIZER_wegmann/tokenizer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f884cc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name(corpus_name, pre_tokenizer, vocab_size):\n",
    "    dir_name = f\"{OUT_PATH}/{corpus_name}-{pre_tokenizer}-{vocab_size}/tokenizer.json\"\n",
    "    return dir_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c033ce9a",
   "metadata": {},
   "source": [
    "### For mixed, 32k, look at overlap vocabulary pre-tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51dec96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_paths = [get_name(\"mixed\", \"ws\", 32000), get_name(\"mixed\", \"gpt2\", 32000), get_name(\"mixed\", \"llama3\", 32000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce3737b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/shared/3/projects/hiatus/TOKENIZER_wegmann/tokenizer/mixed-ws-32000/tokenizer.json',\n",
       " '/shared/3/projects/hiatus/TOKENIZER_wegmann/tokenizer/mixed-gpt2-32000/tokenizer.json',\n",
       " '/shared/3/projects/hiatus/TOKENIZER_wegmann/tokenizer/mixed-llama3-32000/tokenizer.json']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "658d60d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(tokenizer_paths[0])\n",
    "vocab = tokenizer.get_vocab()\n",
    "sorted_tokens = sorted(vocab.items(), key=lambda item: item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "97077aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[UNK]', 0),\n",
       " ('[CLS]', 1),\n",
       " ('[SEP]', 2),\n",
       " ('[PAD]', 3),\n",
       " ('[MASK]', 4),\n",
       " ('!', 5),\n",
       " ('\"', 6),\n",
       " ('#', 7),\n",
       " ('$', 8),\n",
       " ('%', 9),\n",
       " ('&', 10),\n",
       " (\"'\", 11),\n",
       " ('(', 12),\n",
       " (')', 13),\n",
       " ('*', 14),\n",
       " ('+', 15),\n",
       " (',', 16),\n",
       " ('-', 17),\n",
       " ('.', 18),\n",
       " ('/', 19),\n",
       " ('0', 20),\n",
       " ('1', 21),\n",
       " ('2', 22),\n",
       " ('3', 23),\n",
       " ('4', 24),\n",
       " ('5', 25),\n",
       " ('6', 26),\n",
       " ('7', 27),\n",
       " ('8', 28),\n",
       " ('9', 29),\n",
       " (':', 30),\n",
       " (';', 31),\n",
       " ('<', 32),\n",
       " ('=', 33),\n",
       " ('>', 34),\n",
       " ('?', 35),\n",
       " ('@', 36),\n",
       " ('A', 37),\n",
       " ('B', 38),\n",
       " ('C', 39),\n",
       " ('D', 40),\n",
       " ('E', 41),\n",
       " ('F', 42),\n",
       " ('G', 43),\n",
       " ('H', 44),\n",
       " ('I', 45),\n",
       " ('J', 46),\n",
       " ('K', 47),\n",
       " ('L', 48),\n",
       " ('M', 49),\n",
       " ('N', 50),\n",
       " ('O', 51),\n",
       " ('P', 52),\n",
       " ('Q', 53),\n",
       " ('R', 54),\n",
       " ('S', 55),\n",
       " ('T', 56),\n",
       " ('U', 57),\n",
       " ('V', 58),\n",
       " ('W', 59),\n",
       " ('X', 60),\n",
       " ('Y', 61),\n",
       " ('Z', 62),\n",
       " ('[', 63),\n",
       " ('\\\\', 64),\n",
       " (']', 65),\n",
       " ('^', 66),\n",
       " ('_', 67),\n",
       " ('`', 68),\n",
       " ('a', 69),\n",
       " ('b', 70),\n",
       " ('c', 71),\n",
       " ('d', 72),\n",
       " ('e', 73),\n",
       " ('f', 74),\n",
       " ('g', 75),\n",
       " ('h', 76),\n",
       " ('i', 77),\n",
       " ('j', 78),\n",
       " ('k', 79),\n",
       " ('l', 80),\n",
       " ('m', 81),\n",
       " ('n', 82),\n",
       " ('o', 83),\n",
       " ('p', 84),\n",
       " ('q', 85),\n",
       " ('r', 86),\n",
       " ('s', 87),\n",
       " ('t', 88),\n",
       " ('u', 89),\n",
       " ('v', 90),\n",
       " ('w', 91),\n",
       " ('x', 92),\n",
       " ('y', 93),\n",
       " ('z', 94),\n",
       " ('{', 95),\n",
       " ('|', 96),\n",
       " ('}', 97),\n",
       " ('~', 98),\n",
       " ('¡', 99),\n",
       " ('¢', 100),\n",
       " ('£', 101),\n",
       " ('¤', 102),\n",
       " ('¥', 103),\n",
       " ('¦', 104),\n",
       " ('§', 105),\n",
       " ('¨', 106),\n",
       " ('©', 107),\n",
       " ('ª', 108),\n",
       " ('«', 109),\n",
       " ('¬', 110),\n",
       " ('®', 111),\n",
       " ('¯', 112),\n",
       " ('°', 113),\n",
       " ('±', 114),\n",
       " ('²', 115),\n",
       " ('³', 116),\n",
       " ('´', 117),\n",
       " ('µ', 118),\n",
       " ('¶', 119),\n",
       " ('·', 120),\n",
       " ('¸', 121),\n",
       " ('¹', 122),\n",
       " ('º', 123),\n",
       " ('»', 124),\n",
       " ('¼', 125),\n",
       " ('½', 126),\n",
       " ('¾', 127),\n",
       " ('¿', 128),\n",
       " ('Â', 129),\n",
       " ('Ã', 130),\n",
       " ('Ä', 131),\n",
       " ('Å', 132),\n",
       " ('Æ', 133),\n",
       " ('Ç', 134),\n",
       " ('È', 135),\n",
       " ('É', 136),\n",
       " ('Ê', 137),\n",
       " ('Ë', 138),\n",
       " ('Ì', 139),\n",
       " ('Í', 140),\n",
       " ('Î', 141),\n",
       " ('Ï', 142),\n",
       " ('Ð', 143),\n",
       " ('Ñ', 144),\n",
       " ('Ò', 145),\n",
       " ('Ó', 146),\n",
       " ('Ô', 147),\n",
       " ('Õ', 148),\n",
       " ('Ö', 149),\n",
       " ('×', 150),\n",
       " ('Ø', 151),\n",
       " ('Ù', 152),\n",
       " ('Ú', 153),\n",
       " ('Û', 154),\n",
       " ('Ü', 155),\n",
       " ('Ý', 156),\n",
       " ('Þ', 157),\n",
       " ('ß', 158),\n",
       " ('à', 159),\n",
       " ('á', 160),\n",
       " ('â', 161),\n",
       " ('ã', 162),\n",
       " ('ä', 163),\n",
       " ('å', 164),\n",
       " ('æ', 165),\n",
       " ('ç', 166),\n",
       " ('è', 167),\n",
       " ('é', 168),\n",
       " ('ê', 169),\n",
       " ('ë', 170),\n",
       " ('ì', 171),\n",
       " ('í', 172),\n",
       " ('î', 173),\n",
       " ('ï', 174),\n",
       " ('ð', 175),\n",
       " ('ñ', 176),\n",
       " ('ò', 177),\n",
       " ('ó', 178),\n",
       " ('ô', 179),\n",
       " ('Ā', 180),\n",
       " ('ā', 181),\n",
       " ('Ă', 182),\n",
       " ('ă', 183),\n",
       " ('Ą', 184),\n",
       " ('ą', 185),\n",
       " ('Ć', 186),\n",
       " ('ć', 187),\n",
       " ('Ĉ', 188),\n",
       " ('Ď', 189),\n",
       " ('ď', 190),\n",
       " ('Đ', 191),\n",
       " ('đ', 192),\n",
       " ('Ē', 193),\n",
       " ('ē', 194),\n",
       " ('Ĕ', 195),\n",
       " ('ĕ', 196),\n",
       " ('Ė', 197),\n",
       " ('ė', 198),\n",
       " ('Ę', 199),\n",
       " ('ę', 200),\n",
       " ('Ě', 201),\n",
       " ('ě', 202),\n",
       " ('Ĝ', 203),\n",
       " ('ĝ', 204),\n",
       " ('Ğ', 205),\n",
       " ('ğ', 206),\n",
       " ('ġ', 207),\n",
       " ('Ģ', 208),\n",
       " ('ģ', 209),\n",
       " ('Ĥ', 210),\n",
       " ('ĥ', 211),\n",
       " ('Ħ', 212),\n",
       " ('ħ', 213),\n",
       " ('Ĩ', 214),\n",
       " ('ĩ', 215),\n",
       " ('Ī', 216),\n",
       " ('ī', 217),\n",
       " ('Ĭ', 218),\n",
       " ('ĭ', 219),\n",
       " ('Į', 220),\n",
       " ('į', 221),\n",
       " ('İ', 222),\n",
       " ('ı', 223),\n",
       " ('Ĳ', 224),\n",
       " ('ĳ', 225),\n",
       " ('Ĵ', 226),\n",
       " ('ĵ', 227),\n",
       " ('Ķ', 228),\n",
       " ('ķ', 229),\n",
       " ('ĸ', 230),\n",
       " ('Ĺ', 231),\n",
       " ('ĺ', 232),\n",
       " ('Ļ', 233),\n",
       " ('ļ', 234),\n",
       " ('Ľ', 235),\n",
       " ('ľ', 236),\n",
       " ('Ŀ', 237),\n",
       " ('ŀ', 238),\n",
       " ('Ł', 239),\n",
       " ('ł', 240),\n",
       " ('Ń', 241),\n",
       " ('th', 242),\n",
       " ('in', 243),\n",
       " ('er', 244),\n",
       " ('an', 245),\n",
       " ('on', 246),\n",
       " ('the', 247),\n",
       " ('re', 248),\n",
       " ('at', 249),\n",
       " ('ou', 250),\n",
       " ('en', 251),\n",
       " ('or', 252),\n",
       " ('st', 253),\n",
       " ('ing', 254),\n",
       " ('al', 255),\n",
       " ('to', 256),\n",
       " ('is', 257),\n",
       " ('ed', 258),\n",
       " ('it', 259),\n",
       " ('and', 260),\n",
       " ('es', 261),\n",
       " ('ar', 262),\n",
       " ('as', 263),\n",
       " ('le', 264),\n",
       " ('of', 265),\n",
       " ('om', 266),\n",
       " ('ic', 267),\n",
       " ('se', 268),\n",
       " ('ion', 269),\n",
       " ('he', 270),\n",
       " ('be', 271),\n",
       " ('ve', 272),\n",
       " ('ac', 273),\n",
       " ('ly', 274),\n",
       " ('ow', 275),\n",
       " ('ent', 276),\n",
       " ('de', 277),\n",
       " ('il', 278),\n",
       " ('me', 279),\n",
       " ('you', 280),\n",
       " ('for', 281),\n",
       " ('ad', 282),\n",
       " ('ro', 283),\n",
       " ('wh', 284),\n",
       " ('that', 285),\n",
       " ('ut', 286),\n",
       " ('ch', 287),\n",
       " ('ot', 288),\n",
       " ('ay', 289),\n",
       " ('li', 290),\n",
       " ('ge', 291),\n",
       " ('ver', 292),\n",
       " ('id', 293),\n",
       " ('ct', 294),\n",
       " ('ol', 295),\n",
       " ('un', 296),\n",
       " ('ith', 297),\n",
       " ('ig', 298),\n",
       " ('ha', 299),\n",
       " ('âĢ', 300),\n",
       " ('ab', 301),\n",
       " ('ere', 302),\n",
       " ('lo', 303),\n",
       " ('ke', 304),\n",
       " ('im', 305),\n",
       " ('am', 306),\n",
       " ('su', 307),\n",
       " ('with', 308),\n",
       " ('ne', 309),\n",
       " ('ce', 310),\n",
       " ('are', 311),\n",
       " ('ter', 312),\n",
       " ('ld', 313),\n",
       " ('con', 314),\n",
       " ('us', 315),\n",
       " ('pe', 316),\n",
       " ('if', 317),\n",
       " ('tr', 318),\n",
       " ('ir', 319),\n",
       " ('ation', 320),\n",
       " ('we', 321),\n",
       " ('com', 322),\n",
       " ('ur', 323),\n",
       " ('ER', 324),\n",
       " ('mo', 325),\n",
       " ('all', 326),\n",
       " ('out', 327),\n",
       " ('ON', 328),\n",
       " ('ht', 329),\n",
       " ('ex', 330),\n",
       " ('this', 331),\n",
       " ('so', 332),\n",
       " ('was', 333),\n",
       " ('ll', 334),\n",
       " ('SON', 335),\n",
       " ('PER', 336),\n",
       " ('PERSON', 337),\n",
       " ('ate', 338),\n",
       " ('pro', 339),\n",
       " ('ap', 340),\n",
       " ('one', 341),\n",
       " ('do', 342),\n",
       " ('fr', 343),\n",
       " ('not', 344),\n",
       " ('op', 345),\n",
       " ('ould', 346),\n",
       " ('pl', 347),\n",
       " ('have', 348),\n",
       " ('te', 349),\n",
       " ('ill', 350),\n",
       " ('but', 351),\n",
       " ('ma', 352),\n",
       " ('sh', 353),\n",
       " ('The', 354),\n",
       " ('.\"', 355),\n",
       " ('ul', 356),\n",
       " ('ome', 357),\n",
       " ('qu', 358),\n",
       " ('res', 359),\n",
       " ('ers', 360),\n",
       " ('po', 361),\n",
       " ('go', 362),\n",
       " ('ess', 363),\n",
       " ('my', 364),\n",
       " ('ant', 365),\n",
       " ('ment', 366),\n",
       " ('his', 367),\n",
       " ('ust', 368),\n",
       " ('fe', 369),\n",
       " ('up', 370),\n",
       " ('oun', 371),\n",
       " ('od', 372),\n",
       " ('ri', 373),\n",
       " ('can', 374),\n",
       " ('ag', 375),\n",
       " ('00', 376),\n",
       " ('ther', 377),\n",
       " ('wor', 378),\n",
       " ('et', 379),\n",
       " ('ally', 380),\n",
       " ('ti', 381),\n",
       " ('ame', 382),\n",
       " ('ok', 383),\n",
       " ('ain', 384),\n",
       " ('end', 385),\n",
       " ('..', 386),\n",
       " ('our', 387),\n",
       " ('from', 388),\n",
       " ('--', 389),\n",
       " ('art', 390),\n",
       " ('get', 391),\n",
       " ('um', 392),\n",
       " ('now', 393),\n",
       " ('any', 394),\n",
       " ('some', 395),\n",
       " ('ind', 396),\n",
       " ('âĢĻ', 397),\n",
       " ('per', 398),\n",
       " ('ight', 399),\n",
       " ('ist', 400),\n",
       " ('by', 401),\n",
       " ('ack', 402),\n",
       " ('thing', 403),\n",
       " ('use', 404),\n",
       " ('act', 405),\n",
       " ('em', 406),\n",
       " ('cl', 407),\n",
       " ('bo', 408),\n",
       " ('ity', 409),\n",
       " ('like', 410),\n",
       " ('gr', 411),\n",
       " ('int', 412),\n",
       " ('no', 413),\n",
       " ('co', 414),\n",
       " ('ive', 415),\n",
       " ('ort', 416),\n",
       " ('ie', 417),\n",
       " ('oug', 418),\n",
       " ('ard', 419),\n",
       " ('they', 420),\n",
       " ('Th', 421),\n",
       " ('ast', 422),\n",
       " ('der', 423),\n",
       " ('ass', 424),\n",
       " ('ine', 425),\n",
       " ('her', 426),\n",
       " ('your', 427),\n",
       " ('age', 428),\n",
       " ('el', 429),\n",
       " ('able', 430),\n",
       " ('In', 431),\n",
       " ('way', 432),\n",
       " ('ck', 433),\n",
       " ('just', 434),\n",
       " ('iv', 435),\n",
       " ('sp', 436),\n",
       " ('ple', 437),\n",
       " ('very', 438),\n",
       " ('ue', 439),\n",
       " (');', 440),\n",
       " ('will', 441),\n",
       " ('would', 442),\n",
       " ('ound', 443),\n",
       " ('si', 444),\n",
       " ('there', 445),\n",
       " ('str', 446),\n",
       " ('ction', 447),\n",
       " ('**', 448),\n",
       " ('ies', 449),\n",
       " ('work', 450),\n",
       " ('what', 451),\n",
       " ('ich', 452),\n",
       " ('ty', 453),\n",
       " ('has', 454),\n",
       " ('Wh', 455),\n",
       " ('fo', 456),\n",
       " ('ure', 457),\n",
       " ('about', 458),\n",
       " ('ile', 459),\n",
       " ('pr', 460),\n",
       " ('ink', 461),\n",
       " ('know', 462),\n",
       " ('ca', 463),\n",
       " ('ev', 464),\n",
       " ('own', 465),\n",
       " ('It', 466),\n",
       " ('more', 467),\n",
       " ('av', 468),\n",
       " ('ong', 469),\n",
       " ('ice', 470),\n",
       " ('ta', 471),\n",
       " ('had', 472),\n",
       " ('ci', 473),\n",
       " ('//', 474),\n",
       " ('ough', 475),\n",
       " ('pt', 476),\n",
       " ('other', 477),\n",
       " ('when', 478),\n",
       " ('bl', 479),\n",
       " ('time', 480),\n",
       " ('ble', 481),\n",
       " ('ser', 482),\n",
       " ('ase', 483),\n",
       " ('sa', 484),\n",
       " ('pp', 485),\n",
       " ('fer', 486),\n",
       " ('20', 487),\n",
       " (').', 488),\n",
       " ('ans', 489),\n",
       " ('ite', 490),\n",
       " ('cre', 491),\n",
       " ('ear', 492),\n",
       " ('pre', 493),\n",
       " ('ace', 494),\n",
       " ('which', 495),\n",
       " ('ated', 496),\n",
       " ('ach', 497),\n",
       " ('ous', 498),\n",
       " ('est', 499),\n",
       " ('read', 500),\n",
       " ('who', 501),\n",
       " ('ire', 502),\n",
       " ('form', 503),\n",
       " ('them', 504),\n",
       " ('new', 505),\n",
       " ('10', 506),\n",
       " ('An', 507),\n",
       " ('how', 508),\n",
       " ('don', 509),\n",
       " ('ud', 510),\n",
       " ('âĢľ', 511),\n",
       " ('br', 512),\n",
       " ('=\"', 513),\n",
       " ('ions', 514),\n",
       " ('ep', 515),\n",
       " ('she', 516),\n",
       " ('âĢĿ', 517),\n",
       " ('cont', 518),\n",
       " ('want', 519),\n",
       " ('him', 520),\n",
       " ('af', 521),\n",
       " ('their', 522),\n",
       " ('ence', 523),\n",
       " ('set', 524),\n",
       " ('iz', 525),\n",
       " ('low', 526),\n",
       " ('He', 527),\n",
       " ('were', 528),\n",
       " ('pos', 529),\n",
       " ('over', 530),\n",
       " ('ang', 531),\n",
       " ('urn', 532),\n",
       " ('ber', 533),\n",
       " ('dis', 534),\n",
       " ('back', 535),\n",
       " ('ick', 536),\n",
       " ('than', 537),\n",
       " ('day', 538),\n",
       " ('cause', 539),\n",
       " ('ance', 540),\n",
       " ('ob', 541),\n",
       " ('sel', 542),\n",
       " ('been', 543),\n",
       " ('then', 544),\n",
       " ('put', 545),\n",
       " ('val', 546),\n",
       " ('ting', 547),\n",
       " ('port', 548),\n",
       " ('could', 549),\n",
       " ('look', 550),\n",
       " ('ps', 551),\n",
       " ('ide', 552),\n",
       " ('man', 553),\n",
       " ('think', 554),\n",
       " ('into', 555),\n",
       " ('its', 556),\n",
       " ('ext', 557),\n",
       " ('need', 558),\n",
       " ('did', 559),\n",
       " ('ak', 560),\n",
       " ('only', 561),\n",
       " ('tw', 562),\n",
       " ('ory', 563),\n",
       " ('You', 564),\n",
       " ('cr', 565),\n",
       " ('ople', 566),\n",
       " ('je', 567),\n",
       " ('bu', 568),\n",
       " ('ress', 569),\n",
       " ('du', 570),\n",
       " ('see', 571),\n",
       " ('turn', 572),\n",
       " ('We', 573),\n",
       " ('ren', 574),\n",
       " ('ts', 575),\n",
       " ('ree', 576),\n",
       " ('ary', 577),\n",
       " ('ho', 578),\n",
       " ('part', 579),\n",
       " ('fir', 580),\n",
       " ('la', 581),\n",
       " ('fun', 582),\n",
       " ('----', 583),\n",
       " ('?\"', 584),\n",
       " ('This', 585),\n",
       " ('col', 586),\n",
       " ('people', 587),\n",
       " ('ish', 588),\n",
       " ('ign', 589),\n",
       " ('because', 590),\n",
       " ('add', 591),\n",
       " ('cess', 592),\n",
       " ('spe', 593),\n",
       " ('right', 594),\n",
       " ('</', 595),\n",
       " ('fore', 596),\n",
       " ('off', 597),\n",
       " ('dat', 598),\n",
       " ('try', 599),\n",
       " ('say', 600),\n",
       " ('ial', 601),\n",
       " ('self', 602),\n",
       " ('cor', 603),\n",
       " ('where', 604),\n",
       " ('comm', 605),\n",
       " ('play', 606),\n",
       " ('Re', 607),\n",
       " ('ical', 608),\n",
       " ('also', 609),\n",
       " ('year', 610),\n",
       " ('ks', 611),\n",
       " ('ject', 612),\n",
       " ('ens', 613),\n",
       " ('gu', 614),\n",
       " ('every', 615),\n",
       " ('sc', 616),\n",
       " ('wr', 617),\n",
       " ('par', 618),\n",
       " ('og', 619),\n",
       " ('fin', 620),\n",
       " ('cur', 621),\n",
       " ('bet', 622),\n",
       " ('...', 623),\n",
       " ('does', 624),\n",
       " ('should', 625),\n",
       " ('ia', 626),\n",
       " ('ra', 627),\n",
       " ('old', 628),\n",
       " ('),', 629),\n",
       " ('dif', 630),\n",
       " ('said', 631),\n",
       " ('min', 632),\n",
       " ('king', 633),\n",
       " ('St', 634),\n",
       " ('thr', 635),\n",
       " ('ip', 636),\n",
       " ('here', 637),\n",
       " ('first', 638),\n",
       " ('fa', 639),\n",
       " ('most', 640),\n",
       " ('ild', 641),\n",
       " ('If', 642),\n",
       " ('sy', 643),\n",
       " ('ition', 644),\n",
       " ('make', 645),\n",
       " ('down', 646),\n",
       " ('mu', 647),\n",
       " ('ph', 648),\n",
       " ('class', 649),\n",
       " ('iew', 650),\n",
       " ('ry', 651),\n",
       " ('But', 652),\n",
       " ('pu', 653),\n",
       " ('even', 654),\n",
       " ('mat', 655),\n",
       " ('ations', 656),\n",
       " ('really', 657),\n",
       " ('good', 658),\n",
       " ('hel', 659),\n",
       " ('ug', 660),\n",
       " ('19', 661),\n",
       " ('12', 662),\n",
       " ('using', 663),\n",
       " ('red', 664),\n",
       " ('inter', 665),\n",
       " ('==', 666),\n",
       " ('des', 667),\n",
       " ('Con', 668),\n",
       " ('book', 669),\n",
       " ('gre', 670),\n",
       " ('vers', 671),\n",
       " ('ful', 672),\n",
       " ('comp', 673),\n",
       " ('under', 674),\n",
       " ('itt', 675),\n",
       " ('What', 676),\n",
       " ('start', 677),\n",
       " ('coun', 678),\n",
       " ('two', 679),\n",
       " ('ose', 680),\n",
       " ('let', 681),\n",
       " ('che', 682),\n",
       " ('And', 683),\n",
       " ('ef', 684),\n",
       " ('ning', 685),\n",
       " ('much', 686),\n",
       " ('name', 687),\n",
       " ('well', 688),\n",
       " ('after', 689),\n",
       " ('()', 690),\n",
       " ('ors', 691),\n",
       " ('cent', 692),\n",
       " ('ever', 693),\n",
       " ('ath', 694),\n",
       " ('gen', 695),\n",
       " ('>,', 696),\n",
       " ('call', 697),\n",
       " ('quest', 698),\n",
       " ('\",', 699),\n",
       " ('got', 700),\n",
       " ('11', 701),\n",
       " ('going', 702),\n",
       " ('inst', 703),\n",
       " ('Ch', 704),\n",
       " ('may', 705),\n",
       " ('ious', 706),\n",
       " ('htt', 707),\n",
       " ('ange', 708),\n",
       " ('run', 709),\n",
       " ('these', 710),\n",
       " ('again', 711),\n",
       " ('ms', 712),\n",
       " ('ving', 713),\n",
       " ('mon', 714),\n",
       " ('data', 715),\n",
       " ('used', 716),\n",
       " ('point', 717),\n",
       " ('ents', 718),\n",
       " ('ys', 719),\n",
       " ('return', 720),\n",
       " ('come', 721),\n",
       " ('enc', 722),\n",
       " ('lig', 723),\n",
       " ('So', 724),\n",
       " ('cho', 725),\n",
       " ('ult', 726),\n",
       " ('mar', 727),\n",
       " ('ved', 728),\n",
       " ('fl', 729),\n",
       " ('hand', 730),\n",
       " ('app', 731),\n",
       " ('://', 732),\n",
       " ('long', 733),\n",
       " ('var', 734),\n",
       " ('find', 735),\n",
       " ('ss', 736),\n",
       " ('list', 737),\n",
       " ('ied', 738),\n",
       " ('igh', 739),\n",
       " ('before', 740),\n",
       " ('lect', 741),\n",
       " ('line', 742),\n",
       " ('something', 743),\n",
       " ('through', 744),\n",
       " ('tal', 745),\n",
       " ('differ', 746),\n",
       " ('less', 747),\n",
       " ('being', 748),\n",
       " ('15', 749),\n",
       " ('same', 750),\n",
       " ('still', 751),\n",
       " ('help', 752),\n",
       " ('stand', 753),\n",
       " ('char', 754),\n",
       " ('ating', 755),\n",
       " ('sub', 756),\n",
       " ('ict', 757),\n",
       " ('too', 758),\n",
       " ('sm', 759),\n",
       " ('cond', 760),\n",
       " ('au', 761),\n",
       " ('reg', 762),\n",
       " ('16', 763),\n",
       " ('ise', 764),\n",
       " ('ought', 765),\n",
       " ('For', 766),\n",
       " ('function', 767),\n",
       " ('ments', 768),\n",
       " ('side', 769),\n",
       " ('iss', 770),\n",
       " ('num', 771),\n",
       " ('io', 772),\n",
       " ('sure', 773),\n",
       " ('atch', 774),\n",
       " ('test', 775),\n",
       " ('med', 776),\n",
       " ('ater', 777),\n",
       " ('stem', 778),\n",
       " ('sim', 779),\n",
       " ('ual', 780),\n",
       " ('How', 781),\n",
       " ('pers', 782),\n",
       " ('cle', 783),\n",
       " ('cal', 784),\n",
       " ('show', 785),\n",
       " ('happ', 786),\n",
       " ('head', 787),\n",
       " ('att', 788),\n",
       " ('fol', 789),\n",
       " ('feel', 790),\n",
       " ('many', 791),\n",
       " ('ave', 792),\n",
       " ('ific', 793),\n",
       " ('cer', 794),\n",
       " ('pres', 795),\n",
       " ('blic', 796),\n",
       " ('IN', 797),\n",
       " ('lt', 798),\n",
       " ('proble', 799),\n",
       " ('ative', 800),\n",
       " ('mem', 801),\n",
       " ('les', 802),\n",
       " ('ily', 803),\n",
       " ('13', 804),\n",
       " ('fect', 805),\n",
       " ('bit', 806),\n",
       " ('code', 807),\n",
       " ('No', 808),\n",
       " ('pon', 809),\n",
       " ('cept', 810),\n",
       " ('clo', 811),\n",
       " ('ates', 812),\n",
       " ('view', 813),\n",
       " ('tim', 814),\n",
       " ('mer', 815),\n",
       " ('tem', 816),\n",
       " ('ways', 817),\n",
       " ('As', 818),\n",
       " ('14', 819),\n",
       " ('rel', 820),\n",
       " ('file', 821),\n",
       " ('pect', 822),\n",
       " ('Let', 823),\n",
       " ('De', 824),\n",
       " ('bas', 825),\n",
       " ('ff', 826),\n",
       " ('ww', 827),\n",
       " ('lot', 828),\n",
       " ('seem', 829),\n",
       " ('ah', 830),\n",
       " ('take', 831),\n",
       " ('those', 832),\n",
       " ('while', 833),\n",
       " ('\">', 834),\n",
       " ('Is', 835),\n",
       " ('ince', 836),\n",
       " ('18', 837),\n",
       " ('value', 838),\n",
       " ('ually', 839),\n",
       " ('ittle', 840),\n",
       " ('Un', 841),\n",
       " ('del', 842),\n",
       " ('Pro', 843),\n",
       " ('200', 844),\n",
       " ('ask', 845),\n",
       " ('vo', 846),\n",
       " ('ven', 847),\n",
       " ('!\"', 848),\n",
       " ('xt', 849),\n",
       " ('type', 850),\n",
       " ('ters', 851),\n",
       " ('dr', 852),\n",
       " ('sur', 853),\n",
       " ('around', 854),\n",
       " ('arch', 855),\n",
       " ('stru', 856),\n",
       " ('last', 857),\n",
       " ('__', 858),\n",
       " ('real', 859),\n",
       " ('org', 860),\n",
       " ('little', 861),\n",
       " ('tain', 862),\n",
       " ('eng', 863),\n",
       " ('ian', 864),\n",
       " ('incl', 865),\n",
       " ('each', 866),\n",
       " ('->', 867),\n",
       " ('made', 868),\n",
       " ('ward', 869),\n",
       " ('fil', 870),\n",
       " ('different', 871),\n",
       " ('My', 872),\n",
       " ('ody', 873),\n",
       " ('ves', 874),\n",
       " ('There', 875),\n",
       " ('didn', 876),\n",
       " ('years', 877),\n",
       " ('err', 878),\n",
       " ('pri', 879),\n",
       " ('(\"', 880),\n",
       " ('sol', 881),\n",
       " ('though', 882),\n",
       " ('plic', 883),\n",
       " ('place', 884),\n",
       " ('.âĢĿ', 885),\n",
       " ('fact', 886),\n",
       " ('En', 887),\n",
       " ('Al', 888),\n",
       " ('log', 889),\n",
       " ('ason', 890),\n",
       " ('things', 891),\n",
       " ('ular', 892),\n",
       " ('such', 893),\n",
       " ('text', 894),\n",
       " ('Than', 895),\n",
       " ('car', 896),\n",
       " ('ull', 897),\n",
       " ('public', 898),\n",
       " ('cri', 899),\n",
       " ('led', 900),\n",
       " ('She', 901),\n",
       " ('17', 902),\n",
       " ('ton', 903),\n",
       " ('light', 904),\n",
       " ('ds', 905),\n",
       " ('life', 906),\n",
       " ('ix', 907),\n",
       " ('never', 908),\n",
       " ('Se', 909),\n",
       " ('vel', 910),\n",
       " ('fig', 911),\n",
       " ('top', 912),\n",
       " ('ye', 913),\n",
       " ('follow', 914),\n",
       " ('ava', 915),\n",
       " ('Ex', 916),\n",
       " ('http', 917),\n",
       " ('kes', 918),\n",
       " ('ub', 919),\n",
       " ('div', 920),\n",
       " ('arr', 921),\n",
       " ('That', 922),\n",
       " ('aren', 923),\n",
       " (\"',\", 924),\n",
       " ('bre', 925),\n",
       " ('ze', 926),\n",
       " ('ics', 927),\n",
       " ('ings', 928),\n",
       " ('ore', 929),\n",
       " ('resu', 930),\n",
       " ('pol', 931),\n",
       " ('RE', 932),\n",
       " ('os', 933),\n",
       " ('aw', 934),\n",
       " ('ft', 935),\n",
       " ('ces', 936),\n",
       " ('Com', 937),\n",
       " ('jo', 938),\n",
       " ('uck', 939),\n",
       " ('supp', 940),\n",
       " ('They', 941),\n",
       " ('import', 942),\n",
       " ('ash', 943),\n",
       " ('gener', 944),\n",
       " ('found', 945),\n",
       " ('produ', 946),\n",
       " ('ner', 947),\n",
       " ('ield', 948),\n",
       " ('exp', 949),\n",
       " ('person', 950),\n",
       " ('ween', 951),\n",
       " ('To', 952),\n",
       " ('vice', 953),\n",
       " ('cons', 954),\n",
       " ('--------', 955),\n",
       " ('ample', 956),\n",
       " ('fri', 957),\n",
       " ('great', 958),\n",
       " ('open', 959),\n",
       " ('friend', 960),\n",
       " ('ages', 961),\n",
       " ('net', 962),\n",
       " ('ced', 963),\n",
       " ('gl', 964),\n",
       " (',\"', 965),\n",
       " ('between', 966),\n",
       " ('rent', 967),\n",
       " ('body', 968),\n",
       " ('ility', 969),\n",
       " ('30', 970),\n",
       " ('25', 971),\n",
       " ('better', 972),\n",
       " ('ger', 973),\n",
       " ('times', 974),\n",
       " ('won', 975),\n",
       " ('doesn', 976),\n",
       " ('both', 977),\n",
       " ('system', 978),\n",
       " ('stud', 979),\n",
       " ('cial', 980),\n",
       " ('ins', 981),\n",
       " ('problem', 982),\n",
       " ('few', 983),\n",
       " ('without', 984),\n",
       " ('vis', 985),\n",
       " ('load', 986),\n",
       " ('cut', 987),\n",
       " ('pass', 988),\n",
       " ('trans', 989),\n",
       " ('case', 990),\n",
       " ('Ð¾', 991),\n",
       " ('face', 992),\n",
       " ('ically', 993),\n",
       " ('why', 994),\n",
       " ('sha', 995),\n",
       " ('Ar', 996),\n",
       " ('ready', 997),\n",
       " ('ows', 998),\n",
       " ('order', 999),\n",
       " ...]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0db99c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "import os\n",
    "\n",
    "# Load tokenizers and extract vocabularies\n",
    "vocabularies = {}\n",
    "for path in tokenizer_paths:\n",
    "    # Extract the last folder name from the path\n",
    "    folder_name = os.path.basename(os.path.dirname(path))\n",
    "    tokenizer = Tokenizer.from_file(path)\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    \n",
    "    # Sort tokens based on their frequency (ID)\n",
    "    sorted_tokens = sorted(vocab.items(), key=lambda item: item[1])\n",
    "    \n",
    "    # Store sorted tokens (by frequency) as a list of token strings\n",
    "    vocabularies[folder_name] = [token for token, _ in sorted_tokens]\n",
    "\n",
    "# Calculate truly unique tokens for each tokenizer\n",
    "unique_tokens = {}\n",
    "special_unique_tokens = {}\n",
    "\n",
    "for name, vocab_list in vocabularies.items():\n",
    "    # Union of vocabularies from the other tokenizers\n",
    "    others = set.union(*(set(vocabularies[other_name]) for other_name in vocabularies if other_name != name))\n",
    "    \n",
    "    # Subtract the other vocabularies from the current one to get truly unique tokens\n",
    "    unique_tokens[name] = [token for token in vocab_list if token not in others]\n",
    "    \n",
    "    # Special condition for tokenizers with \"ws\" in their name\n",
    "    if \"ws\" in name:\n",
    "        # Tokens not present in others, and not present when prefixed with \"Ġ\"\n",
    "        special_unique_tokens[name] = [token for token in unique_tokens[name] if f\"Ġ{token}\" not in others]\n",
    "\n",
    "# Function to get examples from sorted tokens (assuming order by frequency)\n",
    "def get_ordered_examples(tokens):\n",
    "    n = len(tokens)\n",
    "    return {\n",
    "        \"most_common\": tokens[:10],\n",
    "        \"middle\": tokens[n // 2 - 5: n // 2 + 4],\n",
    "        \"least_common\": tokens[-10:]\n",
    "    }\n",
    "\n",
    "# Prepare the results including token examples with frequency considerations\n",
    "results_with_special_unique = {\n",
    "    'vocab_sizes': {name: len(vocab_list) for name, vocab_list in vocabularies.items()},\n",
    "    'common_token_count_all': len(set.intersection(*(set(vocabularies[name]) for name in vocabularies))),\n",
    "    'unique_tokens_count': {name: len(tokens) for name, tokens in unique_tokens.items()},\n",
    "    'unique_tokens_examples': {name: get_ordered_examples(tokens) for name, tokens in unique_tokens.items()},\n",
    "    'special_unique_tokens_count': {name: len(tokens) for name, tokens in special_unique_tokens.items()},\n",
    "    'special_unique_tokens_examples': {name: get_ordered_examples(tokens) for name, tokens in special_unique_tokens.items()},\n",
    "    'pairwise_common_tokens': {f\"{folder_names[i]} & {folder_names[j]}\": len(set(vocabularies[folder_names[i]]).intersection(set(vocabularies[folder_names[j]]))) for i in range(len(folder_names)) for j in range(i + 1, len(folder_names))}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3e841bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Sizes:\n",
      "mixed-ws-32000: 32000 tokens\n",
      "mixed-gpt2-32000: 32000 tokens\n",
      "mixed-llama3-32000: 32000 tokens\n",
      "\n",
      "Common Tokens Across All: 9576\n",
      "\n",
      "Truly Unique Tokens with Frequency Examples:\n",
      "mixed-ws-32000: 20150 unique tokens\n",
      "Most Common: ['wor', 'fir', 'dif', 'coun', 'htt', 'differ', 'stem', 'blic', 'proble', 'pon']\n",
      "Middle: ['Holo', 'bridges', 'Bry', 'Proposition', 'mixer', 'rookie', 'Hebrew', 'Subscri', 'enhan']\n",
      "Least Common: ['informing', 'Gmail', 'lebih', 'ulis', 'braw', 'feminists', \"#'\", 'leur', 'lya', 'unnoticed']\n",
      "mixed-gpt2-32000: 1752 unique tokens\n",
      "Most Common: ['Ġ1', 'Ġ2', 'ĊĠĠĠ', 'ĊĠĠĠĠĠĠĠĠ', 'ĊĠĠĠĠĠĠĠ', 'Ġ0', 'Ġ3', 'ĊĠ', 'Ġ4', 'Ġ5']\n",
      "Middle: ['ĠMurphy', 'Ġpassions', 'ĠAppendix', 'Ġunsuccessful', 'Ġpsychologist', 'Ġhasht', 'irections', 'Ġdistancing', 'ĠSilicon']\n",
      "Least Common: ['Ġwil', 'Ġyogurt', 'ĠHinata', 'Ġadminister', 'Ġuno', 'Ġslender', 'Ġomit', 'Ġchords', 'Ġamet', 'Ġurgency']\n",
      "mixed-llama3-32000: 2954 unique tokens\n",
      "Most Common: ['.Ċ', '.ĊĊ', ';Ċ', ',Ċ', ')Ċ', '>Ċ', ');Ċ', 'âĢĻs', ':ĊĊ', 'Ġ{Ċ']\n",
      "Middle: ['.all', '.ca', '/www', '-at', '(row', '\")]Ċ', '\"/>ĊĊ', '/LICENSE', '070']\n",
      "Least Common: ['.valueOf', 'âĢľWhen', '.Event', ':Name', '.im', '916', 'lyph', '965', '795', '698']\n",
      "\n",
      "Special Unique Tokens for 'ws' Tokenizers with Frequency Examples:\n",
      "mixed-ws-32000: 8509 special unique tokens\n",
      "Most Common: ['htt', 'blic', 'pon', 'resu', 'possi', 'peri', 'issu', 'respon', 'velo', 'thod']\n",
      "Middle: ['genetically', 'ppm', 'NetBeans', 'rainy', 'MD5', 'PTSD', '(=', 'Seed', 'Freder']\n",
      "Least Common: ['informing', 'Gmail', 'lebih', 'ulis', 'braw', 'feminists', \"#'\", 'leur', 'lya', 'unnoticed']\n",
      "\n",
      "Pairwise Common Tokens:\n",
      "mixed-ws-32000 & mixed-gpt2-32000: 11314 common tokens\n",
      "mixed-ws-32000 & mixed-llama3-32000: 10112 common tokens\n",
      "mixed-gpt2-32000 & mixed-llama3-32000: 28510 common tokens\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "print(\"Vocabulary Sizes:\")\n",
    "for name, size in results_with_special_unique['vocab_sizes'].items():\n",
    "    print(f\"{name}: {size} tokens\")\n",
    "\n",
    "print(f\"\\nCommon Tokens Across All: {results_with_special_unique['common_token_count_all']}\")\n",
    "\n",
    "print(\"\\nTruly Unique Tokens with Frequency Examples:\")\n",
    "for name, tokens in unique_tokens.items():\n",
    "    print(f\"{name}: {len(tokens)} unique tokens\")\n",
    "    examples = results_with_special_unique['unique_tokens_examples'][name]\n",
    "    print(f\"Most Common: {examples['most_common']}\")\n",
    "    print(f\"Middle: {examples['middle']}\")\n",
    "    print(f\"Least Common: {examples['least_common']}\")\n",
    "\n",
    "print(\"\\nSpecial Unique Tokens for 'ws' Tokenizers with Frequency Examples:\")\n",
    "for name, tokens in special_unique_tokens.items():\n",
    "    print(f\"{name}: {len(tokens)} special unique tokens\")\n",
    "    examples = results_with_special_unique['special_unique_tokens_examples'][name]\n",
    "    print(f\"Most Common: {examples['most_common']}\")\n",
    "    print(f\"Middle: {examples['middle']}\")\n",
    "    print(f\"Least Common: {examples['least_common']}\")\n",
    "\n",
    "print(\"\\nPairwise Common Tokens:\")\n",
    "for pair, count in results_with_special_unique['pairwise_common_tokens'].items():\n",
    "    print(f\"{pair}: {count} common tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff2b289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn3\n",
    "\n",
    "# Set sizes and intersections\n",
    "ws_only = 20150\n",
    "gpt2_only = 1752\n",
    "llama3_only = 2954\n",
    "\n",
    "ws_gpt2_exclusive = 11314\n",
    "ws_llama3_exclusive = 10112\n",
    "gpt2_llama3_exclusive = 28510\n",
    "\n",
    "common_all = 9576\n",
    "\n",
    "# Create the Venn diagram\n",
    "venn3(subsets=(ws_only, gpt2_only, ws_gpt2_exclusive, llama3_only, ws_llama3_exclusive, gpt2_llama3_exclusive, common_all),\n",
    "      set_labels=('ws', 'gpt2', 'llama3'))\n",
    "\n",
    "plt.title(\"Venn Diagram of Vocabulary Sizes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a70ad77",
   "metadata": {},
   "source": [
    "### For gpt2, mixed, but varying vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "baf295c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = [500, 1000, 2000, 4000, 8000, 16000, 32000, 64000, 128000, 256000, 512000]\n",
    "tokenizer_paths = [get_name(\"mixed\", \"gpt2\", v_size) for v_size in VOCAB_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a12ad6d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/shared/3/projects/hiatus/TOKENIZER_wegmann/tokenizer/mixed-gpt2-500/tokenizer.json',\n",
       " '/shared/3/projects/hiatus/TOKENIZER_wegmann/tokenizer/mixed-gpt2-1000/tokenizer.json',\n",
       " '/shared/3/projects/hiatus/TOKENIZER_wegmann/tokenizer/mixed-gpt2-2000/tokenizer.json',\n",
       " '/shared/3/projects/hiatus/TOKENIZER_wegmann/tokenizer/mixed-gpt2-4000/tokenizer.json',\n",
       " '/shared/3/projects/hiatus/TOKENIZER_wegmann/tokenizer/mixed-gpt2-8000/tokenizer.json',\n",
       " '/shared/3/projects/hiatus/TOKENIZER_wegmann/tokenizer/mixed-gpt2-16000/tokenizer.json',\n",
       " '/shared/3/projects/hiatus/TOKENIZER_wegmann/tokenizer/mixed-gpt2-32000/tokenizer.json',\n",
       " '/shared/3/projects/hiatus/TOKENIZER_wegmann/tokenizer/mixed-gpt2-64000/tokenizer.json',\n",
       " '/shared/3/projects/hiatus/TOKENIZER_wegmann/tokenizer/mixed-gpt2-128000/tokenizer.json',\n",
       " '/shared/3/projects/hiatus/TOKENIZER_wegmann/tokenizer/mixed-gpt2-256000/tokenizer.json',\n",
       " '/shared/3/projects/hiatus/TOKENIZER_wegmann/tokenizer/mixed-gpt2-512000/tokenizer.json']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dae19fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At pair ('mixed-gpt2-500', 'mixed-gpt2-1000')\n",
      "Getting newly added tokens.\n",
      "At pair ('mixed-gpt2-1000', 'mixed-gpt2-2000')\n",
      "Getting newly added tokens.\n",
      "At pair ('mixed-gpt2-2000', 'mixed-gpt2-4000')\n",
      "Getting newly added tokens.\n",
      "At pair ('mixed-gpt2-4000', 'mixed-gpt2-8000')\n",
      "Getting newly added tokens.\n",
      "At pair ('mixed-gpt2-8000', 'mixed-gpt2-16000')\n",
      "Getting newly added tokens.\n",
      "At pair ('mixed-gpt2-16000', 'mixed-gpt2-32000')\n",
      "Getting newly added tokens.\n",
      "At pair ('mixed-gpt2-32000', 'mixed-gpt2-64000')\n",
      "Getting newly added tokens.\n",
      "At pair ('mixed-gpt2-64000', 'mixed-gpt2-128000')\n",
      "Getting newly added tokens.\n",
      "At pair ('mixed-gpt2-128000', 'mixed-gpt2-256000')\n",
      "Getting newly added tokens.\n",
      "At pair ('mixed-gpt2-256000', 'mixed-gpt2-512000')\n",
      "Getting newly added tokens.\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizers and extract vocabularies\n",
    "vocabularies = {}\n",
    "for path in tokenizer_paths:\n",
    "    # Extract the last folder name from the path\n",
    "    folder_name = os.path.basename(os.path.dirname(path))\n",
    "    tokenizer = Tokenizer.from_file(path)\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    \n",
    "    # Sort tokens based on their frequency (ID) and store them\n",
    "    sorted_tokens = sorted(vocab.items(), key=lambda item: item[1])\n",
    "    vocabularies[folder_name] = [token for token, _ in sorted_tokens]\n",
    "\n",
    "# Calculate the overlap for consecutive tokenizers and find newly added tokens\n",
    "overlap_ratios = {}\n",
    "added_tokens_examples = {}\n",
    "folder_names = list(vocabularies.keys())\n",
    "\n",
    "for i in range(len(folder_names) - 1):\n",
    "    name_1 = folder_names[i]\n",
    "    name_2 = folder_names[i + 1]\n",
    "    print(f\"At pair {(name_1, name_2)}\")\n",
    "    \n",
    "    # Get vocabularies as ordered lists\n",
    "    vocab_1 = vocabularies[name_1]\n",
    "    vocab_2 = vocabularies[name_2]\n",
    "    \n",
    "    # Calculate the overlap\n",
    "    overlap = len(set(vocab_1).intersection(set(vocab_2)))\n",
    "    smaller_vocab_size = min(len(vocab_1), len(vocab_2))\n",
    "    \n",
    "    # Calculate the overlap ratio\n",
    "    overlap_ratio = overlap / smaller_vocab_size\n",
    "    overlap_ratios[f\"{name_1} & {name_2}\"] = overlap_ratio\n",
    "    \n",
    "    print(\"Getting newly added tokens.\")\n",
    "    # Find newly added tokens\n",
    "    added_tokens = [token for token in vocab_2 if token not in vocab_1]\n",
    "    n = len(added_tokens)\n",
    "    \n",
    "    # Get examples of the most frequent, middle, and least frequent added tokens\n",
    "    examples = {\n",
    "        \"most_frequent\": added_tokens[:10],\n",
    "        \"middle\": added_tokens[max(0, n//2 - 5):max(0, n//2 + 5)],\n",
    "        \"least_frequent\": added_tokens[-10:]\n",
    "    }\n",
    "    added_tokens_examples[f\"{name_1} -> {name_2}\"] = examples\n",
    "\n",
    "# Print the results\n",
    "print(\"Overlap Ratios between Consecutive Tokenizers:\")\n",
    "for pair, ratio in overlap_ratios.items():\n",
    "    print(f\"{pair}: {ratio:.2f}\")\n",
    "\n",
    "print(\"\\nExamples of Added Tokens:\")\n",
    "for transition, examples in added_tokens_examples.items():\n",
    "    print(f\"\\nTransition: {transition}\")\n",
    "    print(\"Most Frequent Added Tokens:\", examples['most_frequent'])\n",
    "    print(\"Middle Added Tokens:\", examples['middle'])\n",
    "    print(\"Least Frequent Added Tokens:\", examples['least_frequent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca68920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure showing the % overlap w.r.t. the next tokenizer, expectation is at 100% always ??, i.e, larger vocabulary size \"jsut adds more\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
