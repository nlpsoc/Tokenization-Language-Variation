{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-08T10:56:40.845408Z",
     "start_time": "2025-01-08T10:56:40.814773Z"
    }
   },
   "outputs": [],
   "source": [
    "NUCLE_path = \"/Users/anna/Documents/git projects.nosync/StyleTokenizer/data/NUCLE/release3.3/data/nucle3.2.sgml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def split_into_sentences(paragraph):\n",
    "    \"\"\"\n",
    "    Splits a paragraph into sentences *naively* by ., ?, or !.\n",
    "    Returns a list of (start_char, end_char, sentence_text) tuples,\n",
    "    where start_char/end_char are offsets in the original paragraph.\n",
    "    \"\"\"\n",
    "    # We want to keep track of offsets, so we'll do it manually.\n",
    "    # A simple approach:\n",
    "    sentences = []\n",
    "    current_start = 0\n",
    "    \n",
    "    # Find all positions where there's a sentence-ending punctuation.\n",
    "    # We'll also keep the punctuation with the sentence for clarity.\n",
    "    for match in re.finditer(r'[.?!]', paragraph):\n",
    "        # `match.start()` is the position of the punctuation\n",
    "        end_pos = match.start() + 1  # +1 to include the punctuation\n",
    "        sentence_text = paragraph[current_start:end_pos].strip()\n",
    "        if sentence_text:\n",
    "            sentences.append((current_start, end_pos, sentence_text))\n",
    "        current_start = end_pos\n",
    "    \n",
    "    # If there's leftover text after the last punctuation, add it as well.\n",
    "    if current_start < len(paragraph):\n",
    "        leftover = paragraph[current_start:].strip()\n",
    "        if leftover:\n",
    "            sentences.append((current_start, len(paragraph), leftover))\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def extract_mistakes_from_sgml(filename, csv_out):\n",
    "    \"\"\"\n",
    "    Reads the SGML file, locates mistakes, identifies the containing sentence,\n",
    "    and writes out the CSV with columns: sentence, mistake, correction, character span.\n",
    "    \"\"\"\n",
    "    # 1. Read the SGML file\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        sgml_data = f.read()\n",
    "\n",
    "    # 2. Parse using BeautifulSoup (lxml or html.parser)\n",
    "    soup = BeautifulSoup(sgml_data, 'lxml')\n",
    "\n",
    "    # Prepare a list of rows for our CSV\n",
    "    rows_for_csv = []\n",
    "\n",
    "    # 3. For each <DOC> ...\n",
    "    for doc in soup.find_all('doc'):\n",
    "        text_tag = doc.find('text')\n",
    "        if not text_tag:\n",
    "            continue\n",
    "        \n",
    "        # 4. Extract paragraphs (the <p> tags) as a list of strings\n",
    "        paragraphs = text_tag.find_all('p')\n",
    "        paragraph_texts = [p.get_text() for p in paragraphs]\n",
    "\n",
    "        annotation_tag = doc.find('annotation')\n",
    "        if not annotation_tag:\n",
    "            # Some <DOC> might have no <ANNOTATION>\n",
    "            continue\n",
    "\n",
    "        # 5. For each <MISTAKE> ...\n",
    "        for mistake_tag in annotation_tag.find_all('mistake'):\n",
    "            # Read start/end paragraph, start/end offset\n",
    "            start_par = int(mistake_tag['start_par'])\n",
    "            end_par = int(mistake_tag['end_par'])\n",
    "            start_off = int(mistake_tag['start_off'])\n",
    "            end_off = int(mistake_tag['end_off'])\n",
    "\n",
    "            # Read the correction text\n",
    "            correction = mistake_tag.find('correction').get_text(strip=True)\n",
    "\n",
    "            # 6. Extract the relevant substring (the 'mistake') from the text\n",
    "            #    (assuming the mistake is contained in a single paragraph)\n",
    "            paragraph_text = paragraph_texts[start_par]\n",
    "            mistake_text = paragraph_text[start_off:end_off]\n",
    "\n",
    "            # 7. Identify which sentence this substring belongs to\n",
    "            sentences = split_into_sentences(paragraph_text)\n",
    "            sentence_found = None\n",
    "            char_span_in_sentence = (None, None)\n",
    "\n",
    "            for (sent_start, sent_end, sent_text) in sentences:\n",
    "                if (start_off >= sent_start) and (end_off <= sent_end):\n",
    "                    # Found the sentence containing the error\n",
    "                    sentence_found = sent_text\n",
    "                    # Now find offsets *within* that sentence\n",
    "                    offset_in_sentence = start_off - sent_start\n",
    "                    char_span_in_sentence = (\n",
    "                        offset_in_sentence,\n",
    "                        offset_in_sentence + (end_off - start_off)\n",
    "                    )\n",
    "                    break\n",
    "            \n",
    "            # If for some reason we couldn't find a sentence, \n",
    "            # just use the entire paragraph (fallback)\n",
    "            if not sentence_found:\n",
    "                sentence_found = paragraph_text\n",
    "                # character span = from start_off to end_off in the paragraph\n",
    "                char_span_in_sentence = (start_off, end_off)\n",
    "\n",
    "            # 8. Prepare a row for the CSV\n",
    "            # character_span_str will look like \"start:end\"\n",
    "            char_span_str = f\"{char_span_in_sentence[0]}:{char_span_in_sentence[1]}\"\n",
    "\n",
    "            rows_for_csv.append([\n",
    "                sentence_found.strip(),\n",
    "                mistake_text,\n",
    "                correction,\n",
    "                char_span_str\n",
    "            ])\n",
    "\n",
    "    # 9. Write out the CSV\n",
    "    with open(csv_out, 'w', newline='', encoding='utf-8') as out_f:\n",
    "        writer = csv.writer(out_f)\n",
    "        # Write header\n",
    "        writer.writerow([\"sentence\", \"mistake\", \"correction\", \"character span\"])\n",
    "        # Write rows\n",
    "        writer.writerows(rows_for_csv)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-08T10:56:45.545463Z",
     "start_time": "2025-01-08T10:56:45.493963Z"
    }
   },
   "id": "2b155f930ba3599f"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m sgml_file \u001B[38;5;241m=\u001B[39m NUCLE_path   \u001B[38;5;66;03m# Your SGML input file\u001B[39;00m\n\u001B[1;32m      2\u001B[0m csv_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m    \u001B[38;5;66;03m# Desired output CSV\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m extract_mistakes_from_sgml(sgml_file, csv_output)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCSV written to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcsv_output\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[2], line 77\u001B[0m, in \u001B[0;36mextract_mistakes_from_sgml\u001B[0;34m(filename, csv_out)\u001B[0m\n\u001B[1;32m     73\u001B[0m correction \u001B[38;5;241m=\u001B[39m mistake_tag\u001B[38;5;241m.\u001B[39mfind(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcorrection\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mget_text(strip\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     75\u001B[0m \u001B[38;5;66;03m# 6. Extract the relevant substring (the 'mistake') from the text\u001B[39;00m\n\u001B[1;32m     76\u001B[0m \u001B[38;5;66;03m#    (assuming the mistake is contained in a single paragraph)\u001B[39;00m\n\u001B[0;32m---> 77\u001B[0m paragraph_text \u001B[38;5;241m=\u001B[39m paragraph_texts[start_par]\n\u001B[1;32m     78\u001B[0m mistake_text \u001B[38;5;241m=\u001B[39m paragraph_text[start_off:end_off]\n\u001B[1;32m     80\u001B[0m \u001B[38;5;66;03m# 7. Identify which sentence this substring belongs to\u001B[39;00m\n",
      "\u001B[0;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "sgml_file = NUCLE_path   # Your SGML input file\n",
    "csv_output = \"output.csv\"    # Desired output CSV\n",
    "extract_mistakes_from_sgml(sgml_file, csv_output)\n",
    "print(f\"CSV written to {csv_output}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-08T10:56:48.914788Z",
     "start_time": "2025-01-08T10:56:46.458480Z"
    }
   },
   "id": "d8c1c19a6529126c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fd7d6f2be403353d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
