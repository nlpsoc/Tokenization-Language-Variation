{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-13T13:03:21.574014Z",
     "start_time": "2024-12-13T13:03:17.777495Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bdab4380123e47beb0e7d13760b27a1e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating validation split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9f199f3fedc44d36b579180c63b9aee0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[0;32m~/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages/datasets/builder.py:2011\u001B[0m, in \u001B[0;36mArrowBasedBuilder._prepare_split_single\u001B[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001B[0m\n\u001B[1;32m   2010\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 2011\u001B[0m     writer\u001B[38;5;241m.\u001B[39mwrite_table(table)\n\u001B[1;32m   2012\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m CastError \u001B[38;5;28;01mas\u001B[39;00m cast_error:\n",
      "File \u001B[0;32m~/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages/datasets/arrow_writer.py:585\u001B[0m, in \u001B[0;36mArrowWriter.write_table\u001B[0;34m(self, pa_table, writer_batch_size)\u001B[0m\n\u001B[1;32m    584\u001B[0m pa_table \u001B[38;5;241m=\u001B[39m pa_table\u001B[38;5;241m.\u001B[39mcombine_chunks()\n\u001B[0;32m--> 585\u001B[0m pa_table \u001B[38;5;241m=\u001B[39m table_cast(pa_table, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_schema)\n\u001B[1;32m    586\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membed_local_files:\n",
      "File \u001B[0;32m~/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages/datasets/table.py:2295\u001B[0m, in \u001B[0;36mtable_cast\u001B[0;34m(table, schema)\u001B[0m\n\u001B[1;32m   2294\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m table\u001B[38;5;241m.\u001B[39mschema \u001B[38;5;241m!=\u001B[39m schema:\n\u001B[0;32m-> 2295\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast_table_to_schema(table, schema)\n\u001B[1;32m   2296\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m table\u001B[38;5;241m.\u001B[39mschema\u001B[38;5;241m.\u001B[39mmetadata \u001B[38;5;241m!=\u001B[39m schema\u001B[38;5;241m.\u001B[39mmetadata:\n",
      "File \u001B[0;32m~/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages/datasets/table.py:2254\u001B[0m, in \u001B[0;36mcast_table_to_schema\u001B[0;34m(table, schema)\u001B[0m\n\u001B[1;32m   2249\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CastError(\n\u001B[1;32m   2250\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCouldn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt cast\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mtable\u001B[38;5;241m.\u001B[39mschema\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mto\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mfeatures\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mbecause column names don\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt match\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   2251\u001B[0m         table_column_names\u001B[38;5;241m=\u001B[39mtable\u001B[38;5;241m.\u001B[39mcolumn_names,\n\u001B[1;32m   2252\u001B[0m         requested_column_names\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlist\u001B[39m(features),\n\u001B[1;32m   2253\u001B[0m     )\n\u001B[0;32m-> 2254\u001B[0m arrays \u001B[38;5;241m=\u001B[39m [cast_array_to_feature(table[name], feature) \u001B[38;5;28;01mfor\u001B[39;00m name, feature \u001B[38;5;129;01min\u001B[39;00m features\u001B[38;5;241m.\u001B[39mitems()]\n\u001B[1;32m   2255\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m pa\u001B[38;5;241m.\u001B[39mTable\u001B[38;5;241m.\u001B[39mfrom_arrays(arrays, schema\u001B[38;5;241m=\u001B[39mschema)\n",
      "File \u001B[0;32m~/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages/datasets/table.py:2254\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m   2249\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CastError(\n\u001B[1;32m   2250\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCouldn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt cast\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mtable\u001B[38;5;241m.\u001B[39mschema\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mto\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mfeatures\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mbecause column names don\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt match\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   2251\u001B[0m         table_column_names\u001B[38;5;241m=\u001B[39mtable\u001B[38;5;241m.\u001B[39mcolumn_names,\n\u001B[1;32m   2252\u001B[0m         requested_column_names\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlist\u001B[39m(features),\n\u001B[1;32m   2253\u001B[0m     )\n\u001B[0;32m-> 2254\u001B[0m arrays \u001B[38;5;241m=\u001B[39m [cast_array_to_feature(table[name], feature) \u001B[38;5;28;01mfor\u001B[39;00m name, feature \u001B[38;5;129;01min\u001B[39;00m features\u001B[38;5;241m.\u001B[39mitems()]\n\u001B[1;32m   2255\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m pa\u001B[38;5;241m.\u001B[39mTable\u001B[38;5;241m.\u001B[39mfrom_arrays(arrays, schema\u001B[38;5;241m=\u001B[39mschema)\n",
      "File \u001B[0;32m~/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages/datasets/table.py:1802\u001B[0m, in \u001B[0;36m_wrap_for_chunked_arrays.<locals>.wrapper\u001B[0;34m(array, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1801\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(array, pa\u001B[38;5;241m.\u001B[39mChunkedArray):\n\u001B[0;32m-> 1802\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m pa\u001B[38;5;241m.\u001B[39mchunked_array([func(chunk, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m array\u001B[38;5;241m.\u001B[39mchunks])\n\u001B[1;32m   1803\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages/datasets/table.py:1802\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m   1801\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(array, pa\u001B[38;5;241m.\u001B[39mChunkedArray):\n\u001B[0;32m-> 1802\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m pa\u001B[38;5;241m.\u001B[39mchunked_array([func(chunk, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m array\u001B[38;5;241m.\u001B[39mchunks])\n\u001B[1;32m   1803\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages/datasets/table.py:2018\u001B[0m, in \u001B[0;36mcast_array_to_feature\u001B[0;34m(array, feature, allow_primitive_to_str, allow_decimal_to_str)\u001B[0m\n\u001B[1;32m   2017\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(feature, \u001B[38;5;28mlist\u001B[39m):\n\u001B[0;32m-> 2018\u001B[0m     casted_array_values \u001B[38;5;241m=\u001B[39m _c(array\u001B[38;5;241m.\u001B[39mvalues, feature[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m   2019\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m casted_array_values\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m==\u001B[39m array\u001B[38;5;241m.\u001B[39mvalues\u001B[38;5;241m.\u001B[39mtype:\n",
      "File \u001B[0;32m~/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages/datasets/table.py:1804\u001B[0m, in \u001B[0;36m_wrap_for_chunked_arrays.<locals>.wrapper\u001B[0;34m(array, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1803\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1804\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(array, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages/datasets/table.py:2115\u001B[0m, in \u001B[0;36mcast_array_to_feature\u001B[0;34m(array, feature, allow_primitive_to_str, allow_decimal_to_str)\u001B[0m\n\u001B[1;32m   2109\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m array_cast(\n\u001B[1;32m   2110\u001B[0m         array,\n\u001B[1;32m   2111\u001B[0m         feature(),\n\u001B[1;32m   2112\u001B[0m         allow_primitive_to_str\u001B[38;5;241m=\u001B[39mallow_primitive_to_str,\n\u001B[1;32m   2113\u001B[0m         allow_decimal_to_str\u001B[38;5;241m=\u001B[39mallow_decimal_to_str,\n\u001B[1;32m   2114\u001B[0m     )\n\u001B[0;32m-> 2115\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCouldn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt cast array of type\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00marray\u001B[38;5;241m.\u001B[39mtype\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mto\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mfeature\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mTypeError\u001B[0m: Couldn't cast array of type\nstruct<source: string, id: string, filename: string, story: string, questions: list<item: struct<input_text: string, turn_id: int64, sae_input_text: string>>, answers: list<item: struct<span_start: int64, span_end: int64, span_text: string, input_text: string, turn_id: int64, dialect_input_text: string>>, additional_answers: struct<0: list<item: struct<span_start: int64, span_end: int64, span_text: string, input_text: string, turn_id: int64>>, 1: list<item: struct<span_start: int64, span_end: int64, span_text: string, input_text: string, turn_id: int64>>, 2: list<item: struct<span_start: int64, span_end: int64, span_text: string, input_text: string, turn_id: int64>>>, name: string, dialect_story: string>\nto\n{'source': Value(dtype='string', id=None), 'id': Value(dtype='string', id=None), 'filename': Value(dtype='string', id=None), 'story': Value(dtype='string', id=None), 'questions': [{'input_text': Value(dtype='string', id=None), 'turn_id': Value(dtype='int64', id=None), 'sae_input_text': Value(dtype='string', id=None), 'bad_turn': Value(dtype='string', id=None)}], 'answers': [{'span_start': Value(dtype='int64', id=None), 'span_end': Value(dtype='int64', id=None), 'span_text': Value(dtype='string', id=None), 'input_text': Value(dtype='string', id=None), 'turn_id': Value(dtype='int64', id=None), 'dialect_input_text': Value(dtype='string', id=None), 'bad_turn': Value(dtype='string', id=None)}], 'name': Value(dtype='string', id=None), 'dialect_story': Value(dtype='string', id=None)}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mDatasetGenerationError\u001B[0m                    Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mdatasets\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# load SALT-NLP/CoQA_AppE\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m dataset \u001B[38;5;241m=\u001B[39m datasets\u001B[38;5;241m.\u001B[39mload_dataset(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSALT-NLP/CoQA_AppE\u001B[39m\u001B[38;5;124m\"\u001B[39m, split\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages/datasets/load.py:2609\u001B[0m, in \u001B[0;36mload_dataset\u001B[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001B[0m\n\u001B[1;32m   2606\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m builder_instance\u001B[38;5;241m.\u001B[39mas_streaming_dataset(split\u001B[38;5;241m=\u001B[39msplit)\n\u001B[1;32m   2608\u001B[0m \u001B[38;5;66;03m# Download and prepare data\u001B[39;00m\n\u001B[0;32m-> 2609\u001B[0m builder_instance\u001B[38;5;241m.\u001B[39mdownload_and_prepare(\n\u001B[1;32m   2610\u001B[0m     download_config\u001B[38;5;241m=\u001B[39mdownload_config,\n\u001B[1;32m   2611\u001B[0m     download_mode\u001B[38;5;241m=\u001B[39mdownload_mode,\n\u001B[1;32m   2612\u001B[0m     verification_mode\u001B[38;5;241m=\u001B[39mverification_mode,\n\u001B[1;32m   2613\u001B[0m     num_proc\u001B[38;5;241m=\u001B[39mnum_proc,\n\u001B[1;32m   2614\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39mstorage_options,\n\u001B[1;32m   2615\u001B[0m )\n\u001B[1;32m   2617\u001B[0m \u001B[38;5;66;03m# Build dataset for splits\u001B[39;00m\n\u001B[1;32m   2618\u001B[0m keep_in_memory \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   2619\u001B[0m     keep_in_memory \u001B[38;5;28;01mif\u001B[39;00m keep_in_memory \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m is_small_dataset(builder_instance\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mdataset_size)\n\u001B[1;32m   2620\u001B[0m )\n",
      "File \u001B[0;32m~/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages/datasets/builder.py:1027\u001B[0m, in \u001B[0;36mDatasetBuilder.download_and_prepare\u001B[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001B[0m\n\u001B[1;32m   1025\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m num_proc \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1026\u001B[0m         prepare_split_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_proc\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m num_proc\n\u001B[0;32m-> 1027\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_download_and_prepare(\n\u001B[1;32m   1028\u001B[0m         dl_manager\u001B[38;5;241m=\u001B[39mdl_manager,\n\u001B[1;32m   1029\u001B[0m         verification_mode\u001B[38;5;241m=\u001B[39mverification_mode,\n\u001B[1;32m   1030\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mprepare_split_kwargs,\n\u001B[1;32m   1031\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdownload_and_prepare_kwargs,\n\u001B[1;32m   1032\u001B[0m     )\n\u001B[1;32m   1033\u001B[0m \u001B[38;5;66;03m# Sync info\u001B[39;00m\n\u001B[1;32m   1034\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mdataset_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(split\u001B[38;5;241m.\u001B[39mnum_bytes \u001B[38;5;28;01mfor\u001B[39;00m split \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39msplits\u001B[38;5;241m.\u001B[39mvalues())\n",
      "File \u001B[0;32m~/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages/datasets/builder.py:1122\u001B[0m, in \u001B[0;36mDatasetBuilder._download_and_prepare\u001B[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001B[0m\n\u001B[1;32m   1118\u001B[0m split_dict\u001B[38;5;241m.\u001B[39madd(split_generator\u001B[38;5;241m.\u001B[39msplit_info)\n\u001B[1;32m   1120\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1121\u001B[0m     \u001B[38;5;66;03m# Prepare split will record examples associated to the split\u001B[39;00m\n\u001B[0;32m-> 1122\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_split(split_generator, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mprepare_split_kwargs)\n\u001B[1;32m   1123\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1124\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\n\u001B[1;32m   1125\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot find data file. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1126\u001B[0m         \u001B[38;5;241m+\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmanual_download_instructions \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1127\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mOriginal error:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1128\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(e)\n\u001B[1;32m   1129\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages/datasets/builder.py:1882\u001B[0m, in \u001B[0;36mArrowBasedBuilder._prepare_split\u001B[0;34m(self, split_generator, file_format, num_proc, max_shard_size)\u001B[0m\n\u001B[1;32m   1880\u001B[0m job_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m   1881\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m pbar:\n\u001B[0;32m-> 1882\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m job_id, done, content \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_split_single(\n\u001B[1;32m   1883\u001B[0m         gen_kwargs\u001B[38;5;241m=\u001B[39mgen_kwargs, job_id\u001B[38;5;241m=\u001B[39mjob_id, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m_prepare_split_args\n\u001B[1;32m   1884\u001B[0m     ):\n\u001B[1;32m   1885\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m done:\n\u001B[1;32m   1886\u001B[0m             result \u001B[38;5;241m=\u001B[39m content\n",
      "File \u001B[0;32m~/anaconda3/envs/StyleTokenizer/lib/python3.11/site-packages/datasets/builder.py:2038\u001B[0m, in \u001B[0;36mArrowBasedBuilder._prepare_split_single\u001B[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001B[0m\n\u001B[1;32m   2036\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e, DatasetGenerationError):\n\u001B[1;32m   2037\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[0;32m-> 2038\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m DatasetGenerationError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while generating the dataset\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m   2040\u001B[0m \u001B[38;5;28;01myield\u001B[39;00m job_id, \u001B[38;5;28;01mTrue\u001B[39;00m, (total_num_examples, total_num_bytes, writer\u001B[38;5;241m.\u001B[39m_features, num_shards, shard_lengths)\n",
      "\u001B[0;31mDatasetGenerationError\u001B[0m: An error occurred while generating the dataset"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "# load SALT-NLP/CoQA_AppE\n",
    "dataset = datasets.load_dataset(\"SALT-NLP/CoQA_AppE\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3222655638.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn[3], line 1\u001B[0;36m\u001B[0m\n\u001B[0;31m    import multivalue.\u001B[0m\n\u001B[0m                      ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import multivalue\n",
    "from multivalue.build_coqa_value import build_coqa_value\n",
    "\n",
    "multivalue.build_coqa_value "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-13T12:39:27.823250Z",
     "start_time": "2024-12-13T12:39:27.811643Z"
    }
   },
   "id": "1468c918b0bea81"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7553d519bc5a403c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
