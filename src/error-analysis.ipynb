{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-09T13:00:55.433664Z",
     "start_time": "2025-02-09T13:00:55.375415Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "local_finder_addition = \"/Users/anna/sftp_mount/hpc_disk2/02-awegmann/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "path_gpt2_PAN = os.path.join(local_finder_addition, \"TOKENIZER/output/VAR/train-mixed/base-BERT/mixed-gpt2-32000/749M/steps-45000/seed-42/42/PAN\")\n",
    "path_gpt2_AV = os.path.join(local_finder_addition, \"TOKENIZER/output/VAR/train-mixed/base-BERT/mixed-gpt2-32000/749M/steps-45000/seed-42/42/sadiri\")\n",
    "path_no_AV = os.path.join(local_finder_addition, \"TOKENIZER/output/VAR/train-mixed/base-BERT/mixed-no-32000/749M/steps-45000/seed-42/42/sadiri\")\n",
    "path_twitter_AV = os.path.join(local_finder_addition, \"TOKENIZER/output/VAR/train-mixed/base-BERT/twitter-gpt2-32000/749M/steps-45000/seed-42/42/sadiri\")\n",
    "path_wikipedia_AV = os.path.join(local_finder_addition, \"TOKENIZER/output/VAR/train-mixed/base-BERT/wikipedia-gpt2-32000/749M/steps-45000/seed-42/42/sadiri\")\n",
    "path_llama3_PAN = os.path.join(local_finder_addition, \"TOKENIZER/output/VAR/train-mixed/base-BERT/mixed-llama3-32000/749M/steps-45000/seed-42/42/PAN\")\n",
    "path_wsorg_PAN = os.path.join(local_finder_addition, \"TOKENIZER/output/VAR/train-mixed/base-BERT/mixed-wsorg-32000/749M/steps-45000/seed-42/42/PAN\")\n",
    "path_ws_PAN = os.path.join(local_finder_addition, \"TOKENIZER/output/VAR/train-mixed/base-BERT/mixed-ws-32000/749M/steps-45000/seed-42/42/PAN\")\n",
    "path_128k_AV = os.path.join(local_finder_addition, \"TOKENIZER/output/VAR/train-mixed/base-BERT/mixed-gpt2-128000/749M/steps-45000/seed-42/42/sadiri\")\n",
    "tokenizer_gpt2 = os.path.join(local_finder_addition, \"TOKENIZER/tokenizer/mixed-gpt2-32000\")\n",
    "tokenizer_llama3 = os.path.join(local_finder_addition, \"TOKENIZER/tokenizer/mixed-llama3-32000\")\n",
    "tokenizer_wsorg = os.path.join(local_finder_addition, \"TOKENIZER/tokenizer/mixed-wsorg-32000\")\n",
    "tokenizer_ws = os.path.join(local_finder_addition, \"TOKENIZER/tokenizer/mixed-ws-32000\")\n",
    "tokenizer_no = os.path.join(local_finder_addition, \"TOKENIZER/tokenizer/mixed-no-32000\")\n",
    "tokenizer_500 = os.path.join(local_finder_addition, \"TOKENIZER/tokenizer/mixed-gpt2-500\")\n",
    "tokenizer_4k = os.path.join(local_finder_addition, \"TOKENIZER/tokenizer/mixed-gpt2-4000\")\n",
    "tokenizer_64k = os.path.join(local_finder_addition, \"TOKENIZER/tokenizer/mixed-gpt2-64000\")\n",
    "tokenier_128k = os.path.join(local_finder_addition, \"TOKENIZER/tokenizer/mixed-gpt2-128000\")\n",
    "tokenizer_twitter = os.path.join(local_finder_addition, \"TOKENIZER/tokenizer/twitter-gpt2-32000\")\n",
    "tokenizer_pubmed = os.path.join(local_finder_addition, \"TOKENIZER/tokenizer/pubmed-gpt2-32000\")\n",
    "tokenizer_wikipedia = os.path.join(local_finder_addition, \"TOKENIZER/tokenizer/wikipedia-gpt2-32000\")\n",
    "path_gpt2_webbook_PAN = os.path.join(local_finder_addition, \"TOKENIZER/output/VAR/base-BERT/mixed-gpt2-32000/749M/steps-45000/seed-42/42/PAN\")\n",
    "path_llama3_webbook_PAN = os.path.join(local_finder_addition, \"TOKENIZER/output/VAR/base-BERT/mixed-llama3-32000/749M/steps-45000/seed-42/42/PAN\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-09T13:48:41.424880Z",
     "start_time": "2025-02-09T13:48:41.417998Z"
    }
   },
   "id": "8ae404da341aa907"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "path_wiki_NUCLE = os.path.join(local_finder_addition, \"TOKENIZER/output/VAR/train-mixed/base-BERT/wikipedia-gpt2-32000/749M/steps-45000/seed-42/42/NUCLE\")\n",
    "path_mixed_NUCLE = os.path.join(local_finder_addition, \"TOKENIZER/output/VAR/train-mixed/base-BERT/mixed-gpt2-32000/749M/steps-45000/seed-42/42/NUCLE\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-09T14:05:54.084909Z",
     "start_time": "2025-02-09T14:05:54.075034Z"
    }
   },
   "id": "1eb50e93c5bca452"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-09 14:19:55,793 - styletokenizer.utility.custom_logger - DEBUG - Loading previously fitted tokenizer from local path: /Users/anna/sftp_mount/hpc_disk2/02-awegmann/TOKENIZER/tokenizer/mixed-no-32000/tokenizer.json\n",
      "2025-02-09 14:19:55,852 - styletokenizer.utility.custom_logger - DEBUG - Loading previously fitted tokenizer from local path: /Users/anna/sftp_mount/hpc_disk2/02-awegmann/TOKENIZER/tokenizer/mixed-wsorg-32000/tokenizer.json\n",
      "2025-02-09 14:19:55,910 - styletokenizer.utility.custom_logger - DEBUG - Loading previously fitted tokenizer from local path: /Users/anna/sftp_mount/hpc_disk2/02-awegmann/TOKENIZER/tokenizer/mixed-ws-32000/tokenizer.json\n",
      "2025-02-09 14:19:55,970 - styletokenizer.utility.custom_logger - DEBUG - Loading previously fitted tokenizer from local path: /Users/anna/sftp_mount/hpc_disk2/02-awegmann/TOKENIZER/tokenizer/mixed-gpt2-32000/tokenizer.json\n",
      "2025-02-09 14:19:56,032 - styletokenizer.utility.custom_logger - DEBUG - Loading previously fitted tokenizer from local path: /Users/anna/sftp_mount/hpc_disk2/02-awegmann/TOKENIZER/tokenizer/mixed-llama3-32000/tokenizer.json\n",
      "2025-02-09 14:19:56,110 - styletokenizer.utility.custom_logger - DEBUG - Loading previously fitted tokenizer from local path: /Users/anna/sftp_mount/hpc_disk2/02-awegmann/TOKENIZER/tokenizer/mixed-gpt2-500/tokenizer.json\n",
      "2025-02-09 14:19:56,150 - styletokenizer.utility.custom_logger - DEBUG - Loading previously fitted tokenizer from local path: /Users/anna/sftp_mount/hpc_disk2/02-awegmann/TOKENIZER/tokenizer/mixed-gpt2-4000/tokenizer.json\n",
      "2025-02-09 14:19:56,195 - styletokenizer.utility.custom_logger - DEBUG - Loading previously fitted tokenizer from local path: /Users/anna/sftp_mount/hpc_disk2/02-awegmann/TOKENIZER/tokenizer/mixed-gpt2-64000/tokenizer.json\n",
      "2025-02-09 14:19:56,292 - styletokenizer.utility.custom_logger - DEBUG - Loading previously fitted tokenizer from local path: /Users/anna/sftp_mount/hpc_disk2/02-awegmann/TOKENIZER/tokenizer/mixed-gpt2-128000/tokenizer.json\n",
      "2025-02-09 14:19:56,436 - styletokenizer.utility.custom_logger - DEBUG - Loading previously fitted tokenizer from local path: /Users/anna/sftp_mount/hpc_disk2/02-awegmann/TOKENIZER/tokenizer/twitter-gpt2-32000/tokenizer.json\n",
      "2025-02-09 14:19:56,497 - styletokenizer.utility.custom_logger - DEBUG - Loading previously fitted tokenizer from local path: /Users/anna/sftp_mount/hpc_disk2/02-awegmann/TOKENIZER/tokenizer/pubmed-gpt2-32000/tokenizer.json\n",
      "2025-02-09 14:19:56,592 - styletokenizer.utility.custom_logger - DEBUG - Loading previously fitted tokenizer from local path: /Users/anna/sftp_mount/hpc_disk2/02-awegmann/TOKENIZER/tokenizer/wikipedia-gpt2-32000/tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "# laod tokeinzer\n",
    "from train_bert import load_tokenizer\n",
    "no_tok, _ = load_tokenizer(tokenizer_no)\n",
    "wsorg_tok, _ = load_tokenizer(tokenizer_wsorg)\n",
    "ws_tok, _ = load_tokenizer(tokenizer_ws)\n",
    "gpt2_tok, _ = load_tokenizer(tokenizer_gpt2)\n",
    "llama3_tok, _ = load_tokenizer(tokenizer_llama3)\n",
    "tok_500, _ = load_tokenizer(tokenizer_500)\n",
    "tok_4k, _ = load_tokenizer(tokenizer_4k)\n",
    "tok_64k, _ = load_tokenizer(tokenizer_64k)\n",
    "tok_128k, _ = load_tokenizer(tokenier_128k)\n",
    "tok_twitter, _ = load_tokenizer(tokenizer_twitter)\n",
    "tok_pubmed, _ = load_tokenizer(tokenizer_pubmed)\n",
    "tok_wikipedia, _ = load_tokenizer(tokenizer_wikipedia)\n",
    "\n",
    "TOKENIZERS = {\"gpt2\": gpt2_tok, \"llama3\": llama3_tok, \"wsorg\": wsorg_tok, \"ws\": ws_tok, \"no\": no_tok, \"500\": tok_500, \"4k\": tok_4k, \"64k\": tok_64k, \"128k\": tok_128k, \"twitter\": tok_twitter, \"pubmed\": tok_pubmed, \"wikipedia\": tok_wikipedia}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-09T13:19:56.673551Z",
     "start_time": "2025-02-09T13:19:55.738865Z"
    }
   },
   "id": "fc4951e844e8f379"
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized gpt2:\n",
      "['Ġth', 'i', 'is', 'Ġte', 'h', 'Ġl', 'g', 'bt', 'q', 'Ġj', 'umper']\n",
      "Tokenized llama3:\n",
      "['Ġth', 'i', 'is', 'Ġte', 'h', 'Ġl', 'g', 'bt', 'q', 'Ġj', 'umper']\n",
      "Tokenized wsorg:\n",
      "['Ġ', 'th', 'i', 'is', 'Ġ', 'te', 'h', 'Ġ', 'lg', 'bt', 'q', 'Ġ', 'jum', 'per']\n",
      "Tokenized ws:\n",
      "['Ġth', 'i', 'is', 'Ġte', 'h', 'Ġl', 'g', 'bt', 'q', 'Ġj', 'umper']\n",
      "Tokenized no:\n",
      "['Ġth', 'i', 'is', 'Ġte', 'hĠ', 'l', 'g', 'bt', 'qĠ', 'j', 'um', 'per']\n",
      "Tokenized 500:\n",
      "['Ġth', 'i', 'is', 'Ġt', 'e', 'h', 'Ġl', 'g', 'b', 't', 'q', 'Ġj', 'um', 'p', 'er']\n",
      "Tokenized 4k:\n",
      "['Ġth', 'i', 'is', 'Ġte', 'h', 'Ġl', 'g', 'bt', 'q', 'Ġj', 'um', 'per']\n",
      "Tokenized 64k:\n",
      "['Ġthi', 'is', 'Ġteh', 'Ġlg', 'bt', 'q', 'Ġjumper']\n",
      "Tokenized 128k:\n",
      "['Ġthi', 'is', 'Ġteh', 'Ġlgbt', 'q', 'Ġjumper']\n",
      "Tokenized twitter:\n",
      "['Ġthi', 'is', 'Ġte', 'h', 'Ġlgbt', 'q', 'Ġj', 'umper']\n",
      "Tokenized pubmed:\n",
      "['Ġthi', 'is', 'Ġte', 'h', 'Ġl', 'g', 'b', 't', 'q', 'Ġj', 'um', 'per']\n",
      "Tokenized wikipedia:\n",
      "['Ġth', 'i', 'is', 'Ġte', 'h', 'Ġl', 'g', 'b', 't', 'q', 'Ġj', 'umper']\n"
     ]
    }
   ],
   "source": [
    "for key, tok in TOKENIZERS.items():\n",
    "    print(f'Tokenized {key}:')\n",
    "    print(tok.tokenize(\" thiis teh lgbtq jumper\")) # St. ma'am /home  SAY CAN Hello 1) ballots: narcotics Guy c00l lymphadenopathy')}')\n",
    "    # ain't noboday got time for that.\n",
    "    # lgbtq+ y'all queer b4 ;\\n 5 a.m.  lgbtq+ authorised  559  tumour\n",
    "    # and/or well-being \n",
    "    # endl;\\n\n",
    "    #  X-ray multilingual ballots in Europe/the US\n",
    "    # partisan\n",
    "    # in the U.S. Mr. e.g. en.wikipedia.org/wiki U.S.   monk /home\n",
    "    # e-mail  \n",
    "    #  \n",
    "    # make-up\n",
    "    # 588\n",
    "    # Difference gpt2/llama3: \n",
    "    #   Minorities: queer, \n",
    "    #   British vs American: jelly, tumour\n",
    "    # Difference 32k and 128k: \n",
    "    #   Minorities: lgbtq+\n",
    "    #   Dialectal differences: jumper, trainers, yall, biscuit \n",
    "    #   youth lang: lmao, bruh, noo\n",
    "    #   content words: ballots\n",
    "    # Difference Twitter vs. Wikipedia\n",
    "    #   Minorities: lgbtq+\n",
    "    #   Dialectal differences: yall, aint, cant\n",
    "    #   Youth lang: lmao, lol, bruh, noo, OMG\n",
    "    #   content words: ballots, precursor\n",
    "    # Difference ws merge character categories\n",
    "    #   internet/youth language: ok. b4 y'all"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-09T17:05:37.389648Z",
     "start_time": "2025-02-09T17:05:37.333296Z"
    }
   },
   "id": "f2f02ed6e0b53bfe"
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "# loading classifications for tasks requiring sensitivity to language variation\n",
    "import pandas as pd\n",
    "df_gpt2 = pd.read_csv(os.path.join(path_gpt2_PAN, \"2025-01-21_eval_dataset.tsv\"), sep=\"\\t\")\n",
    "df_llama3 = pd.read_csv(os.path.join(path_llama3_PAN, \"2025-01-22_eval_dataset.tsv\"), sep=\"\\t\")\n",
    "df_wsorg = pd.read_csv(os.path.join(path_wsorg_PAN, \"2025-01-25_eval_dataset.tsv\"), sep=\"\\t\")\n",
    "df_ws = pd.read_csv(os.path.join(path_ws_PAN, \"2025-01-26_eval_dataset.tsv\"), sep=\"\\t\")\n",
    "\n",
    "df_no_av = pd.read_csv(os.path.join(path_no_AV, \"eval_dataset.tsv\"), sep=\"\\t\")\n",
    "\n",
    "df_wb_gpt2 = pd.read_csv(os.path.join(path_gpt2_webbook_PAN, \"2025-01-10_eval_dataset.tsv\"), sep=\"\\t\")\n",
    "df_wb_llama3 = pd.read_csv(os.path.join(path_llama3_webbook_PAN, \"2025-01-10_eval_dataset.tsv\"), sep=\"\\t\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-09T14:23:16.639366Z",
     "start_time": "2025-02-09T14:22:59.092849Z"
    }
   },
   "id": "b4033b7b5d4980b4"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "df_gpt2_av = pd.read_csv(os.path.join(path_gpt2_AV, \"eval_dataset.tsv\"), sep=\"\\t\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-09T13:50:48.972616Z",
     "start_time": "2025-02-09T13:50:39.888740Z"
    }
   },
   "id": "1ce0ca0ae5959897"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "df_no_av = pd.read_csv(os.path.join(path_no_AV, \"eval_dataset.tsv\"), sep=\"\\t\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-09T13:31:27.836393Z",
     "start_time": "2025-02-09T13:31:15.216394Z"
    }
   },
   "id": "18ed95d8c5be93dc"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "df_twitter_av = pd.read_csv(os.path.join(path_twitter_AV, \"eval_dataset.tsv\"), sep=\"\\t\")\n",
    "df_wikipedia_av = pd.read_csv(os.path.join(path_wikipedia_AV, \"eval_dataset.tsv\"), sep=\"\\t\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-09T13:22:47.602375Z",
     "start_time": "2025-02-09T13:22:29.018300Z"
    }
   },
   "id": "77e428ce4f9ca9df"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "df_128k = pd.read_csv(os.path.join(path_128k_AV, \"eval_dataset.tsv\"), sep=\"\\t\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-09T13:48:53.446065Z",
     "start_time": "2025-02-09T13:48:44.334482Z"
    }
   },
   "id": "c900fbc7b17251c6"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "df_wiki_NUCLE = pd.read_csv(os.path.join(path_wiki_NUCLE, \"2025-01-23_eval_dataset.tsv\"), sep=\"\\t\")\n",
    "df_mixed_NUCLE = pd.read_csv(os.path.join(path_mixed_NUCLE, \"2025-01-23_eval_dataset.tsv\"), sep=\"\\t\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-09T14:05:59.923646Z",
     "start_time": "2025-02-09T14:05:58.206128Z"
    }
   },
   "id": "526edf3a2ba82bba"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "# find cases where llama3 predicted correctly, but gpt2 did not\n",
    "df_llama3_correct_gpt2_wrong = df_llama3[(df_llama3[\"label\"] == df_llama3[\"predictions\"]) & (df_gpt2[\"label\"] != df_gpt2[\"predictions\"])]\n",
    "df_wb_gpt2_correct_llama3_wrong = df_wb_gpt2[(df_wb_gpt2[\"label\"] == df_wb_gpt2[\"predictions\"]) & (df_llama3[\"label\"] != df_llama3[\"predictions\"])]\n",
    "df_ws_correct_gpt2_wrong = df_ws[(df_ws[\"label\"] == df_ws[\"predictions\"]) & (df_gpt2[\"label\"] != df_gpt2[\"predictions\"])]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T16:35:30.327999Z",
     "start_time": "2025-02-06T16:35:30.319364Z"
    }
   },
   "id": "ce34de363b917860"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "745\n"
     ]
    }
   ],
   "source": [
    "df_no_correct_gpt2_wrong_AV = df_no_av[(df_no_av[\"label\"] == df_no_av[\"predictions\"]) & (df_gpt2_av[\"label\"] != df_gpt2_av[\"predictions\"])]\n",
    "print(len(df_no_correct_gpt2_wrong_AV))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-07T14:50:34.844112Z",
     "start_time": "2025-02-07T14:50:34.809590Z"
    }
   },
   "id": "de47c235248f60fd"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "def compare_predictions(df_correct, df_incorrect, first_tok, second_tok, first_name, second_name, text1=\"text 1\", text2=\"text 2\", classification_at_1=\"Same Author\", n_examples=10, nbr_char_to_show=300):\n",
    "    i = 0\n",
    "    # parse label and predictions\n",
    "    df_first_correct_second_wrong = df_correct[(df_correct[\"label\"] == df_correct[\"predictions\"]) & (df_incorrect[\"label\"] != df_incorrect[\"predictions\"])]\n",
    "    for index, row in df_first_correct_second_wrong.iterrows():\n",
    "        print(f\"=={classification_at_1}==\" if row[\"predictions\"] == 1 else f\"==not {classification_at_1}==\")\n",
    "        print(f\"Text 1: {row[text1][:nbr_char_to_show]}\")\n",
    "        print(f\"\\t Tokenized {first_name}:{' '.join(first_tok.tokenize(row[text1][:nbr_char_to_show]))}\")\n",
    "        print(f\"\\t Tokenized {second_name}:{' '.join(second_tok.tokenize(row[text1][:nbr_char_to_show]))}\")\n",
    "        print(f\"\\t Unique Tokens {first_name}: {set(first_tok.tokenize(row[text1])) - set(second_tok.tokenize(row[text1]))}\")\n",
    "        print(f\"\\t Unique Tokens {second_name}: {set(second_tok.tokenize(row[text1])) - set(first_tok.tokenize(row[text1]))}\")\n",
    "        if text2 is not None:\n",
    "            print(f\"Text 2: {row[text2][:nbr_char_to_show]}\")   \n",
    "            print(f\"\\t Tokenized {first_name}:{' '.join(first_tok.tokenize(row[text2][:nbr_char_to_show]))}\")\n",
    "            print(f\"\\t Tokenized {second_name}:{' '.join(second_tok.tokenize(row[text2][:nbr_char_to_show]))}\")\n",
    "            print(f\"\\t Unique Tokens {first_name}: {set(first_tok.tokenize(row[text2])) - set(second_tok.tokenize(row[text2]))}\")\n",
    "            print(f\"\\t Unique Tokens {second_name}: {set(second_tok.tokenize(row[text2])) - set(first_tok.tokenize(row[text2]))}\")\n",
    "        i += 1\n",
    "\n",
    "        if i > n_examples: break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-09T13:45:39.017006Z",
     "start_time": "2025-02-09T13:45:38.943987Z"
    }
   },
   "id": "456056bb193af8fc"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==Same Author==\n",
      "Text 1: size 4. This size is generally available in sizes of 12mm (½ in), 16mm (  in), 20mm (¾ in), 25mm (1 in), 30mm (1¼ in) and 40mm (1½ in).\n",
      " A 3.5mm in metric is equivalent to an imperial gauge size 6. This size is generally available in sizes of 12mm (½ in), 16mm (5/3 in), 20mm (¾ in), 25mm (1 in), 30m\n",
      "\t Tokenized twitter:s ize Ġ4 . ĠThis Ġsize Ġis Ġgenerally Ġavailable Ġin Ġs izes Ġof Ġ12 mm Ġ( Â ½ Ġin ), Ġ16 mm Ġ( Ġ Ġin ), Ġ20 mm Ġ( Â ¾ Ġin ), Ġ25 mm Ġ( 1 Ġin ), Ġ30 mm Ġ( 1 Â ¼ Ġin ) Ġand Ġ40 mm Ġ( 1 Â ½ Ġin ). Ċ ĠA Ġ3 . 5 mm Ġin Ġmet ric Ġis Ġequivalent Ġto Ġan Ġim perial Ġg aug e Ġsize Ġ6 . ĠThis Ġsize Ġis Ġgenerally Ġavailable Ġin Ġs izes Ġof Ġ12 mm Ġ( Â ½ Ġin ), Ġ16 mm Ġ( 5 / 3 Ġin ), Ġ20 mm Ġ( Â ¾ Ġin ), Ġ25 mm Ġ( 1 Ġin ), Ġ30 m\n",
      "\t Tokenized wiki:size Ġ4 . ĠThis Ġsize Ġis Ġgenerally Ġavailable Ġin Ġsizes Ġof Ġ12 mm Ġ( Â½ Ġin ), Ġ16 mm Ġ( Ġ Ġin ), Ġ20 mm Ġ( Â ¾ Ġin ), Ġ25 mm Ġ( 1 Ġin ), Ġ30 mm Ġ( 1 Â ¼ Ġin ) Ġand Ġ40 mm Ġ( 1 Â½ Ġin ). Ċ ĠA Ġ3 . 5 mm Ġin Ġmetric Ġis Ġequivalent Ġto Ġan Ġimperial Ġgauge Ġsize Ġ6 . ĠThis Ġsize Ġis Ġgenerally Ġavailable Ġin Ġsizes Ġof Ġ12 mm Ġ( Â½ Ġin ), Ġ16 mm Ġ( 5 / 3 Ġin ), Ġ20 mm Ġ( Â ¾ Ġin ), Ġ25 mm Ġ( 1 Ġin ), Ġ30 m\n",
      "\t Unique Tokens twitter: {'Ġcommon', 'Ġder', 'perial', 'ish', '[UNK]', 'ingu', 'Ġregard', 'Ġcontain', 'ions', 'ric', 'izes', 'ers', 'ant', 'ERSON', 'ize', 'ĠAb', 'ur', 'thet', 'ient', 'Ġ_', 'itive', 'e', 'es', 'ics', 'Ġg', 'Ġinst', 'ĠS', 'Ġim', 'imate', 'cept', 'ided', 'ed', 'Ġaspect', 'Ġapprox', 'Ġscrew', 'Ġdist', 'Ġmanufact', 'Cr', 'aug', 'Ġ##', 'Ġinc', 'Ġsub', 'ly', 'ĠBri', 'Ġsuggest', 'Ġmet', 'iti', 'ual', 'Ġs', 'ove', 'ip', 'Ġrec', 'ef', 'Ġconcept', 'umm', 're', 'que', 'Ġdiff', 'ary', 'ing', 'div', 'Ġparticip', 'ance'}\n",
      "\t Unique Tokens wiki: {'Ġdistinguish', 'Ġimperial', 'ON', 'Ġmetric', 'Ġsc', 'ERS', 'Ġsubdivided', '>', 'size', 'C', 'Ġinstance', 'Â½', 'Ġ<', 'Ġregarded', 'uit', 'Ġcontaining', 'Ġsizes', '#', 'Ġcommonly', 'Ġrecipient', 'Ġconceptual', 'Ġgauge', 'ĠAbove', 'Ġincre', 'ique', 'ceptions', 'rit', 'ĠBrief', 'Ġaesthetics', 'Ġdiffers', 'Ġsuggests', 'Ġapproximate', 'Ġparticipant', 'Ġaspects', 'Ġ#', 'Ġderive', 'ĠSummary', 'rews', 'Ġmanufacturers'}\n",
      "Text 2: is for distance vision.\n",
      " The lens segment, technically be referred to as a 'seg', is used for near-vision correction and comes in a variety of shapes. It can be half-moon (or a flat-top), a straight-top (D-segment), a round segment, a rectangular (or ribbon) area or the entire lower half of a bifoca\n",
      "\t Tokenized twitter:is Ġfor Ġdistance Ġvision . Ċ ĠThe Ġl ens Ġse gment , Ġtechnically Ġbe Ġreferred Ġto Ġas Ġa Ġ' se g ', Ġis Ġused Ġfor Ġnear - vision Ġcorrect ion Ġand Ġcomes Ġin Ġa Ġvariety Ġof Ġsh apes . ĠIt Ġcan Ġbe Ġhalf - moon Ġ( or Ġa Ġflat - top ), Ġa Ġstraight - top Ġ( D - se gment ), Ġa Ġround Ġse gment , Ġa Ġre ct ang ular Ġ( or Ġri bb on ) Ġarea Ġor Ġthe Ġentire Ġlower Ġhalf Ġof Ġa Ġb if oca\n",
      "\t Tokenized wiki:is Ġfor Ġdistance Ġvision . Ċ ĠThe Ġlens Ġsegment , Ġtechnically Ġbe Ġreferred Ġto Ġas Ġa Ġ' se g ', Ġis Ġused Ġfor Ġnear - vision Ġcorrection Ġand Ġcomes Ġin Ġa Ġvariety Ġof Ġshapes . ĠIt Ġcan Ġbe Ġhalf - moon Ġ( or Ġa Ġflat - top ), Ġa Ġstraight - top Ġ( D - se gment ), Ġa Ġround Ġsegment , Ġa Ġrectangular Ġ( or Ġribbon ) Ġarea Ġor Ġthe Ġentire Ġlower Ġhalf Ġof Ġa Ġb if oca\n",
      "\t Unique Tokens twitter: {'CK', 'IP', 'rative', 'Ġri', 'ecutive', 'ens', 'dy', 'ics', 'es', 'IF', 'ĠEMB', 'ĠST', 'enses', 'hol', 'st', 'ĠEx', 'IED', 'ends', 'ĠBL', 'Ġsepar', 'id', 'ion', 'ANK', 'AIN', 'Double', 'MY', 'Ġback', 'bro', 'Ġem', 'ward', 'CHES', 'Ġcorrect', 'Ġident', 'co', 'ular', 'ITCH', 'ang', 'OUBLE', 'oop', 'ĠCH', 'apes', 'wh', 'IDER', 'ing', 'ially', 'lin', 'l', 'Ġde', 'ROSS', 'ĠDAR', 'Ġmechan', 'ips', 'urpose', 'por', 'TCH', 'M', 'ct', 'ating', 'Ġ##', 'ĠCLI', 'Ġsh', 'Ġsew', 'ĠDE', 'Ġt', 'TON', 'Ġspec', 'Ġadjust', 'ical', 'ates', 'ĠHE', 'HO', 'p', 'EW', 'ACK', 'AST', 'bb', 'ĠBACK', 'IND', 'ĠFrank', 'LE', 'Ġeye', 'IT', 'ocals', 'ur', 'NING', 'Ġ_', 'Ġl', 'Ġport', 'VER', 'ments', 'ed', 'ATCH', 'ĠBUT', 'ĠWH', 'Ġre', 'lets', 'on', 'cor'}\n",
      "\t Unique Tokens wiki: {'Ġdecorative', 'Ġribbon', 'EM', 'TC', 'TI', 'als', 'Dou', 'Ġseparating', 'NH', 'TO', 'Ġbackward', 'ts', 'HA', 'T', 'Ġspecially', 'ĠFranklin', 'y', 'Ġcorrection', 'ER', 'Ġ', 'MB', 'oped', 'AC', 'OS', 'ID', 'ED', 'Ġshapes', 'hip', 'U', 'Ġupwards', 'W', 'h', 'ĠCA', 'Ġlenses', 'ĠCR', 'Ġportion', 'urd', 'BL', 'ble', 'FI', 'ĠBAS', 'Ġembroid', 'H', 'KS', 'AR', 'E', 'Ġlens', 'IC', 'ĠDO', 'Ġmechanics', 'Ġincorporates', 'ĠDI', 'Ġsegments', 'ĠSE', 'ele', 'S', 'PS', 'ĠExecutive', 'HI', 'L', 'LA', 'wing', 'lo', 'purpose', 'ĠCL', 'ES', 'Ġidentical', 'OL', 'Ġsegment', 'V', 'N', 'IN', 'UT', 'oles', 'stit', 'Ġey', 'Ġtends', '#', 'ĠH', 'Ġadjusted', 'K', 'ĠW', 'w', 'oc', 'Ġrectangular', 'Ġ#', 'ch'}\n",
      "==not Same Author==\n",
      "Text 1: the template code through its paces. The `assert_select` assertion call confirms that the resulting page contains the expected number of `div` elements with a class of `story`:\n",
      "     assert_select 'div#content div.story', count: 1\n",
      " And that, dear reader, is the last test I'll make you write! Well, fo\n",
      "\t Tokenized twitter:the Ġtempl ate Ġcode Ġthrough Ġits Ġp aces . ĠThe Ġ` ass ert _ se lect ` Ġass ert ion Ġcall Ġconfir ms Ġthat Ġthe Ġresult ing Ġpage Ġcont ains Ġthe Ġexpected Ġnumber Ġof Ġ` div ` Ġelements Ġwith Ġa Ġclass Ġof Ġ` story ` : ĊĠĠĠĠ Ġass ert _ se lect Ġ' div # content Ġdiv . story ', Ġcount : Ġ1 Ċ ĠAnd Ġthat , Ġdear Ġreader , Ġis Ġthe Ġlast Ġtest ĠI 'll Ġmake Ġyou Ġwrite ! ĠWell , Ġfo\n",
      "\t Tokenized wiki:the Ġtem plate Ġcode Ġthrough Ġits Ġp aces . ĠThe Ġ ` ass ert _ se lect ` Ġassert ion Ġcall Ġconf irms Ġthat Ġthe Ġresulting Ġpage Ġcontains Ġthe Ġexpected Ġnumber Ġof Ġ ` div ` Ġelements Ġwith Ġa Ġclass Ġof Ġ ` story ` : Ċ Ġ Ġ Ġ Ġ Ġassert _ se lect Ġ' div # content Ġdiv . story ', Ġcount : Ġ1 Ċ ĠAnd Ġthat , Ġde ar Ġreader , Ġis Ġthe Ġlast Ġtest ĠI 'll Ġmake Ġyou Ġwrite ! ĠWell , Ġfo\n",
      "\t Unique Tokens twitter: {'Ġourselves', 'Ġloc', 'Ġfail', 'Ġinteg', 'atory', 'Ġdevelop', 'rations', 'ms', 'cks', 'Ġcompreh', 'ene', 'Ġcom', 'Ġ`', 'Ġprof', 'ctional', 'Ġexcept', 'Ġbugs', 'ate', 'ig', 'ors', 'b', 'ĠT', 'Ġski', 'ise', 'ĠDe', 'Ġfl', 'ĠWelcome', 'mission', 'en', 'ite', 'ĠR', 'Ġsu', 'ally', 'rake', 'ains', 'Ġexpand', 'ĠBen', 'ails', 'Ġdear', 'ps', 'Ġar', 'Ġcont', 'Ġ20', 'umm', 'ĠSu', '187', 'l', 'ration', '650', 'Ġresult', 'ils', 'Ġdev', 'Ġspeedy', 'Ġyourself', 'may', 'lic', 'Ġdis', 'ic', 'Ġ##', 'Ġarch', '06', 'oted', 'Ġmig', 'ma', 'ures', 'ĊĠĠĠĠ', 'esting', 'ensive', 'ity', 'Ġerr', 'Ġ_', 'ĠS', 'Ġtags', 'ĠApp', 'ive', 'Ġasc', 'Ġ6', 'Ġsub', 'Ġre', 'ertain', 'Ġcongratul', 'ni', 'Ġra', 'Ġconfir', '................', 'Ġtempl', 'aws', 'ary', 'ch'}\n",
      "\t Unique Tokens wiki: {'ations', 'Ġours', 'Ġsuite', 'Ġfailures', 'Ġdism', 'Ġspe', 'ra', 'Ġlocate', 'Ġrails', 'ar', 'ugs', 'cer', 'Ġdevoted', 'irms', 'Ġde', 'a', 'Ġcontains', 'Ġarchive', 'ulatory', 'elves', 'Ġtem', 'ips', 'Ġdeveloper', 'Ġassert', 'ĠWel', 'Ġ60', '...', 'plate', 'tained', 'ĠRail', 'laws', 'Ġresulting', 'Ġerrors', 'Ġcomm', 'come', 'ĠDeb', 'ke', 'Ġsubmission', 'Ġprofic', 'Ġ207', 'ay', '..', 'ĠApplic', 'Ġconfig', 'Ġexceptions', 'Ġintegration', 'ĠSuite', 'Ġsk', 'ourself', '50', 'ks', '18', 'ec', 'Ġrein', 'Ġarise', 'Ġcomprehensive', 'Ġf', '6', 'Ġcongrat', 'ĠBench', 'Ġb', 'len', 'Ġdisplayed', 'Ġmigr', 'enn', 'edy', 'ĠSummary', 'ially', 'Ġfunctionality', 'Ġexpanded', 'Ġy'}\n",
      "Text 2: Hello,\n",
      "\n",
      "My comment was aimed to the fact of removing shared BP.\n",
      "\n",
      "If we have single BP per application then it's SMACK label is fixed and BP\n",
      "do not need to handle anything by itself.\n",
      "Privileges to files are enforced by platform.\n",
      "\n",
      "In actual situation, we have shared browser process. BP is needed to be\n",
      "\t Tokenized twitter:Hello , Ċ Ċ My Ġcomment Ġwas Ġaim ed Ġto Ġthe Ġfact Ġof Ġremoving Ġshared ĠBP . Ċ Ċ If Ġwe Ġhave Ġsingle ĠBP Ġper Ġapplication Ġthen Ġit 's ĠSMA CK Ġlabel Ġis Ġfixed Ġand ĠBP Ċ do Ġnot Ġneed Ġto Ġhandle Ġanything Ġby Ġitself . Ċ Priv ile ges Ġto Ġfiles Ġare Ġen for ced Ġby Ġplatform . Ċ Ċ In Ġactual Ġsituation , Ġwe Ġhave Ġshared Ġbrows er Ġprocess . ĠBP Ġis Ġneeded Ġto Ġbe\n",
      "\t Tokenized wiki:H ello , Ċ Ċ My Ġcomment Ġwas Ġaimed Ġto Ġthe Ġfact Ġof Ġremoving Ġshared ĠBP . Ċ Ċ If Ġwe Ġhave Ġsingle ĠBP Ġper Ġapplication Ġthen Ġit 's ĠSM AC K Ġlabel Ġis Ġfixed Ġand ĠBP Ċ do Ġnot Ġneed Ġto Ġhandle Ġanything Ġby Ġitself . Ċ Pr iv ile ges Ġto Ġfiles Ġare Ġenforced Ġby Ġplatform . Ċ Ċ In Ġactual Ġsituation , Ġwe Ġhave Ġshared Ġbrowser Ġprocess . ĠBP Ġis Ġneeded Ġto Ġbe\n",
      "\t Unique Tokens twitter: {'for', 'er', 'ĠRP', 'Hello', 'ĊĊ', '[UNK]', 'work', 'CK', 'ced', 'ed', 'Ġframe', 'Ġen', 'Priv', 'ĠSMA', 'ERSON', 'Ġaim', 'Ġbrows'}\n",
      "\t Unique Tokens wiki: {'Ġenforced', 'Ġframework', '>', 'AC', '<', 'Ġbrowser', 'H', 'Pr', 'K', 'ĠR', 'iv', 'Ġaimed', 'ON', 'ello', 'ĠSM', 'ERS'}\n",
      "==not Same Author==\n",
      "Text 1: argument over which one of us had previously been the meanest when drunk. That meant we started \"bragging\" about the times when we had been the most awful humans while intoxicated, because we all wanted to \"win\" that \"competition.\" Nice, huh? (And don't ask me who won, because I'm not telling.)\n",
      " Thr\n",
      "\t Tokenized twitter:arg ument Ġover Ġwhich Ġone Ġof Ġus Ġhad Ġpreviously Ġbeen Ġthe Ġmean est Ġwhen Ġdrunk . ĠThat Ġmeant Ġwe Ġstarted Ġ\" bra gging \" Ġabout Ġthe Ġtimes Ġwhen Ġwe Ġhad Ġbeen Ġthe Ġmost Ġawful Ġhumans Ġwhile Ġinto xic ated , Ġbecause Ġwe Ġall Ġwanted Ġto Ġ\" win \" Ġthat Ġ\" compet ition .\" ĠNice , Ġhuh ? Ġ( And Ġdon 't Ġask Ġme Ġwho Ġwon , Ġbecause ĠI 'm Ġnot Ġtelling .) Ċ ĠTh r\n",
      "\t Tokenized wiki:arg ument Ġover Ġwhich Ġone Ġof Ġus Ġhad Ġpreviously Ġbeen Ġthe Ġmean est Ġwhen Ġdrunk . ĠThat Ġmeant Ġwe Ġstarted Ġ\" b rag ging \" Ġabout Ġthe Ġtimes Ġwhen Ġwe Ġhad Ġbeen Ġthe Ġmost Ġaw ful Ġhumans Ġwhile Ġinto x icated , Ġbecause Ġwe Ġall Ġwanted Ġto Ġ\" win \" Ġthat Ġ\" compet ition .\" ĠNice , Ġh uh ? Ġ( And Ġdon 't Ġask Ġme Ġwho Ġwon , Ġbecause ĠI 'm Ġnot Ġtelling .) Ċ ĠTh r\n",
      "\t Unique Tokens twitter: {'Ġping', 'Ġsupposed', '[UNK]', 'zi', 'Ġlaughing', 'Ġkindly', 'opped', 'eremony', 'ated', 'Ġal', 'ighty', 'ĠImm', 'Ġstarving', 'ERSON', 'das', 'ur', 'thank', 'enan', 'Ġfirm', 'asc', 'e', 'Ġwhisper', 'able', 'Ġm', 'Ġhadn', 'Ġhuh', 'Ġlosers', 'vision', 'ed', 'az', 'ight', 'ĠTweets', 'igans', 'irty', 'Ġgigg', 'Ġgossip', 'ven', 'Ġsh', 'Ġwoke', 'Ġforgot', 'odka', 'gging', 'Ġe', 'Ġc', 'igh', 'Ġth', 'Ġ', 'Ġdesperately', 'Ġawful', 'xic', 'bra', 'ty', 'tar', 'Ġen', 'edi', 'Ġele'}\n",
      "\t Unique Tokens wiki: {'ig', 'Ġeighty', 'b', 'ĠT', '<', 'Ġdesper', 'Ġthirty', 'we', 'Ġshe', 'pered', 'ON', 'ers', 'Ġhop', 'Ġaw', 'ets', 'ERS', 'nan', 'Ġsupposedly', 'ped', '>', 'Ġmasc', 'icated', 'Ġlaugh', 'Ġg', 'Ġ<', 'Ġenvisioned', 'ful', 'azz', 'ving', 'urable', 'n', 'Ġw', 'Ġfirmly', 'uh', 'ka', 'od', 'Ġwh', 'eight', 'ans', 'igg', 'ot', 'Ġkind', 'ank', 'Ġforg', 'Ġs', 'ging', 'Ġlos', 'oss', 'oke', 'ip', 'i', 'is', 'Ġforty', 'rag', 'th', 'ĠImmediately', 'Ġaltar', 'ing', 'Ġceremony', 'Ġp', 'as', 'x', 'Ġstar', 'Ġweighed', 'Ġeleven'}\n",
      "Text 2: <PERSON> was looking forward to having his Occidental College team play conference rival Claremont in the final of the Tiger Classic tournament last weekend at Occidental. So much so that when Occidental beat Christ College of Irvine, 110-64, <PERSON> only briefly scouted Claremont's opponent--East \n",
      "\t Tokenized twitter:[UNK] P ERSON [UNK] Ġwas Ġlooking Ġforward Ġto Ġhaving Ġhis ĠO cc ident al ĠCollege Ġteam Ġplay Ġconference Ġrival ĠCl are mont Ġin Ġthe Ġfinal Ġof Ġthe ĠTiger ĠClassic Ġtournament Ġlast Ġweekend Ġat ĠO cc ident al . ĠSo Ġmuch Ġso Ġthat Ġwhen ĠO cc ident al Ġbeat ĠChrist ĠCollege Ġof ĠI rv ine , Ġ110 - 64 , Ġ [UNK] P ERSON [UNK] Ġonly Ġbrief ly Ġsco ut ed ĠCl are mont 's Ġopponent -- East Ġ\n",
      "\t Tokenized wiki:< P ERS ON > Ġwas Ġlooking Ġforward Ġto Ġhaving Ġhis ĠOcc idental ĠCollege Ġteam Ġplay Ġconference Ġrival ĠCla remont Ġin Ġthe Ġfinal Ġof Ġthe ĠTiger ĠClassic Ġtournament Ġlast Ġweekend Ġat ĠOcc idental . ĠSo Ġmuch Ġso Ġthat Ġwhen ĠOcc idental Ġbeat ĠChrist ĠCollege Ġof ĠIrvine , Ġ110 - 64 , Ġ< P ERS ON > Ġonly Ġbriefly Ġscout ed ĠCla remont 's Ġopponent -- East Ġ\n",
      "\t Unique Tokens twitter: {'ere', 'Ġpre', 'ident', 'Ġru', 'verted', 'nament', 'int', 'ress', 'ĠRe', 'our', 'vantage', 'ad', 'Zero', 'arters', 'ĠF', 'ĠGl', 'ly', 'Ġcon', 'rv', 'con', 'y', 'enta', 'Ġs', 'Ġ', 'are', '?:', 'ounds', 'time', 'ĠNot', 'aging', 'ĠCl', 'al', 'ĠT', 'Ġtour', 'nas', 'ĠSac', 'ERSON', 'rained', 'ĠP', 'ine', 'ola', 'Which', 'ĠO', 'Ġfried', 'Ch', 'Ġv', 'ĠL', 'Ġsco', 'Ġopen', 'ĠFl', 'Ġrespect', 'Ġagg', 'er', 'Ġref', 'ney', 'ena', 'struction', 'Ġemer', 'mont', 'Ġaver', 'm', 'Ġbrief', 'ĠCre', 'ĠHor', 'ount', 'ut', 'igers', 'acing', 'Ġqu', 'Ġdis', 'le', 'ic', 'ĠConn', 'Ġdefensive', 'season', 'cc', 'Ġincompetent', 'shoot', 'aged', 'cons', 'De', 'Ġreb', 'idel', 're', 'Trading', 'sc', 'ars', 'Ġover', 'oy', '[UNK]', 'ect', 'ium', 'ins', 'asad', 'anging', 'ity', 'ĠMar', 'end', 'ĠA', 'izon', 'shall', 'red', 'Ġre', 'ging', 'cember', 'Ġrepl'}\n",
      "\t Unique Tokens wiki: {'Ġscout', 'remont', 'Trad', '<', 'Ġdefens', 'Z', 'Ġemerging', 'mount', 'Ġtourn', 'ĠMarshall', 'ra', 'Ġresh', 'ĠFalcons', 'ĠFlint', 'ament', 'ON', 'ich', 'ERS', 'ourn', 'a', 'Ġgymnasium', 'ried', 'December', 'ĠRey', 'Wh', '>', 'Ġpreseason', 'Ġopener', 'ĠIrvine', 'Ġquarters', 'pet', 'Ġvarsity', 'Ġ<', 'Ġconverted', 'Ġaveraged', '?', 'ĠAle', 'ĠConnecticut', 'idental', 'ĠGlend', 'Ġincom', 'ĠNotre', 'ent', 'Chang', 'ĠCrescent', 'Ġbriefly', 'Ġruins', 'Ġaveraging', 'Ġovertime', 'Ġdisadvantage', 'Ġreconstruction', 'Ġrespectively', 'Ġf', 'ey', 'ĠTigers', 'ĠSacred', 'Ġreplacing', 'ĠLoyola', 'ero', 'adic', 'Ġsidel', 'ĠCla', 'oot', 'Ġrefere', 'ĠHorizon', 'Ġaggress', 'ĠOcc', 'Ġrebounds', 'ĠPasadena'}\n",
      "==Same Author==\n",
      "Text 1: <PERSON>. <PERSON> pleaded with his brother to destroy the documents after he realized what had happened.\n",
      " For five years, <PERSON> continued to demand that his brother return his rightful share of the business.\n",
      " As a last resort, <PERSON> and his sons filed suit against <PERSON>'s side of the famil\n",
      "\t Tokenized twitter:[UNK] P ERSON [UNK] . Ġ [UNK] P ERSON [UNK] Ġple aded Ġwith Ġhis Ġbrother Ġto Ġdestroy Ġthe Ġdocuments Ġafter Ġhe Ġrealized Ġwhat Ġhad Ġhappened . Ċ ĠFor Ġfive Ġyears , Ġ [UNK] P ERSON [UNK] Ġcontinued Ġto Ġdemand Ġthat Ġhis Ġbrother Ġreturn Ġhis Ġright ful Ġshare Ġof Ġthe Ġbusiness . Ċ ĠAs Ġa Ġlast Ġres ort , Ġ [UNK] P ERSON [UNK] Ġand Ġhis Ġsons Ġfiled Ġsuit Ġagainst Ġ [UNK] P ERSON [UNK] ' s Ġside Ġof Ġthe Ġfamil\n",
      "\t Tokenized wiki:< P ERS ON > . Ġ< P ERS ON > Ġpleaded Ġwith Ġhis Ġbrother Ġto Ġdestroy Ġthe Ġdocuments Ġafter Ġhe Ġrealized Ġwhat Ġhad Ġhappened . Ċ ĠFor Ġfive Ġyears , Ġ< P ERS ON > Ġcontinued Ġto Ġdemand Ġthat Ġhis Ġbrother Ġreturn Ġhis Ġright ful Ġshare Ġof Ġthe Ġbusiness . Ċ ĠAs Ġa Ġlast Ġresort , Ġ< P ERS ON > Ġand Ġhis Ġsons Ġfiled Ġsuit Ġagainst Ġ< P ERS ON > ' s Ġside Ġof Ġthe Ġfam il\n",
      "\t Unique Tokens twitter: {'Ġhold', 'Ġsurv', 'Ġple', 'Ġimmig', 'er', 'Ġres', 'suit', 'ĠPhil', 'offs', 'Ġr', '[UNK]', 'ort', 'ĠG', 'Ġ197', 'Ġalleg', 'ĠBR', 'reece', 'lier', 'ated', 'ĠFE', 'Ġdi', 'Ġsuper', 'ERSON', 'irs', 'asket', '0', 'Ġ198', 'Ġl', 'ung', 'ted', 'OT', 'ame', 'enced', 'Ġspin', 'Ġm', '1', 'Ġco', 'Ġsent', 'ud', 'UD', 'pper', 'Ġmalicious', 'ior', 'rup', 'atri', 'ĠMad', 'acked', 'Ġlaw', 'ĠPROM', 'Ġsupp', 'ĠTUR', 'Ġre', 'raud', 'vert', 'ĠO', 'ntario', 'Ġtrans', 'market', 'rants', 'formed', 'iving', 'Ġ199', 'ip', 'Ġ', 'ings', 'Ġfe', 'ĠFAMILY', 'ĠCor', 'p', 'inst', 'ĠINTO', 'own', 'ĠSuper', 'aded', 'RS', 'HE'}\n",
      "\t Unique Tokens wiki: {'Ġsupplier', 'ĠT', 'Ġresort', '<', 'ra', 'ĠOntario', 'Ġ1984', 'ĠCorp', 'ĠMadame', 'Ġmal', 'Ġpleaded', 'ĠFA', 'ON', 'Ġheirs', 'Ġreinstated', 'TH', 'ĠSuperior', 'owner', 'Y', 'ERS', 'ĠPhilip', 'Ġrack', 'Ġdiver', '>', 'Ġsurviving', 'ri', 'Ġ1990', 'ĠBasket', 'ĠP', 'Ġcopper', 'ino', 'Ġholdings', 'ff', 'Ġfeud', 'Ġ<', 'Ġsupermarket', 'TO', 'uded', 'rupted', 'M', 'ĠGreece', 'Ġsp', 'Ġtransformed', 'ĠF', 'Ġ1993', 'Ġimmigrants', 'ĠIN', 'UR', 'RO', 'ting', 'Ġalleging', 'Ġlawsuit', 'Ġsentenced', 'Ġ1971', 'EU', 'Ġlung', 'IL', 'D', 'Ġmat', 'icious'}\n",
      "Text 2: in need, they visit a government assistance center. When they have a health problem, they expect a government-funded health system to take care of them. When they are unemployed, they look to the government to pay them—and to tide them over until they find another job.\n",
      " If unemployment is statistica\n",
      "\t Tokenized twitter:in Ġneed , Ġthey Ġvisit Ġa Ġgovernment Ġassistance Ġcenter . ĠWhen Ġthey Ġhave Ġa Ġhealth Ġproblem , Ġthey Ġexpect Ġa Ġgovernment - fund ed Ġhealth Ġsystem Ġto Ġtake Ġcare Ġof Ġthem . ĠWhen Ġthey Ġare Ġunemploy ed , Ġthey Ġlook Ġto Ġthe Ġgovernment Ġto Ġpay Ġthem âĢĶ and Ġto Ġt ide Ġthem Ġover Ġuntil Ġthey Ġfind Ġanother Ġjob . Ċ ĠIf Ġunemployment Ġis Ġstat ist ica\n",
      "\t Tokenized wiki:in Ġneed , Ġthey Ġvisit Ġa Ġgovernment Ġassistance Ġcenter . ĠWhen Ġthey Ġhave Ġa Ġhealth Ġproblem , Ġthey Ġexpect Ġa Ġgovernment - funded Ġhealth Ġsystem Ġto Ġtake Ġcare Ġof Ġthem . ĠWhen Ġthey Ġare Ġunemployed , Ġthey Ġlook Ġto Ġthe Ġgovernment Ġto Ġpay Ġthem âĢĶ and Ġto Ġtide Ġthem Ġover Ġuntil Ġthey Ġfind Ġanother Ġjob . Ċ ĠIf Ġunemployment Ġis Ġstat ist ica\n",
      "\t Unique Tokens twitter: {'Ġtw', 'Ġlegisl', 'Ġcoinc', 'ide', 'ute', 'Ġmark', 'ation', 'vern', 'ĠOr', 'Ġcon', 'cl', 'osition', 'eteen', 'th', 'rip', 'etal', 'ators', 'Ġcons', 'tty', 'ate', 'pense', 'bor', 'tter', 'olid', 'ist', 'Ġparent', 'Ġsp', 'v', 'Ġsen', 'enti', 'ounts', 'fund', 'ically', 'Ġflood', 'Ġprincip', 'lar', 'orm', 'ized', 'Europe', 'itor', 'een', 'able', 'ut', 'UST', 'ided', 'Ġdomin', 'gu', 'Ġex', 'Ġgrad', 'ĠSchool', 'Ġam', 'ĠAr', 'Ġdial', 'Ġcompet', 'igh', 'Ġnin', 'ually', 'ect', 'ns', 'ĠGo', 'ur', 'Ġcustom', 'Ġpa', 'Ġinvolve', 'ments', 'Ġkingdom', 'Ġunemploy', 'eth', 'ed', 'o', 'stit', 'hood', 'ment', 'Ġnumer', 'Ġsub', 'ĠNe', 'Ġhappily', 'leans', 'ging', 'Ġra', 'ĠGOING', 'Ġeight', 'an', 'udes', 'Ġimp', 'Ġen', 'Ġparticip'}\n",
      "\t Unique Tokens wiki: {'Ġexpense', 'Ġcustoms', 'utable', 'Ġpatterns', 'Ġnineteenth', 'Ġmarked', 'ily', 'Ġconsolidation', 'Ġgradually', 'Ġlegislation', 'Ġkingdoms', 'Ġamounts', 'Ġcentralized', 'Ġunemployed', 'Ġenl', 'ĠArg', 'ĠSchools', 'ri', 'Ġtide', 'Ġrav', 'istically', 'Ġcompetitor', 'Ġspot', 'Ġnumerous', 'US', 'Ġhapp', 'pet', 'Ġparticipation', 'funded', 'Ġflooding', 'urable', 'Ġinvolvement', 'uably', 'Ġprincipal', 'Ġtwentieth', 'Ġdominate', 'ĠNeighborhood', 'Ġparental', 'ĠOrleans', 'Ġsenators', 'T', 'ĠGO', 'Ġcoincided', 'position', 'arg', 'European', 'Ġdialects', 'Ġconcludes', 'Ġsubstitute', 'ty', 'ING', 'Ġeighteenth', 'Ġenormous', 'ĠGovernment'}\n",
      "==not Same Author==\n",
      "Text 1: close enough to see the coast. He didn't tell her that he'd quit the salt mines because, sea air notwithstanding, he could not bear to be so far underground, the white walls dwarfing him, his own shadow a stranger as he hunched over and shuttled carts of supplies to white men similar to how he imagi\n",
      "\t Tokenized twitter:close Ġenough Ġto Ġsee Ġthe Ġcoast . ĠHe Ġdidn 't Ġtell Ġher Ġthat Ġhe 'd Ġquit Ġthe Ġsalt Ġmines Ġbecause , Ġsea Ġair Ġnot with standing , Ġhe Ġcould Ġnot Ġbear Ġto Ġbe Ġso Ġfar Ġunder ground , Ġthe Ġwhite Ġwalls Ġd war f ing Ġhim , Ġhis Ġown Ġshadow Ġa Ġstranger Ġas Ġhe Ġh unch ed Ġover Ġand Ġsh utt led Ġc arts Ġof Ġsupplies Ġto Ġwhite Ġmen Ġsimilar Ġto Ġhow Ġhe Ġim agi\n",
      "\t Tokenized wiki:close Ġenough Ġto Ġsee Ġthe Ġcoast . ĠHe Ġdidn 't Ġtell Ġher Ġthat Ġhe 'd Ġquit Ġthe Ġsalt Ġmines Ġbecause , Ġsea Ġair Ġnot with standing , Ġhe Ġcould Ġnot Ġbear Ġto Ġbe Ġso Ġfar Ġunderground , Ġthe Ġwhite Ġwalls Ġdwarf ing Ġhim , Ġhis Ġown Ġshadow Ġa Ġst ranger Ġas Ġhe Ġhun ched Ġover Ġand Ġsh utt led Ġcart s Ġof Ġsupplies Ġto Ġwhite Ġmen Ġsimilar Ġto Ġhow Ġhe Ġimag i\n",
      "\t Unique Tokens twitter: {'Ġreason', 'Ġguess', 'olly', 'ul', 'Ġcoward', 'our', 'ground', 'arts', 'st', 'ight', 'ĠF', 'Ġoblig', 'ene', 'od', 'us', 'are', 'Ġstranger', 'ingh', 'ĠNeg', 'itch', 'al', 'ations', 'ouse', 'Ġk', 'cro', 'Ġd', 'olve', 'ers', 'ERSON', 'ably', 'Ġbr', 'brew', 'Ġneighborhood', 'Ġappeal', 'amy', 'unch', 'Ġboard', 'Ġunder', 'v', 'Ġi', 'ĠM', 'Ġth', 'Ġh', 'om', 'ian', 'Ġsoda', 'ĠE', 'Ġpockets', 'ar', 'Ġdamn', 'Ġste', 'ared', 'est', 'Ġad', 'ries', 'Ġther', 'ĠEgypt', 'airs', 'Ġimagined', 'assi', 'Ġfe', 'Ġf', 'Ġyours', 'Ġbang', 'Ġsumm', 'ooth', 'ĠMc', '[UNK]', 'ried', 'ter', 'Ġ_', 'Gee', 'war', 'Ġfold', 'Ġslept', 'Ġbrave', 'Ġc', 'ile', 'ew', 'irt', 'Ġgrateful', 'Ġmod', 'Ġen', 'tte', 'Ġabs'}\n",
      "\t Unique Tokens wiki: {'ĠWyoming', 'ĠMolly', 'Ġthir', 'ockets', 'ned', 'ĠNegro', 'ON', 'Ġappealed', 'ined', 'ĠMcG', 'ard', 'ĠHebrew', 'ERS', 'arries', 'ranger', 'Ġbrass', 'tery', 'Ġiv', 'ĠEgyptian', 'Ġcow', 'Ġdried', '>', 'Ġrental', 'Ġfare', 'Ġgr', 'Ġsle', 'ess', 'Ġ<', 'Ġfooth', 'Ġsummers', 'ours', 'Ġdam', 'ee', 'ang', 'pt', 'Ġabsol', 'Ġdwarf', 'r', 'Ġboarding', 'odus', 'Ġhun', 'Ġfeared', 'Ġbra', 'Ġobligations', 'Ġst', 'Ġfolded', 'Ġimag', 'essed', 'Ġneighborhoods', 'oda', 've', 'Ġsteam', 'ĠEight', 'Ġs', 'ette', 'i', 'ĠMile', 'ateful', 'Ġbour', 'house', 'ched', 'ties', 'Ġmodest', 'Ġkitchen', 'Ġgu', 'ĠFew', 'Ġreasonably', 'Ġunderground', 'Ġenc', 'Ġadul', 'stairs', 'Ġy'}\n",
      "Text 2: I think a lot about how different everything would be if just the smallest thing were changed, like how the whole world would be an entirely different place if it hadn't been for one crazy guy saying cats were familiars of the devil, or how my life might have been totally different if I had just bee\n",
      "\t Tokenized twitter:I Ġthink Ġa Ġlot Ġabout Ġhow Ġdifferent Ġeverything Ġwould Ġbe Ġif Ġjust Ġthe Ġsmall est Ġthing Ġwere Ġchanged , Ġlike Ġhow Ġthe Ġwhole Ġworld Ġwould Ġbe Ġan Ġentirely Ġdifferent Ġplace Ġif Ġit Ġhadn 't Ġbeen Ġfor Ġone Ġcrazy Ġguy Ġsaying Ġcats Ġwere Ġfam ili ars Ġof Ġthe Ġdevil , Ġor Ġhow Ġmy Ġlife Ġmight Ġhave Ġbeen Ġtotally Ġdifferent Ġif ĠI Ġhad Ġjust Ġbee\n",
      "\t Tokenized wiki:I Ġthink Ġa Ġlot Ġabout Ġhow Ġdifferent Ġeverything Ġwould Ġbe Ġif Ġjust Ġthe Ġsmallest Ġthing Ġwere Ġchanged , Ġlike Ġhow Ġthe Ġwhole Ġworld Ġwould Ġbe Ġan Ġentirely Ġdifferent Ġplace Ġif Ġit Ġhad n 't Ġbeen Ġfor Ġone Ġcra zy Ġguy Ġsaying Ġcats Ġwere Ġfam ili ars Ġof Ġthe Ġdevil , Ġor Ġhow Ġmy Ġlife Ġmight Ġhave Ġbeen Ġtotally Ġdifferent Ġif ĠI Ġhad Ġjust Ġbee\n",
      "\t Unique Tokens twitter: {'ĠĠĠĠĠĠĠ', 'est', 'think', 'Ġhadn', 'ionally', 'Ġreg', 'Ġcrazy', 'Ġsmall'}\n",
      "\t Unique Tokens wiki: {'Ġregion', 'Ġ', 'Ġcra', 'n', 'zy', 'Ġsmallest', 'th', 'ink', 'ally'}\n",
      "==not Same Author==\n",
      "Text 1: Hi guys, this is my first post here, so let me start by saying it's a real pleasure to be a part of this community. I'm a newbie Mac developer and am looking forward to learning a lot and (hopefully) helping others along the way.\n",
      "\n",
      "I'm adding AquaticPrime to my first app, and setting up the PHP scrip\n",
      "\t Tokenized twitter:Hi Ġguys , Ġthis Ġis Ġmy Ġfirst Ġpost Ġhere , Ġso Ġlet Ġme Ġstart Ġby Ġsaying Ġit 's Ġa Ġreal Ġpleasure Ġto Ġbe Ġa Ġpart Ġof Ġthis Ġcommunity . ĠI 'm Ġa Ġnew bie ĠMac Ġdevelop er Ġand Ġam Ġlooking Ġforward Ġto Ġlearning Ġa Ġlot Ġand Ġ( hope fully ) Ġhelping Ġothers Ġalong Ġthe Ġway . Ċ Ċ I 'm Ġadding ĠAqu atic Prime Ġto Ġmy Ġfirst Ġapp , Ġand Ġsetting Ġup Ġthe ĠP HP Ġsc rip\n",
      "\t Tokenized wiki:H i Ġguys , Ġthis Ġis Ġmy Ġfirst Ġpost Ġhere , Ġso Ġlet Ġme Ġstart Ġby Ġsaying Ġit 's Ġa Ġreal Ġpleasure Ġto Ġbe Ġa Ġpart Ġof Ġthis Ġcommunity . ĠI 'm Ġa Ġnew bie ĠMac Ġdeveloper Ġand Ġam Ġlooking Ġforward Ġto Ġlearning Ġa Ġlot Ġand Ġ( h ope fully ) Ġhelping Ġothers Ġalong Ġthe Ġway . Ċ Ċ I 'm Ġadding ĠAqu atic Pr ime Ġto Ġmy Ġfirst Ġapp , Ġand Ġsetting Ġup Ġthe ĠP HP Ġsc rip\n",
      "\t Unique Tokens twitter: {'ate', 'er', 'Ġactiv', 'Ġdevelop', 'ray', '[UNK]', 'ers', 'Ġsc', 'rat', 'ERSON', 'Hi', 'her', 'ation', 'ts', 'Ġgener', 'Ġoccur', 'Ġar', 'Ġint', 'ring', 'hope', ')?', 'Prime', 'rip'}\n",
      "\t Unique Tokens wiki: {'<', 'Ġgenerate', 'H', 'ON', 'Ġarray', 'ERS', '>', 'Ġdeveloper', 'h', 'ime', 'Ġpoin', 'r', 'Ġoccurring', 'Ġintend', 'ope', 'ather', 'Ġscripts', 'ters', 'i', 'Pr', 'Ġactivation'}\n",
      "Text 2: I'm in a long distance relationship as well. In my opinion there should always be a long term plan: when will you close the distance? What are you both doing, together and individually, to work on the problems that are keeping you apart? Who will move to where, is there family or careers to consider\n",
      "\t Tokenized twitter:I 'm Ġin Ġa Ġlong Ġdistance Ġrelationship Ġas Ġwell . ĠIn Ġmy Ġopinion Ġthere Ġshould Ġalways Ġbe Ġa Ġlong Ġterm Ġplan : Ġwhen Ġwill Ġyou Ġclose Ġthe Ġdistance ? ĠWhat Ġare Ġyou Ġboth Ġdoing , Ġtogether Ġand Ġindivid ually , Ġto Ġwork Ġon Ġthe Ġproblems Ġthat Ġare Ġkeeping Ġyou Ġapart ? ĠWho Ġwill Ġmove Ġto Ġwhere , Ġis Ġthere Ġfamily Ġor Ġcare ers Ġto Ġconsider\n",
      "\t Tokenized wiki:I 'm Ġin Ġa Ġlong Ġdistance Ġrelationship Ġas Ġwell . ĠIn Ġmy Ġopinion Ġthere Ġshould Ġalways Ġbe Ġa Ġlong Ġterm Ġplan : Ġwhen Ġwill Ġyou Ġclose Ġthe Ġdistance ? ĠWhat Ġare Ġyou Ġboth Ġdoing , Ġtogether Ġand Ġindividually , Ġto Ġwork Ġon Ġthe Ġproblems Ġthat Ġare Ġkeeping Ġyou Ġapart ? ĠWho Ġwill Ġmove Ġto Ġwhere , Ġis Ġthere Ġfamily Ġor Ġcareers Ġto Ġconsider\n",
      "\t Unique Tokens twitter: {'Ġcare', 'Ġindivid', 'Dist', 'ually', 'ĠĠĠĠ', 'ers', 'ance'}\n",
      "\t Unique Tokens wiki: {'Ġ', 'istance', 'Ġindividually', 'D', 'Ġcareers'}\n",
      "==Same Author==\n",
      "Text 1: After checking the docs, it doesn't seem like groff offers a register or any other way for checking if it is being run in unsafe mode.  Would this not be a good idea?  I have some macros that use unsafe requests to create and update an index.   I'd like for these macros to be silent or issue maybe o\n",
      "\t Tokenized twitter:After Ġchecking Ġthe Ġdo cs , Ġit Ġdoesn 't Ġseem Ġlike Ġgro ff Ġoffers Ġa Ġregister Ġor Ġany Ġother Ġway Ġfor Ġchecking Ġif Ġit Ġis Ġbeing Ġrun Ġin Ġun safe Ġmode . Ġ ĠWould Ġthis Ġnot Ġbe Ġa Ġgood Ġidea ? Ġ ĠI Ġhave Ġsome Ġmac ros Ġthat Ġuse Ġun safe Ġrequests Ġto Ġcreate Ġand Ġupdate Ġan Ġind ex . ĠĠ ĠI 'd Ġlike Ġfor Ġthese Ġmac ros Ġto Ġbe Ġsilent Ġor Ġissue Ġmaybe Ġo\n",
      "\t Tokenized wiki:After Ġcheck ing Ġthe Ġdoc s , Ġit Ġdoesn 't Ġseem Ġlike Ġgro ff Ġoffers Ġa Ġregister Ġor Ġany Ġother Ġway Ġfor Ġcheck ing Ġif Ġit Ġis Ġbeing Ġrun Ġin Ġuns afe Ġmode . Ġ ĠW ould Ġthis Ġnot Ġbe Ġa Ġgood Ġidea ? Ġ ĠI Ġhave Ġsome Ġmac ros Ġthat Ġuse Ġuns afe Ġrequests Ġto Ġcreate Ġand Ġupdate Ġan Ġindex . Ġ Ġ ĠI 'd Ġlike Ġfor Ġthese Ġmac ros Ġto Ġbe Ġsilent Ġor Ġissue Ġmaybe Ġo\n",
      "\t Unique Tokens twitter: {'ex', 'Ġdiagn', 'ost', 'ĊĊ', 'ĠĠ', 'es', 'Ġsequ', 'safe', 'Ġun', 'ic', 'ĠWould', 'Ġind', 'Ġdistribut', 'Ġlear', 'Ġchecking', 'cs', 'y', 'ence'}\n",
      "\t Unique Tokens wiki: {'utes', 's', 'Ġindex', 'Ġsequence', 'Ġdoc', 'afe', 'Ġle', 'Ġuns', 'ould', 'ary', 'ing', 'Ġdiagnostic', 'Ġdistrib', 'ĠW', 'Ġcheck'}\n",
      "Text 2: Given a .JOBNAME request in the input document, groff -Thtml creates multiple output HTML files, splitting the input at every .NH 1 and .SH.\n",
      "\n",
      "Groff's ms's .SH takes an optional numeric argument n whereby .SH n typesets in a style (font size) similar to .NH n.   (By default, n = 1.)\n",
      "\n",
      "I think that sug\n",
      "\t Tokenized twitter:G iven Ġa Ġ. J OB N AME Ġrequest Ġin Ġthe Ġin put Ġdocument , Ġgro ff Ġ- T ht ml Ġcreates Ġmultiple Ġout put ĠH T ML Ġfiles , Ġsp li tting Ġthe Ġin put Ġat Ġevery Ġ. NH Ġ1 Ġand Ġ. SH . Ċ Ċ Gro ff 's Ġms 's Ġ. SH Ġtakes Ġan Ġopt ional Ġn um eric Ġargument Ġn Ġwhere by Ġ. SH Ġn Ġtypes ets Ġin Ġa Ġstyle Ġ( f ont Ġsize ) Ġsimilar Ġto Ġ. NH Ġn . ĠĠ Ġ( By Ġdefault , Ġn Ġ= Ġ1 .) Ċ Ċ I Ġthink Ġthat Ġs ug\n",
      "\t Tokenized wiki:Given Ġa Ġ. J O B NA ME Ġrequest Ġin Ġthe Ġinput Ġdocument , Ġgro ff Ġ- Th t m l Ġcreates Ġmultiple Ġoutput ĠHT ML Ġfiles , Ġsplitting Ġthe Ġinput Ġat Ġevery Ġ. NH Ġ1 Ġand Ġ. SH . Ċ Ċ Gro ff 's Ġm s 's Ġ. SH Ġtakes Ġan Ġoptional Ġnum eric Ġargument Ġn Ġwhereby Ġ. SH Ġn Ġtypes ets Ġin Ġa Ġstyle Ġ( font Ġsize ) Ġsimilar Ġto Ġ. NH Ġn . Ġ Ġ Ġ( By Ġdefault , Ġn Ġ= Ġ1 .) Ċ Ċ I Ġthink Ġthat Ġsug\n",
      "\t Unique Tokens twitter: {'Ġwhere', 'put', 'ml', 'tting', 'ional', 'li', 'Ġopt', 'G', 'N', 'Ġout', 'ĊĊ', 'Ġsp', 'ĠH', 'by', 'T', 'Ġsuggest', 'um', 'ont', 'AME', 'f', 'ĠĠ', 'OB', 'ht', 'Ġms', 'iven'}\n",
      "\t Unique Tokens wiki: {'l', 'Ġinput', 'font', 'O', 'm', 'Ġoutput', 'B', 'Ġm', 'Ġwhereby', 't', 'Ġoptional', 'ME', 'Ġnum', 'ĠHT', 'Ġ', 'Ġsuggests', 'Th', 'Ġsplitting', 'Given', 'NA'}\n",
      "==Same Author==\n",
      "Text 1: <PERSON>,\n",
      "Under the old?Law when a player substituted 2H for 1H (1D-1S-1H) you didn't know what he held.? Under the new Law when a player substitutes 2H he has his 2H bid...otherwise he would Dbl.? Almost all hands that bid 1H fit either Dbl or 2H in many systems.\n",
      "\n",
      "The whole idea of allowing a chang\n",
      "\t Tokenized twitter:[UNK] P ERSON [UNK] , Ċ Under Ġthe Ġold ? Law Ġwhen Ġa Ġplayer Ġsub stit ut ed Ġ2 H Ġfor Ġ1 H Ġ( 1 D - 1 S - 1 H ) Ġyou Ġdidn 't Ġknow Ġwhat Ġhe Ġheld .? ĠUnder Ġthe Ġnew ĠLaw Ġwhen Ġa Ġplayer Ġsub stit utes Ġ2 H Ġhe Ġhas Ġhis Ġ2 H Ġbid ... other wise Ġhe Ġwould ĠD bl .? ĠAlmost Ġall Ġhands Ġthat Ġbid Ġ1 H Ġfit Ġeither ĠD bl Ġor Ġ2 H Ġin Ġmany Ġsystems . Ċ Ċ The Ġwhole Ġidea Ġof Ġallowing Ġa Ġchang\n",
      "\t Tokenized wiki:< P ERS ON > , Ċ Under Ġthe Ġold ? Law Ġwhen Ġa Ġplayer Ġsubstituted Ġ2 H Ġfor Ġ1 H Ġ( 1 D - 1 S - 1 H ) Ġyou Ġdidn 't Ġknow Ġwhat Ġhe Ġheld . ? ĠUnder Ġthe Ġnew ĠLaw Ġwhen Ġa Ġplayer Ġsubstit utes Ġ2 H Ġhe Ġhas Ġhis Ġ2 H Ġbid ... other wise Ġhe Ġwould ĠD bl . ? ĠAlmost Ġall Ġhands Ġthat Ġbid Ġ1 H Ġfit Ġeither ĠD bl Ġor Ġ2 H Ġin Ġmany Ġsystems . Ċ Ċ The Ġwhole Ġidea Ġof Ġallowing Ġa Ġchang\n",
      "\t Unique Tokens twitter: {'u', 'ise', 'fficient', '[UNK]', 'ĠTHAT', '.?', 'ERSON', 'ĠĊ', 'Ġprec', 'ĠLAW', 'ut', 'ately', 'ed', 'ĊĊ', 'stit', 'Ġaccur', 'vey', 'Ġsub', 'Ġcon', 'Ġins', 'ĠNEW', 'uthor', 'Ġuna'}\n",
      "\t Unique Tokens wiki: {'ĠT', '<', 'Ġaccurately', 'ON', 'Ġprecise', 'ERS', '>', 'W', 'Ġun', 'Ġconvey', 'ĠNE', 'Ġ<', 'HA', 'Ġsubstit', 'T', 'Ġinsufficient', 'author', 'ĠLA', 'Ġsubstituted'}\n",
      "Text 2: I want to play bridge the way the ACBL guidelines tell us to -- by being as \n",
      "helpful and forthcoming as I can in revealing my understandings to my opponents, \n",
      "and expecting the same in return.  I don't want to play bridge where Secretary \n",
      "Birds hide behind an obscure and questionable minute to claim\n",
      "\t Tokenized twitter:I Ġwant Ġto Ġplay Ġbridge Ġthe Ġway Ġthe ĠAC BL Ġguidelines Ġtell Ġus Ġto Ġ-- Ġby Ġbeing Ġas Ġ Ċ help ful Ġand Ġforth coming Ġas ĠI Ġcan Ġin Ġrevealing Ġmy Ġunderstand ings Ġto Ġmy Ġoppon ents , Ġ Ċ and Ġexpecting Ġthe Ġsame Ġin Ġreturn . Ġ ĠI Ġdon 't Ġwant Ġto Ġplay Ġbridge Ġwhere ĠSecretary Ġ Ċ Bird s Ġhide Ġbehind Ġan Ġobs cure Ġand Ġquestion able Ġminute Ġto Ġclaim\n",
      "\t Tokenized wiki:I Ġwant Ġto Ġplay Ġbridge Ġthe Ġway Ġthe ĠAC BL Ġguidelines Ġtell Ġus Ġto Ġ-- Ġby Ġbeing Ġas Ġ Ċ hel p ful Ġand Ġforthcoming Ġas ĠI Ġcan Ġin Ġrevealing Ġmy Ġunderstand ings Ġto Ġmy Ġopponents , Ġ Ċ and Ġexpecting Ġthe Ġsame Ġin Ġreturn . Ġ ĠI Ġdon 't Ġwant Ġto Ġplay Ġbridge Ġwhere ĠSecretary Ġ Ċ B irds Ġhide Ġbehind Ġan Ġobscure Ġand Ġquestion able Ġminute Ġto Ġclaim\n",
      "\t Unique Tokens twitter: {'help', 'ations', 'Ġpro', 'Ġinterpret', 'ĊĊ', 'coming', 'bab', 'cure', 'Ġoppon', 's', '[UNK]', 'Ġobs', 'Bird', 'ilities', 'ERSON', 'ents', 'Ġforth'}\n",
      "\t Unique Tokens wiki: {'Ġopponents', 'Ġprob', '>', '<', 'Ġinterpretations', 'Ġforthcoming', 'p', 'Ġobscure', 'B', 'irds', 'ON', 'hel', 'abilities', 'ERS'}\n",
      "==not Same Author==\n",
      "Text 1: Hi.\n",
      "I need a quick way to get the ip address of the esternal esmtp client that\n",
      "submit emails to my courier.\n",
      "\n",
      "Is there a way to force courier to generate a unique custom mail header\n",
      "with the ip address of the esmtp sender?\n",
      "\n",
      "I need to classify mails from maildrop filter rules by incoming mails ip\n",
      "addr\n",
      "\t Tokenized twitter:Hi . Ċ I Ġneed Ġa Ġquick Ġway Ġto Ġget Ġthe Ġi p Ġaddress Ġof Ġthe Ġest ern al Ġes mtp Ġclient Ġthat Ċ sub mit Ġemails Ġto Ġmy Ġco ur ier . Ċ Ċ Is Ġthere Ġa Ġway Ġto Ġforce Ġco ur ier Ġto Ġgener ate Ġa Ġunique Ġcustom Ġmail Ġheader Ċ with Ġthe Ġi p Ġaddress Ġof Ġthe Ġes mtp Ġsend er ? Ċ Ċ I Ġneed Ġto Ġclass ify Ġm ails Ġfrom Ġmai ld rop Ġfilter Ġrules Ġby Ġincoming Ġm ails Ġi p Ċ ad dr\n",
      "\t Tokenized wiki:H i . Ċ I Ġneed Ġa Ġquick Ġway Ġto Ġget Ġthe Ġ ip Ġaddress Ġof Ġthe Ġ estern al Ġes m t p Ġclient Ġthat Ċ sub mit Ġem ails Ġto Ġmy Ġcou rier . Ċ Ċ Is Ġthere Ġa Ġway Ġto Ġforce Ġcou rier Ġto Ġgenerate Ġa Ġunique Ġcustom Ġmail Ġheader Ċ with Ġthe Ġ ip Ġaddress Ġof Ġthe Ġes m t p Ġse nder ? Ċ Ċ I Ġneed Ġto Ġclass ify Ġm ails Ġfrom Ġma ild rop Ġfilter Ġrules Ġby Ġincoming Ġm ails Ġ ip Ċ add r\n",
      "\t Unique Tokens twitter: {'ate', 'Hi', 'ern', 'er', 'ĊĊ', 'Ġgener', 'ier', 'mtp', 'Ġest', 'ld', 'Ġemails', 'Ġi', 'Ġco', 'Ġsend', 'Thanks', 'Ġmai', 'ur'}\n",
      "\t Unique Tokens wiki: {'nder', 'ip', 'Ġ', 'Ġse', 'Ġgenerate', 'i', 'ild', 'H', 'm', 'Ġma', 't', 'Ġem', 'Th', 'Ġcou', 'rier', 'anks', 'estern'}\n",
      "Text 2: You cannot send data received from your external application to a web page by trying to respond to the web page's GET request with request.body of POST request received from the external application. The simplest way to do what you're trying to achieve would be something like this.\n",
      "app.use(myParser.\n",
      "\t Tokenized twitter:You Ġcannot Ġsend Ġdata Ġreceived Ġfrom Ġyour Ġex ternal Ġapplication Ġto Ġa Ġweb Ġpage Ġby Ġtrying Ġto Ġrespond Ġto Ġthe Ġweb Ġpage 's ĠGET Ġrequest Ġwith Ġrequest . body Ġof ĠPOST Ġrequest Ġreceived Ġfrom Ġthe Ġex ternal Ġapplication . ĠThe Ġsimpl est Ġway Ġto Ġdo Ġwhat Ġyou 're Ġtrying Ġto Ġachieve Ġwould Ġbe Ġsomething Ġlike Ġthis . Ċ app . use ( my P ars er .\n",
      "\t Tokenized wiki:You Ġcannot Ġsend Ġdata Ġreceived Ġfrom Ġyour Ġexternal Ġapplication Ġto Ġa Ġweb Ġpage Ġby Ġtrying Ġto Ġrespond Ġto Ġthe Ġweb Ġpage 's ĠG ET Ġrequest Ġwith Ġrequest . body Ġof ĠP OS T Ġrequest Ġreceived Ġfrom Ġthe Ġexternal Ġapplication . ĠThe Ġsimpl est Ġway Ġto Ġdo Ġwhat Ġyou 're Ġtrying Ġto Ġachieve Ġwould Ġbe Ġsomething Ġlike Ġthis . Ċ app . use ( my P ars er .\n",
      "\t Unique Tokens twitter: {'Ġsock', 'fun', 'ase', 'Ġdat', 'ets', 'ĠPOST', 'ction', 'ĊĠĠĠ', 'ternal', 'rie', 'ended', 'ext', 'Ġex', 'ĠGET', 've', 'Ġbrows', '))', ';', 'ab'}\n",
      "\t Unique Tokens wiki: {'ex', 'Ġdatabase', 'Ġ', 'ET', 'function', 'OS', 'e', 'Ġbrowser', 'ockets', 'ĠG', 'ĠP', 'Ġexternal', 'T', ');', 'tended', '})', 'riev', 'Ġs'}\n",
      "==not Same Author==\n",
      "Text 1: Hello,\n",
      "\n",
      "I'm just testing Ivy, and I encountered a first problem.\n",
      "\n",
      "My corporate \"repository\" is shared through a network directory.\n",
      "\n",
      "So, I understand I have to customize Ivy settings to handle this.\n",
      "\n",
      "My problem is the \"ivy.shared.default.root\" is dynamic... It looks like:\n",
      "//builds/5_greatest/win64_x6\n",
      "\t Tokenized twitter:Hello , Ċ Ċ I 'm Ġjust Ġtesting ĠI vy , Ġand ĠI Ġencounter ed Ġa Ġfirst Ġproblem . Ċ Ċ My Ġcorporate Ġ\" re pos itory \" Ġis Ġshared Ġthrough Ġa Ġnetwork Ġdirect ory . Ċ Ċ So , ĠI Ġunderstand ĠI Ġhave Ġto Ġcustom ize ĠI vy Ġsettings Ġto Ġhandle Ġthis . Ċ Ċ My Ġproblem Ġis Ġthe Ġ\" ivy . sh ared . de fault . root \" Ġis Ġdynamic ... ĠIt Ġlooks Ġlike : Ċ // build s / 5 _ great est / win 64 _ x 6\n",
      "\t Tokenized wiki:H ello , Ċ Ċ I 'm Ġjust Ġtesting ĠIvy , Ġand ĠI Ġencountered Ġa Ġfirst Ġproblem . Ċ Ċ My Ġcorporate Ġ\" re pos itory \" Ġis Ġshared Ġthrough Ġa Ġnetwork Ġdirectory . Ċ Ċ So , ĠI Ġunderstand ĠI Ġhave Ġto Ġcustom ize ĠIvy Ġsettings Ġto Ġhandle Ġthis . Ċ Ċ My Ġproblem Ġis Ġthe Ġ\" iv y . sh ared . def ault . ro ot \" Ġis Ġdynamic ... ĠIt Ġlooks Ġlike : Ċ // build s / 5 _ great est / win 64 _ x 6\n",
      "\t Unique Tokens twitter: {'de', ').', 'Ġencounter', 'root', 'pl', 'direct', 'Ġmark', 'Ġpar', 'ivy', 'vy', 'ed', 'fault', 'ĊĊ', 'Ġsub', 'maybe', 'am', 'Ġdirect', 'eter', 'atform', 'Hello', 'Thanks'}\n",
      "\t Unique Tokens wiki: {'H', 'Ġdirectory', 'Ġmarked', 'be', 'ello', 'def', 'irect', 'ault', \"').\", 'iv', 'may', 'plat', 'Ġencountered', 'Ġparameter', 'anks', 'ot', 'y', 'ĠIvy', 'Th', 'form', 'Ġsubd', 'ro'}\n",
      "Text 2: Each and every project is different, however as a rule of thumb here are the core things that I try to have done prior to letting code go out to the wild.\n",
      "In no particular order:\n",
      "1) A version identification in place where it can be found by a user later, this must be unique to this release. (very ty\n",
      "\t Tokenized twitter:E ach Ġand Ġevery Ġproject Ġis Ġdifferent , Ġhowever Ġas Ġa Ġrule Ġof Ġthumb Ġhere Ġare Ġthe Ġcore Ġthings Ġthat ĠI Ġtry Ġto Ġhave Ġdone Ġprior Ġto Ġletting Ġcode Ġgo Ġout Ġto Ġthe Ġwild . Ċ In Ġno Ġparticular Ġorder : Ċ 1 ) ĠA Ġversion Ġident ification Ġin Ġplace Ġwhere Ġit Ġcan Ġbe Ġfound Ġby Ġa Ġuser Ġlater , Ġthis Ġmust Ġbe Ġunique Ġto Ġthis Ġrelease . Ġ( very Ġty\n",
      "\t Tokenized wiki:Each Ġand Ġevery Ġproject Ġis Ġdifferent , Ġhowever Ġas Ġa Ġrule Ġof Ġthumb Ġhere Ġare Ġthe Ġcore Ġthings Ġthat ĠI Ġtry Ġto Ġhave Ġdone Ġprior Ġto Ġletting Ġcode Ġgo Ġout Ġto Ġthe Ġwild . Ċ In Ġno Ġparticular Ġorder : Ċ 1 ) ĠA Ġversion Ġidentification Ġin Ġplace Ġwhere Ġit Ġcan Ġbe Ġfound Ġby Ġa Ġuser Ġlater , Ġthis Ġmust Ġbe Ġunique Ġto Ġthis Ġrelease . Ġ( very Ġty\n",
      "\t Unique Tokens twitter: {'Ġdocument', 'ational', 'ĠDe', 'Ġaka', 'set', 'E', 'ity', 'ĠCand', 'Ġident', 'ending', 'ource', 'fect', 'es', 'Ġapp', 'hot', 'ive', 'Ġoff', 'ed', 'ating', 'Ġtyp', 'Ġoper', 'Ġfir', 'Ġlibr', 'ps', 'red', 'ĠCould', 'ically', 'ctional', 'ach', 've', 'id', 'fer', 'are', 'aries', 'mw', 's', 'Ġsna', 'should', 'tted', 'ĠNot', 'Ġdistribut', 'ification', 'Have'}\n",
      "\t Unique Tokens wiki: {'Ġoperational', 'H', 'utable', 'Ġsnap', 'Ġoffset', 'Each', 'ĠCandid', 'ect', 'Ġoperating', 'shot', 'sh', 'nding', 'ferred', 'Ġarchive', 'Ġfirm', 'ave', 'ĠC', 'ted', 'Ġlibraries', 'Ġinst', 'Ġdefect', 'Ġidentification', 'Ġtypically', 'Ġdistrib', 'source', 'ĠDef', 'ka', 'ĠNotes', 'ould', 'Ġdocumented', 'ware', 'ble', 'alla', 'Ġappe', 'Ġfunctionality', 'vet'}\n",
      "==Same Author==\n",
      "Text 1: Hello <PERSON>,\n",
      "\n",
      "Quite new to PMWicki, I already start to love it and am thankful to you, \n",
      "<PERSON> and all the other contributors and this very helpful community for \n",
      "their work.\n",
      "\n",
      "Glad to hear about your wife's good prognosis.\n",
      "\n",
      "I wish her and you all the best. Take all the time you need to comfort \n",
      "\t Tokenized twitter:Hello Ġ [UNK] P ERSON [UNK] , Ċ Ċ Qu ite Ġnew Ġto ĠPM W ick i , ĠI Ġalready Ġstart Ġto Ġlove Ġit Ġand Ġam Ġthankful Ġto Ġyou , Ġ Ċ [UNK] P ERSON [UNK] Ġand Ġall Ġthe Ġother Ġcontribut ors Ġand Ġthis Ġvery Ġhelpful Ġcommunity Ġfor Ġ Ċ their Ġwork . Ċ Ċ Glad Ġto Ġhear Ġabout Ġyour Ġwife 's Ġgood Ġpro gn osis . Ċ Ċ I Ġwish Ġher Ġand Ġyou Ġall Ġthe Ġbest . ĠTake Ġall Ġthe Ġtime Ġyou Ġneed Ġto Ġcomfort Ġ\n",
      "\t Tokenized wiki:H ello Ġ< P ERS ON > , Ċ Ċ Qu ite Ġnew Ġto ĠPM W icki , ĠI Ġalready Ġstart Ġto Ġlove Ġit Ġand Ġam Ġthank ful Ġto Ġyou , Ġ Ċ < P ERS ON > Ġand Ġall Ġthe Ġother Ġcontributors Ġand Ġthis Ġvery Ġhelpful Ġcommunity Ġfor Ġ Ċ their Ġwork . Ċ Ċ G lad Ġto Ġhear Ġabout Ġyour Ġwife 's Ġgood Ġpro gnosis . Ċ Ċ I Ġwish Ġher Ġand Ġyou Ġall Ġthe Ġbest . ĠTake Ġall Ġthe Ġtime Ġyou Ġneed Ġto Ġcomfort Ġ\n",
      "\t Unique Tokens twitter: {'ors', 'Hello', 'i', 'Ġthankful', 'gn', 'your', '[UNK]', 'Glad', 'Ġworry', 'ick', 'osis', 'ERSON', 'Ġcontribut'}\n",
      "\t Unique Tokens wiki: {'Ġthank', 'our', '>', '<', 'ry', 'gnosis', 'H', 'lad', 'G', 'Ġcontributors', 'ON', 'Ġ<', 'icki', 'ello', 'ful', 'y', 'ERS', 'Ġwor'}\n",
      "Text 2: Hello,\n",
      "\n",
      "I want to create a multilingual site using MultiLanguageViews, and I \n",
      "have editors from different countries. How can I set the wiki interface \n",
      "language via the XLPage for each editor?\n",
      "MultiLanguageViews proposes to do it according to the language view \n",
      "chosen by the user (editor), but I thin\n",
      "\t Tokenized twitter:Hello , Ċ Ċ I Ġwant Ġto Ġcreate Ġa Ġmult iling ual Ġsite Ġusing ĠMult iL anguage View s , Ġand ĠI Ġ Ċ have Ġedit ors Ġfrom Ġdifferent Ġcountries . ĠHow Ġcan ĠI Ġset Ġthe Ġw iki Ġinter face Ġ Ċ l anguage Ġvia Ġthe ĠX LP age Ġfor Ġeach Ġeditor ? Ċ Mult iL anguage View s Ġprop oses Ġto Ġdo Ġit Ġaccording Ġto Ġthe Ġlanguage Ġview Ġ Ċ ch osen Ġby Ġthe Ġuser Ġ( ed itor ), Ġbut ĠI Ġthin\n",
      "\t Tokenized wiki:H ello , Ċ Ċ I Ġwant Ġto Ġcreate Ġa Ġmult ilingual Ġsite Ġusing ĠMulti L anguage View s , Ġand ĠI Ġ Ċ have Ġeditors Ġfrom Ġdifferent Ġcountries . ĠHow Ġcan ĠI Ġset Ġthe Ġw iki Ġinterface Ġ Ċ language Ġvia Ġthe ĠX LP age Ġfor Ġeach Ġeditor ? Ċ Mult i L anguage View s Ġproposes Ġto Ġdo Ġit Ġaccording Ġto Ġthe Ġlanguage Ġview Ġ Ċ ch osen Ġby Ġthe Ġuser Ġ( editor ), Ġbut ĠI Ġthin\n",
      "\t Unique Tokens twitter: {'ig', 'ors', 'l', 'better', 'itor', 'Ġdes', 'iL', 'face', 'Ġedit', 'iling', 'ed', 'Ġconf', 'Ġprop', 'Ġedits', 'looks', 'ired', 'Ġcond', 'ual', 'oses', 'itional', 'Hello', 'Ġinter', 'Thanks', 'ĠMult'}\n",
      "\t Unique Tokens wiki: {'ĠMulti', 'L', 'Ġeditors', 'ilingual', 'H', 'look', 'ello', 'ter', 'Ġed', 'its', 'editor', 'Ġproposes', 'Ġinterface', 'Ġconditional', 'bet', 'Ġconfig', 'anks', 'language', 'i', 'Th', 'Ġdesired'}\n"
     ]
    }
   ],
   "source": [
    "compare_predictions(df_twitter_av, df_wikipedia_av, tok_twitter, tok_wikipedia, \"twitter\", \"wiki\", classification_at_1=\"Same Author\", text1=\"query_text\", text2=\"candidate_text\", n_examples=10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-09T13:31:29.061381Z",
     "start_time": "2025-02-09T13:31:28.994302Z"
    }
   },
   "id": "8707de2c9ce0ddd0"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==Same Author==\n",
      "Text 1: . <PERSON> to <PERSON>, 26 June 1927, Phelps Stokes Papers, Yale.\n",
      " . <PERSON>, \"Impressions of the Port Elizabeth Conference,\" _Universitas_ 8:8 (August 1928).\n",
      " . <PERSON> to <PERSON>, 24 October 1928, NAACP Papers, Library of Congress.\n",
      " ### NOTES TO CHAPTER 4\n",
      " . \"<PERSON> of Africa Is Guest of the \n",
      "\t Tokenized no:.Ġ < P ERS ON > ĠtoĠ < P ERS ON >,Ġ 26Ġ JuneĠ 19 27 ,ĠP helpsĠ St ok esĠ Pap ers,Ġ Y al e.Ċ Ġ .Ġ < P ERS ON > ,Ġ\" Im pression sĠofĠtheĠ PortĠ ElizabethĠ Con fer ence ,\"Ġ _ Universit as _Ġ 8: 8 Ġ( AugustĠ 19 28 ).Ċ Ġ .Ġ < P ERS ON > ĠtoĠ < P ERS ON >,Ġ 24Ġ OctoberĠ 19 28 ,Ġ NA AC PĠ Pap ers,Ġ Libr aryĠofĠ Congres s.Ċ Ġ ###Ġ NOTE SĠ TOĠ CHAPTERĠ 4 ĊĠ .Ġ\" < P ERS ON >Ġ ofĠ Afric aĠ IsĠ Gu estĠ ofĠtheĠ\n",
      "\t Tokenized wiki:. Ġ< P ERS ON > Ġto Ġ< P ERS ON > , Ġ26 ĠJune Ġ1927 , ĠPhel ps ĠStokes ĠPapers , ĠYale . Ċ Ġ. Ġ< P ERS ON > , Ġ\" Imp ress ions Ġof Ġthe ĠPort ĠElizabeth ĠConference ,\" Ġ _ Univers itas _ Ġ8 : 8 Ġ( August Ġ1928 ). Ċ Ġ. Ġ< P ERS ON > Ġto Ġ< P ERS ON > , Ġ24 ĠOctober Ġ1928 , ĠNAA CP ĠPapers , ĠLibrary Ġof ĠCongress . Ċ Ġ# # # ĠN OT ES ĠTO ĠC HA PT ER Ġ4 Ċ Ġ. Ġ\" < P ERS ON > Ġof ĠAfrica ĠIs ĠGuest Ġof Ġthe Ġ\n",
      "\t Unique Tokens no: {',Ġ\"', 'RedĠ', 's,\"Ġ', 'Tor', 'on,Ġ', 'O', 'edĠbyĠtheĠ', 's:Ġ', '.ĠT', 'MC', 'edĠbyĠ', 'Com', 'MaxĠ', 'WorkĠ', 'FĠ', '74', 'opyĠ', '_D', '27', 'Friend', '>ĊĠ', '),Ġ', 'the', '.ĠOnĠ', '_TheĠ', '_Ġ', 'thĠ', 'AprilĠ', 'AC', 'ĊĠ', 'et', 'a', 'ismĠandĠ', '.ĠM', '26', \"an'sĠ\", 'TOĠ', '2', 'Document', 'ĠtoĠ\"', 'estĠ', 'Ex', 'ireĠ', 'ir', 'ing', 'Count', 'seeĠ', 'ational', '3', 'St', 'align', 'ualĠ', 'SchoolĠ', 'AmericanĠ', ',ĠM', ',ĠP', '1997', '.A', 'NewĠ', 'riesĠ', 'ialĠ', 'M', 'Intern', 's.ĠTheĠ', 'ay', '8:', 'ial', 'Revolution', 'ence,Ġ', 'Meet', 'even', 'worthyĠ', 'Kat', 'CA', 'NewĠYork', 'ceĠ', '7Ġ', 'ReportĠ', 'ateĠ', '.d', 'ributionĠ', '27Ġ', 'GeneralĠ', '88', 'inĠ', 'aryĠofĠ', 'ChristianĠ', 'ElizabethĠ', \"'Ġ\", ':5', 'select', 'medi', 'Room', 'conferenceĠ', '4,Ġ', 'esĠ', 'ementĠ', 'erĠ', 'a.', 'sĠofĠtheĠ', 'tract', '.ĊĠ', 'Nor', 'Town', 'Tri', ',ĠD', 'Task', 'es', 'Afric', 'Correspond', 'âĢĵĠ', ',Ġ19', 'Se', 'Bas', \"'TheĠ\", 'Arch', 'PortĠ', '2,Ġ', 'ement', 'yĠofĠ', 'notesĠ', 'opt', '._Ġ', 'FromĠtheĠ', 'leĠandĠ', 'Hel', 'Gu', 'sesĠ', 'AĠ', 'ottĠ', 'Congres', '.Ġ\"', 'Pap', 'e.Ċ', 'LifeĠ', 'Lov', 'angle', '.J', 'entĠ', 'Con', ',ĠandĠ', 'Contain', 'InĠ', ',Ġ', 'IsĠ', '>,Ġ', 'note', 'olderĠ', 'Polic', 'ic', '43', 'LordĠ', 'Project', '19', 'Ox', 'a,', 'additionĠtoĠ', 'ourĠ', '.ĠC', 'orsĠ', 'copyĠ', '.Ġ', 'as', 'erg', 'Polit', '31', 'DearĠ', 'NativeĠ', 'ProblemĠ', 'sĠandĠ', 'court', 'amaĠ', 'vol', 'JanuaryĠ', 'istĠ', 'ers,Ġ', 'New', '.,Ġ', 'ofĠtheĠ', 'ision', 'Educ', '.ĠItĠ', 'Colon', 'Mand', 'Pres', 'forĠtheĠ', 'ĠtoĠ', 'o:Ġ', 'es,Ġ', ',ĠF', 'upĠtoĠ', 'per', 'ation', '26Ġ', 'apeĠ', '1960', 'att', 'Libr', 'enceĠofĠ', 'ativeĠ', '28', '24Ġ', 'Div', 'siz', 'InternationalĠ', 'ĠandĠ', 'ers', ').Ċ', 'ityĠ', 'edit', 'Bio', '5Ġ', 'book,Ġ', '>Ġ', 'MarchĠ', '9Ġ', '97', 'eĠofĠ', 'aleĠ', 'NOTE', 'y,', 'antĠ', 'ationalĠ', '.Ġ(', 'fer', 'aĠ', ':ĠTheĠ', 'University', 'Collection', 'AugustĠ', ':Ġ', 'a:Ġ', 'assĠ', 'Sem', 'Mis', 'mer', 'itĠofĠ', 'ha', '._', ',\"Ġ', 'ĠmadeĠ', 'ary,Ġ', 'Re', 'fordĠ', 'yĠ', 'AsianĠ', 'laĠ', 's_', 'ondon', 'Ad', 'JimmyĠ', 're', 'ix', 'yĠinĠ', 'India', 'YearĠ', 'SupportĠ', 'was', '(C', 'itingĠ', 'onĠ', 'Y', 'ok', 'PĠ', '.ĠSeeĠ', 'mentĠofĠ', 'yp', 'andĠ', 'arĠ', 'onĠaĠ', '_N', '50', '###Ġ', 'Ent', 's', 'Princ', 'an', 'kk', '13', 'sĠ', 'inter', 'Land', 'olar', 'aryĠ', 'SouthĠ', 's,Ġ', 'WashingtonĠ', 'UniversityĠ', ')', 'ĠthoroughlyĠ', 'Universit', 'edĠaĠ', 'cont', '1,Ġ', '(', 'ev', 's,', 'SĠ', 'um', 'ont', 'graph', 'al', '22Ġ', 'TheĠ', 'Sch', 'date,Ġ', ',Ġ_', 'ofĠ', 'OctoberĠ', '.Ġ_', 'sĠfromĠtheĠ', 'W', 'Dist', 'al,Ġ', 'isionĠ', 'SouthĠAfric', '64', 'Stud', 'v', 'Gen', 'MayĠ', 'edĠandĠ', 'ution', 's.Ċ', '2:', '(L', '19Ġ', 'CHAPTERĠ', 'yield', 'Question', ',ĠW', 'IV', 'asternĠ', 'able', 'dĠ', '28Ġ', 'G', 'iv', 'gramĠ', 'ton,Ġ', 'n', 'pression', 'nĠ', 'pon', 'ed.Ġ', 'onĠtheĠ', 'helpsĠ', 'Vo', 'JuneĠ', 'Need', 'c', 'eĠandĠ', 'sion', 'WorldĠ', 'NA', 'e,Ġ', 'Mov', ',ĠtoĠ', 'Sen', '_C', 'Instit', 'i-', 'yĠofĠtheĠ', 'PacificĠ', 'Relation', 'AfricanĠ', 'atĠ', 'archĠ', '22', 'BoxĠ', 'Spo', 'Lif', 'keĠ', 'ary', 'InstituteĠofĠ', 'UniversityĠofĠ'}\n",
      "\t Unique Tokens wiki: {'Ġcopy', 'phas', 'ative', 'ĠGuest', 'ĠPress', 'perial', 'Ġ1922', 'Ġ1924', 'ions', 'Ġfrom', 'ant', 'ress', 'eds', 'ĠOxford', 'Ġand', 'CP', 'Ġa', 'our', 'ĠWork', 'Ġup', 'Ġ1997', 'ĠPapers', 'OT', 'ĠIV', 'ĠInternational', 'Ġedited', 'Ġ3', '1928', 'ĠDoc', 'ĠBox', 'ĠThe', 'ĠSp', 'ĠArchives', 'Ġ<', '.,', 'ĠRoom', 'ĠLands', 'Ġ1927', 'ĠF', 'HA', 'ĠMarch', 'Imp', 'ent', 'A', 'ĠFrom', 'ĠFriends', 'uma', 'ear', 'ĠAsian', 'ĠSemi', 'ĠInstitute', 'ER', 'ĠAfrican', 'ĠâĢĵ', 'oke', 'ual', 'Ġ\"', 'ignment', 'ĠIt', 'ott', \"'s\", 'Ġat', 'ism', 'Ġto', 'Ġla', 'MCA', 'ĠLov', 'ĠDivision', 'ĠRevolutionary', 'ĠJune', 'Ġselected', 'ors', 'ĠT', 'ĠLibrary', 'ĠIndia', 'Ġ1928', 'tern', 'ĠG', 'ĠSee', 'heastern', \"Ġ'\", 'Ġ24', 'ĠElizabeth', 'ĠBiography', 'onial', 'ĠFar', 'Ġ5', 'ĠScholars', 'ĠCom', 'Ġ50', 'Ġed', 'ĠN', 'ĠNew', 'ayama', 'Ġbook', 'Ġaddition', 'ĠPolicy', 'August', 'Ġmade', 'ĠOv', 'it', 'utions', 'ĠAfrica', 'ĠConference', 'Ċ', 'ĠQuestion', 'racts', 'ĠColonial', 'ps', 'ĠNews', 'my', 'ĠLife', 'Ġwas', 'Ġnotes', 'ĠCommunist', 'ĠPacific', 'ĠTown', 'ĠCable', 'ĠM', 'ĠMax', 'ĠApril', 'Ext', 'ĠAd', 'Ġ1926', 'Ġthe', 'tain', 'Ġsee', 'ian', 'ĠMovement', 'ĠTy', 'Ġn', 'ĠPhel', 'older', 'ĠMay', 'ĠTO', 'Ġ97', 'er', 'blem', 'ĠCommunists', ').', '1923', 'ĠYale', 'Ġconference', 'izes', 'ĠNative', 'ĠDivinity', 'itas', 'ĠProject', 'ĠGeneva', '.:', 'ĠIm', 'ĠC', 'Ġ1964', 'ĠVo', 'Ġ1974', 'ĠKat', 'Ġ84', 'mediate', 'ses', 'ĠToronto', 'ĠAmerican', 'Jim', 'gan', 'ĠIs', 'eat', 'ĠWorld', 'ĠStokes', 'ĠSchool', 'ĠUniversity', 'inc', 'ĠSen', 'Ġ22', 'ĠSeventh', 'Ġ7', 'ĠInstit', 'ĠLord', 'Ġin', 'ĠYork', 'Ġcontribution', 'ĠD', 'Ġ13', 'op', 'ĠPrinceton', 'ĠYear', 'Ġ191', 'eton', 'ĠCorrespond', 'Ġ26', 'ĠMeeting', 'Pr', 'ĠChristian', '1943', 'ĠReal', 'tle', 'ĠCol', 'ĠStudent', 'ĠRed', 'gram', 'ĠCountries', 'Ġon', 'ĠTriangle', 'Ġby', 'ĠBas', 'Ġnoteworthy', \"'\", 'ale', 'ĠPolitics', 'Univers', 'ĠEm', 'ĠEnt', 'ĠPort', 'd', '-', 'Ġthoroughly', 'American', 'asks', 'ire', 'Ġ1960', '),', 'ĠCommerce', 'ES', 'ĠNAA', 'ĠSouth', 'pes', 'kers', 'Ġvol', 'ĠWashington', 'Ġcourtesy', 'ted', 'ape', 'ĠPro', 'ĠCopy', 'ĠGeneral', 'ĠCedar', 'Ġciting', 'Ġ88', 'ĠOctober', 'Ġ4', 'ĠCongress', 'Ġyielded', 'ĠMass', 'ĠSixth', 'PT', 'ons', 'Ġ1919', 'ĠLondon', 'ĠMissionary', 'ĠA', 'ĠJanuary', 'and', 'ĠOn', 'Ġof', 'ĠSupport', ',\"', 'ĠHel', '#', '5', 'As', 'uments', 'ĠNe', 'Ġ2', 'Ġ9', 'Ġ8', 'ington', ':', 'Ġdate', 'ĠW', 'ĠY', 'ĠIn', 'Report', 'ĠEducational', 'Ġfor', 'Ġ.', 'Ġ31', 'ĠCon', 'ĠDist', 'J', 'Ġ#', 'ĠMand', 'ĠCollection', 'ĠRelations', 'rek', 'Ġ1929'}\n",
      "Text 2: \"We'd better go back before she teases him to death,\" <PERSON> said.\n",
      " \"He just needs to know how to defend himself,\" <PERSON> said. \"If she tries it again, <PERSON>, tickle her bottom.\"\n",
      " \"I don't have a bottom,\" the mermaid pointed out. \"Just a nice piece of tail.\"\n",
      " <PERSON> sighed. \"Then he must be\n",
      "\t Tokenized no:\" We 'dĠ betterĠ goĠbackĠ beforeĠsheĠ te asesĠ himĠtoĠ death ,\"Ġ < P ERS ON > Ġsaid .ĊĠ \" HeĠ justĠ needsĠtoĠ knowĠhowĠtoĠ defendĠ himself ,\"Ġ < P ERS ON > Ġsaid .Ġ\" If ĠsheĠ tri esĠ itĠ again,Ġ < P ERS ON > ,Ġt ick leĠ herĠ bottom .\" ĊĠ\" IĠdon'tĠ haveĠaĠ bottom ,\"ĠtheĠ mer ma idĠ pointedĠout .Ġ\" JustĠ aĠniceĠ pieceĠ ofĠt ail .\" ĊĠ < P ERS ON > Ġsigh ed.Ġ\" ThenĠ heĠ mustĠ be\n",
      "\t Tokenized wiki:\" We 'd Ġbetter Ġgo Ġback Ġbefore Ġshe Ġte ases Ġhim Ġto Ġdeath ,\" Ġ< P ERS ON > Ġsaid . Ċ Ġ\" He Ġjust Ġneeds Ġto Ġknow Ġhow Ġto Ġdefend Ġhimself ,\" Ġ< P ERS ON > Ġsaid . Ġ\" If Ġshe Ġtries Ġit Ġagain , Ġ< P ERS ON > , Ġtick le Ġher Ġbottom .\" Ċ Ġ\" I Ġdon 't Ġhave Ġa Ġbottom ,\" Ġthe Ġm erm aid Ġpointed Ġout . Ġ\" Just Ġa Ġnice Ġpiece Ġof Ġtail .\" Ċ Ġ< P ERS ON > Ġs ighed . Ġ\" Then Ġhe Ġmust Ġbe\n",
      "\t Unique Tokens no: {'might', 's,\"Ġ', 'fatherĠ', '.ĠT', 'ignor', 'Cap', '.ĠB', 'endĠtheĠ', 'March', 'signal', 'some', 'alreadyĠ', 'od', 'laugh', 'ĠsaidĠ', 'y', \"YouĠdon'tĠ\", 'SoĠ', 'edĠthroughĠ', 'fromĠ', 'smile', 'ig', 'aĠbigĠ', 'ĊĠ', 'lost', ',\"ĠsheĠ', \"you'reĠ\", 'or', 'itter', 'ideĠ', 's.ĠItĠ', 'needĠto', 'sĠtoĠgetĠ', 'ĠsheĠ', \"we'reĠ\", 'ooĠ', 'Post', 'clim', 'usĠ', 'itsĠ', 'stepĠ', 'needsĠtoĠ', 'accompani', 'pieceĠ', 'aĠniceĠ', '.ĠWeĠ', 'leĠthatĠ', 'allyĠtheĠ', 'riceĠ', 'lin', 'ĠtoldĠ', 'ais', 'aroundĠ', 'ellĠ', 'goĠbackĠ', 'deck', 'said', 'TheyĠ', 'eddingĠ', 'itĠwillĠ', 'M', 'ingĠtoĠ', 'ĠtimeĠ', 'defendĠ', 'realizeĠthatĠ', 'mĠ', 'dressedĠ', 'Man', 'HeĠwasĠ', 'enough', 'moreĠthanĠ', 'we', 'Mel', 'atĠtheĠ', 'hereĠandĠ', '.ĠBut', 'sheĠwasĠ', '.\"ĠSheĠ', 'NowĠ', 'Ġsay', 'edĠ', 'asĠ', 'ousĠ', '.ĠH', 'clud', 'fromĠtheĠ', 'sal', 'IĠsaidĠ', 'seemsĠ', 'soĠ', 'instead', 'wentĠ', 'esĠ', 'env', 'substitut', 'said,Ġ', 'knowĠhowĠtoĠ', 'needĠaĠ', 'fetchĠ', 'look', '.ĊĠ', 'ĠwithĠ', 'right', 'again', 'whileĠtheĠ', 'Norm', 'WhatĠ', 'needĠtoĠ', 'cast', ';Ġ', 'aĠsmile', 'ĠtheĠsameĠ', 'eĠtimeĠtoĠ', 'glanc', 'Only', 'AllĠright,Ġ', 'walk', 'asked,Ġ', 'eĠtoĠ', 'getĠtheĠ', 'ausedĠ', 'didĠ', 'windĠ', 'it.', 'aĠg', 'JustĠ', 'busyĠ', 'ou', 'wasĠtheĠ', 'water', 'whoĠ', 'HeĠ', 'boat', 'play', 'sesĠ', 'aĠc', 'ĠthenĠ', 'te', '.Ġ\"', \"You'reĠ\", 'harmon', 'ĠtheyĠ', 'onight', 'hereĠinĠ', ',\"ĠtheĠ', 'Hon', 'got', 'byĠ', 'be', 'Cast', 'onto', 'ĠtoĠtheĠ', 'abul', 'hy', 'allĠ', 'EndĠ', ',Ġ', 'song', 'Har', '>,Ġ', 'ic', 'doĠit', 'isĠ', 'S', 'Ġtime.', 'op', 'edĠtheĠ', 'leĠt', 'ĊĠ\"', '.Ġ', 'aĠs', 'haveĠaĠ', '.\"Ġ', 'wasĠ', 'bad', 'isĠthat', 'edĠatĠ', 'him', 'it,Ġ', 'sĠtheĠ', 'e.', 'getĠ', 'broughtĠ', 'had', 'sheĠ', 'F', '.ĠItĠ', 'elĠ', 'found', \",Ġthat'sĠ\", 'dog', 'Ġtravel', 'lyĠtheĠ', 'forĠtheĠ', 'ru', 'notĠtoĠ', 'agre', 'ĠtoĠ', 'pant', 'Brid', 'edĠthem', 'ĠtoĠhisĠ', 'Ġsigh', 'akĠ', '!Ġ', 'gleĠ', 'her', 'maid', 'ButĠ', 'qu', 'weĠneedĠ', 'betterĠ', 'ilyĠ', 'enoughĠ', 'ationsĠ', 'atĠherĠ', 'herĠ', 'eveĠ', 'edĠup', 'here.Ġ', 'stuck', \"'llĠ\", ',ĠwhileĠ', 'depart', \"'sĠ\", 'bedĠ', 'pointedĠout', 'lookedĠ', 'justĠ', 'weĠ', 'b', 'youngĠ', 'ĠandĠ', 'place', 'ingĠtheĠ', 'backĠintoĠtheĠ', 'ask', 'ed.Ċ', 'Ġwhisper', 'ĠsaysĠ', 'mustĠbeĠ', 'pr', '>Ġ', 'MarchĠ', 'manag', 'tri', 's!Ġ', 'it', 'aĠf', 'edĠherĠ', 'yĠandĠ', 'ore', 'ed.Ġ\"', 'sitĠ', 'aĠ', ',Ġt', 'asesĠ', 'udden', 'fetch', '?\"ĠheĠ', 'er', 'below', 'wedd', 'YouĠcan', 'mer', 'ĠcanĠ', 'amaz', ',\"Ġ', 'atelyĠ', 'fat', 'yĠ', 'one,Ġ', '!ĠIĠ', 'impres', 'ever', '.ĠF', \"IĠdon'tĠ\", 'ma', 'Ġwhat', 'needĠ', 'p', 'pau', 'ick', 're', 'about', 'step', 'was', 'mon', 'onĠ', 'carefullyĠ', 'downĠtheĠ', 'essĠ', 'leĠ', 'andĠ', 'ior', 'y.Ġ', 'withĠ', 'now', 'yourĠ', 'again,Ġ', 'br', 'Princ', 'sĠ', 'beforeĠsheĠ', 'aredĠ', 'overĠtoĠ', 'room', 'gettingĠtoĠ', \"'dĠ\", 'ail', 'quicklyĠ', '.ĠThen', 'don', 'isĠtheĠ', 'hadĠtoĠ', 'heĠ', 'Br', ',ĠyourĠ', 'knowĠ', 'isĠtoĠ', 'kis', 'Ġcon', 'isĠnotĠ', 'wouldĠ', 'Ġ', 'boatĠ', 'hear', 'th', 'eĠthereĠ', 'ItĠ', 'TheĠ', \"He'sĠ\", 'SheĠ', 'Ġcorrect', 'ĠthemĠ', 'notic', 'govern', 'dry', 'ofĠ', 'finallyĠ', 'W', 'inĠtheĠ', 'intoĠ', 'sed', 'sad', 'BestĠ', 'eĠthatĠtheĠ', 'girl', 'bey', 'ble', 'Ġclos', 'Ch', 'D', 'walkĠ', 'upĠtheĠ', 'aĠsmallĠ', 'etĠ', 'pe', 'way', '?\"Ġ', 'idĠ', ',ĠandĠtheĠ', \"it's\", 'doĠthatĠ', 'ingĠup', 'in,Ġ', 'ĠthereĠ', 'bottom', 'G', 'ĠthisĠwasĠ', 'jobĠ', 'brokeĠ', 'ThenĠ', 'soonĠ', 'oreĠ', 'seeingĠ', 'Ġsh', 'R', 'death', 'doĠ', 'keysĠ', 'ought', 's,ĠbutĠ', 'fac', 'himself', 'ort', 'wr', \"youĠdidn'tĠ\", '.ĠSheĠ', 'appearedĠ', 'aidĠ', 'IĠwasĠjustĠ', 'in', 'o', 'himĠtoĠ', 'flowersĠ', \"It'sĠaĠ\", 'ofĠt', 'crossedĠ', 'backĠ', '.ĠIt', 'w', 'setĠ', 'itĠ', 'hairĠ', '.ĠWe', 'on', 'mineĠ'}\n",
      "\t Unique Tokens wiki: {'He', 'ak', 'Now', 'ĠBrid', 'Ġhis', 'tering', 'Ġagreed', 'Ġstuck', 'Ġthis', 'Ġbut', 'Ġfrom', 'Ġchore', 'Ġwind', 'eve', 'She', 'Ġknow', 'Ġaccompanied', 'Ġand', 'Ġa', 'Ġwater', 'Ġup', 'Ġinstead', 'Wh', 'Ġwould', 'Ġbetter', 'Ġas', 'ĠThe', 'Ġfather', 'Ġdidn', 'Ġyou', 'Ġgo', 'vy', 'Ġstep', 'hed', 'ately', 'Ġsmall', 'ight', 'Ġfab', 'Ġits', 'ĠF', 'Ġsays', 'ĠMarch', 'ĠB', 'Ġthen', 'Ġflowers', 'aid', 'Ġhere', 'Ġwith', 'oring', 'Ġhe', 'Ġs', 'Ġall', 'You', 'Ġsm', 'Ġrehearsal', 'Ġcorrectly', 'ĠBr', 'Ġdefend', 'Ġgl', 'ĠIt', \"'s\", 'Ġat', 'Ġgirl', 'Ġtravels', 'Ġcarefully', 'Ġto', 'All', 'Ġasked', 'Ġla', 'Ġpointed', 'Ġsign', 'Ġhad', 'Ġp', 'Ġbottom', 'ug', 'Ġshore', 'quet', 'Ġdon', 'Ġyour', 'ĠThen', 'Ġback', 'Ġwhile', 'Then', 'But', ',', 'Ġdepart', 'ily', 'Ġway', 'key', 'Ġbrought', 'Ġset', 'Ġchair', 'ally', 'erm', '?\"', 'Ġfaced', 'Ġdog', 'used', 'Ġpe', 'Ġbelow', 'Ġhow', 'Ġm', 'Ġabout', 'Ġnice', 'atever', 'Just', \"'t\", 'Ġdressed', 'ĠHonor', 'Ġton', 'Ġwalked', 'Ġclimbed', 'ĠDell', 'Win', 'Ċ', '.', 'Ġneed', 'ĠCap', 'Ġis', 'Ġmore', 'Ġbig', 'Ġtit', 'Ġsad', 'Ġte', 'rin', 'Ġjust', 'ĠToo', 'Ġfate', 'om', 'Ġwill', 'Ġnotice', 'Ġquickly', 'is', 'Ġthe', 'Ġsubstitute', 'Ġlooked', 'Ġboat', 'Ġlined', ';', 'Ġthat', 'Ġaround', 'Ġcan', 'Ġhim', 'ĠCh', 'Ġaisle', 'Ġdry', 'Ġwent', 'ĠFort', 'Ġcrossed', 'Ġalready', 'Ġplays', 'Ġmight', 'ats', 'Ġobey', 'ulous', 'pered', 'Ġbou', 'Ġagain', 'ĠNorm', 'antry', 'Ġdo', 'Ġbe', 'ighed', 'Ġget', 'Ġright', 'Ġplace', 'iors', 'onica', 'Ġfoundations', 'ĠPrincess', 'Ġought', 'aled', 'ĠEnd', 'Ġseeing', 'Ġthan', 'Ġign', 'Ġnow', 'Ġyoung', 'Ġout', 'ĠMelody', 'Ġthey', 'Ġmust', 'ĠHarmony', 'Ġcastle', 'Ġsoon', 'ĠGro', 'Ġdown', 'Ġbroke', 'Ġher', 'Ġbefore', 'Ġsit', 'igg', 'ĠBut', 'Ġinto', 'Ġtries', 'ĠCastle', 'den', 'Ġthere', 'Ġsong', 'Ġtold', 'Ġin', 'Ġrealize', 'ĠWedding', 'ĠShe', 'Ġkiss', 'ched', 'Ġwho', 'oster', 'Ġfinally', 'Ġsomeone', 'Ġseems', 'anced', 'Ġover', 'Ġfet', 'Ġby', 'Ġon', 'ĠSo', 'Ġblew', 'ĠWe', 'Ġgetting', 'Ġpron', 'ĠSud', 'Ġmine', 'Ġnot', 'Ġshe', \"'re\", \"'d\", 'Ġpiece', 'Ġgot', 'Ġmanaging', '!', 'ĠMan', 'Ġbride', 'Ġdeath', 'Ġamaz', 'to', 'Ġme', 'Ġwe', 'Ġg', 'rice', 'Ġpa', 'Ġclose', 'Ġlost', 'ops', 'Ġdeck', 'Ġneeds', 'Ġjob', 'ases', 'Ġconcluded', 'Ġtail', 'ĠOnly', 'Best', 'Ġbusy', 'It', 'Ġappeared', 'Ġwedding', 'Ġsame', 'Ġharm', 'Ġof', ',\"', 'Ġw', 'ĠHe', 'Ġthrough', 'Ġend', 'cess', 'Ġbad', 'Ġwh', 'Ġgovern', 'ĠThey', 'Ġhave', 'Ġit', 'Ġre', 'ĠHelp', 'ĠMaid', 'ile', 'Ġimpressed', 'Ġdid', 'Ġcruel', 'Ġfor', 'ĠI', 'Ġenough', 'Ġen', 'ĠRhythm', 'Ġus', 'Ġhimself', 'ared'}\n",
      "==not Same Author==\n",
      "Text 1: the template code through its paces. The `assert_select` assertion call confirms that the resulting page contains the expected number of `div` elements with a class of `story`:\n",
      "     assert_select 'div#content div.story', count: 1\n",
      " And that, dear reader, is the last test I'll make you write! Well, fo\n",
      "\t Tokenized no:theĠt emplateĠ cod eĠthroughĠ itsĠ pac es.ĠTheĠ ` assert _ select `Ġ asser tion ĠcallĠ confirm sĠthatĠtheĠ resultingĠ pageĠ containsĠtheĠ expectedĠ numberĠofĠ ` div `Ġ elementsĠ withĠaĠ classĠofĠ ` story ` : ĊĠĠĠĠĠ assert _ selectĠ ' div # contentĠ div . story ',Ġ count :Ġ1 ĊĠ And Ġthat,Ġ dearĠ read er,Ġ isĠtheĠ last ĠtestĠ I'llĠ makeĠ youĠw rite !Ġ Well,Ġ fo\n",
      "\t Tokenized wiki:the Ġtem plate Ġcode Ġthrough Ġits Ġp aces . ĠThe Ġ ` ass ert _ se lect ` Ġassert ion Ġcall Ġconf irms Ġthat Ġthe Ġresulting Ġpage Ġcontains Ġthe Ġexpected Ġnumber Ġof Ġ ` div ` Ġelements Ġwith Ġa Ġclass Ġof Ġ ` story ` : Ċ Ġ Ġ Ġ Ġ Ġassert _ se lect Ġ' div # content Ġdiv . story ', Ġcount : Ġ1 Ċ ĠAnd Ġthat , Ġde ar Ġreader , Ġis Ġthe Ġlast Ġtest ĠI 'll Ġmake Ġyou Ġwrite ! ĠWell , Ġfo\n",
      "\t Unique Tokens no: {'a-', 'work', 'es.Ċ', 'goodĠatĠ', 'profileĠ', 'youĠareĠ', 'ing,Ġ', 'edĠbyĠtheĠ', 's:Ġ', 'yourselfĠ', 'ĠtoĠtheĠnextĠ', 'debugg', 'ingĠyourĠ', 'con', 'withĠaĠ', '),Ġ', 'option', 'ailsĠ', '_Ġ', '#Ġ', 'ĊĠ', \"you'reĠ\", 'makeĠ', 'againĠ', 's.ĠTh', 's.ĠItĠ', 'low', 'Wel', 'listĠ', 'Ġtool', 'suiteĠ', 'sub', 'InĠthisĠ', 'itsĠ', 'funĠ', 'pac', 'focusĠ', 'applicationsĠ', 'ingĠourĠ', 'Finish', 'pageĠ', 'hapterĠ', 'anyĠ', 'seeĠ', 'beĠ', '.ĠTheĠ', 'ench', 'may', 'ĠthatĠ', 'numberĠofĠ', 'willĠ', 'provid', 'notĠ', 'lessonĠ', 'howeverĠ', 'help', 'nobodyĠ', 'view', 'De', 'isĠthatĠ', 'lik', '7Ġ', 'useĠofĠtheĠ', 'your', 'ainedĠ', 'webĠ', 'theĠt', 'orĠ', 'inĠandĠ', '0Ġ', 'edĠ', 'asĠ', 'ed,Ġ', 'ofĠcourse', 'haveĠ', 'eur', 'expectedĠ', 'developmentĠ', 'select', 'comprehensiveĠ', 'learnĠ', 'bugg', 'configur', 'iseĠ', 'assert', 'responsibleĠ', 'passed', '.ĊĠ', 'ateĠtheĠ', 'seedĠ', 'es', 's,ĠandĠ', 'assureĠ', 'pre', '509', 'fix', 'bug', '45Ġ', 'howĠtheĠ', 'codeĠ', 'flaw', 'resultsĠinĠ', 'yet', 'fail', 'er,Ġ', 'display', 'ĊĠĠĠĠĠ', 'viaĠ', 'IfĠ', 'wholeĠ', 'ĠthisĠisĠtheĠ', \"let'sĠ\", 'regardlessĠofĠ', 'selectĠ', 'RunĠ', 'extendĠ', 'aĠc', 'ourselv', 'soĠthatĠ', 'developingĠ', 'storyĠ', 'youĠcanĠ', 'lookedĠatĠ', 'Well,Ġ', 'edĠtoĠtheĠ', 'onĠyourĠ', ',ĠsoĠ', 'errorĠmess', 'classĠofĠ', '11', 'probablyĠtheĠ', 'ĠtestsĠ', 'alwaysĠ', 'it!', 'againstĠtheĠ', 'job', 'aĠsingleĠ', 'finishedĠ', ',Ġ', 'double-', 'forĠ', 'cert', 'exception', 'down', 'contentĠ', 'ChapterĠ', 'comeĠtoĠ', 'ic', 'comm', 'rail', 'containsĠtheĠ', 'isĠ', 'ĠtrulyĠ', 'ToĠ', '9.', 'op', 'edĠtheĠ', 'atoryĠ', 'ourĠ', \".ĠIt'sĠ\", 's/', 'WhenĠ', \"'\", 'forĠs', 'very', 'apply', 'bott', 'Ġthat,Ġ', 'sĠandĠ', 'SpringĠ', 'Application', 'B', 'develop', 'ingĠthingsĠ', 'iallyĠ', 'erĠinĠ', 'ures,Ġ', 'youĠmightĠ', 'mayĠ', 'ofĠtheĠ', 'withoutĠ', 'whereĠ', 'YourĠ', '.ĠAndĠ', 'Run', 'ning', 'like', 'chapter', 'cod', 'ĠtoĠ', 'ĠcallĠ', 'Ġtop', '$Ġ', 'eĠtimesĠ', '!Ġ', 'richĠ', '0.8', 'per', 'guide', 'aboutĠ', 'betterĠ', 'ĠĠĠĠĠ', 'sĠtoĠ', 'severalĠ', 'forĠanyĠ', 'new', 'stuck', 'Test', 'need', 'read', 'areĠ', 'back', \"'sĠ\", 'readĠ', 'weĠ', 'ientĠ', 'processĠ', 'code', 'itĠtoĠ', 'sĠthatĠtheĠ', 'loc', 'ingĠtheĠ', '`Ġ', 'andĠtheĠ', 'wrong', 'speed', 'jumpĠ', 'allĠtheĠtim', 'missionĠ', 'ones', 'dearĠ', 'functionalityĠ', 'migr', 'allĠofĠtheseĠ', 'resultingĠ', 'buildingĠaĠ', 'lineĠofĠ', 'integration', 'yĠandĠ', 'stillĠhaveĠ', 'aĠ', '56', 'ĠworkĠ', 'pretend', 'willĠbeĠtheĠ', 'checkĠ', 'eĠthroughĠ', 'application,Ġ', ':Ġ', 'ksĠ', 'agesĠ', '3Ġ', 'gratul', 'icsĠ', 'yĠ', 'eĠt', 'r', 'asĠwellĠasĠ', 'yourĠcodeĠ', 'OnceĠ', 'Our', 'environmentĠ', 'Su', 'deĠ', 'necessaryĠ', 'ifĠyouĠ', 'p', 're', 'about', 'rig', 'agĠ', 'errorsĠ', 'toĠ', 'issu', 'passĠ', 'addĠ', 'install', 'ag', 'runĠtheĠ', \"doesn'tĠ\", 'codeĠtoĠ', 'ing,ĠandĠ', 'ake', 'OfĠcourse', 'pluginĠ', 'usingĠ', ',Ġthough', 'RunningĠ', 'ics,Ġ', 'andĠ', 'ifĠthereĠareĠ', ',ĠthereĠ', 'youĠcouldĠ', 'ingĠ', 'emplateĠ', 'ations,Ġ', 'yourĠ', 'bugsĠ', 'count', 'get', 'sĠonĠ', '60', 'elementsĠ', \"'dĠ\", 's,Ġ', '.ĠIf', ':Ġ1', 'And', 'goneĠ', 'onlyĠ', 'whenĠyou', 'iteĠ', 'applicationĠ', 'isĠtheĠ', 'separatedĠ', 'hadĠtoĠ', '(', 'ĠtechnicalĠ', \"you'dĠ\", 'invent', '.ĊĠĠĠĠĠ', 'plugin', 'Ġcon', 'confirm', 'isĠnotĠ', '.ĠAgain', 'dis', '.ĠTh', 'exploreĠ', 'bookĠ', 'remainingĠ', 'asser', 'es.ĠTheĠ', 'expand', 'beginĠ', 'production', 's-', 'youĠ', 'ĠtoĠtalkĠ', 'asĠaĠ', 'forĠthisĠ', '-freeĠ', 'prof', 'inĠtheĠ', 'usingĠanĠ', 'rite', 'youĠw', 'Runn', 'ĠtoĠfindĠ', 'error', 'perfect', 'esĠthatĠ', 'hand', ',ĠtheĠ', 'form', 's.Ċ', 'anyway', \"you'llĠ\", 'data', 'way', '.ĠAfterĠ', 'hardĠ', 'wheel', 'handĠtoĠ', '.ĠWhen', 'runningĠ', 'giveĠ', 'run', 'howĠ', 'R', 'plug', 'allĠofĠ', 'onĠtheĠ', 'edĠinĠ', 'application', 'own', '4', 'learn', ',ĠtheĠfirstĠ', ',ĠweĠ', 'beĠthatĠ', 'last', 'archiveĠ', '##Ġ', 'e,Ġ', \"I'llĠ\", 'ĠtestĠ', 'ĠtheĠt', 'ahead', \"',Ġ\", 'Summary', 'TestĠ', 'book', 'devot', 'soleĠ', \"we'llĠ\", 'ĠtoĠtakeĠaĠ', 'tion', 'aĠb', 'atĠ', 'ofĠt', 'existingĠ', '--', 'stopĠ', 'eas', 'ip', 'doneĠ', '................', 'case,Ġ', '1.0', '0'}\n",
      "\t Unique Tokens wiki: {'Ġstuck', '05', 'Ġoptions', 'Ġelements', 'Ġgive', 'ure', 'ĠThe', 'Ġfun', 'Ġdevelopment', 'Ġstill', 'Ġ11', 'Ġwith', 'par', 'Ġat', 'the', 'ĠTo', 'ations', 'Ġstop', ',', 'a', 'Ġnobody', 'Ġtools', 'Ġwheel', 'tained', 'ĠOnce', 'Ġbook', 'laws', 'Ġresulting', 'Ġweb', 'Ġseveral', \"'t\", 'Ġlesson', 'Ġhard', 'Ġmay', 'Ċ', 'ĠApplic', 'Ġfirst', 'ĠSuite', 'ourself', 'is', 'Ġlooked', 'ĠSummary', '3', 'Ġdone', 'Ġrails', 'Ġ45', 'Ġdoesn', 'Ġcould', 'Ġdata', 'Ġbe', 'Ġif', 'let', 'Ġexplore', 'ay', 'Ġconfig', 'Ġwithout', 'Ġwhole', 'Ġon', 'Ġanyway', 'we', 'Ġadd', 'Ġass', 'Ġconsole', 'Ġpage', 'aces', 'ĠAfter', 'ĠYour', 'Ġyet', 'ige', 'Ġwrite', 'ĠRunning', 'Ġthrough', 'Ġbott', 'Ġwork', 'ert', 'content', 'ass', 'Ġfailures', 'Ġlast', 'Ġ0', 'ient', 'Ġbetter', 'Ġwhen', 'Ġresponsible', 'Ġits', 'ĠAgain', 'Ġregardless', 'Ġall', \"'s\", 'Ġbit', 'Ġpretend', 'Ġare', 'Ġto', 'Ġhad', 'ĠWell', 'Ġtag', 'Ġfunctionality', 'Ġy', 'Ġtime', 'Ġours', 'Ġagainst', 'Ġyour', 'Ġback', 'Ġcode', 'Ġprofile', 'Ġway', 'Ġtruly', 'Ġmessages', 'Ġfinished', 'Ġbuilding', 'Ġprovided', 'Ġhow', 'Ġwell', 'Ġabout', 'Ġcomm', 'Ġthese', 'Ġ207', 'lect', 'Ġbegin', 'Ġlearn', 'Ġthough', 'Ġthe', 'Ġ(', 'edy', 'Ġlike', 'Ġexpanded', 'Ġcase', 'Ġcount', 'Ġlocate', \"',\", 'Ġwrong', 'Ġtechnical', 'Ġeasy', 'Ġnecessary', 'Ġour', 'Ġhelp', 'Ġpat', 'ĠDeb', 'ĠFin', 'Ġvery', 'Ġconf', 'Ġthings', 'Ġrich', 'Ġerror', 'Ġslow', 'Ġline', 'Ġhand', 'ĠIf', 'se', 'Ġvia', 'ĠChapter', 'Ġan', 'Ġapplications', 'Ġfind', 'Ġdism', 'Ġspe', '),', 'Ġrun', 'ĠWhen', 'Ġjob', 'Ġso', 'Ġgood', 'ĠSpring', 'Ġresults', 'ĠRun', 'Ġ1', 'Ġnew', 'uring', 'ks', 'Ġcomprehensive', 'Ġreader', 'Ġ#', 'probably', 'ĠThese', 'Ġpre', 'Ġchapters', 'Ġ$', 'Ġapplication', 'ions', 'Ġplug', 'Ġand', 'Ġa', 'elves', 'Ġassert', 'ĠWel', 'plate', 'ation', 'Ġas', 'Ġyou', 'Ġcourse', 'Ġsubmission', 'vent', 'Ġnumber', 'Ġguide', 'Ġexisting', 'ĠIt', 'ĠOur', 'ĠBench', 'Ġb', 'Ġp', 'Ġuse', 'Ġhands', 'Ġ49', 'free', 'Ġfix', 'Ġtake', 'Ġfocus', 'Ġremaining', 'ĠTest', 'Ġwill', 'Ġsee', 'Ġmigr', 'Ġahead', 'Ġthat', 'ially', 'Ġproduction', 'Ġsuite', 'Ġor', 'irms', 'Ġcall', 'Ġagain', 'Ġissues', 'Ġdeveloper', 'Ġget', 'Ġper', 'come', 'Ġ--', 'Ġprofic', 'Ġtopics', 'Ġthere', '/', 'Ġin', 'Ġsingle', 'Ġjump', 'Ġprocess', 'Ġmake', 'Ġr', \"'re\", \"'d\", 'ĠAnd', 'ur', 'Ġtimes', 'Ġ60', 'Ġ73', 'Ġtalk', 'Ġhave', 'Ġit', '50', 'Ġrein', 'Ġarise', 'ĠIn', 'Ġhowever', 'Ġviews', 'Ġpassed', 'Ġfor', 'Ġany', 'ra', 'ugs', \"'ll\", 'Ġthis', 'ated', 'Ġtem', 'ugging', 'Ġclass', 'Ġperfect', 'Ġdouble', 'Ġstory', 'ion', 'Ġcongrat', '8', 'Ġinstall', 'Ġwhere', 'Ġdevoted', \"Ġ'\", 'Ġread', 'Ġruns', 'Ġones', 'Ġerrors', 'ke', 'Ġdeveloping', 'Ġneed', 'Ġis', 'Ġenvironment', 'Ġpass', 'Ġown', 'Ġlist', 'Ġintegration', 'Ġcan', 'de', 'Ġdisplay', 'Ġmight', 'Ġdiv', 'ĠOf', 'Ġde', 'Ġcontains', 'ulatory', 'ips', 'check', 'Ġapplying', 'Ġtests', 'Ġdown', 'Ġchapter', 'Ġworked', '....', 'Ġnext', 'Ġdeb', 'Ġ...', 'Ġf', 'Ġdisplayed', 'Ġform', 'Ġextend', 'Ġby', 'Ġexceptions', 'Ġ90', '-', 'Ġgone', 'Ġnot', 'Ġonly', 'cer', 'Ġrunning', 'Ġarchive', 'Ġexpected', 'ĠRail', 'in', 'ed', 'Ġof', 'ished', 'Ġalways', 'Ġlikes', 'ĠI', 'Ġusing'}\n",
      "Text 2: Hello,\n",
      "\n",
      "My comment was aimed to the fact of removing shared BP.\n",
      "\n",
      "If we have single BP per application then it's SMACK label is fixed and BP\n",
      "do not need to handle anything by itself.\n",
      "Privileges to files are enforced by platform.\n",
      "\n",
      "In actual situation, we have shared browser process. BP is needed to be\n",
      "\t Tokenized no:Hello,ĊĊ MyĠ commentĠ wasĠ aim edĠtoĠtheĠ fac tĠofĠ removingĠ sharedĠ BP .ĊĊ If ĠweĠhaveĠ singleĠ B PĠ perĠ application ĠthenĠ it'sĠ SM ACKĠ labelĠ isĠ fix edĠandĠ BP Ċ doĠnotĠ needĠtoĠ handleĠ anythingĠ byĠ itself .Ċ Pri vi leg esĠtoĠ filesĠareĠ enforc edĠbyĠ platform .ĊĊInĠ actual Ġsitu ation,Ġ weĠhaveĠ sharedĠ browserĠ process.Ġ B PĠ isĠ neededĠtoĠ be\n",
      "\t Tokenized wiki:H ello , Ċ Ċ My Ġcomment Ġwas Ġaimed Ġto Ġthe Ġfact Ġof Ġremoving Ġshared ĠBP . Ċ Ċ If Ġwe Ġhave Ġsingle ĠBP Ġper Ġapplication Ġthen Ġit 's ĠSM AC K Ġlabel Ġis Ġfixed Ġand ĠBP Ċ do Ġnot Ġneed Ġto Ġhandle Ġanything Ġby Ġitself . Ċ Pr iv ile ges Ġto Ġfiles Ġare Ġenforced Ġby Ġplatform . Ċ Ċ In Ġactual Ġsituation , Ġwe Ġhave Ġshared Ġbrowser Ġprocess . ĠBP Ġis Ġneeded Ġto Ġbe\n",
      "\t Unique Tokens no: {'securityĠ', '.ĊĊRegards,Ċ', 'request', \"it'sĠ\", 'wo', 'singleĠ', 'foreĠ', 'ĠweĠhaveĠ', 'perĠ', 'ation,Ġ', 'model', 'actual', 'fix', 'edĠbyĠ', 'needĠtoĠ', 'labelĠ', 'parallel', 'hadĠ', 'Ġsitu', 'sharedĠ', 'itself', 'removingĠ', 'Hello,ĊĊ', '(whichĠ', 'alĠc', 'leg', 'useĠ', 'appĠ', 'process.Ġ', 'just', 'processĠ', 'en', 'anythingĠ', 'itĠtoĠ', 'ĠthoseĠt', 'filesĠareĠ', 'lim', 'tĠofĠ', 'SM', 'Pri', 'soĠthatĠ', 'ACKĠ', 'ĠthenĠ', 'edĠandĠ', '.Ċ', 'platform', 'justĠoneĠ', 'edĠtoĠtheĠ', '.ĊĊInĠ', 'of', 'byĠ', 'esĠtoĠ', 'browserĠ', 'handleĠ', 'commentĠ', 'awareĠofĠ', \"it'sĠnotĠ\", 'addition', 'aim', 'whichĠ', 'neededĠtoĠ', 'RP', 'isĠ', 'beĊ', 'application', 'BP', 'weĠhaveĠ', 'sĠinĠ', '.ĊĊ', 'fileĠ', 'fac', 'ĠthoseĠtwoĠ', 'easyĠtoĠ', 'issu', ')Ġ', 'framework', 'wasĠ', 'odeĠ', 'ĠthinkĠ', 'enforc', 'PĠ', 'maintain', '>ĊĊ', 'edĠ', 'forceĠ', 'MyĠ', 'B', 'usingĠ', 'doĠnotĠ', '.ĊĊIĠ', 'vi'}\n",
      "\t Unique Tokens wiki: {'Ġmodels', 'Ġapplication', 'so', 'which', 'Ġadditional', 'Ġrequest', 'ello', 'Ġfiles', ')', 'Ġthink', 'Ġand', 'Ġenforce', 'My', 'Ġitself', 'ges', 'Ġshared', 'Ġthen', 'Ġissued', 'Ġenforced', \"'s\", 'Ġare', 'Ġto', 'Ġhad', 'AC', 'Ġuse', 'Ġcode', ',', 'ĠR', 'aware', 'Ġmaintain', 'Ġframework', '.', 'Ġneed', 'fore', 'Ġis', 'Ġwas', 'In', 'Ġjust', 'do', 'Ġthe', 'Ġ(', 'Ġthat', 'H', 'I', 'Ġwhich', 'Ġremoving', 'Ġeasy', '>', 'Ġsecurity', 'Ġbrowser', 'Ġbe', 'Ġsituation', 'iv', 'Ġper', 'Ġapp', 'ĠBP', 'Ġfile', 'Ġaimed', 'Ġparallel', 'Ġactual', 'Ġin', 'Ġone', 'Ġthose', 'Pr', 'Ġlimit', 'Reg', 'Ġsingle', 'Ġby', 'ĠSM', 'Ġprocess', 'Ġtwo', 'Ġnot', 'Ġfact', 'Ġwe', 'Ġneeded', 'Ġcomment', 'Ġhandle', 'Ġof', 'K', 'Ġplatform', 'Ġhave', 'Ġit', 'Ġanything', 'ile', 'Ġlabel', 'Ġfixed', 'Ġusing', 'ards'}\n",
      "==not Same Author==\n",
      "Text 1: burden it was.\n",
      " Yes, there had indeed been an \"incident,\" but nothing to do with gas leaks or an Airbnb, although <PERSON> still didn't know what the actual nature of it was. <PERSON> had finally resurfaced on the phone just after <PERSON> had picked the girls up at the airport. He hadn't sounded li\n",
      "\t Tokenized no:burdenĠ itĠwas .ĊĠ Yes ,ĠthereĠ hadĠ indeedĠ beenĠ anĠ\" incident ,\"Ġ butĠ nothingĠtoĠdoĠwithĠ gasĠ leak sĠorĠ anĠ Air b nb ,ĠalthoughĠ < P ERS ON >Ġ stillĠ didn'tĠ know ĠwhatĠtheĠ actualĠ natureĠofĠ itĠwas .Ġ < P ERS ON >Ġ hadĠ finallyĠ resur fac edĠonĠtheĠ phoneĠ justĠ afterĠ < P ERS ON >Ġ hadĠ pick edĠtheĠ girl sĠupĠ atĠtheĠ airport .ĠHeĠ hadn'tĠ soundedĠ li\n",
      "\t Tokenized wiki:bur den Ġit Ġwas . Ċ ĠYes , Ġthere Ġhad Ġindeed Ġbeen Ġan Ġ\" inc ident ,\" Ġbut Ġnothing Ġto Ġdo Ġwith Ġgas Ġle aks Ġor Ġan ĠAir bn b , Ġalthough Ġ< P ERS ON > Ġstill Ġdidn 't Ġknow Ġwhat Ġthe Ġactual Ġnature Ġof Ġit Ġwas . Ġ< P ERS ON > Ġhad Ġfinally Ġresur faced Ġon Ġthe Ġphone Ġjust Ġafter Ġ< P ERS ON > Ġhad Ġpicked Ġthe Ġgirls Ġup Ġat Ġthe Ġairport . ĠHe Ġhad n 't Ġsounded Ġl i\n",
      "\t Unique Tokens no: {'Pal', 'stillĠ', 'es.Ċ', '.ĠThereĠwasĠ', 'smallerĠ', 'soundedĠ', 'edĠbyĠtheĠ', 'eĠtheĠ', 'ĠandĠheĠ', 'smileĠ', 'panic', 'ishĠ', 'wher', 'ĠsaidĠ', 'ShouldĠ', 'usedĠtoĠ', 'edĠitĠ', 'likeĠ', 'aĠt', 'dragg', 'hadĠbeenĠ', 'payingĠ', 'easy', 'lost', 'ĊĠ', ',\"ĠsheĠ', 'behindĠ', '.\"ĠHeĠ', ',ĠasĠ', 'ival', 'ri', 'ings,Ġ', 'somethingĠ', 'Ġm', \"eĠthey'reĠ\", 'Ġtheat', 'usĠ', 'blind', 'edĠthereĠ', 'sĠtoĠtheĠ', 'bl', 'sĠinĠtheĠ', 'silenceĠ', 'feltĠ', 'daughter', 'phoneĠ', 'ir', 'is', 'airport', 'incident', 'heĠwas', 'will', 'Yes', '.ĠTheĠ', 'leak', 'fallenĠ', 'ĠwasĠ', 'lessĠofĠ', 'said', 'ĠthatĠ', 'alĠ', 'ĠwhatĠtheĠ', 'said.Ġ\"', 'kissesĠ', 'ay', 'aĠh', 'handsĠ', 'AnotherĠ', 'atĠtheĠ', 'ickĠ', 'caughtĠ', 'goingĠ', 'rear', 'backĠofĠtheĠ', 'feelingĠofĠ', 'edĠ', 'ousĠ', 'inĠ', 'moreĠofĠaĠ', 'etĠal', 'phone', '.ĠOrĠ', 'medi', 'picture', 'automaticallyĠ', 'sâĢĶ', 'esĠ', 'lad', 'onĠs', 'dar', 'dark', '.ĊĠ', 'ĠwithĠ', 'wereĠtheĠ', 'Ġtime,Ġ', 'WhatĠ', 'sĠupĠ', 'erĠofĠtheĠ', 'ĠtheirĠ', 'jeal', 'seeĠtheĠ', 'Pol', 'sĠorĠ', 'id', 'ianĠ', 'yĠofĠ', ',ĠalthoughĠ', 'eĠtoĠbeĠ', 'dad', 'pumpĠ', 'actualĠ', 'ĠtryingĠtoĠ', 'eĠtoĠ', 'youĠtoĠ', 'lookingĠ', 'noise', 'usualĠ', 'me', 'outĠofĠ', 'newĠ', 'lif', '.ĠHisĠ', 'JustĠ', 'etimeĠ', 'wasĠtheĠ', 'full', 'known', 'self', 'AĠ', '.ĠHeĠ', '.ĠItĠwasĠ', 'allerĠ', 'neverĠ', 'ownershipĠ', '.Ġ\"', 'ian', 'Well,Ġ', \"hadn'tĠ\", 'ifĠ', ',\"ĠtheĠ', 'ago', 'heel', 'byĠ', 's.ĠHeĠ', 'myĠ', 'suspect', 'phones', 'onto', 'ĠtoĠtheĠ', 'nothingĠtoĠdoĠwithĠ', 'allĠ', 'seĠtoĠ', ',Ġ', 'burdenĠ', 't', 'forĠ', '>,Ġ', 'Dun', 'ac', 'heĠwasĠ', 'iou', 'GetĠ', 'edĠtheĠ', '.ĠC', 'butĠ', 'ĊĠ\"', '.Ġ', 'whateverĠ', 'ling', 'aĠs', 'concert', 'saw', 'ous', 'keepĠ', 'wasĠ', 'ĠthinkĠ', 'althoughĠ', 'Ġthat,Ġ', 'pas', 'spott', 'aĠsmileĠ', 'hisĠ', 'ol', '.ĠMr', 'and', 'closeĠ', 'Noth', 'notĠtheĠ', 'loom', 'rightĠ', 'sheĠ', 'oc', 'not', '.ĠItĠ', 'elĠ', 'mustĠ', 'aĠmom', 'ch', 'could', 'becauseĠ', 'Ġcha', 'age,Ġ', 'ĠtoĠ', 'lovelyĠ', 'rev', 'ingĠherĠ', 'whiteĠ', '?Ġ', 'eachĠotherĠ', 'st', 'hisĠt', \"Can'tĠ\", 'followedĠ', 'hadĠ', 'beenĠ', 'NiceĠ', 'WhyĠ', 'carĠandĠ', \"'sĠ\", 'ly,Ġ', 'bedĠ', '.ĠTheyĠ', 'justĠ', 'over', 'someoneĠ', 'andĠtheĠ', 'ask', 'couldĠhaveĠ', 'seas', '>Ġ', 'pr', 'ent,Ġ', 'natureĠofĠ', 'aĠf', 'sid', 'dat', 'gasĠ', 'antĠ', '.Ġ(', 'aĠ', 'e.Ġ', 'Sil', 'haveĠbeenĠ', 'FromĠ', 'pick', 'sightĠ', 'saying', 'ing_', 'ratherĠthanĠ', 'no', 'ar', 'ingĠthem', 'apparent', ',\"Ġ', 'rust', 'you', 'airĠ', 'eĠturn', 'here.', 'inĠfrontĠofĠtheĠ', 'r', 'eight', 'inglyĠ', 'WasĠ', 'caseĠtheĠ', 'here,Ġ', 'moment', 'rangeĠofĠ', 'attention', 'about', 'withĠtheseĠ', \"I'm\", 'arilyĠ', 'incidentĠ', 'know', \"he'dĠ\", 'pl', 'ag', 'NotĠ', 'ur', 'ake', 'verĠ', 'deathĠ', 'lossĠofĠ', 'anĠ\"', 'bleĠ', 'andĠ', 'Air', 'driv', 'untilĠsheĠ', ',ĠthereĠ', 'sĠwereĠ', 'ingĠ', 'withĠ', 'ingĠforĠ', \"didn'tĠ\", 'home.Ġ', 'worryĠ', 'yourĠ', 'girlsĠ', 'br', 'edĠonĠtheĠ', 'an', 'sayingĠ', 'Hello,Ġ', 'passion', 'He', 'saidĠtoĠ', 'asleep', \"'dĠ\", 'goingĠtoĠ', 's,Ġ', 'ingĠthemĠ', 'at', 'endĠupĠ', 'summer', 'ĠtoĠtheirĠ', 'heĠ', 'glancedĠ', 'swift', 'ight', 'greet', 'sĠofĠ', 'oneĠ', 'listen', 'been', 'depth', 'lyĠ', 'um', 'nb', 'should', 'iesĠthatĠ', 'al', '-r', 'TheĠ', 'thought', 'stretch', 'ĠthemĠ', 'finallyĠ', 'ofĠ', 's.Ġ', 'ong', 'inĠtheĠ', 'fe', 'greatĠ', 'because,Ġ', 'icalĠ', 'magic', 'girl', \"I'veĠgotĠ\", 'un', '.ĠHeĠhadĠ', 'hand', 'ror', ',ĠtheĠ', 'how', 'knew', 'inĠtoĠtheĠ', 'Ch', 'herĠheadĠ', 'anĠ', 'Christ', 'anyway', 'untilĠtheyĠ', 'gar', 'evenĠ', 'indeedĠ', 'you,Ġ', 'non', 'furtherĠ', '.)Ġ', '?\"Ġ', 'afterĠ', 'callĠ', 'edĠhisĠ', 'kind', 'wer', 'smilingĠ', 'asĠtheyĠ', 'death', 'ed.Ġ', 'AnyoneĠ', 'onĠtheĠ', 'resur', 'whichĠwasĠ', 'irĠ', 'sedĠtheĠ', 'singĠ', 'fak', 'urnĠ', 'fac', 'e,Ġ', 'himself', 'grinĠ', 'whenĠheĠ', 'Bir', 'downĠ', \"It'sĠ\", 'aĠsign', 'uf', 'iesĠ', 'suspic', 'ouch', 'EverythingĠ', 'atĠ', 'itĠwas', 'w', 'doneĠ', 'bothĠ', 'looksĠ', 'itĠ', 'ed,'}\n",
      "\t Unique Tokens wiki: {'Ġhis', 'Ġthink', 'Ġstage', 'Ġfallen', 'ĠThe', 'sided', 'ad', 'Ġsilence', 'Ġch', 'ĠOr', 'Ġnon', 'Ġstill', 'Ġincident', 'Ġhere', 'Ġwith', 'y', 'Ġrange', 'Ġgl', 'Ġat', 'arily', 'Ġgreeted', 'Ġd', ',', 'Ġpassion', 'ocr', 'Ġnature', 'aks', 'Ġstretched', 'Ġthought', 'ry', 'Ġage', 'Ġago', 'Ġfurther', \"'t\", 'Ġownership', 'Ċ', 'Ġmore', 'Ġar', 'auff', 'Ġjust', 'ĠYes', 'Ġwere', \"'m\", 'Ġgar', 'Ġsmaller', 'Ġdone', 'H', 'ĠPolish', 'Ġswift', 'ĠMr', 'Ġcould', 'Ġbe', 'Ġalthough', 'Ġif', 'Ġout', 'ice', 'Ġdark', 'season', '.)', 'Ġon', 'Ġsaying', 'Ġlifetime', 'Ġtheatrical', 'Ġanyway', 'Ġsuspicious', 'Ġtone', 'Ġdeath', 'Ġme', 'Ġet', 'Ġwings', 'Ġlost', 'Ġturning', '?', 'Ġnoise', ',\"', 'Ġsaw', 'Ġkind', 'Ġairport', 'Ġphone', 'Ġgas', 'Ġgreat', 'Ġfake', 'Ġlooking', 'ant', 'Ġup', 'ĠCong', 'es', 'Ġhead', 'Ġwhen', 'Ġdidn', 'Ġpump', 'Ġusual', 'Ġlooks', 'Ġknown', 'Ġdragged', 'Ġfollowed', 'thing', 'Ġall', 'Ġworried', 'chal', \"'s\", 'Ġto', 'Ġhad', 'Ġtime', 'Ġyour', 'Ġback', 'Ġcar', 'Ġal', 'Ġattention', 'Ġabout', 'Ġdr', 'Ġthese', 'Ġunwilling', 'umb', 'Ġmirror', 'Ġwas', 'Ġindeed', 'ĠAny', 'Ġthe', 'el', 'Ġpaying', 'Ġ(', 'Ġlike', 'Ġother', 'Ġturn', 'Ġcase', 'Ġeasy', 'Ġdo', 'Ġphones', 'Ġless', 'Every', 'Ġthan', 'Ġfront', 'Ġapparently', 'Can', 'inc', 'Ġactual', 'den', 'Ġsake', 'Ġcaught', 'Ġhand', 'Ġsick', 'se', 'Ġfinally', 'Ġan', 'Ġgoing', 'Ġpron', 'Ġshe', 'sing', 'Ġgot', 'Ġshoulder', 'N', 'Ġfelt', 'Ġmed', 'Ġconcert', 'Ġdriving', 'Ġmy', 'Ġnew', 'Ġafter', 'Ġtouching', 'uring', 'Ġnever', 'Ġdate', 'usted', 'Ġhimself', 'Ġsomething', 'Ġsignal', 'ches', 'which', 'Ġknow', 'Not', 'Ġsight', 'Ġand', 'Ġa', 'faced', 'Ġspot', 'Ġas', 'Ġyou', 'Ġwhatever', 'Ġmoment', 'Ġbeen', 'Ġwhite', 'Ġsm', 'ĠIt', 'Ġb', 'Ġjealous', 'Ġboth', 'Ġhands', 'Ġair', 'Ġself', 'Ġpicture', 'ĠPalace', 'What', 'Ġgirls', 'Ġresur', 'Ġsee', 'Ġthat', 'er', 'Ġor', 'Ġstir', 'Ġsuspected', 'Ġcall', 'bur', 'Ġright', \"'ve\", 'iling', 'ĠAnother', 'Ġthey', 'ĠChan', 'going', 'ies', 'ting', 'Ġwhat', 'Ġin', 'Ġkiss', 'Ġeven', 'Ġshow', 'anced', 'Ġover', 'Ġr', \"'re\", \"'d\", 'els', 'ĠHis', 'ĠBir', 'Ġend', 'Ġhave', 'Ġit', 'oming', 'Ġbecause', 'ile', 'i', 'Ġpassed', 'Ġfor', 'Ġfeeling', 'Ġpl', 'Ġpanic', 'Ġdepths', 'ident', 'Ġbut', 'ello', 'Ġlov', 'Ġgr', 'Ġ<', 'Ġfull', 'A', 'ly', 'Ġdaughters', 'Ġtrying', 'Ġhe', 'Ġautomatically', 'Ġ', 'Ġblind', 'ely', 'Ġasked', 'ĠNot', 'Ġholiday', 'ician', 'Ġwhere', 'Ġbag', 'Well', 'unn', 'Just', 'Ġsounded', 'ĠChrist', 'arl', '.', 'Why', 'ble', 'Ġeach', 'D', 'Ġused', 'Ġmight', 'Ġnothing', 'Ġle', 'othing', 'Ġbehind', 'Ġuntil', 'land', 'e', 'Get', 'iv', 'Ġloss', 'rict', 'Ġknew', 'Ġmust', 'n', 'list', 'Ġdown', 'Ġher', 'leep', 'Ġpicked', 'Ġone', 'Ġtheir', 'ĠFrom', 'ĠShould', 'Ġtall', 'Ġsomeone', 'Ġrevival', 'Ġby', 'Ġrather', 'Ġladies', 'Ġnot', 'Ġsummer', 'ening', 'to', 'ĠThere', 'Ġclose', 'Ġbring', 'in', 'Ġkeep', 'ed', 'ĠSilver', 'It', 'o', 'Ġof', 'Ġrear', 'ĠHe', 'ĠThey', 'bn', 'Ġhome', 'Ġeight', 'ĠWas', 'ĠI', 'Ġmag', 'ĠAir', 'Ġlo'}\n",
      "Text 2: @White Buffalo ~ As a landscaper for twenty years I've watched both the flora and fauna on the move up here in the northern Adirondacks. Intrusions of so called invasive species has been doubling every few years. My opinion on this is that it's somewhat like the finger in the leaky dam, trying to ho\n",
      "\t Tokenized no:@ WhiteĠ Buff al oĠ ~Ġ AsĠ aĠl and scap erĠ for ĠtwentyĠ yearsĠ I'veĠ watchedĠ bothĠtheĠ fl or aĠandĠ fa unaĠ onĠtheĠ moveĠ upĠ hereĠinĠtheĠ northernĠ Ad ir ond ack s.ĠIn tr usion sĠofĠ soĠ calledĠ invas iveĠ speciesĠ hasĠbeenĠ dou blingĠ everyĠ fewĠ years.Ġ MyĠ opinionĠ onĠthisĠ isĠthatĠ it'sĠ somewhatĠ likeĠtheĠ fing erĠinĠtheĠ leak yĠ dam ,ĠtryingĠtoĠ ho\n",
      "\t Tokenized wiki:@ White ĠBuffalo Ġ~ ĠAs Ġa Ġlandsc aper Ġfor Ġtwenty Ġyears ĠI 've Ġwatched Ġboth Ġthe Ġflora Ġand Ġfauna Ġon Ġthe Ġmove Ġup Ġhere Ġin Ġthe Ġnorthern ĠAd ir ond acks . ĠInt rus ions Ġof Ġso Ġcalled Ġinvasive Ġspecies Ġhas Ġbeen Ġdoubling Ġevery Ġfew Ġyears . ĠMy Ġopinion Ġon Ġthis Ġis Ġthat Ġit 's Ġsomewhat Ġlike Ġthe Ġfinger Ġin Ġthe Ġleak y Ġdam , Ġtrying Ġto Ġho\n",
      "\t Unique Tokens no: {'al', 'for', 'arĠ', 'likeĠtheĠ', \"it'sĠ\", 'AsĠ', 'bothĠtheĠ', 'iveĠ', 'or', 'becomingĠ', 'Buff', 'fl', 'tr', '.ĠTheĠ', 'orĠ', 'leak', 'watchedĠ', 'dam', 'yearsĠ', 'W', 'C', 'ableĠ', 'fing', 'somethingĠ', 'onĠthisĠ', 'MyĠ', 'ĠtwentyĠ', 'yĠ', 'somewhatĠ', 'upĠ', 'blingĠ', 'occasion', 'hasĠbeenĠ', 'everyĠ', 'invas', 'and', 'hereĠinĠtheĠ', 'sĠofĠ', 'usion', 'ack', 'scap', 'northernĠ', 'aĠandĠ', 'olfĠ', 'occasionalĠ', 'opinionĠ', 'holdĠ', \"I'veĠ\", 'aĠl', 'soĠ', 'calledĠ', ',ĠtryingĠtoĠ', '~Ġ', 'erĠinĠtheĠ', 'WhiteĠ', 'Ad', 'fewĠ', 'al.', 'lessĠ', 'areĠ', 'onĠtheĠ', 'unaĠ', 'dou', 'inevit', 'oĠ', 'isĠthatĠ', 'back', 'erĠ', 'fa', 'moveĠ', 's.ĠIn', 'speciesĠ', 'years.Ġ', 'oug'}\n",
      "\t Unique Tokens wiki: {'Ġhold', 'Ġoccasional', 'Ġleak', 'ĠMy', 'Ġboth', 'gar', 'Ġspecies', 'aper', 'Ġback', 'ĠWolf', 'Ġor', 'ions', 'ĠInt', 'Ġthis', ',', 'Ġopinion', 'rus', 'Ġand', 'Ġa', 'Ġup', 'Ġevery', 'Ġwatched', 'Ġnorthern', 'Ġmove', 'ĠThe', 'Ġless', 'Ġon', 'Ġso', \"'ve\", 'Ġdam', 'Ġinevitable', 'Ġyears', 'White', 'Ġflora', 'ĠCou', 'Ġof', '.', 'Ġlandsc', 'Ġis', 'Ġsomewhat', 'Ġbecoming', 'Ġbeen', 'Ġhere', 'Ġit', 'Ġfinger', 'ĠBuffalo', 'Ġtrying', 'y', 'Ġdoubling', 'Ġfauna', 'Ġinvasive', 'Ġ~', 'ĠAs', 'Ġin', 'ĠAd', 'Ġfew', 'Ġthe', 'Ġfor', \"'s\", 'acks', 'Ġare', 'Ġtwenty', 'ĠI', 'Ġcalled', 'Ġto', 'Ġhas', 'Ġthat', 'Ġlike', 'Ġsomething'}\n",
      "==not Same Author==\n",
      "Text 1: railway bridges close to the river. Around these bridges were some desolate areas that had once contained small industrial units, now mostly empty. It was an ideal place for an attack. The hunters each had a gun concealed in a shoulder holster, loaded with silver bullets. Though the Guild preferred \n",
      "\t Tokenized no:rail wayĠ bridg esĠ closeĠtoĠtheĠ river .ĠA round ĠtheseĠ bridg esĠwereĠ someĠ des ol ateĠ areas ĠthatĠhadĠ onceĠ containedĠ smallĠ industrialĠ unit s,Ġ now Ġm ostlyĠ empty .ĠItĠwasĠ anĠ idealĠ placeĠ forĠanĠ attack .ĠTheĠ hun tersĠ eachĠ hadĠaĠ gun Ġcon ce al edĠinĠaĠ shoulderĠ hol st er,Ġ loadedĠ withĠs il verĠ bullet s.ĠTh ough ĠtheĠ Gu ildĠ preferredĠ\n",
      "\t Tokenized wiki:rail way Ġbridges Ġclose Ġto Ġthe Ġriver . ĠAround Ġthese Ġbridges Ġwere Ġsome Ġdes olate Ġareas Ġthat Ġhad Ġonce Ġcontained Ġsmall Ġindustrial Ġunits , Ġnow Ġmostly Ġempty . ĠIt Ġwas Ġan Ġideal Ġplace Ġfor Ġan Ġattack . ĠThe Ġhunters Ġeach Ġhad Ġa Ġgun Ġconcealed Ġin Ġa Ġshoulder Ġhol ster , Ġloaded Ġwith Ġsilver Ġbullets . ĠThough Ġthe ĠGuild Ġpreferred Ġ\n",
      "\t Unique Tokens no: {'menĠ', 'rulingĠ', 'andĠheĠ', 'onĠt', 'NoĠ', 'dy', 'es,', 'athĠtheĠ', 'eĠtheĠ', 'fromĠherĠ', 'hol', 'lingĠ', 'sideĠ', 'orderĠ', 'cl', 'Ġs', 'edĠitĠ', 'withĠaĠ', 'mistak', 'fulĠ', 'fromĠ', 'fi', 's.ĠTh', 'againĠ', 'terĠ', 'gun', 'otherĠtwoĠ', 'ri', 'slipp', 'Ġm', 'off', 'complain', 'enĠ', 'recover', '.ĠA', 'olfĠ', 'bl', 'placeĠ', 'wh', '.ĠS', 'sĠfrom', 'makingĠ', 'runĠ', 'heĠwas', 'ing.Ġ', 'ingĠorĠ', 'foodĠ', 'her.ĠSheĠ', 'ĠthatĠhisĠ', 'brok', 'gaveĠ', 'eĠtheyĠwereĠ', '.ĠTheĠ', 'cent', 'ĠwasĠnotĠ', 'ostlyĠ', 'ĠwasĠ', 'lastĠ', 'esid', 'let', 'ĠthatĠ', 's.ĠTheĠ', 'redĠtoĠ', 'aĠh', 'ĠtookĠ', 'normallyĠ', 'gotĠ', 'steppedĠ', 'ĠthatĠtheyĠ', 'Ġtak', 'sen', 'lik', 'someĠ', 'attack', 'edĠinĠaĠ', 'e.ĠThisĠ', 'ce', 'ingĠandĠ', 'ateĠ', 'man', 'sheĠwasĠ', 'edĠ', 'smallĠ', 'lastĠnight', 'asĠ', 'vanish', 'systemĠandĠ', 'angĠ', 'inĠ', 'ed,Ġ', 'dread', 'fromĠtheĠ', 'river', 'ensesĠ', 'smash', 'soĠ', 'ĠtriedĠtoĠ', 'ump', 'wentĠ', 'wallĠ', 'esĠ', 'erĠ', 'sĠofĠtheĠ', '.ĊĠ', 'again', 'escap', 'noĠoneĠ', 'alt', 'mightĠbeĠ', 'pulledĠ', 'ĠtheirĠ', 'e.ĠTheyĠ', 'killĠ', 'augh', 'forĠanĠ', 'amountĠofĠ', 'wantingĠtoĠ', 'er,Ġ', 'abov', 'mouth', 'dead', 'LookĠ', 'eĠtoĠ', 'didĠ', 'atedĠ', 'bridgeĠ', 'isingĠ', 'possibility', 'clearĠ', 'polic', 'fellĠ', 'Gu', 'feet', 'hunt', 'whetherĠ', '.ĠHeĠ', 'backward', '.ĠItĠwasĠ', 'feetĠ', 'closeĠtoĠtheĠ', 'bornĠ', 'ĠtheirĠs', 'edĠtoĠtheĠ', 'eachĠ', 'byĠ', 'other', 'bloodyĠ', 'ough', 'felt', 'ĠtoĠtheĠ', 'decidedĠtoĠ', 'crack', ',ĠandĠ', ',Ġ', 'scram', 'bene', 'round', 'forĠ', 'down', 'forĠthem', 'iou', 'firstĠ', 'ĠĠ', 'edĠtheĠ', 'butĠ', 'speedĠ', '.Ġ', 'atis', 'ungĠ', 'kill', 'edĠfromĠtheĠ', 'saw', 'beneathĠ', 'wasĠ', 'en,Ġ', 'himĠ', 'B', 'disappearedĠ', 'hisĠ', 'istĠ', 'oneĠthatĠ', 'ol', 'leader', 'ness.Ġ', 'second,Ġ', 'empty', 'rightĠ', 'sheĠ', 'ofĠtheĠ', 'whereĠ', '.ĠItĠ', 'beforeĠ', 'elĠ', 'forward', 'forĠtheĠ', 'ĠthatĠhadĠ', 'Real', 'ingĠherĠ', 'her', 'ersĠ', 'st', 'girlĠ', 'wentĠtoĠ', 'killed', 'hadĠ', '-Ġ', 'offĠ', '.ĠBeforeĠ', 'herĠ', 'faceĠandĠ', 'stamp', 'orĠnot', 'anyoneĠ', 'closedĠ', 'edĠtoĠ', \"'sĠ\", 'youngĠ', 'began', 'olv', 'pursu', 'sĠthenĠ', 'andĠtheĠ', 'vom', 'flightĠ', '.ĠWhenĠ', 'immedi', 'ingĠtheir', 'neckĠ', '>Ġ', 'pr', 'd,Ġ', 'industrialĠ', 'edĠherĠ', 'il', 'overĠtoĠtheĠ', 'shoulderĠ', 'aĠ', 'e.Ġ', 'farĠtooĠ', 'des', 'kindĠ', 'mer', 'neg', 'tersĠ', 'sense', 'hisĠheadĠ', 'faceĠ', 'bodi', 'shadow', 'cornerĠ', 'wayĠ', 'scal', 'ItĠwasĠ', 'fromĠs', 'prefer', 'lead', 'whil', '.ĠWhenĠtheĠ', 'insid', 'her,Ġ', 'withĠs', 'leftĠ', '\"', 'ĠwithĠtheĠ', 'itingĠ', 'BloodĠ', \"he'dĠ\", 'bullet', 'onĠ', 'letĠhimĠ', 'humanĠ', '.ĠHerĠ', 'verĠ', 'andĠ', 'studi', 'arelyĠ', 'gasp', 'sĠwereĠ', 'remainedĠ', 's.ĠSheĠ', 'now', 'consc', 'fight', 'OneĠ', 'illĠtheĠ', 'es.Ġ', 'overĠ', 'readyĠ', 'second', 's,Ġ', 'goingĠtoĠ', 'quicklyĠ', 'letĠ', 'ĠthreeĠ', 'fireĠ', 'g', 'at', ',ĠhisĠ', 'ĠtoĠthem', 'rott', ',ĠandĠnotĠ', 'ground', 'importantĠtoĠ', 'bsĠ', 'familyĠ', 'loadedĠ', 'he', 's.ĠS', 'oneĠ', 'Ġcon', 'ĠwithoutĠ', 's,', 'chestĠ', 'wouldĠ', 'ĠtheseĠ', 'thirdĠ', 'inclin', 'quickĠ', 'hear', 'strain', 'leaderĠ', 'bringĠtheĠ', 'any', 'al', 'ingĠaĠ', 'aĠd', '<', '\"Ġ', 'SheĠ', 'asĠaĠ', ',ĠoneĠ', 'sp', 'elyĠtoĠ', 'ofĠ', 's.Ġ', 'inĠtheĠ', 'intoĠtheĠ', 'couldĠ', 'ke', 'onceĠ', 'memberĠ', 'edĠhimĠ', 'girl', 'erĠandĠ', 'ĠthereĠwasĠnoĠ', 'hatedĠ', 'pull', 'ĠtraceĠ', 'probablyĠ', 'un', 'areas', 's.Ċ', 'anĠ', 'ĠtooĠ', 'hap', 'squ', 'land', 'in,Ġ', '.ĠAtĠtheĠ', 'afterĠ', 'runningĠ', 'Ġshould', 'brokeĠ', 'wer', 'whichĠ', 'hadĠaĠ', 'eĠtheirĠ', 'threeĠ', 'fullĠ', 'asĠtheyĠ', 'fromĠhisĠ', 'allĠofĠ', 'Ġt', 'ontoĠhisĠ', 'onĠtheĠ', 'containedĠ', 'unit', 'prevent', 'f', 'hun', 'sedĠtheĠ', 'strongĠ', 'kick', 'hard', 'handĠ', 'e,Ġ', 'sprint', 'hunter', 'viol', 'idealĠ', 'bridg', 'ildĠ', 'hurt', '.ĠSheĠ', 'shot', 'jacket', 'downĠ', 'sl', 'esĠwereĠ', 'forceĠ', 'maybeĠ', 'ĠtheĠ', 'atĠ', 'ledĠ', ',ĠtheĠonlyĠ', 'archĠ', 'weak', 'almostĠ', 'comingĠtoĠ', ',ĠthenĠ', 'anxietyĠ'}\n",
      "\t Unique Tokens wiki: {'Ġmen', 'Ġhis', 'ated', 'Ġfire', 'Ġbut', 'Ġfrom', 'Ġlast', 'Ġdecided', 'adows', 'ied', 'Ġand', 'Ġa', 'olves', 'Ġkick', 'Ġclear', 'Ġquick', 'Ġkeen', 'Ġleft', 'Ġhead', 'Ġwould', 'Ġgave', 'Ġcontained', 'ĠThe', 'Ġanyone', 'Ġtoo', 'Ġas', 'ĠThough', 'ster', 'ful', 'Ġthird', 'Ġriver', 'Ġareas', 'Ġsmall', 'Ġwanting', 'Ġfull', 'ts', 'sters', 'Ġmostly', 'ram', 'ent', 'ping', 'Ġempty', 'Ġthen', 'ly', 'Ġspeed', 'Ġwith', 'Ġvan', 'Ġchest', 'Ġhunters', 'Ġstudied', 'Ġbroken', 'Ġby', 'Ġhe', 'Ġconcealed', 'Ġall', 'Ġhalt', 'Ġ', 'read', 'Ġsm', 'Ġ\"', 'ĠIt', 'Ġgirl', 'Ġat', 'Ġhurt', 'Ġmaking', 'Ġstamp', 'Ġthree', 'Ġto', 'Ġpolice', 'Ġhad', 'Ġhuman', 'Ġpulled', 'Ġround', 'Ġwhere', 'Ġshot', 'ĠHer', 'Ġshape', 'Ġstepped', 'Ġwhile', 'Ġd', 'ĠSat', 'ĠBefore', 'Ġonce', 'Ġsh', 'iting', 'Ġhunted', 'Ġfell', 'Ġbeneath', 'ung', 'rang', 'Ġprobably', 'Ġkicked', 'Ġpreferred', 'Ġfood', 'Ġpurs', 'Ġthese', 'ist', 'Ġsystem', 'Ġneg', 'Ġhard', 'Ġsilver', 'Ġseconds', 'Ċ', 'Ġsp', 'Ġcomplain', 'Ġbridges', 'Ġindustrial', '.', 'Ġbullets', 'Ġsecond', 'Ġfirst', 'ashed', 'Ġborn', 'Ġlet', 'Ġmember', 'Ġloaded', 'Ġslipped', 'Ġsome', 'Ġfeet', 'Ġgun', 'Ġh', 'Ġabove', 'ble', 'Ġmouth', 'Ġbridge', 'Ġquickly', 'Ġthe', 'el', 'isf', 'Ġno', 'Ġeach', 'Ġwere', 'Ġonto', 'olate', 'Ġsquare', 'Ġruling', 'Ġthat', 'Ġimportant', 'Ġother', 'ĠGuild', 'Ġhim', 'Ġwent', 'Ġvom', 'Ġmight', 'er', 'Ġflight', 'Ġor', 'Ġtill', 'Ġweakness', 'Ġviol', 'Ġside', 'Ġwhich', 'Ġdes', 'Ġagain', 'Ġsc', 'Ġlanded', 'way', 'Ġbodies', 'ets', 'Ġready', 'Ġinclined', 'Ġimmediately', 'Ġcould', 'Look', 'Ġbe', 'Ġescape', 'Ġright', 'Ġplace', 'Ġjack', 'Ġdead', 'Ġstrong', 'Ġmaybe', 'Ġfight', 'Ġnormally', 'Ġoff', 't', 'Ġinside', 'Ġnow', 'Ġyoung', 'Ġsenses', 'Ġribs', 'Ġ-', 'Ġwhether', 'Ġsens', 'Ġcoming', 'Ġthey', 'Ġhunter', 'Ġdown', 'Ġbroke', 'Ġher', 'Ġscaled', 'Ġbefore', 'ising', 'alle', 'Ġorder', 'Ġarch', 'Ġliked', 'Ġmistake', 'Ġdying', 'Ġanxiety', 'Ġinto', 'Ġamount', 'Ġattack', 'Ġthere', 'Ġpossibility', 'Ġin', 'Ġhand', 'Ġwithout', 'Ġmer', 'Ġone', 'ĠShe', 'Ġf', 'Ġleader', 'Ġface', 'ĠNo', 'Ġbloody', 'Ġcl', 'ĠReal', 'Ġtro', 'Ġan', 'Ġunconscious', 'Ġfar', 'Ġon', 'umped', 'Ġgoing', 'Ġover', 'ĠThis', \"'\", 'ling', 'Ġbackwards', 'Ġremained', 'Ġtwo', 'u', 'Ġpreventing', 'Ġnight', 'Ġnot', 'Ġrecover', 'Ġonly', 'Ġcrack', 'Ġshe', 'Ġdisappeared', 'Ġcorner', \"'d\", 'ĠAround', 'Ġgot', 'wards', 'Ġrunning', 'Ġalmost', 'ĠAt', 'Ġfamily', 'Ġrun', 'Ġpull', 'ted', 'Ġhol', 'Ġg', 'Ġattacking', 'Ġunits', 'ĠWhen', 'Ġshoulder', 'Ġfelt', 'Ġclose', 'Ġkill', 'Ġkilled', 'Ġneck', 'Ġfled', 'Ġforce', 'Ġso', 'Ġbring', 'ining', 'ed', 'Ġtook', 'ĠOne', 'Ġtaking', 'Ġof', 'Ġhearing', 'ĠBesides', 'Ġdaughter', 'Ġsaw', 'Ġtrace', 'ĠBlood', 'ĠHe', 'Ġwh', 'Ġideal', 'Ġafter', 'ĠThey', 'Ġit', 'Ġbegan', 'Ġkind', 'Ġground', 'Ġguns', 'Ġgas', 'Ġsl', 's', 'Ġclosed', 'Ġdid', 'Ġtried', 'ished', 'Ġfor', 'Ġsprin', 'Ġman', 'Ġwall', 'Ġstra', 'Ġany'}\n",
      "Text 2: \n",
      "Don't Make Me Say It,\n",
      "**Author's Note:**\n",
      "> I hope you enjoy....\n",
      ">\n",
      "> It's really cheesy I'm so sorry.\n",
      ">\n",
      "> [Don't repost my work]\n",
      "\"So~, Bakugou.\" Katsuki froze in his seat on the couch, a scowl crawling onto his face as he caught sight of Mina's smile. He resisted the urge to hike his shoulders up an\n",
      "\t Tokenized no:Ċ Don'tĠ MakeĠ MeĠ SayĠ It ,Ċ **Author'sĠNote:**Ċ>Ġ IĠhopeĠ youĠ enjoy ... .Ċ>Ċ>Ġ It'sĠ reallyĠ che es yĠ I'mĠsoĠ s orr y.Ċ >Ċ>Ġ [ Don'tĠ re postĠ myĠ work ]Ċ \" So ~ ,ĠB ak ug ou .\"Ġ Kat su kiĠ fro zeĠ inĠhisĠ seatĠ onĠtheĠ couch ,ĠaĠ sc ow lĠ craw lingĠ ontoĠ hisĠfaceĠ asĠheĠ caughtĠ sightĠ ofĠ Min a'sĠ smile .ĠHeĠ resist edĠtheĠ urg eĠtoĠ h ikeĠ hisĠ shoulder sĠupĠ an\n",
      "\t Tokenized wiki:Ċ Don 't ĠMake ĠMe ĠSay ĠIt , Ċ * * A uthor 's ĠNote : * * Ċ > ĠI Ġhope Ġyou Ġenjoy .... Ċ > Ċ > ĠIt 's Ġreally Ġche esy ĠI 'm Ġso Ġsor ry . Ċ > Ċ > Ġ[ Don 't Ġrep ost Ġmy Ġwork ] Ċ \" So ~ , ĠBak ug ou .\" ĠKats uki Ġfro ze Ġin Ġhis Ġseat Ġon Ġthe Ġcou ch , Ġa Ġsc owl Ġc raw ling Ġonto Ġhis Ġface Ġas Ġhe Ġcaught Ġsight Ġof ĠM ina 's Ġsm ile . ĠHe Ġresisted Ġthe Ġur ge Ġto Ġh ike Ġhis Ġshoulders Ġup Ġan\n",
      "\t Unique Tokens no: {',Ġ\"', 'stillĠ', 'abilityĠtoĠ', 'work', 'es.Ċ', 'legĠ', 'fingerĠ', 'bitter', 'up', 'his', 'reaction', 'likeĠheĠ', 'HisĠ', 'seemĠtoĠbeĠ', 'face.Ċ', 'ing.Ċ', 'lingĠ', 'sĠthatĠ', 'ingĠitĠ', 'ent', 'backĠtoĠhisĠ', 'butĠthenĠ', 'ppedĠ', 'laugh', 'acrossĠ', 'brown', 'likeĠ', 'aĠt', 'completely', 'untilĠ', 'hadĠbeenĠ', 'grow', 'fromĠ', 'ĠteethĠ', 'atĠallĠ', 'smile', 'gri', 'herĠeyesĠ', 'mus', 'DoĠ', \"you'reĠ\", 'makeĠ', 'fli', '.\"ĠHeĠ', 'ableĠtoĠ', 'a', 'sh', 'cheekĠ', 'feltĠtheĠ', 'overlyĠ', 'somethingĠ', 'Ġm', 'ki', 'sound', 'couch', 'SometimesĠ', '.ĠA', 'responseĠtoĠtheĠ', 'headĠ', 'stepĠ', 'growingĠ', 'bl', 'flick', 'ingĠwithĠtheĠ', 'silenceĠ', 'feltĠ', 'findĠ', 'placeĠ', 'ir', ',ĠB', 'sittingĠ', 'pageĠ', 'Ġtimes,Ġ', 'jo', 'goĠandĠ', 'insideĠofĠ', '.Ċ\"', 'lipĠ', 'gettingĠ', 'aroundĠ', 'gripp', 'tingĠ', 'worriedĠ', 'parkingĠ', 'aĠwasĠ', 'lookĠ', 'makeĠyouĠ', 'lip', ',ĠM', 'stoodĠ', 'againstĠhisĠ', 'onĠallĠ', 'feelingĠ', 'MeĠ', 'endĠ', 'oneĠtoĠ', 'inĠhisĠ', 'TheyĠ', 'swe', 'jok', 'aysĠ', 'ingĠtoĠ', 'backĠtheĠ', 'nextĠtoĠ', 'ĠtookĠ', 'at,Ġ', 'allow', 'notĠ', 'eĠthinkĠ', 'Kat', 'steppedĠ', '.Ċ>Ċ>Ġ', 'help', 'mĠ', 'ppingĠ', 'MaybeĠ', 'e.Ċ\"', 'madeĠhisĠ', 'argumentĠ', '[', 'ateĠ', 'explosion', 'atĠtheĠ', 'edĠasĠaĠ', 'caughtĠ', 'ickĠ', 'pressingĠ', 'edĠ', 'around', 'usedĠ', 'secondsĠ', 'ouchĠ', 'sm', '.ĠH', 'inĠ', 'burn', 'ed,ĠtheĠ', 'moreĠofĠaĠ', 'fromĠtheĠ', 'ed,Ġ', 'filter', 'resist', 'staringĠ', 'hisĠhandĠ', 'dart', 'attempt', 'stomach', 'soĠ', 'IĠhopeĠ', 'kno', 'aseĠ', 'bo', 'esĠ', 'pink', 'erĠ', '?ĠWhyĠ', 'ass', 'ĠthingsĠ', 'passed', 'relax', 'kneeĠ', 'urlĠ', 'spec', 'awayĠ', 'y,Ġ', 'right', 'whatĠheĠ', 'actionĠ', 'ingĠdownĠ', 'es', 'still', 'eĠthatĠ', 'sul', 'pre', 'orr', 'satĠ', 'supposedĠtoĠ', 'aĠhy', 'aliz', 'ear', 'ignoreĠ', 'itself', 'stomachĠ', 'IĠ', 'useĠ', 'glanc', 'word,Ġ', 'allowĠ', 'sure', 'heldĠ', 'finally', 'sn', 'ancedĠ', 'cre', 'eĠtoĠ', ',ĠS', 'quirk', 'mind', 'eĠ', 'me', 'ĠturnedĠ', 'ense', 'breathĠ', 'co', '.ĠHisĠ', 'manageĠ', 'underĠ', 'onĠitsĠ', 'fightĠ', 'direc', 'sesĠ', '.ĠEveryoneĠ', 'unchĠ', '.ĠHeĠ', 'aĠc', 'ingĠinĠtheĠ', 'haveĠtoĠ', 'suddenĠ', 'ayĠ', 'neverĠ', 'eĠinĠ', 'stiff', 'arm', 'dr', 'triedĠtoĠ', 'attentionĠ', \"You'reĠ\", 'onlyĠtoĠ', '.Ġ\"', \"a'sĠ\", 'selfĠ', 'slamm', 'likeĠaĠ', '.ĠButĠ', 'ed,ĠandĠ', 'kiĠ', 'approach', ',ĠtheirĠ', 'byĠ', 'noteĠ', 'myĠ', 'enĠtheĠ', 'lĠ', 'fist', 'mightĠ', 'aĠsaidĠ', 'againstĠtheĠ', 'annoy', 'enjoy', 'enseĠ', 'leanedĠ', 'm', 'decidedĠtoĠ', 'asĠsheĠ', 'downĠonĠtheĠ', 'grac', ',ĠandĠ', 'ap', 'well', 'inĠanotherĠ', 't', 'yellowĠ', 'az', 'forĠ', 'down', 'msĠ', 'heĠwasĠ', \"I'mĠsoĠ\", 'wist', 'deprec', 'many', 'sĠnotĠ', 'lackĠofĠ', 'ann', ',ĠheĠ', 'heartĠ', 'allĠtheĠ', 'edĠtheĠ', 'fle', 'asĠtheĠ', 'otherw', 'ariĠ', 'sc', 'calmĠ', 'ably,Ġ', 'ĠtoĠme', 'che', 'every', 'clesĠ', 'otherĠ', 'froz', 'himĠtoĠtheĠ', 'a.Ċ', '.\"Ġ', 'scared', '.ĠItĠjustĠ', 'feelĠ', 'sĠandĠ', 'absenceĠofĠ', 'overĠtheĠ', 'boyĠ', 'himĠ', 'him', 'hisĠ', 'istĠ', 'ifyingĠ', \"wasn'tĠ\", '.ĠK', 'and', 'preparedĠ', 'yĠthatĠ', 'leaningĠ', 'disgust', 'Noth', 'dro', 'soft', 'pal', 'red', 'youĠs', 'rightĠ', \"**Author'sĠNote:**Ċ>Ġ\", 'ofĠtheĠ', 'brush', 'leav', 'nerv', 'whereĠ', 'start', 'straightĠ', 'iĠwouldĠ', 'postĠ', 'ingĠfromĠ', 'eĠto', 'pitĠ', 'fro', 'arn', \"i'sĠ\", 'doĠitĠ', 'forĠtheĠ', 'ru', ',ĠbutĠheĠ', 'Ġcompar', 'hisĠlip', \"couldn'tĠ\", 'challeng', 'umbledĠ', 'skinĠ', 'shouldersĠ', 'norm', 'itt', 'aboutĠ', 'st', 'forĠaĠmoment', 'sent', 'overĠhisĠ', 'offĠ', 'chew', 'ativeĠ', 'slid', 'alĠs', 'iĠs', 'areĠ', ']Ċ', 'halfĠ', 'craw', 'makesĠyouĠ', '?\"ĠHeĠ', ',ĠK', 'ly,Ġ', 'armsĠ', 'readĠ', 'ĠcouldĠ', 'edĠoffĠ', 'lookedĠ', 'justĠ', 'anxiet', 'YouĠ', 'videoĠ', 'ign', 'itĠtoĠ', 'sĠmeĠ', 'over', 'whin', 'edĠasĠ', 'ingĠtheĠ', 'andĠtheĠ', 'backĠintoĠtheĠ', 'allĠtoĠ', 'quietĠ', 'Min', 'ĠsaysĠ', 'fastĠ', 'pr', 'ed,Ġ\"', 'eĠtheyĠ', 'gripĠ', 'eĠofĠ', 'd,Ġ', 'tri', 'it', 'eyes', 'ingĠmeĠ', 'overĠtoĠtheĠ', 'shoulderĠ', 'hisĠfaceĠ', 'openĠ', 'sayĠ', 'aĠ', 'e.Ġ', 'imagin', 'Ser', '.Ċ', 'wereĠt', 'butter', ',Ġt', 'sightĠ', 'affection', 'burst', 'er', 'eyes,Ġ', 'gl', 'no', 'boy', 'ikeĠ', 'ar', 'meantĠ', 'Ġcall', '>Ċ>Ġ', 'stareĠ', ',\"Ġ', 'cornerĠ', 'eĠturn', 'airĠ', 'ThereĠ', 'ondĠ', 'asĠsoonĠasĠ', 'yĠ', 'alk', 'exactlyĠ', 'reallyĠ', 'edgeĠ', 'heĠhadĠ', 'nextĠtoĠtheĠ', 'inglyĠ', 'su', 'iĠwasĠ', 'happyĠ', 'ingĠhisĠ', 'tionsĠ', 'insid', 'moment', 'her,Ġ', 'eyesĠ', 're', 'ĠwithĠherĠ', 'clockĠ', 'enedĠ', 'persistentĠ', 'd', 'softĠ', 'formedĠ', 'purpose', 'stood', 'els', 'onĠ', 'ag', \"doesn'tĠ\", 'outĠofĠhisĠ', 'deathĠ', 'atĠanyĠ', 'onĠit', 'bleĠ', ',ĠthisĠ', '.ĊItĠ', 'chuck', 'ingĠt', 'startingĠtoĠ', 's.ĊTheĠ', 'BeingĠ', 'ingĠ', 'withĠ', 'off,Ġ', '.\"Ċ', 'stare', 'peopleĠ', 'MakeĠ', 'edĠonĠtheĠ', 'Ġthere,Ġ', 'intoĠhisĠ', 'overĠ', 'ab', 'did', 'adm', 'tedĠ', 'es.', 'ak', 'overĠtoĠ', 'aryĠ', 'ingĠthemĠ', 's,Ġ', 'ated', 'ComeĠon,Ġ', 'fold', 'wait', 'letĠ', 'Ġthem,Ġ', 'fireĠ', 'goneĠ', 'inn', ',ĠhisĠ', 'ĠtoĠthem', 'leaveĠ', 'inĠdifferentĠ', 'voiceĠ', 'heĠ', 'onĠhisĠ', 'upĠ', 'e,Ġ\"', 'ight', 'he', 'oneĠ', 'placedĠ', 'aĠl', 'atingĠ', 'utterĠ', 'lyĠ', 'sur', 'Ġ', 'atchingĠ', 'iĠw', 'himĠandĠ', 'gr', 'th', 'sat', 'beforeĠheĠ', 'IĠthinkĠ', 'youĠ', 'utĠ', 'TheĠ', '\"Ġ', 'ĠtoĠgetĠ', 'browĠ', 'SheĠ', 'ell', 'moreĠt', 'ittedĠ', 'ingĠinĠ', 'sp', 'ofĠ', 's.Ġ', 'bloodĠ', 'clench', 'rib', 'inĠtheĠ', 'heatĠ', 'h', 'spaceĠ', 'intoĠtheĠ', 'ilĠ', 'saidĠ', 'couldĠ', 'intoĠ', 'summerĠ', 'ageĠ', 'ing.Ċ\"', 'ckingĠ', 'aĠmist', 'curledĠ', 'burningĠ', 'seatĠ', 'cheek', \"I'mĠnotĠ\", 'comeĠ', \"What'sĠ\", 'human', 'un', 'holid', 'familiar', 'againstĠ', 'agingĠ', 'hand', ',ĠtheĠ', 'havingĠ', 'irkĠ', 'whisper', 'palm', 'anĠ', 'Ġthigh', 'SayĠ', 'meĠ', 'nowĠ', 'evenĠ', \"you'llĠ\", 'opĠ', 'kĠ', 'fall', 'fl', 'onĠherĠ', 'aĠlittl', 'onsĠ', '.ĊHeĠ', '?\"Ġ', 'shoulder', 'overĠaĠ', ',ĠandĠtheĠ', 'blon', 'out', 'softlyĠ', 'edĠhisĠ', '\"S', '\"t', 'enter', 'Ġtim', 'aĠhadĠ', 'oĠandĠ', 'watch', 'themĠ', 'zeĠ', 'asĠtheyĠ', 'feltĠlikeĠ', '.\"ĠTheĠ', 'iouslyĠ', 'game.Ġ', 'apĠ', 'plo', 've', 'continuedĠtoĠ', 'sheĠhadĠ', 'onĠtheĠ', 'f', 'spok', 'oĠ', 'betweenĠ', 'hard', 'handĠ', 'underĠtheĠ', 'reactionĠ', ',ĠaĠ', 'fac', \"Don'tĠ\", 'y.Ċ', 'e,Ġ', 'ort', 'himself', 'ĠsmallĠ', 'mass', 'whenĠheĠ', 'edĠatĠtheĠ', 'wr', 'inĠaĠ', ',ĠitĠ', 'ontoĠ', \"It'sĠ\", 'park', 'shiftingĠ', '...', 'question', 'eredĠ', 'book', 'clos', 'dude', 'in', 'slideĠ', 'anceĠ', ',Ċ', 'upĠandĠ', 'o', 'doĠsomethingĠ', 'atĠ', 'Ġthrow', 'urg', 'crossedĠ', 'ledĠ', 'ofĠhisĠ', ',ĠtryingĠtoĠ', 'scaredĠ', 'itĠwas', 'kingĠ', 'doneĠ', 'loos', 'edly', 'ise,Ġ', 'away', 'aĠminuteĠ', 'setĠ', 'almostĠ', 'itĠ', 'front', 'asĠheĠ'}\n",
      "\t Unique Tokens wiki: {'Ġtw', 'Ġdro', 'ĠK', 'Ġhis', 'Ġquestioned', 'Ġfire', 'Ġfrom', 'cious', 'Ġthink', 'Ġcow', 'ĠThe', 'Ġsilence', 'Ġstep', 'Ġch', 'Ġanother', 'ping', 'Ġstill', 'Ġtense', 'Ġj', 'Ġwith', 'Ġgame', 'Ġhappy', 'Ġsor', 'Ġgl', 'Ġbetween', 'Ġat', 'inge', 'Ġla', 'Ġacross', 'Ġsoft', 'ate', 'aring', 'Ġfil', 'Ġd', 'ĠKats', 'Ġcouldn', 'Ġbr', '?\"', 'Ġhy', 'ry', 'Ġhelping', 'Ġmany', 'Ġbook', 'Ġpit', \"'t\", 'king', 'Ġhard', 'Ġann', 'Ġseem', 'Ġwaiting', 'Ġlip', 'Ġlight', 'Ġmore', 'ari', 'Ġjust', 'Ġsudden', 'Ġcome', 'unk', 'Ġabsent', 'Ġlooked', 'Ġwere', 'Ġonto', 'Ġerup', 'Ġ[', \"'m\", 'inned', 'Ġopen', 'Ġcrossed', 'Ġdone', 'Ġnote', 'Ġsc', 'Ġentering', 'Ġdoesn', 'ima', 'Ġcould', 'Ġbe', 'Ġun', 'Ġdifferent', 'Ġplace', 'Ġcou', '.\"', 'Ġout', 'Ġtouch', 'Ġignore', 'Ġleave', 'Ġheld', 'Ġfinger', 'anned', 'Ġanxiety', 'Ġur', 'Ġcir', 'Ġon', 'Ġprey', 'Ġburst', 'oy', 'Ġformed', 'minded', 'izing', 'Ġcorner', 'Ġtelling', 'Ġtick', 'Ġdeath', 'Ġsitting', 'Ġunn', 'Ġpage', 'Ġme', 'Ġshallow', 'Ġsay', 'Ġable', 'Ġwatched', 'Ġg', 'Ġresisted', 'one', 'Ġshoulders', '?', 'Ġquiet', 'Ġtook', 'Ġquestioning', ',\"', 'ek', 'owed', 'Ġenjoy', 'Ġaction', 'Ġmeant', 'Ġturned', 'Ġdid', 'Ġwork', 'Ġstomach', 'May', 'She', 'Ġcur', 'Ġup', 'ike', 'Ġbrown', 'Ġapproaching', 'Ġwhen', 'ina', 'Ġspace', 'hed', 'Ġits', 'Ġstarting', 'ushing', 'Ġsul', 'Ġswe', 'Ġall', 'Ġpersistent', 'are', 'Ġworried', 'use', 'Ġare', 'rec', 'Ġto', 'ero', 'Ġhad', 'Ġhuman', 'Ġholidays', 'Ġslam', 'Ġag', 'Ġagainst', 'Ġgra', 'Ġfl', 'Ġback', 'Ġal', 'Ġattention', 'ably', 'Ġdra', 'Ġsn', 'Ġfeel', 'Ġwell', 'Ġabout', 'Ġmade', 'ze', 'Ġmakes', 'Ġsp', 'Ġunder', 'Ġlet', 'Ġwas', 'Ġpeople', 'uthor', 'uki', 'Ġthe', 'Ġlike', 'ost', 'Ġother', 'raw', 'Ġeyes', 'Ġwrist', 'Ġpressing', 'Say', 'pered', 'Ġslide', 'erving', 'inary', 'Ġsw', 'Ġdo', 'Ġhelp', 'Ġche', 'Ġtent', 'Ġburning', 'Ġleaving', 'Ġfight', 'all', 'Ġoff', 'Ġdirections', 'Ġqu', 'Ġthings', 'Ġlook', 'Ġsoon', 'ĠBut', 'Ġmistake', 'Ġinto', 'S', 'Ġbl', 'Ġcaught', 'Ġhand', 'Ġface', 'Ġfinally', 'Ġlack', 'Ġan', 'ling', 'ĠSay', 'Ġshr', 'Ġfind', 'Ġshe', 'head', 'ened', 'Ġbrow', 'Ġlittle', 'erting', 'Ġgrip', 'Ġstood', 'ted', 'rawl', 'Ġshoulder', 'N', 'Ġfelt', 'Ġso', 'Ġhalf', 'ons', 'ĠWhy', 'Ġcre', 'ge', 'Ġmy', 'Ġreigning', 'Ġnever', 'rick', 'ks', 'Ġmanage', 'ching', 'Ġsliding', 'itting', 'Ġhimself', 'Ġchallenge', 'Ġsomething', 'Ġedge', 'ative', 'Ġmass', 'ĠNote', 'ushed', 'age', 'Ġsight', 'Ġand', 'Ġa', ']', 'Ġnormal', 'Ġprepared', 'Ġwould', 'Ġas', 'Ġyou', 'Ġgo', 'Ġvoice', 'ms', 'Ġitself', 'Ġsmall', 'Ġyellow', 'Ġotherwise', 'Ġhun', 'Ġaffection', 'Ġmoment', 'Ġbeen', 'Ġplaced', 'Ġins', 'Don', 'Ġsm', 'Ġarm', 'ĠIt', 'Ġthrows', 'Ġp', 'isting', 'Ġfalling', 'Ġsure', 'Ġstepped', 'utter', 'Ġair', 'Ġsnap', 'Ġself', 'ined', 'Ġleg', 'Ġset', '*', 'Ġevery', 'Ġdep', 'ometimes', 'ming', 'Ġrelaxed', 'il', 'Ġfolded', 'What', 'arted', 'Ġred', 'ĠM', 'Ġbitter', 'Ġargument', 'Ġreaction', 'ĠEvery', 'oses', 'Ġveins', 'Be', 'Ġbutterflies', 'Ġstraight', 'ust', 'tered', 'Ġthat', 'ered', 'Ġaround', 'Ġknee', 'Ġabsence', 'ena', 'osen', 'ase', 'Ġspark', 'Ġgrow', 'Ġget', 'Ġright', 'ĠBak', 'Ġinside', 'Ġelse', 'Ġthey', 'Ġpurpose', 'ifying', 'Ġbefore', 'ting', 'Ġlean', 'Ġwhat', 'Ġin', 'Ġcontinued', 'Ġeven', 'ched', 'Ġcl', 'anced', 'Ġover', 'Ġmake', 'Ġbreath', 'Ġcatching', \"'re\", 'ĠHis', 'The', 'Ġalmost', 'Ġheat', 'ped', 'ok', 'Ġshifting', 'Ġtimes', 'ists', 'Ġco', 'Ġcalling', 'ĠA', 'Ġsounds', 'Ġcomparing', 'Ġhaving', 'Ġend', 'hearted', 'Ġdanced', 'Ġst', 'Ġwh', 'Ġwasn', 'Ġit', 'Ġexactly', 'Ġhave', 'ides', 'Ġimag', 'Ġpalm', 'Ġfronts', ':', 'ile', 'Ġminute', 'Ġtried', 'Ġpassed', 'Ġfor', 'Come', 'Ġfeeling', 'Ġpl', 'Ġany', 'Ġstar', 'He', 'Ġunw', 'Ġability', 'Ġstart', 'Ġthis', \"'ll\", 'Ġdecided', 'Ġbut', 'Ġgr', 'A', 'Ġsays', 'Ġfamiliar', 'aning', 'Ġtrio', 'Ġtrying', 'Ġhe', 'rit', 'Ġ\"', 'Ġgrowing', 'Ġboys', 'Ġdisg', 'ĠSpark', 'Ġwarned', 'aging', 'ance', 'Ġwhere', 'Do', 'Ġread', 'ighter', 'Ġrub', 'Ġblood', 'Ġpal', 'ingly', 'Ġseconds', '.', 'Ġreally', 'arks', 'Ġthem', 'itated', 'ĠMake', 'Ġth', 'Ġh', 'Ġcompletely', 'ble', 'Ġn', 'Ġhim', 'Ġused', 'Ġspoke', 'Ġmight', 'Ġsaid', 'ish', 'Ġsupposed', 'ats', 'I', 'Ġcalm', 'Ġadmit', 'Ġle', 'Ġteeth', 'othing', 'Ġuntil', '>', 'ips', 'e', 'Ġseat', 'Ġresponse', 'Ġnow', 'Ġaway', 'Ġsat', 'Ġburned', 'Ġdown', 'Ġher', 'tal', 'Ġrep', 'aze', 'Ġknocking', 'led', 'Ġvideo', '....', 'ĠKir', 'Ġatop', 'Ġskin', 'Ġone', 'irk', 'Ġspec', 'Ġnext', 'Ġtheir', 'Ġf', 'Ġhope', 'orted', 'Ġby', 'Ġexplosion', 'Ġgetting', 'Ġgone', 'Ġfro', 'Ġnot', 'Ġonly', 'umbled', 'Ġsummer', 'Ġbo', 'ude', 'Ġl', 'ĠThere', 'Ġclock', 'ĠS', 'Ġclose', 'Ġattempt', 'esy', 'ed', 'Ġrib', 'Ġof', 'Ġword', 'Ġpink', 'ĠHe', 'aut', 'Ġfast', 'ĠThey', 'Ġsk', 'owl', 'Ġboy', 'Ġarms', 'Ġmuscles', 'Ġstiff', 'Ġheart', 'ĠI', 'Ġflee', 'ĠMe', 'ared', 'Ġlo'}\n",
      "==Same Author==\n",
      "Text 1: <PERSON>. <PERSON> pleaded with his brother to destroy the documents after he realized what had happened.\n",
      " For five years, <PERSON> continued to demand that his brother return his rightful share of the business.\n",
      " As a last resort, <PERSON> and his sons filed suit against <PERSON>'s side of the famil\n",
      "\t Tokenized no:< P ERS ON > .Ġ < P ERS ON >Ġ ple ad edĠwithĠ hisĠ brother ĠtoĠ destro yĠtheĠ documentsĠ afterĠheĠ realizedĠ whatĠ hadĠ happen ed.Ċ Ġ ForĠ fiveĠ years,Ġ < P ERS ON > Ġcontinu edĠtoĠ dem andĠthatĠ hisĠ brotherĠ returnĠ hisĠ right fulĠ shareĠ ofĠtheĠ business .ĊĠ AsĠ aĠlastĠ resort ,Ġ < P ERS ON > ĠandĠhisĠ son sĠ filedĠ suitĠ againstĠ < P ERS ON > 'sĠ sideĠofĠtheĠ famil\n",
      "\t Tokenized wiki:< P ERS ON > . Ġ< P ERS ON > Ġpleaded Ġwith Ġhis Ġbrother Ġto Ġdestroy Ġthe Ġdocuments Ġafter Ġhe Ġrealized Ġwhat Ġhad Ġhappened . Ċ ĠFor Ġfive Ġyears , Ġ< P ERS ON > Ġcontinued Ġto Ġdemand Ġthat Ġhis Ġbrother Ġreturn Ġhis Ġright ful Ġshare Ġof Ġthe Ġbusiness . Ċ ĠAs Ġa Ġlast Ġresort , Ġ< P ERS ON > Ġand Ġhis Ġsons Ġfiled Ġsuit Ġagainst Ġ< P ERS ON > ' s Ġside Ġof Ġthe Ġfam il\n",
      "\t Unique Tokens no: {'ation.Ġ', 'rulingĠ', '.ĠAnd', 'es.Ċ', 'aĠlos', 'storesĠ', ',ĠtheyĠ', 'edĠbyĠtheĠ', 'member', 'surviv', 'ad', 'cle', 'alsoĠ', 'boughtĠ', 'against', 'payĠtheĠ', 'legalĠ', 'Ġs', 'EĠ', 'edĠitĠ', 'dragg', 'delay', 'untilĠ', 'yearsĠinĠ', 'hadĠbeenĠ', 'grow', 'fulĠ', \"It's\", 'liquid', 'fromĠ', '#Ġ', '14Ġ', 'better', 'AsĠ', 'ĊĠ', 'et', 'happen', 'ableĠtoĠ', 'aĠmod', 'Ġsu', 'off', 'madeĠaĠ', 'keptĠ', 'overĠandĠ', 'ir', 'percentĠ', 'pageĠ', 'Ġtimes,Ġ', \"s'Ġ\", 'sellingĠ', 'aĠnewĠ', 'aĠfamilyĠ', 'tingĠ', 'sĠfor', 'vacation', '.ĠTheĠ', 'ĠwasĠ', 'buy', 'againstĠhisĠ', 'PRO', 'ĠshareĠ', 'FE', 'ĠthatĠ', 'INTOĠ', 'alĠ', '50Ġ', 'immediatelyĠ', 'destro', 'provid', ',ĠlikeĠ', 'intoĠaĠ', 'companiesĠ', 'ed\"Ġ', 'attack', 'defenseĠ', 'edĠonĠ', 'companyĠ', 'usesĠ', 'dĠtheĠ', 'ĠthemselvesĠ', 'provideĠ', 'orĠ', '0Ġ', 'edĠ', 'ServicesĠ', 'asĠ', 'historyĠandĠ', 'inĠ', 'fromĠtheĠ', 'chain', \"'Ġ\", 'soĠ', 'Ġc', 'pli', 'bo', 'di', 'forĠexample', 'decid', 'incom', 'erĠ', 'resort', 'longestĠ', 'percentĠofĠtheĠ', '.ĊĠ', 'mis', 'right', 'rul', 'son', 'Ġtime,Ġ', 'shareĠ', 's,ĠandĠ', 'erĠofĠtheĠ', 'ĠtheirĠ', 'offĠtheĠ', 'derĠ', 'Ont', 'eightĠ', 'withĠtheĠ', 'iorĠ', 'Tak', 'Dur', '8Ġ', 'Super', '198', 'eĠtoĠ', 'whatĠ', 'didĠ', 'it.', 'outĠofĠ', 'newĠ', 'aĠlastĠ', 'atedĠ', 'bank', 'oneĠofĠtheĠ', 'law', 'sol', 'sĠforĠ', 'UD', ',ĠwhenĠ', 'ipĠ', 'anotherĠ', 'AĠ', 'rack', 'oldĠ', 'payment', 'el', 'e.Ċ', 'ifĠ', 'ĠtheyĠ', 'ple', 'ForĠ', 'ingĠthatĠ', 'byĠ', 'other', 'ilsĠ', 'caseĠ', 'allĠ', ',ĠandĠ', 'InĠ', 'backĠtoĠtheĠ', ',Ġ', 'aĠsup', 'forĠ', 'cop', 'ic', 'aĠsimpleĠ', 'edĠforĠ', 'yĠtheĠ', 'itĠinĠ', 'iou', 'aĠlaw', 'YĠ', 'sentenc', 'ĠmillionĠinĠ', 'clearĠthatĠ', 'heartĠ', 'couldĠbeĠ', 'edĠtheĠ', 'butĠ', '.Ġ', 'aĠm', 'LikeĠ', 'bad', 'sĠareĠ', '$5', 'business', 'N', 'B', 'it,Ġ', 'hisĠ', \"wasn'tĠ\", 'and', 'immigrant', 'Ġtransform', 'erĠinĠ', 'Ġterm', 'generatingĠ', 'years.Ċ', 'order', '.,Ġ', 'ofĠtheĠ', 'edĠupĠ', 'andĠthatĠ', 'F', 'sinceĠ', 'returnĠ', 'sĠfromĠ', '2008', 'forĠtheĠ', 'ru', 'Ġcha', 'soldĠ', 'yĠtoĠ', 'ĠtoĠ', 'Court', 'perĠ', 'diedĠ', 'steal', 'groceryĠ', 'fav', 'ing,ĠbutĠ', 'JusticeĠ', 'sĠtoĠ', 'father', 'hadĠ', 'new', 'herĠ', 'edĠintoĠ', 'businessĠandĠ', 'arioĠ', 'edĠtoĠ', 'parent', \"'sĠ\", '.ĠTheyĠ', 'vs.Ġ', 'intention', 'suitĠ', 'becomesĠ', 'promis', 'ask', 'ed.Ċ', '>Ġ', 'MarchĠ', 'famil', 'tri', 'fiveĠ', 'inĠturn', 'ingsĠ', 'del', 'er', 'lung', 'oneĠofĠthemĠ', '4.', 'control', \"it'sĠnotĠ\", 'ThereĠ', 'andĠthenĠ', 'heĠhadĠ', 'orĠofĠ', 'realizedĠ', 'long-', 'inst', 're', 'understand', 'cond', 'prisonĠ', 'GreekĠ', 'sideĠofĠtheĠ', 'coreĠ', 'ĠmillionĠ', 'securedĠ', 'caseĠofĠ', 'onĠ', 'outĠofĠhisĠ', 'ĠwhileĠ', '2011', 'civil', '2007', 'AM', 'family', 'documentsĠ', 'ud', 'e-', 'andĠ', 'dem', 'becameĠtheĠ', 'asset', 'becauseĠheĠ', 'diver', 'itĠandĠ', 'afterĠheĠ', 'businessĠ', 'calledĠ', 'ingĠ', 'super', 'keep', 'sĠ', 'pocket', 'inĠofĠ', 'ing,ĠtheĠ', 'ingĠthemĠ', 's,Ġ', 'hold', 'Ġsuper', 'at', 'bon', 'edĠaĠ', 'marketĠ', 'sonĠ', '(', 'heĠ', 'childrenĠ', 'familyĠ', 'filedĠ', 'he', 'SĠ', 'accusedĠ', 'wouldĠ', 'lux', 'Ġ', 'brotherĠ', '199', 'years,Ġ', ')ĠandĠ', 'al', 'ĠandĠhisĠ', 'uryĠ', 'TheĠ', 'profitsĠ', 'ingĠinĠ', 'ofĠ', 'Phil', 's.Ġ', 'OctoberĠ', 'sĠfromĠtheĠ', \"Ġwasn'tĠ\", 'fraud', 'prof', 'inĠtheĠ', 'fe', 'intoĠ', '197', 'TwoĠ', 'Ġcontract', 'stock', 'dup', 'orderedĠ', 'un', 'againstĠ', ',ĠtheĠ', 'de', 'ofĠaĠ', 'ĠtooĠ', 'evenĠ', 'half-', 'fall', 'ameĠ', 'judgeĠ', 'eachĠother', 'MarketĠ', 'bill', 'ĠthereĠ', 'outĠ', 'Greec', 'check', 'atĠaĠ', 'spin', 'rupt', 'Other', 'Corp', 'alleg', 'forĠ$', 'led', 'edĠinĠ', 'bought', 'wereĠ', 'own', 'ĠmembersĠ', 'MIS', 'ern', 'e,Ġ', 'y.Ċ', 'edĠwithĠ', '.ĠInĠ', 'Ġcontinu', '1Ġ', 'anc', 'originalĠs', 'family,Ġ', 'brother', '$', 'iĠ', 'atĠ', 'archĠ', 'TUR', 'oftenĠ', 'almostĠ', 'Mad', 'itĠ', 'ingĠthat', 'launch', 'edĠthemĠ', 'mal'}\n",
      "\t Unique Tokens wiki: {'ra', 'Ġhis', 'Ġ$', 'Ġbills', 'Ġoriginal', 'Ġreinstated', 'Ġfrom', 'Ġpleaded', 'Ġlast', 'Ġbut', 'Ġdecided', 'Ġuncle', 'Ġand', 'Ġa', ')', 'Ġintentions', 'Ġup', 'Ġclear', 'Ġcontrolled', 'Ġmisunder', 'Ġbusiness', 'ĠOther', 'Ġbetter', 'Ġwould', 'ĠBasket', 'Ġgrocery', 'Ġ2008', 'ation', 'Ġwhen', 'ĠThe', 'Ġfather', 'Ġtoo', 'Ġas', 'ino', '.,', 'ful', 'TO', 'rupted', 'ĠGreece', 'Ġsold', 'Ġcancer', 'ĠF', 'Ġanother', 'ĠJustice', 'ĠMarch', 'ĠB', 'Ġthen', 'Ġdragged', 'Ġlongest', 'Ġbeen', 'Ġwith', 'Ġdestroy', 'Ġ2007', 'Ġdocuments', 'Ġcompanies', 'Ġcompany', 'Ġhe', 'Ġcond', 'Ġall', 'Ġsimple', 'ĠAs', 'Ġlawsuit', 'Ġ\"', 'Ġterms', 'page', 'Ġalso', 'ĠIt', 'Ġson', \"'s\", 'Ġat', 'Ġsentenced', 'Ġare', 'Ġgrowing', 'Ġcivil', 'Ġto', 'Ġ1971', 'Ġlung', 'Ġexample', 'Ġpocket', 'Ġhad', 'Ġordered', 'Ġbank', 'Ġtime', 'Ġliquid', 'ĠT', 'Ġagainst', 'ĠFor', 'Ġstock', 'Ġwhile', 'Ġback', 'Ġdel', 'Ġdef', ',', 'Ġmal', 'Ġlong', 'ĠSuperior', 'Ġhistory', 'Ġ50', 'Ġprovided', 'ĠP', 'Ġaccused', 'Ġparents', 'Ġchain', 'Ġholdings', 'ff', 'Ġmade', 'Ġfeud', 'Ġjudge', \"'t\", 'Ġrealized', 'Ġsupermarket', 'uded', 'ĠLike', 'Ċ', 'Ġtransformed', 'Ġsp', '.', 'Ġfall', 'Ġimmigrants', 'ĠDuring', 'Ġwas', 'Ġmember', 'UR', 'Ġluxury', 'Ġthem', 'ĠTwo', 'Ġvacation', 'Ġassets', 'Ġbecame', 'Ġshare', 'Ġthe', 'Ġeach', 'Ġwere', 'NS', 'Ġcalled', 'EU', 'Ġselling', 'ing', 'Ġ(', 'D', 'Ġruling', 'Ġthat', 'Ġlike', 'Ġother', 'Ġturn', 'Ġhappened', 'Ġcase', 'Ġprovide', 'ĠGreek', 'Ġor', 'Ġpayment', 'ĠFA', 'gener', 'Ġside', 'ĠTake', 'ils', 'ĠPhilip', 'Ġfavor', 'ĠCourt', 'Ġdiver', 'Ġuntil', 'Ġkept', 'Ġcheck', 'Ġimmediately', 'Ġcould', 'Ġbrothers', 'Ġbe', 'Ġprofits', 'Ġif', 'Ġ1990', 'Ġright', 'Ġloss', 'Ġ14', 'ĠMarket', 'Ġbon', 'Ġoff', 'Ġdemand', 'Ġyears', 'M', 'Ġvs', 'Ġfiled', 'ating', 'Ġout', 'Ġthey', 'Ġfive', 'Ġher', 'Ġsuit', 'Ġinto', 'Ġdefense', 'ting', 'Ġthere', 'Ġwhat', 'Ġattack', 'Ġin', 'Ġalleging', 'Ġone', 'Ġcontinued', 'Ġeven', 'Ġbecomes', 'Ġmodern', 'ĠIN', 'Ġbuying', 'Ġover', 'Ġby', 'Ġon', 'Ġsince', 'Ġchildren', 'Ġsupplier', 'Ġmembers', \"'\", 'Ġresort', 'd', 'Ġlegal', 'Ġlaunched', 'ĠOntario', 'ISE', 'Ġ1984', 'Ġnot', 'uses', 'Ġpercent', 'ĠMadame', 'ĠCorp', 'Ġpromise', 'Ġheirs', 'Ġbought', 'Ġbo', 'Ġowner', 'Ġdied', 'Ġstealing', 'ĠAnd', 'Y', 'Ġrack', 'Ġalmost', 'Ġfamily', 'Ġpay', 'Ġbrother', 'Ġprison', 'Ġtrial', 'Ġsurviving', 'Ġable', 'Ġtimes', 'ĠThere', 'Ġprofit', 'ĠServices', 'Ġcopper', 'Ġoften', 'ĠOctober', 'Ġ4', 'Ġso', 'Ġhalf', 'Ġkeep', 'Ġstores', 'ĠA', 'Ġincome', 'Ġsons', 'Ġof', 'Ġ1993', 'Ġfees', '5', 'standing', 'Ġreturn', 'Ġbad', 'Ġ2011', 'Ġnew', 'Ġafter', 'Ġwasn', 'Ġdelayed', 'Ġit', 'Ġbecause', 'Ġ8', 'ĠThey', 'arch', 'Ġcore', 'Ġcontracts', 'ĠIn', 'Ġthemselves', 's', 'Ġdid', 'Ġruled', 'i', 'Ġfor', 'Ġheart', 'Ġeight', 'Ġsecured', 'Ġelder', 'Ġ#', 'Ġowned', 'Ġmat', 'icious'}\n",
      "Text 2: in need, they visit a government assistance center. When they have a health problem, they expect a government-funded health system to take care of them. When they are unemployed, they look to the government to pay them—and to tide them over until they find another job.\n",
      " If unemployment is statistica\n",
      "\t Tokenized no:inĠ need ,ĠtheyĠ visitĠ aĠ governmentĠ assistanceĠ center .ĠWhen ĠtheyĠ haveĠaĠ healthĠ problem ,ĠtheyĠ expectĠ aĠ government - fund edĠ health Ġsystem ĠtoĠtakeĠ careĠ ofĠthem .ĠWhen ĠtheyĠareĠ unemploy ed ,ĠtheyĠ look ĠtoĠtheĠ government ĠtoĠ pay Ġthem âĢĶ and ĠtoĠt id eĠthemĠ overĠ untilĠtheyĠ findĠ anotherĠ job .ĊĠ IfĠ un employmentĠ isĠ statistic a\n",
      "\t Tokenized wiki:in Ġneed , Ġthey Ġvisit Ġa Ġgovernment Ġassistance Ġcenter . ĠWhen Ġthey Ġhave Ġa Ġhealth Ġproblem , Ġthey Ġexpect Ġa Ġgovernment - funded Ġhealth Ġsystem Ġto Ġtake Ġcare Ġof Ġthem . ĠWhen Ġthey Ġare Ġunemployed , Ġthey Ġlook Ġto Ġthe Ġgovernment Ġto Ġpay Ġthem âĢĶ and Ġto Ġtide Ġthem Ġover Ġuntil Ġthey Ġfind Ġanother Ġjob . Ċ ĠIf Ġunemployment Ġis Ġstat ist ica\n",
      "\t Unique Tokens no: {'s.', 'ilitaryĠ', '.ĠAnd', 'evidenceĠthatĠ', 'whenĠtheyĠ', 'nation', 'ĊĠTheĠ', ',ĠtheyĠ', 'contract', 'eĠthemĠ', 'domin', 'powerĠ', 'tionĠofĠ', 'edĠbyĠ', 'politicalĠ', 'overcomeĠ', 'principal', 'moreĠlikelyĠtoĠ', 'ans', 'y', 'helpful', 'hadĠbeenĠ', 'fromĠ', '.ĠPeopleĠ', 'likeĠtheĠ', '#Ġ', 'ĊĠ', 'et', 'or', 'everĠ', 'ĠtoĠt', 'lowerĠ', 'Ġm', 'growthĠ', 'WarĠ', 'off', '.ĠInĠfact,Ġ', 'hasĠbeenĠ', 'SometimesĠ', 'federalĠ', 'later,Ġ', 'visitĠ', 'aĠlong', 'findĠ', 'mayĠbeĠ', '.ĠS', 'anyĠ', 'acceptedĠ', 'consolid', 'uccessfulĠ', 'decentĠ', 'aĠwasĠ', 'lookĠ', 'ĠthanĠtheĠ', 'cent', 'ĠthemĠinĠ', 'ĠwasĠ', 'peopleĠareĠ', 'mentsĠ', 'NewĠ', 'EuropeanĠ', 'ederalĠ', 'alĠ', 'unemploy', 'ĠthereĠisĠ', 'busines', 'ST', 'confid', 'sĠwith', 'gu', 'appeared', 'problem', 'amountsĠofĠ', 'Ġcompet', 'help', 'ĠthatĠtheyĠ', 'sen', 'StatesĠ', 'enth', 'fullĠofĠ', 'ually', 'itiesĠofĠ', 'atĠtheĠ', '.ĠBut', 'Ġmin', 'volum', 'marriedĠ', 'expectĠ', 'edĠ', '200', 'dial', 'ousĠ', 'stat', 'inĠ', 'Govern', 'liveĠ', 'haveĠ', \"'Ġ\", 'and,Ġ', 'ĠtheyĠareĠ', 'longer', 'oney,Ġ', 'produceĠ', 'byĠtheĠ', 'incom', 'enormousĠ', 'bar', 'Help', 'sĠofĠtheĠ', 'particip', 'look', 'awayĠ', '.ĊĠ', 'y,Ġ', 'rav', 'right', 'exp', 'higher', 'es', 's,ĠandĠ', 'decisionsĠ', '.ĠP', 'ies,ĠandĠ', 'ofĠthemĠ', 'elec', 'action', 'careĠ', 'ityĠisĠ', 'involv', 'id', 'ement', 'er,Ġ', 'ate,Ġ', 'finally', 'e,ĠandĠ', 'IfĠ', 'cre', 'first', 'reasonĠtoĠ', 'co', 'ĠtoĠanĠ', 'center', 'resum', 'substit', 'assu', 'money', 'ationĠandĠ', 'forcesĠ', 'anotherĠ', 'expectĠtheĠ', 'lessĠ', 'grad', 'feetĠ', 'pastĠ', 'governmentĠ', 'ĠmoreĠ', '.ĠAr', 'ĠtheyĠ', 'numerousĠ', 'suspect', 'enĠtheĠ', 'tr', 'EuropeĠ', 'ĠtoĠtheĠ', 'health', 'problems.Ġ', 'job', 'cameĠ', 'incid', ',Ġ', 'forĠ', 'state.Ġ', 'forĠthem', 'author', 'isĠ', 'se', 'allĠtheĠ', 'quiteĠ', 'parentsĠ', 'ably,Ġ', 'ĠtoĠtakeĠ', 'esĠofĠ', 'spendĠ', 'haveĠaĠ', 'happilyĠ', 'larg', 'leadĠ', 'esĠinĠtheĠ', 'suchĠasĠtheĠ', 'spott', 'behind', 'from', 'enc', 'overĠtheĠ', 'it,Ġ', 'getĠ', 'nin', 'goodĠ', 'inĠtheĠt', 'high', 'fixĠtheĠ', 'ofĠtheĠ', 'atĠtheĠendĠofĠtheĠ', 'crimeĠ', 'cred', 'whoseĠ', 'ch', '.ĠAndĠ', 'GO', 'pay', 'wealth', 'result', 'withĠ\"', 'im', 'ĠtoĠ', 'es,Ġ', 'Or', '.ĠByĠ', 'ationĠofĠ', 'BU', 'canĠ', 'betterĠ', 'historyĠ', 'sĠtoĠ', 'pattern', 'farĠmoreĠ', 'individualĠ', 'offĠ', 'local', 'need', 'areĠ', 'YetĠ', 'central', 'parent', \"'sĠ\", 'measur', '.ĠTheyĠ', 'ator', 'en', 'bor', 'du', 'Betwe', 'ingĠtheĠ', 'lives', 'endedĠ', 'authorityĠ', 'famil', 'spentĠ', 'it', 'everyĠ', 'Americ', 'aĠf', 'aleĠ', 'followingĠ', 'enseĠofĠ', 'fund', 'ĠthereĠisĠnoĠ', 'aĠ', 'UnionĠ', 'And,Ġ', 'haveĠbeenĠ', 'localĠ', 'went', 'AllĠ', 'families', ':Ġ', 'kingdom', 'ifĠthereĠisĠaĠ', 'rat', 'if', 'reason', 'government', 'le', 'fromĠs', 'soundĠ', 'eight', 'SupremeĠCourtĠ', 'ofĠthem', 'PeopleĠ', 'ustom', 'INGĠ', 'aĠhugeĠ', 'leftĠ', 'afford', 'uteĠ', 'world,Ġ', 'startĠ', 'earn', 'sometimesĠ', 'ect', 'individual', 'ur', 'westernĠ', 'end', 'ies,Ġ', 'statisticallyĠ', 'fur', 'ometim', 'mark', 'eth', ',ĠthisĠ', 'andĠ', 'ĠtoĠthink', 'naturalĠ', 'onĠtheirĠ', 'y.Ġ', 'likeĠtheyĠ', 'i', 'count', 'inĠtheĠmiddleĠofĠtheĠ', 'peopleĠ', 'overĠ', 'employmentĠ', 'assistanceĠ', '.ĠN', 'imm', 'successfulĠ', 's,Ġ', '.ĠThen', 'intendedĠtoĠ', 'eĠtax', 'withĠsomeĠ', 'moneyĠ', 'familyĠ', 'alizedĠ', 'states', 's.ĠS', 'performanceĠ', 'aĠl', 'dis', 'Ġ', 'likelyĠtoĠbeĠ', '300Ġ', 'th', 'ationĠofĠtheĠ', 'moreĠ', 'sĠ(', ')ĠandĠ', 'areasĠ', 'ug', 'expand', 'TheĠ', '\"Ġ', 'aĠfew', 'But', 'UnitedĠStat', 'mostĠ', 'asĠaĠ', '7,Ġ', 'ĠthemĠ', 'ingsĠofĠ', 'major', 's.Ġ', 'wouldĠbeĠ', 'yearsĠ', 'inĠtheĠ', 'ifĠtheĠ', 'conclud', 'esĠthatĠtheĠ', 'able,Ġ', 'ationĠinĠtheĠ', 'familiesĠ', 'erĠandĠ', 'un', 'hand', ',ĠtheĠ', 'ast', 'centralĠ', 'untilĠtheyĠ', 'l', 'evenĠ', 'fall', 'furtherĠ', 'itor', 'eigh', 'way', 'able', 'e', 'acceptĠ', 'outĠ', '.ĠWhen', 'ut', 'largerĠ', 'whichĠ', 'canĠbeĠ', 'run', 'ĠthatĠtheĠ', '.ĠItsĠ', 'centr', 'wereĠ', 'helpĠ', 'strongĠ', 'ĠmembersĠ', 'School', 'buildingĠ', 'y.Ċ', 'centuryĠ', 'studyĠ', 'edĠwithĠ', 'ingĠofĠ', 'createdĠ', 'ĠtowardĠ', 'ĠtestĠ', 'ed', 'localĠc', 'flood', 'hood', 'directĠ', 'positionĠofĠ', 'backĠ', 'sĠwithĠ', 'ĠthereĠareĠ', 'doneĠ', 'marri', 'itĠ', 'redĠ', 'healthĠ', 'legis'}\n",
      "\t Unique Tokens wiki: {'Ġexpense', 'Ġfrom', 'Ġcentralized', 'Ġthink', 'Ġsuspect', 'ĠThe', 'Ġpower', 'ful', 'Ġareas', 'Ġanother', 'Ġhuge', 'Ġwith', 'Ġ2007', 'Ġcreated', 'Ġdialects', 'Ġat', 'ĠAmerica', 'ĠSometimes', 'Ġmarked', ',', 'ily', 'Ġd', 'Ġelection', 'Ġhistory', 'Ġexpand', 'Ġstate', 'istically', 'Ġfurther', 'Ġhapp', 'Ġmoney', 'Ġbusinesses', 'Ġcrime', 'Ġmay', 'Ċ', 'Ġmilitary', 'Ġfirst', 'Ġmore', 'Ġfeet', 'position', 'ings', 'Ġpolitical', 'Ġwere', 'ing', 'Ġcame', 'Ġdone', 'ugal', 'Ġbe', 'Ġif', '.\"', 'Ġyears', 'ĠYet', 'Ġstat', 'Ġout', 'Ġevidence', 'Ġminor', 'Ġparental', 'ĠOrleans', 'Ġdecent', 'Ġfew', 'arg', 'Ġperformance', 'Ġconcludes', 'Ġovercome', 'ING', 'Ġfar', 'Ġon', 'Ġspent', 'Ġmembers', 'ĠSuccess', 'Ġdisappeared', 'ĠSchools', 'US', 'Ġtax', 'Ġlikely', 'Ġdisaster', 'Ġaction', 'Ġwealth', 'Ġstable', 'Ġgovernment', 'Ġnineteenth', 'Ġtale', 'Ġgrowth', 'pro', 'Ġleft', 'Ġbetter', 'Ġcompetitor', 'Ġwhen', 'Ġmarried', 'Ġhigh', 'Ġmajor', 'Ġall', \"'s\", 'Ġare', 'Ġto', 'Ġhad', 'Ġback', 'me', 'Ġbuilding', 'Ġparents', 'Ġaccept', 'Ġwas', 'Ġpeople', 'Ġsome', 'ities', 'Ġthe', 'Ġsubstitute', 'Ġeighteenth', 'Ġlonger', 'Ġ(', 'Ġassured', 'Ġlike', 'Ġenormous', 'Ġproblems', 'ĠUnion', 'Ġspend', 'Ġcount', 'Ġimm', 'Ġlegislation', 'duce', 'Ġlater', 'ĠCourt', 'Ġlives', 'Ġhelp', 'ĠEurope', 'Ġnumerous', 'Ġoff', 'Ġthan', 'less', 'ĠEuropean', 'Ġlook', 'Ġtwentieth', 'Ġways', 'ĠBetween', 'ĠBut', 'ĠIf', 'Ġsound', 'Ġauthority', 'Ġfinally', 'Ġan', 'as', 'Ġintended', \"'\", 'Ġfind', 'ĠUnited', 'Ġlive', 'ous', 'Ġunemployed', 'Ġrun', 'Ġtide', 'ĠWhen', 'Ġjob', 'Ġgood', 'Ġresults', 'Ġincome', 'Ġsenators', 'Ġsuch', 'Ġdirect', 'Ġ#', 'Ġreason', 'which', 'Ġamounts', 'Ġaccepted', 'Ġand', 'Ġa', 'Ġcentral', 'Ġwould', 'when', 'Ġspot', 'Ġas', 'ĠSupreme', 'Ġrights', 'ĠWar', 'Ġdominate', 'ĠB', 'Ġhealth', 'Ġbeen', 'Ġlarger', 'Ġhands', 'Ġunemployment', 'Ġstates', 'uch', 'Ġfix', 'Ġtake', 'ĠNew', 'Ġevery', 'Ġim', 'funded', 'Ġlocal', 'Ġfall', 'Ġfollowing', 'Ġhigher', 'Ġcoincided', 'Ġno', 'Ġended', 'Ġthat', 'Ġcenter', 'rif', 'Ġor', 'Ġenl', 'Ġget', 'Ġstrong', 'Ġthey', 'ies', 'Ġthere', 'Ġin', 'Ġcreation', 'Ġeven', 'Ġover', 'ĠBy', 'Ġever', 'Ġconsolidation', 'ĠAnd', 'Ġfamilies', 'Ġfamily', 'Ġpay', 'Ġbar', 'Ġrav', 'Ġassistance', 'Ġproblem', 'Ġend', 'Ġpast', 'Ġhave', 'Ġit', 'ĠHelp', ':', 'European', 'ĠIn', 'Ġfor', 'Ġlun', 'Ġany', 'Ġearn', 'Ġpatterns', 'Ġquite', 'Ġthis', 'Ġstart', 'Ġlower', 'Ġrates', 'Ġkingdoms', 'Ġtoward', ')', 'Ġvisit', 'Ġmiddle', 'ĠPeople', 'Ġfull', 'T', 'Ġsuccessful', 'Ġnations', 'Ġcentury', 'um', 'Ġ\"', 'Ġfederal', 'Ġwhose', 'ĠGovernment', 'ĠPres', 'ĠThen', 'utable', 'Ġlong', 'ĠArg', 'uably', 'Ġneed', '.', 'Ġis', 'Ġcontract', 'Ġwestern', 'ty', 'Ġhas', 'Ġcan', 'Ġcustoms', 'ĠAll', 'Ġtest', 'Ġsometimes', 'Ġgradually', 'Ġbehind', 'Ġlead', 'Ġuntil', 'ĠStates', 'Ġmost', 'umin', 'Ġparticipation', 'Ġflooding', 'Ġprincipal', 'urable', 'Ġinvolvement', 'Ġdecisions', 'ĠNeighborhood', 'ates', 'Ġf', 'Ġnation', 'Ġafford', 'Ġby', 'Ġcredit', 'Ġindividual', 'Ġtrend', 'Ġfact', 'Ġvol', 'Ġexpect', 'Ġcare', 'Ġ300', 'Ġof', 'Ġforces', 'Ġworld', 'ĠThey', 'ĠGO', 'ĠIts', 'Ġhelpful', 'Ġconfidence', 'away', 'Ġstudy', 'Ġnatural'}\n",
      "==Same Author==\n",
      "Text 1: Jubal [...] past, MS; _AM_ does not indent line  \n",
      " 64c | beside?] beside – _MS_  \n",
      " 64d | god –] god, _MS, AM, MM_  \n",
      " 64e | skies –] skies, _MS_  \n",
      " 64f | aught else for] as other _MS, AM, MM_  \n",
      " 64g | _her]_ her _MS_ , _AM_ , _MM_  \n",
      " 64h | Buried within] And buried in _MS_  \n",
      " 64i | may sleep] that li\n",
      "\t Tokenized no:J ub alĠ [... ]Ġ past ,ĠM S ;Ġ _ AM _Ġ doesĠnotĠ ind entĠ lineĠ Ġ ĊĠ 64 cĠ |Ġ bes ide ? ]Ġ besideĠ âĢĵĠ _ MS _ ĠĠ ĊĠ 64 dĠ |Ġ godĠ âĢĵ ]Ġ god ,Ġ_ MS ,Ġ AM ,ĠM M_ ĠĠ ĊĠ 64 eĠ | Ġsk iesĠ âĢĵ ] Ġsk ies,Ġ _ MS _ ĠĠ ĊĠ 64 fĠ |Ġ aughtĠ elseĠ for ]Ġ asĠ otherĠ _ MS ,Ġ AM ,ĠM M_ ĠĠ ĊĠ 64 gĠ |Ġ _ her ] _Ġ herĠ _ MS _Ġ ,Ġ_ AM _Ġ ,Ġ_ MM _ ĠĠ ĊĠ 64 hĠ |Ġ Bur i edĠwith in ]Ġ AndĠ buri edĠinĠ _ MS _ ĠĠ ĊĠ 64 iĠ | ĠmayĠ sleep ] ĠthatĠ li\n",
      "\t Tokenized wiki:J ub al Ġ[...] Ġpast , ĠMS ; Ġ _ AM _ Ġdoes Ġnot Ġind ent Ġline Ġ Ġ Ċ Ġ64 c Ġ| Ġbeside ? ] Ġbeside ĠâĢĵ Ġ _ MS _ Ġ Ġ Ċ Ġ64 d Ġ| Ġgod ĠâĢĵ ] Ġgod , Ġ _ MS , ĠAM , ĠM M _ Ġ Ġ Ċ Ġ64 e Ġ| Ġsk ies ĠâĢĵ ] Ġsk ies , Ġ _ MS _ Ġ Ġ Ċ Ġ64 f Ġ| Ġa ught Ġelse Ġfor ] Ġas Ġother Ġ _ MS , ĠAM , ĠM M _ Ġ Ġ Ċ Ġ64 g Ġ| Ġ _ her ] _ Ġher Ġ _ MS _ Ġ, Ġ _ AM _ Ġ, Ġ _ MM _ Ġ Ġ Ċ Ġ64 h Ġ| ĠBur ied Ġwithin ] ĠAnd Ġburied Ġin Ġ _ MS _ Ġ Ġ Ċ Ġ64 i Ġ| Ġmay Ġsleep ] Ġthat Ġl i\n",
      "\t Unique Tokens no: {'sĠofĠtheĠ', 'for', 'ThisĠ', '.ĊĠ', 's,Ġ', 'right', 'age,Ġ', '73', 'age', 'ide', 'es,Ġ', 'past', 'tt', 'fru', 'Month', 'agu', 'M_', 'ĠtheirĠ', 'povert', 'fruit', 'hol', '75', 'loveĠandĠ', 'âĢĵĠ', 's]', 'withĠtheĠ', ';Ġ', 'ly', 'extĠ', 'herĠ', 's,', 'y', 'stormĠ', 'Ġs', 'Mr', '-w', 'love', '>ĊĠ', 'ind', 'hear', 'edĠtoĠ', 'OfĠ', \"'sĠ\", 'Pin', 'notesĠ', 'detail', 'meansĠ', '8Ġ', '_Ġ', 'besideĠ', 'ate', 'pot', 'thĠ', '#Ġ', 'TheĠ', 'genĠ', 'Ben', 'DecemberĠ', 'ĊĠ', 'cĠ', 'ĠandĠ', 'eĠ', ']ĠĠ', 'edĠwith', 'everĠ', 'Ġcorrect', 'andĠtheĠ', '[...', 'low', ',Ġ_', '.Ġ_', 'godĠ', 'ĠmayĠ', 'inĠtheĠ', '>Ġ', 'LoveĠ', 'Ġm', '9Ġ', 'ree', 'jĠ', '}Ġ', 'Atl', 'atch', \"y'sĠ\", 'hasĠbeenĠ', 'ine', '64', 'ageĠ', 'girl', 'eyes', '65', 'y,', 'whichĠareĠ', 'lieĠ', 'FranceĠ', 'cott', 'aj', 'ThatĠ', 'ineĠ', 'al_', 'aĠ', 'e.Ġ', 'om', '186', '1)Ġ', 'SantaĠ', 'el', 'referredĠtoĠ', ',ĠtheĠ', 'âĨĴ', 'edg', 'ro', 'HolyĠ', '187', 'bĠ', 'hill', 'eyes,Ġ', 'outh', 'ar', 'obviousĠ', 'noteĠ', 'grav', 'gaveĠ', 'wood', 'aboveĠ', 'lĠ', 'hĠ', 'ksĠ', 'kĠ', 'pe', ']Ġ', 'taĠ', 'Ã¤', ',ĠM', 'est', 'in,Ġ', 'entĠ', '67', 'dĠ', 'wardĠ', 'afterĠ', 'se-', '7', 'god', 'giv', 'ĠthatĠ', 's]Ġ', ',Ġ', 'World', 'old', 'alĠ', '-h', 'tĠ', '-t', 'eĠt', 'forĠ', 'r', 'hasĠnoĠ', 'giveĠ', '>,Ġ', 'ros', 'hillĠ', ',ĠC', \"'ĠĠ\", 'Mari', '-th', 's_', 'Bo', 'isĠ', 'S', 'ownĠ', 'Rh', 'MaryĠ', 'onĠtheĠ', 'mĠ', 'her,Ġ', 'ĠĠ', 'se', 'edĠinĠ', 'a,', 'own', 'OldĠ', 'hungĠ', '.Ġ', 'comparedĠ', 'respectivelyĠ', 'soul', 'sleep', 'th,Ġ', 'toĠ', '4Ġ', 'anticĠ', 'otherĠ', 'Mel', '[', 'head', 'ĠwhichĠ', 'elseĠ', 'bes', '.\"Ġ', 'pl', 'loveĠ', 'ag', '/Ġ', 'worldĠ', '####', 'anc', 'ep', '.ĠSeeĠ', 'edition', 'edĠ', 'boyĠ', 'âĨ', 'belowĠ', 'iesĠ', 'asĠ', 'ies,Ġ', 'Ġco', \"en'sĠ\", 'doesĠnotĠ', '|Ġ', 'it,Ġ', 'fĠ', 'JanuaryĠ', 'e-', 'e.', 'reeĠ', 'e,', 'inĠ', '---', 'in', 'iĠ', 'ĠshowsĠ', 'andĠ', 'ĠtheĠ', 'ody', 'buri', 'storm', 'AndĠ', 'Ġmon', 'gĠ', 'aughtĠ', \"'Ġ\", 'slo', 'y.Ġ', 'editionĠ', 'ste', 'Dut', 'Ġc', 'potent', 'ni', 'byĠtheĠ', 'mistakeĠ', 'ow', 'dut', 'F', 'girlsĠ', 'bothĠ', 'Bur', 'ĠwithinĠ', 'odyĠ', 'copy', 'sĠ', 'lineĠ'}\n",
      "\t Unique Tokens wiki: {'Ġcopy', 'Ġold', 'Ġ75', 'arth', 'Ġmen', 'Ġ|', 'ĠBur', 'ĠJub', ')', 'Ġa', 'Ġand', 'ied', 'Ġgate', 'Ġgive', 'Ġhead', 'uty', 'Ġgave', 'Ġas', 'ĠThe', 'ĠFrance', 'Ġ<', 'ĠMonthly', 'ĠMary', 'ĠMrs', 'â', 'Ã¤r', 'ĠThat', 'ĠF', 'Ġsouth', 'ent', 'Ġbeen', 'ans', 'ot', 'Ġgod', 'Ġhe', 'ĠâĢĵ', 'Ġrespectively', 'jar', 'Ġare', 'Ġto', 'ught', 'ĠBoy', 'ĨĴ', '.]', 'both', 'Santa', 'Ġdoes', 'al', 'b', 'Ġ1878', 'Ġ1874', 'Ġmonks', 'ĠSee', 'Ġcottage', \"Ġ'\", 'Ġslope', 'me', 'ĠMaria', 'tree', 'France', 'ĠOpen', 'Atlantic', 'j', 'Ġbelow', 'rose', 'Ġmelody', 'Ġfruit', 'Ġlow', 'That', 'Ġbeside', 'Ġcompared', 'that', 'Ġ{', 'Ġ;', 'Ġmay', 'ĠOld', 'Ċ', 'Ġlie', '.', '71', 'Ġis', 'Ġnotes', 'Ġ[...]', '}', 'Ġgirls', 'ĠM', 'Ġdetails', 'Ġh', 'Ġabove', 'Ġown', 'Ġthe', 'Ġno', 'Ġ[', 'Ġhas', 'woods', 'Ġthat', 'Ġshows', 'Ġother', 'Ġeyes', 'ĠRhine', 'ged', 'l', 'Ġplag', 'December', 'e', 'Ġeditions', 'm', 'Ġright', 'ĠCross', '.\"', 'Ġelse', 'ct', 'Ġhill', 'ĠMelody', 'note', 'Ġ-', 'Ġcorrected', 'ĠWorld', 'Ġher', 'Ġind', 'Ĩ', 'Ġsoul', 'Ġ74', 'ies', 'Ġline', 'Ġmistake', 'Ġ1869', 'Ġin', 'ĠD', 'f', 'Ġsouls', 'ched', 'Ġburied', 'Ġ65', 'c', 'Ġpoverty', 'ĠMS', 'Ġby', 'Ġon', 'ĠSan', 'Ġsouthwest', 'ĠThis', 'Ġrose', 'd', 'Ġobvious', 'Ġnot', 'Ġomn', 'ĠAM', 'Ġ,', 'Ġduty', 'Ġ64', 'Ġwoods', 'Ġwithin', 'ened', 'Of', 'Ġgrave', 'ĠAnd', 'Ġ67', 'Ġlove', 'Ġstorm', 'B', 'Ġ73', 'Ġtree', 'Ġ/', 'ĠPine', 'Ġcows', 'ĠJanuary', 'Ġof', 'Ġsleep', '#', 'ĠLove', 'Ġpast', '--', 'Ġafter', 'ĠHoly', 'world', 'Ġboy', 'ip', 's', 'Ġfor', 'text', 'Ġreferred', 'Ġ#', 'Ġsteep', 'Ġâ', 'Ġhollow', 'Ġedition'}\n",
      "Text 2: fields,\n",
      " Instead of sipping at the heart of flowers,\n",
      " Poising in sunshine, fluttering towards its bride,\n",
      " Should fast and speculate, considering\n",
      " What were if it were not? or what now is\n",
      " Instead of that which seems to be itself?\n",
      " Its deepest wisdom surely were to be\n",
      " A sipping, marrying, blue-winge\n",
      "\t Tokenized no:field s, ĊĠ In steadĠofĠ sip pingĠ atĠtheĠ heartĠofĠ flow ers ,ĊĠ Po is ingĠinĠ sun sh ine,Ġ flutter ingĠtowardsĠ itsĠ br ide ,ĊĠ ShouldĠ fastĠandĠ specul ate,Ġ consider ing ĊĠ WhatĠ wereĠ ifĠitĠ wereĠ not ?Ġ orĠwhatĠ nowĠ is ĊĠ In steadĠ ofĠthatĠ whichĠ seemsĠtoĠbeĠ itself ? ĊĠ ItsĠ deep estĠ wis dom Ġs urelyĠ wereĠtoĠ be ĊĠ AĠs ipp ing,Ġ m arry ing,Ġ blue -w ing e\n",
      "\t Tokenized wiki:fields , Ċ ĠInstead Ġof Ġs ipping Ġat Ġthe Ġheart Ġof Ġflowers , Ċ ĠPo ising Ġin Ġsun shine , Ġfl utter ing Ġtowards Ġits Ġbride , Ċ ĠShould Ġfast Ġand Ġspec ulate , Ġconsidering Ċ ĠWhat Ġwere Ġif Ġit Ġwere Ġnot ? Ġor Ġwhat Ġnow Ġis Ċ ĠInstead Ġof Ġthat Ġwhich Ġseems Ġto Ġbe Ġitself ? Ċ ĠIts Ġdeep est Ġwisdom Ġsure ly Ġwere Ġto Ġbe Ċ ĠA Ġs ipping , Ġmarrying , Ġblue - wing e\n",
      "\t Unique Tokens no: {',Ġ\"', 'more,Ġ', 'put', 'liv', 'argument', 'with', 'ĊĠTheĠ', 'NoĠ', 'Ġthink', 'pass', 'ing,Ġ', 'spark', 'es,', 'ad', 'standard', 'ent', 'while', 'par', 'deep', '\"IĠ', 'y', 'ShouldĠ', 'love', 'e)', 'delay', 'Ob', 'sac', 'OfĠ', 'wis', 'isĠheĠ', 'ise', 'enumĠ', 'ĊĠ', 'orĠotherĠ', 'or', 'makeĠ', 'fineĠ', 'under', 'ingĠtheirĠ', 'ingĠtoĠbeĠ', 'sh', 'day', 'icĠandĠ', 'graceĠ', 'pain', 'seemsĠtoĠbeĠ', 'inĠyourĠ', 'itsĠ', 'decimalĠ', 'by', 'unĠ', 'bl', 'In', 'estĠ', 'want', 'Conclusion', 'ThatĠ', 'pray', 'clust', 'ir', 'is', \"s'Ġ\", 'anyĠ', 'If', ':ĊĠ', 'char', 'WhoĠ', 'icallyĠ', 'word', 'analog', 'beĠ', 'sĠyouĠ', 'lip', ',ĠisĠ', 'agree', 'flow', 'Mak', 'phrase', 'bru', 'ĠthatĠ', 'ice', 'ivesĠ', 'regul', 'analysis', 'ence,Ġ', 'urelyĠ', 'notĠ', 'exist', ',ĊĠ', 'ace', '125', 'Ġsup', 'figureĠ', 'flu', 'bb', 'breast', 'AnĠ', 'Forc', 'x', 'doctr', 'or,Ġ', 'your', 'atĠtheĠ', 'forĠmyĠ', 'orĠ', 'homogen', 'absolut', 'AĠs', 'edĠ', 'molec', 'inĠ', 'fellow', 'ship', 'fromĠtheĠ', 'resist', 'cal', 'Analys', 'ump', 'dotĠ', 'ĠwouldĠ', 'esĠ', 'borneĠ', 'erĠ', 'nameĠofĠ', 'content', 'noĠ', 'objectsĠ', 'hint', 'sĠofĠtheĠ', 'dark', 'ipp', 'worseĠ', 'y,Ġ', '.ĊĠ', 'OurĠ', 'icĠ', 'ill', 'jud', 'WhatĠ', 'eĠthatĠ', 'cu', 'pre', 'We', 'ĠtheirĠ', 'ByĠ', 'oceanĠ', 'âĢĵĠ', 'ĠmustĠ', 'field', ';Ġ', 'AĠc', 'blue', 'itself', 'ĠthoseĠwhoĠ', 'yĠofĠ', 'Tak', 'IĠ', 'ItsĠ', 'useĠ', 'ate,Ġ', 'isĠnoĠ', 'flutter', 'needsĠ', 'relationĠ', 'flyĠ', ':ĠaĠ', 'justĠthatĠ', 'Phys', 'stay', 'eĠtoĠ', 'cĠ', 'lookingĠ', 'sĠofĠaĠ', 'reachedĠ', 'For', 'steadĠ', 'isingĠ', 'law', 'hasĠ', 'clusion', 'known', 'AĠ', 'aĠc', 'posed', 'lessĠ', 'find', 'SuchĠ', 'HasĠ', 'influ', 'byĠ', 'SinceĠ', 'other', 'be', 'insightĠ', 'ine,Ġ', 'mightĠ', 'ereĠ', 'm', 'wardĠ', 'allĠ', 'suck', 'InĠ', 'all', ',Ġ', 'FinalĠ', 'letter', 'down', 's,ĠtheĠ', 'firstĠ', 'heartĠofĠ', 'âĢĵ', 'isĠ', 'S', 'se', 'sun', 'op', 'asĠtheĠ', 'saveĠ', 'sign', 'Will', 'ĠtoĠbeĠ', 'butĠ', 'aĠm', 'reach', 'WhateverĠ', 'Po', 'yourĠs', 'ĠtakingĠ', 'byĠaĠ', 'edĠatĠ', 'N', 'missingĠ', 'fĠ', 'it,Ġ', 'propert', 'carryĠ', 'entalĠ', 'po', 'rightĠ', 'withoutĠ', 'negativ', 'F', 'oc', 'not', 'Bur', ',ĠthatĠ', 'ĠthisĠ', 'easyĠ', 'syl', 'fro', 'forĠtheĠ', 'pingĠ', 'ĠtoĠ', 'es,Ġ', 'orĠwhatĠ', 'shownĠ', '?Ġ', 'her', '.ĠThatĠ', 'qu', 'ButĠ', 'ationĠonĠ', 'eĠthisĠ', 'ence.', 'sĠtoĠ', 'pattern', 'aid', 'conversation', 'earnĠ', 'ach', 'need', '-w', 'Ill', 'All', 'back', \"'sĠ\", 'ly,Ġ', ';ĊĠ', 'bringsĠ', 'weĠ', 'youĠcallĠ', 'sharpĠ', 'ers', 'arry', 'bubble', 'ingĠtheĠ', 'andĠtheĠ', 'Pers', 'struggleĠ', 'ofĠthatĠ', 'imag', ',ĠanĠ', 'forĠher', '>Ġ', 'ist', 'ine', 'withinĠtheĠ', 'mean', 'metaph', 'ingĠmeĠ', 'erĠofĠ', 'What', 'equival', 'food', 'philos', 'aĠ', 'us-', 'Be', 'butter', 'weightĠ', ',ĠwhoĠ', 'FromĠ', 's)', 'sĠwhereĠ', 'barĠ', 'bĠ', 'er', ':Ġ', 'ĠwillĠ', 'esĠtheĠ', 'weight', 'E', '.h', 'eningĠ', 'intenseĠ', 'ThereĠ', 'Re', 'r', 'Ġcarri', 'uni', 'inglyĠ', 'standĠ', 'IĠwillĠ', 'Ġten', 'bound', 'musicĠ', 'forĠherĠ', '\"', 'toĠ', 'startĠ', 'pl', 'onĠ', 'NotĠ', 'e,ĠaĠ', 'humanĠ', 'relativ', 'band', 'dom', 'specul', 'e,', 'orĠanyĠ', 'andĠ', 'yingĠ', 'mor', 'BeingĠ', 'ingĠ', 'withĠ', 'ifĠitĠ', 'yourĠ', 'wereĠtoĠ', 'br', '(TheĠ', 'OnĠ', 'strength', 'isĠnot', 'sĠ', 'bu', 'room', 'GoodĠ', 'shr', 'hold', 'ĠthreeĠ', 'standingĠ', 'ide', 'ule', 'youĠtheĠ', 'showĠ', 'Wh', 'This', 'blingĠ', 'strictĠ', 'loadedĠ', 'ingĠthroughĠ', 'knowĠ', 'oneĠ', 'phys', 'consider', 'sip', 'Ġcon', 's,', 'T', 'dis', 'Ġ', 'space', 'gr', 'th', 't,Ġ', 'figuredĠ', 'steadĠofĠ', 'ance', 'ingĠaĠ', 'youĠ', '<', 'TheĠ', 'root', 'mostĠ', 'lateĠ', 'actedĠ', 'demand', 'asĠaĠ', 'ingĠinĠ', 'fastĠandĠ', 'ofĠ', 's.Ġ', 'atesĠtheĠ', 'worth', 'W', 'solelyĠ', 'inĠtheĠ', 'That', 'basisĠ', 'intoĠ', 'sole', 'Which', '.g', 'putĠ', 'MayĠ', 'edĠinĠtheĠ', 'judic', 'Pri', 'sortĠ', 'comeĠ', 'log', 'un', 'enceĠ', 'aĠmanĠ', 'differenceĠ', 'ofĠtheirĠ', 'againstĠ', 'sĠIĠ', 'spareĠ', ',ĠtheĠ', 'La', 'idd', 'filledĠ', 'WithĠ', 'ish', 'nowĠ', 'meĠ', 'name', 'etĠ', 'of', 'WouldĠ', ',ĠIĠ', 'stĠ', 'fly', 'dimin', 'dĠ', 'ableĠ', 'G', 'ut', 'whichĠ', 'bet', 'n', 'painĠ', 'add', 'pit', 'ofĠyourĠ', 'wereĠ', 'gras', 'ChurchĠ', 'go,Ġ', 'own', 'eĠandĠ', 'Wr', 'remov', 'alpha', 'y.Ċ', 'say,Ġ', 'rhetor', 'ingĠofĠ', 'trueĠ', ',ĠtoĠ', 'persist', 'joy', '(noĠ', 'PutĠ', 'protest', 'in', 'ĠtheĠ', 'AndĠ', 'each', 'choic', 'ledĠ', 'aĠth', 'oughtĠ', 'ingĠtowardsĠ', 'itĠ', 'fel'}\n",
      "\t Unique Tokens wiki: {'ĠWhat', 'Ġfrom', 'ishing', 'ĠThe', 'Ġdemands', 'Ġbackward', 'ping', 'Ġwith', 'Ġgrace', 'Ġstrengthening', 'ĠâĢĵ', 'shine', 'Ġat', 'Ġsolely', 'Ġprayer', 'ĠFor', 'Ġspeculation', 'ulate', 'Ġcluster', 'ĠG', 'Ġsy', 'istically', 'Ġmetaph', 'Ġconversation', 'Ċ', 'Ġlaw', 'Ġfirst', 'Ġmore', 'Ġanalysis', 'ĠAnalysis', 'Ġjust', 'und', 'Ġcome', 'Ġwere', 'Ġsuc', 'Ġdeep', 'Ġblown', 'Ġnote', 'Ġwhich', 'Ġbe', 'Ġun', 'Ġif', 'Ġmeanwhile', 'ering', 'ĠPriest', 'Ġbasis', 'Ġchar', 'Ġdark', 'ould', 'rought', 'ĠMaking', 'Ġpun', 'Ġthose', 'Ġbutterfly', 'Ġwithout', 'Ġpain', 'ubb', 'Ġon', 'ĠWe', 'ured', 'Ġweight', 'Ġadd', 'Ġbride', 'Ġflux', 'Ġlove', 'Ġwe', 'Ġsay', 'Ġneeds', 'Ġabsolute', 'Ġtaking', 'Ġocean', 'Ġthrough', 'ipping', 'Ġmissing', 'Ġc', 'ĠCon', 'ker', 'ĠGood', 'Ġarguments', 'Ġstrict', 'ĠSince', 'Ġlooking', 'again', 'imal', 'Ġfine', 'Ġphys', 'ence', 'Ġits', 'ĠThat', 'Ġ125', 'Ġknown', 'ĠSuch', 'ying', 'Ġall', 'Ġill', 'Ġmeet', 'Ġconclusions', 'There', 'Ġthree', 'Ġto', 'Ġdot', 'Ġsign', 'Ġreached', 'Ġhuman', 'ors', 'Ġreach', 'Ġfl', 'Ġlate', 'Ġyour', 'wise', 'Ġexistence', 'Ġworse', 'Ġbound', 'Ġsort', 'ĠBeing', 'Ġinsight', 'Ġimages', 'Ġproperty', 'Ġdoctrine', 'Ġconsidering', 'Ġsp', 'Ġband', 'ĠFinal', 'ara', 'ĠHas', 'Ġpers', 'Ġthe', 'ĠReason', 'Ġ(', 'Ġother', 'Ġphrase', 'Ġprotest', 'Ġeasy', 'Ġshown', 'Ġliving', 'Ġdimin', 'inate', 'Ġthin', 'Ġmusic', 'ĠPers', 'Ġput', 'less', 'Ġqu', 'ac', 'Ġsun', 'ĠBut', 'ĠLa', 'Ġinto', 'Ġbl', 'ĠIf', 'Ġrelation', 'Ġan', 'ĠThis', \"'\", 'che', 'Ġshr', 'Ġfind', 'ĠTeaching', 'Ġwithin', 'ity', 'll', 'Ġscale', 'Ġfelt', 'Ġfigure', 'Ġacted', 'ĠWhy', 'Ġmy', 'Ġletters', 'Ġresist', 'og', 'Ġhold', 'ru', 'Ġknow', 'Ġand', 'Ġa', 'Ġut', 'Ġwould', 'Ġas', 'Ġyou', 'Ġitself', 'Ġtrue', 'ĠPut', 'Ġrooted', 'ives', 'Ġb', 'Ġrhetoric', 'ĠOur', 'Ġp', 'isting', 'ĠWh', 'Ġsure', 'Ġuse', 'en', 'utter', 'ade', 'Ġdays', 'Ġintense', 'fields', 'ld', 'Ġfood', 'Ġdec', 'atever', 'cing', 'Ġstanding', 'Ġwill', 'Ġno', 'ĠIl', ';', 'ĠWere', 'ĠE', 'Ġthat', 'ĠWho', 'Ġalphabet', 'no', 'Ġor', 'Ġbent', 'Ġroom', 'Ġspark', 'ich', 'Ġsees', 'Ġequival', 'Ġright', 'Ġsave', 'Ġbreast', 'Ġjoy', 'ĠInstead', 'Ġwants', 'Ġworthy', 'Ġdis', 'clusions', 'Ġsac', 'Ġregul', 'Ġin', 'Ġshow', 'Ġwho', 'ĠNo', 'Ġcarry', 'Ġfellowship', 'Ġmother', 'Ġcarries', 'Ġbrings', 'Ġmake', 'ĠBy', 'wing', 'The', 'ĠAnd', 'Ġmarrying', 'Ġtenor', 'Ġsharp', 'Ġbar', 'ĠNor', 'Ġcalling', 'ĠA', 'Ġtowards', 'istence', 'Ġspare', 'Ġit', 'Ġbubble', 'ĠOb', 'Ġir', 'ĠIn', 'ogene', 'Ġfor', 'ĠSaid', 'Ġpl', 'Ġcontent', 'Ġany', 'Ġearn', 'ĠBur', 'ĠWill', 'Ġstart', 'Ġstays', 'Ġbut', 'int', 'eless', ')', 'Ġagree', 'Ġgr', 'Ġdelay', 'Ġ<', 'ocus', 'Ġthinking', 'Ġflowers', 'ly', 'Ġhe', 'um', 'Ġ\"', 'Ġinflux', 'Ġrelative', 'ĠNot', 'Ġwhere', 'ĠChurch', 'Ġstand', 'Ġpattern', 'Ġmetaphor', 'Ġstandard', 'rid', 'ĠFed', 'Ġwisdom', 'Ġhom', 'Ġpassing', 'Ġmolecules', '.', 'Ġprejudice', 'Ġneed', 'Ġis', 'Ġloaded', 'Ġfilled', 'Ġh', 'Ġn', 'Ġhas', 'ĠMay', 'Ġphilosophers', 'Ġmight', 'ĠAll', 'Ġsupposed', 'I', 'ĠOf', 'Ġfig', 'Ġanalog', 'ips', '>', 'able', 'ental', 'Ġnow', 'Ġmust', 'ising', 'Ġher', 'Ġdown', 'Ġname', 'ĠPhys', 'led', 'Ġdance', 'Ġone', 'Ġspec', 'Ġtheir', 'ates', 'ĠFrom', 'Ġchoice', 'ĠShould', 'Ġnegative', 'Ġblue', 'Ġseems', 'ov', 'Ġby', 'Ġdifference', 'ĠTakes', '-', 'Ġfro', 'Ġnot', 'Ġstruggle', 'ĠPo', 'rem', 'Ġl', 'bling', 'borne', 'most', 'ed', 'ĠAn', 'ĠOn', 'Ġof', 'Ġword', 'Ġfast', 'ĠIts', 'ĠW', 'thy', 'ip', 'Ġobjects', 'Ġheart', 'ĠWith', 'Ġman', 'umps', 'ĠI'}\n",
      "==not Same Author==\n",
      "Text 1: Hi,\n",
      "     I am using fligthgear 0.9.4. I am having three queries.\n",
      "1.  For scenery database  I have downloaded flightgear world scenery\n",
      "which is in *.btg file format.I would like to know why *.btg files are\n",
      "called within *.stg file.\n",
      "2.  Can I use Multi Gen creator which gives *.flt files to model the\n",
      "\n",
      "\t Tokenized no:Hi ,ĊĠĠĠĠĠ IĠamĠusingĠ fl ig th gearĠ 0. 9. 4 .ĠIĠamĠ having ĠthreeĠ queries .Ċ 1 .ĠĠ For Ġsc ener yĠ databaseĠ Ġ IĠhaveĠ downloadedĠ flight gear ĠworldĠ sc ener yĊ whichĠisĠ inĠ *. bt gĠ fileĠ format . IĠwouldĠlikeĠtoĠ knowĠwhyĠ *. bt gĠ filesĠ areĊ calledĠ withinĠ *. st gĠ file .Ċ2 .ĠĠ CanĠ IĠuseĠ Mult iĠ G enĠ cre ator ĠwhichĠ givesĠ *. fl tĠ file sĠtoĠ model ĠtheĊ\n",
      "\t Tokenized wiki:H i , Ċ Ġ Ġ Ġ Ġ ĠI Ġam Ġusing Ġfl ig th ge ar Ġ0 . 9 . 4 . ĠI Ġam Ġhaving Ġthree Ġqu eries . Ċ 1 . Ġ ĠFor Ġscenery Ġdatabase Ġ ĠI Ġhave Ġdownload ed Ġflight ge ar Ġworld Ġscenery Ċ which Ġis Ġin Ġ* . b t g Ġfile Ġformat . I Ġwould Ġlike Ġto Ġknow Ġwhy Ġ* . b t g Ġfiles Ġare Ċ called Ġwithin Ġ* . st g Ġfile . Ċ 2 . Ġ ĠCan ĠI Ġuse ĠMulti ĠGen Ġcreator Ġwhich Ġgives Ġ* . fl t Ġfiles Ġto Ġmodel Ġthe Ċ\n",
      "\t Unique Tokens no: {'s.', 'fileĠ', 'withinĠ', 'ator', 'file', 'filesĠ', 'areĊ', 'having', 'flight', 'cre', 'ĠcanĠ', 'ĠwhichĠ', 'Ġsc', 'ĠthreeĠ', 'ĠtheĊ', 'givesĠ', 'ĠworldĠ', '.ĠĠ', '0.', 'knowĠwhyĠ', 'Hi', 'CanĠ', 'rainĠ', 's,Ċ', 'ener', 'object', 'For', 'regard', 'model', 'G', 'databaseĠ', 'IĠhaveĠ', 'yĠ', 'iĠ', 'enĠ', 'inĠ', 'tĠ', 'downloadedĠ', 'bt', 'andĠ', '?Ċ', '.ĠWhatĠ', 'sĠtoĠ', 'WhetherĠ', '*.', 'gĠ', ',ĊĠĠĠĠĠ', 'files', 'IĠuseĠ', 'IĠamĠusingĠ', 'IĠwouldĠlikeĠtoĠ', 'calledĠ', '9.', 'areĠtheĠ', '.Ċ2', 'gear', 'gearĠ', 'supportedĠbyĠ', 'Mult', 'importĠ', '.Ċ', 'whichĠisĠ', 'format', 'sĠ', 'yĊ', 'sc', '.ĠIĠamĠ', 'queries', '.ĊĊ'}\n",
      "\t Unique Tokens wiki: {'ĠWhat', 'b', 'ĠMulti', 'Ġimport', 'Ġformat', 'ĠFor', 'Ġ*', 'Ġfl', 'Ġuse', 'Ġscenery', 'H', 'ar', 'Ġflight', 'I', ',', 'ther', 'which', 'Ġwhich', 'Ġwithin', 'g', 'Ġfiles', 'Ġknow', 'Ġsupported', 'Ġand', 'Ġ0', 'Ġdatabase', 'Ġwould', 'reg', 'Ġwhy', 't', 'ed', 'Ġlike', '?', 'rain', '2', 'Ġqu', 'Ġfile', 'called', 'Whe', 'Ċ', 'ge', 'Ġhaving', 'ĠCan', 'Ġcreator', 'Ġworld', 'Ġis', 'Ġhave', 'Ġam', 'Ġby', 'Ġmodel', 'ĠGen', 'Ġin', 'Ġobjects', 'i', 'Ġthe', 'Ġformats', 'Ġdownload', 'Ġare', 'Ġthree', 'Ġto', 'ĠI', '9', 'Ġusing', 'ards', 'Ġgives', 'Ġcan', 'eries'}\n",
      "Text 2: Hi,\n",
      "\n",
      "There is a problem only in GeoPackage layers.\n",
      "\n",
      "I have realized that when you put any expression in Provider feature filter\n",
      "(like \"id\" > 1), Layer is loaded successfully according to filter.\n",
      "\n",
      "On the other hand, identifying any feature on this layer, gives wrong\n",
      "results. There is a sequence shift\n",
      "\t Tokenized no:Hi,ĊĊ ThereĠisĠaĠ problemĠ onlyĠinĠ Geo Pack ageĠ lay ers .ĊĊIĠhaveĠ realizedĠthatĠ whenĠyouĠ putĠ anyĠ expressionĠ inĠ Provid erĠ featureĠ filter Ċ( likeĠ\" id \"Ġ >Ġ 1),Ġ Lay erĠisĠ loadedĠ successfullyĠ accordingĠtoĠ filter .ĊĊ OnĠtheĠotherĠ hand,Ġ ident ifyingĠ anyĠ featureĠ onĠthisĠ lay er,Ġ givesĠ wrong Ċ result s.Ġ ThereĠisĠaĠ sequenceĠ shift\n",
      "\t Tokenized wiki:H i , Ċ Ċ There Ġis Ġa Ġproblem Ġonly Ġin ĠGeo P ack age Ġlayers . Ċ Ċ I Ġhave Ġrealized Ġthat Ġwhen Ġyou Ġput Ġany Ġexpression Ġin ĠProv ider Ġfeature Ġfilter Ċ ( like Ġ\" id \" Ġ> Ġ1 ), ĠLay er Ġis Ġloaded Ġsuccessfully Ġaccording Ġto Ġfilter . Ċ Ċ On Ġthe Ġother Ġhand , Ġidentifying Ġany Ġfeature Ġon Ġthis Ġlayer , Ġgives Ġwrong Ċ res ults . ĠThere Ġis Ġa Ġsequence Ġshift\n",
      "\t Unique Tokens no: {'Ċ(', 'ident', '\"Ġ', 'likeĠ\"', '1),Ġ', '.ĊĊInĠ', 'result', 'WithoutĠ', 'Provid', 'ers', 'Lay', 'lay', 'whenĠyouĠ', 'givesĠ', 'Geo', 'wrong', 'Bon', '.ĠĠThisĠ', 's.Ġ', 'afterĠ', 'realizedĠthatĠ', 'Gir', 'Hi,ĊĊ', '>Ġ', 'onĠthisĠ', 'Regards,ĊĊ', 'onlyĠinĠ', 'ifyingĠ', 'inĠ', 'erĠisĠ', 'loadedĠ', 'featureĠ', 'OnĠtheĠotherĠ', 'n', 'ageĠ', 'problem', 'filter', 'putĠ', 'hand,Ġ', 'problemĠisĠ', 'ĠthereĠisĠnoĠ', 'ingĠ', 'ThereĠisĠaĠ', 'shift', 'filterĠ', 'onĠtheĠ', 'successfullyĠ', 'aĠ', 'er,Ġ', 'expressionĠ', 'Pack', 'resultsĠareĠ', 'problemĠ', 'OK', 'sequenceĠ', 'erĠ', 'anyĠ', 'occur', '.ĊĊIĠhaveĠ', 'on', '.ĊĊ', 'accordingĠtoĠ'}\n",
      "\t Unique Tokens wiki: {'\"', 'er', 'On', 'like', 'P', 'ĠOK', 'ĠProv', 'H', 'I', 'Ġonly', '),', 'Ġthis', 'ĠG', 'Ġshift', ',', 'ards', 'ĠGeo', 'iron', 'Ġidentifying', 'age', 'a', 'Ġa', 'Ġwrong', 'Ġocc', 'Ġlayer', 'out', 'ĠThere', 'With', '(', 'ĠBonn', 'Ġwhen', 'Ġyou', 'Ġfilter', 'Ġon', 'Ġrealized', 'Ġput', 'Ġresults', 'ults', 'Ġthat', 'ack', 'Ġ>', 'Ġsequence', '.', 'ĠLay', 'Ġis', 'Ġproblem', 'Ġaccording', 'Ġ1', 'Ġafter', 'Ġhave', 'Ġloaded', 'In', 'uring', 'Ġany', 'Ġthere', 'Ġlayers', 'Ġ', 'Ġin', 'Ġ\"', 'Ġversion', 'Ġhand', 'i', 'Ġthe', 'Ġno', 'ider', 'Ġare', 'There', 'res', 'Ġto', 'Reg', 'Ġsuccessfully', 'Ġfeature', 'Ġgives', 'Ġexpression', 'Ġother', 'ĠThis'}\n",
      "==not Same Author==\n",
      "Text 1: Thanks <PERSON>, and the others on this list that raised the issue.  I\n",
      "managed to nuke the MBL database here before any customers complained :)\n",
      "\n",
      "One thing I'd recommend to others though is to check your logs to see how\n",
      "many emails get caught by MBL (other than MBL_144360) - in my case there\n",
      "were non\n",
      "\t Tokenized no:ThanksĠ < P ERS ON > ,ĠandĠtheĠ othersĠ onĠthisĠ list ĠthatĠ ra i sedĠtheĠ issu e.ĠĠ IĊ managedĠtoĠ n uk eĠtheĠ MB LĠ databaseĠ hereĠ beforeĠ anyĠ customersĠ compl ainedĠ :)ĊĊ On eĠthingĠ I'dĠ recommend ĠtoĠ others ĠthoughĠ isĠtoĠ checkĠ yourĠ log sĠtoĠ seeĠ how Ċ manyĠ email sĠ getĠ caughtĠ byĠ MB L Ġ( otherĠthanĠ MB L_ 14 43 60 )Ġ-Ġ inĠmyĠ cas eĠthere Ċ wereĠ non\n",
      "\t Tokenized wiki:Th anks Ġ< P ERS ON > , Ġand Ġthe Ġothers Ġon Ġthis Ġlist Ġthat Ġraised Ġthe Ġissue . Ġ ĠI Ċ man aged Ġto Ġn uke Ġthe ĠM BL Ġdatabase Ġhere Ġbefore Ġany Ġcustomers Ġcomplained Ġ: ) Ċ Ċ One Ġthing ĠI 'd Ġrecommend Ġto Ġothers Ġthough Ġis Ġto Ġcheck Ġyour Ġlogs Ġto Ġsee Ġhow Ċ many Ġem ails Ġget Ġcaught Ġby ĠM BL Ġ( other Ġthan ĠM BL _ 14 43 60 ) Ġ- Ġin Ġmy Ġcase Ġthere Ċ were Ġnon\n",
      "\t Unique Tokens no: {'securityĠ', 'e.ĠĠ', 'ra', 'ĠthoughĠ', 'foreĠ', 'useful', 'noneĠ', 'there', 'ĠtoĠ', 'edĠthem', 'O', 'completelyĠ', 'eĠtheĠ', 'databaseĠ', 'onĠthisĠ', 'eĠthere', 'email', 'eĠthanĠtheĠ', 'otherĠthanĠ', 'isĠtoĠ', 'sĠtoĠ', '-Ġ', ')Ġ-Ġ', 'inĠtheĠlastĠ', \"Ġthey'reĠ\", 'abl', 'MB', 'ativesĠ', 'moreĠ', 'LĠ', 'On', 'othersĠ', 'provenĠ', 'uk', 'Ċ<', 'runningĠtheĠ', 'ĠandĠ', \"I'dĠ\", 'altern', 'month', 'systemĠ', '>Ġ', 'others', '.ĠĊ', 'manyĠ', 'log', 'inĠmyĠ', 'how', 'checkĠ', 'find', 'anyĠ', 'reli', 'seeĠ', 'customersĠ', 'byĠ', ',ĠandĠtheĠ', 'san', 'ĠthatĠ', 'forĠ', 'n', 'list', 'cas', '.ĊĊAndĠ', 'IĊ', 'wereĠ', 'sedĠtheĠ', 'ThanksĠ', 'hereĠ', 'remov', 'onceĠagain', 'L', 'L_', 'issu', 'ainedĠ', ',Ġthank', 'caughtĠ', 'compl', 'eĠthingĠ', 'managedĠtoĠ', 'thersĠ', 'getĠ', 'recommend', \"I'veĠ\", 'mayĠ', 'now', 'yourĠ', 'i', 'beforeĠ', ':)ĊĊ', 'sĠ'}\n",
      "\t Unique Tokens wiki: {'Ġthis', 'Ġlast', 'And', 'Ġraised', ')', 'Ġand', 'pro', 'Ġalternatives', 'Ġ<', 'were', 'Ġcustomers', 'Ġhere', 'Ġ', 'Ġto', 'Ġreliable', 'Ġyour', 'Ġem', ',', 'Ġmonth', 'Ġonce', 'Ġdatabase', 'Ġtherefore', 'Ġhow', 'Ġsystem', 'One', 'ails', 'Ġmay', '.', 'ven', 'Ġis', 'Ġmore', 'anks', 'Ġthem', 'ĠM', 'BL', 'Ġcompletely', 'Ġthough', 'Ġthe', 'Th', 'Ġsee', 'Ġsan', 'Ġn', 'Ġlist', 'Ġnone', 'Ġcase', 'other', 'Ġagain', 'Ġcheck', 'Ġ:', 'Ġget', \"'ve\", 'Ġthan', 'Ġnow', 'Other', 'Ġrecommend', 'Ġuseful', 'Ġ-', 'Ġthey', 'uke', 'Ġthanks', 'Ġbefore', 'many', 'Ġthere', 'aged', 'Ġcaught', 'Ġin', 'Ġissue', 'Ġby', 'Ġon', 'Ġfind', 'Ġthing', \"'re\", '_', \"'d\", 'man', 'Ġrunning', 'Ġothers', 'Ġlogs', 'Ġcomplained', 'Ġmy', 's', 'Ġfor', 'ĠI', 'Ġremoved', 'Ġany', 'security'}\n",
      "Text 2: I read that Amarok 2.3 has dynamic collections back in, which excites me\n",
      "because I've been waiting for this feature to return!  I can't get it to\n",
      "work however.  I'm running off Git, and what I'm observing is that if I plug\n",
      "in an external drive, do a full scan, then unmount the drive, the songs are\n",
      "s\n",
      "\t Tokenized no:IĠ readĠthatĠ Am ar okĠ 2. 3Ġ hasĠ dynamic Ġcollection sĠ backĠ in ,ĠwhichĠ excit esĠ me Ċ becauseĠ I'veĠbeen ĠwaitingĠ forĠthisĠ feat ureĠtoĠ return !ĠĠ IĠcan'tĠ getĠit ĠtoĊ workĠ however .ĠĠI'mĠ runningĠ offĠ G it,ĠandĠ whatĠ I'mĠ observ ingĠ isĠthatĠ ifĠIĠ plug Ċ inĠanĠ externalĠ driv e,Ġ doĠ aĠfullĠ scan ,ĠthenĠ un mount ĠtheĠ drive ,ĠtheĠ song sĠ areĊ s\n",
      "\t Tokenized wiki:I Ġread Ġthat ĠAmar ok Ġ2 . 3 Ġhas Ġdynamic Ġcollections Ġback Ġin , Ġwhich Ġexc ites Ġme Ċ because ĠI 've Ġbeen Ġwaiting Ġfor Ġthis Ġfeature Ġto Ġreturn ! Ġ ĠI Ġcan 't Ġget Ġit Ġto Ċ work Ġhowever . Ġ ĠI 'm Ġrunning Ġoff ĠG it , Ġand Ġwhat ĠI 'm Ġobserving Ġis Ġthat Ġif ĠI Ġplug Ċ in Ġan Ġexternal Ġdrive , Ġdo Ġa Ġfull Ġscan , Ġthen Ġunm ount Ġthe Ġdrive , Ġthe Ġsongs Ġare Ċ s\n",
      "\t Unique Tokens no: {'stillĠ', 'workĠ', ',ĠwhichĠ', 'becauseĠ', 'areĊ', 'connectĠtoĠ', 'checkĠtheĠ', \"it'sĠ\", 'actingĠ', 'Also,Ġ', 'again', 'ĠtheĊ', 'miss', \".ĠĠI'mĠ\", 'observ', 'databaseĠ', 'dynamic', 'canĠ', 'So,Ġ', 'soĠIĠcanĠ', 'excit', \"ĠtheyĠdon'tĠ\", 'amĠIĠ', 'upĠ', '?ĊĊ', 'atĊ', 'withĠtheĠ', '-Ġ', 'alsoĠ', 'offĠ', 'USB', 'inĠanĠ', 'IĠ', 'behaviorĠ', 'visibleĠ', 'scan', 'identifi', 'ifĠIĠ', 'sĊ', 'likeĠtheĠ', 'ĠwaitingĠ', 'IfĠ', 'whatĠ', 'ers', 'me', 'getĠit', 'asĠaĠ', 'ingĠinĠ', 'forĠthisĠ', 'disappear', 'separateĠ', 'wayĠtoĠ', 'showsĠ', 'collection', 'inĠtheĠ', 'hasĠ', 'IĠdoĠ', 'onceĠ', 'appearĠ', \"IĠcan'tĠ\", 'anotherĠ', 'driveĠ', 'Ġsome', \"I'mĠ\", 'deviceĠ', 'feat', 'un', ',ĠtheĠ', 'aĠfullĠ', 'quer', 'how', 'mysq', '!ĠĠ', '?ĠĠ', 'assĠ', 'mount', 'evenĠ', 'beĠ', 'andĊ', '3Ġ', ')ĠtheĠ', 'G', 'runningĠ', 'okĠ', 'allĠ', 'song', 'M', 'yĠtheĠ', 'Am', '.ĠĠIfĠ', 'doĠ', 'plug', 'something', 'isĠ', 'aĠgoodĠ', 'ureĠtoĠ', \"I'veĠbeen\", 'isĠthatĠ', \"itĠdoesn'tĠ\", 're', 'seemĠ', 'kick', 'singĠ', '.ĊĊ', 'readĠthatĠ', ',Ġbut', 'e,Ġ', '?ĊĊThank', 'relevant', '.ĠĠ', 'however', '>ĊĊ', '2.', 'Ġcol', 'isĠthereĠ', '(orĠ', 'storage', ',ĠthisĠ', 'ĠtheĠ', 'driv', 'leĠ', 'externalĠ', 'ĠtoĊ', 'backĠ', 'ingĠ', 'esĠ', 'res', 'return', 'it,ĠandĠ', 'restart', 'lectionĠ', 'sĠ', 'inter', ',ĠthenĠ', 'ĠthisĠ'}\n",
      "\t Unique Tokens wiki: {'Ġmass', 'Ġdisappear', 'work', 'Ġthis', 'Ġplug', 'Ġbut', 'Ġreappe', ')', 'Ġa', 'Ġand', 'Ġup', 'still', 'col', 'qu', 'Ġas', 'Ġfull', 'Ġanother', 'A', 'ĠUSB', 'ĠAlso', 'Ġbeen', 'Ġwith', 'q', 'Ġseparate', 'Ġ', 'Ġalso', \"'s\", 'Ġat', 'Ġare', 'Ġto', 'Ġinteract', 'Ġmiss', 'Ġdon', 'ites', 'Ġdrive', 'Ġback', 'Ġcollections', 'ĠG', ',', 'or', 'Ġonce', 'Ġway', 'ifiers', 'Ġread', 'Ġdatabase', 'Ġident', \"'t\", 'it', 'Ġseem', 'mar', '.', 'Ġobserving', 'Ġwaiting', 'Ġis', 'ery', 'anks', 'ĠM', 'Ġstorage', 'Ġsomehow', 'Ġdynamic', 'Ġthe', 'Th', 'Ġhas', 'Ġ(', \"'m\", 'Ġthat', 'If', 'Ġlike', 'Ġshows', 'Ġcan', 'ing', 'Ġscan', '3', 'Ġres', 'So', 'I', 'Ġconnect', 'Ġwhich', 'Ġagain', 'Ġdoesn', 'Ġcheck', 'Ġdevice', 'Ġdo', '>', 'Ġbe', 'Ġget', 'Ġif', 'Ġexc', 'Ġexternal', 'ount', 'all', \"'ve\", 'Ġoff', 'Ġunm', 'le', 'Ġthey', 'Ġ-', 'lection', 'Ġam', 'Ġthere', 'because', 'Ġwhat', 'Ġin', 'can', 'ĠIf', 'Ġmount', 'ĠAmar', 'Ġeven', 'Ġfeature', 'Ġan', 'Ġsongs', 'sing', 'Ġrunning', '!', 'Ġme', 'Ġvisible', 'Ġgood', 'Ġso', 'Ġrelevant', '?', 'Ġreturn', 'ys', 'Ġrestart', 'Ġ2', 'Ġit', 'Ġhowever', 'Ġfor', 'Ġkicking', 'ĠI', 'Ġbehavior', 'Ġsomething'}\n",
      "==not Same Author==\n",
      "Text 1: Hi guys, this is my first post here, so let me start by saying it's a real pleasure to be a part of this community. I'm a newbie Mac developer and am looking forward to learning a lot and (hopefully) helping others along the way.\n",
      "\n",
      "I'm adding AquaticPrime to my first app, and setting up the PHP scrip\n",
      "\t Tokenized no:HiĠ guys ,ĠthisĠisĠ myĠfirstĠ postĠ here,Ġ soĠ letĠmeĠ startĠ byĠ sayingĠ it'sĠaĠ realĠ pleas ureĠtoĠ beĠaĠ partĠofĠthisĠ community .ĠI'mĠ aĠnew bieĠ MacĠ develop erĠandĠ amĠ lookingĠforwardĠtoĠ learn ingĠaĠ lo tĠandĠ ( hopefully )Ġ helpingĠ othersĠ alongĠtheĠ way .ĊĊI'mĠ addingĠ Aqu atic Prim eĠtoĠ myĠfirstĠ app ,ĠandĠ settingĠ upĠtheĠ PHP Ġs crip\n",
      "\t Tokenized wiki:H i Ġguys , Ġthis Ġis Ġmy Ġfirst Ġpost Ġhere , Ġso Ġlet Ġme Ġstart Ġby Ġsaying Ġit 's Ġa Ġreal Ġpleasure Ġto Ġbe Ġa Ġpart Ġof Ġthis Ġcommunity . ĠI 'm Ġa Ġnew bie ĠMac Ġdeveloper Ġand Ġam Ġlooking Ġforward Ġto Ġlearning Ġa Ġlot Ġand Ġ( h ope fully ) Ġhelping Ġothers Ġalong Ġthe Ġway . Ċ Ċ I 'm Ġadding ĠAqu atic Pr ime Ġto Ġmy Ġfirst Ġapp , Ġand Ġsetting Ġup Ġthe ĠP HP Ġsc rip\n",
      "\t Unique Tokens no: {'knowĠhowĠtoĠ', 'erĠtoĠ', 'ĠthoughĠ', 'int', 'PHPĠ', 'whereĠtheĠ', 'purchase', 'les', 'letĠmeĠ', 'lookingĠforwardĠtoĠ', 'sĠbyĠ', 'track', '?Ġ', 'ersĠ', 'ation', 'generateĠ', '.ĊĊMyĠ', 'questionĠ', 'is,Ġ', 'PrimeĠ', 'atĠthisĠpointĠ', 'ĠsinceĠ', 'backendĠ', 'sĠtoĠ', ',ĊĊ', 'aĠl', 's,', 'tĠandĠ', 'Aqu', 'inĠsomeĠ', 'sĠorĠ', 'Ġskill', '.ĠTh', 'app', 'anyoneĠ', 'bieĠ', 'andĠmyĠ', 'ingĠaĠ', 'othersĠ', 'edĠthisĠ', 'addingĠ', 'eĠtoĠ', 'rough', 'aĠbit', 'me', 'ingĠinĠ', 'ingĠtheĠ', 'errorĠ', 'beĠaĠ', 'ask', 'ofĠ', 'wouldĠbeĠ', 'myĠfirstĠ', 'greatĠ', 'it.Ġ', 'occurr', 'point', 'community', 'sĠforĠ', 'AnyĠ', \".ĠI'mĠ\", 'figureĠoutĠ', 'downĠtoĠtheĠ', ',ĠthisĠisĠ', 'erĠandĠ', 'HiĠ', \"I'mĠ\", 'goalĠisĠtoĠ', 'doesĠ', 'activ', \"Ġthere'sĠ\", \".ĊĊI'mĠ\", 'guys', 'Regard', 'alĠandĠ', 'anĠ', '.ĊĊB', 'either', 'Ġscript', 'pleas', '()', 'ĠtooĠ', 'ratherĠthanĠ', 'upĠtheĠ', 'Ġtest', 'file', 'functionĠ', 'PHP', 'problemĠis', 'byĠ', 'way', 'asĠanĠ', 'passingĠ', 'est', 'helpingĠ', 'isĠbeingĠ', 'afterĠ', 'enseĠ', '.ĠWhen', 'endĠ', 'oneĠtoĠ', ',ĠandĠ', 'arrayĠ', \"IĠdon'tĠ\", 'ic', 'amĠ', 'Ġtri', 'suggestion', 'settingĠ', 'partĠofĠthisĠ', 'ureĠtoĠ', 'help', 'here,Ġ', 'ĠcodeĠ', 'function', 'generated', 'ĠĠ', \"it'sĠaĠ\", 'alongĠtheĠ', 'someĠkindĠofĠ', 'MacĠ', 'learn', 'argumentĠ', 'fileĠ', '-', 'startĠ', ')Ġ', 'lo', 'justĠanĠ', 'onĠ', 'ag', 'sĠareĠ', 'aĠnew', 'it,Ġ', 'usingĠ', 'develop', 'createĠ', \"I'veĠ\", ',ĠnotĠ', 'alter', 'realĠ', 'hopefully', '()Ġ', 'soĠ', 'withĠ', 'Perhap', 'Prim', 'sayingĠ', 'get', 'postĠ', 'noĠ'}\n",
      "\t Unique Tokens wiki: {'bie', 'Ġthis', 'Ġstart', 'Ġlooking', 'Ġknow', 'Ġand', 'Ġa', 'Ġup', 'Ġwould', 'My', 'Ġas', 'Ġanyone', 'Ġtoo', 'HP', 'ĠThrough', 'ĠReg', 'Ġintend', 'Ġhere', 'Ġwith', 'ters', 'Ġ', \"'s\", 'Ġbit', 'Ġat', 'Ġare', 'Ġto', 'Ġbeing', 'Ġdoes', 'Ġwhere', 'Ġdon', 'Ġcommunity', 'Ġback', 'Ġsetting', ',', 'Ġway', 'Ġarray', 'Ġquestion', 'Ġhelping', 'ĠP', 'Ġhow', 'h', 'Ġalong', \"'t\", 'ĠAqu', 'Ġpassing', 'ĠMac', 'Ċ', '.', 'Ġis', 'Ġfirst', 'Ġoccurring', 'Ġlet', 'Ġpurchase', 'ope', 'Ġpart', 'Ġsome', 'Ġjust', 'ĠAny', 'Ġargument', 'Ġthough', 'ĠPerhaps', 'Ġthe', 'Ġno', 'Ġtracked', 'ing', 'Ġ(', \"'m\", 'Ġactivation', 'Ġgenerated', 'H', 'I', 'Ġor', 'Ġadding', 'Ġlicense', 'Ġdeveloper', 'Ġbe', 'Ġget', 'Ġhelp', 'Ġpoint', 'Ġapp', 'ime', 'Ġpoin', \"'ve\", 'Ġasks', 'Ġthan', 'Ġfile', 'Ġout', 'r', 'Ġ-', 'Ġdown', 'Ġcreate', 'Ġerror', 'ather', 'Ġam', 'Ġthere', 'Ġone', 'Ġin', 'Ġgoal', 'Pr', 'Ġguys', 'Ġan', 'Ġby', 'Ġon', 'Ġsince', 'Ġsaying', 'Ġgenerate', 'fully', 'Ġnot', 'ager', 'Ġlearning', 'Ġforward', 'Ġtesting', 'Ġreal', 'Ġtrial', 'Ġme', 'end', 'ĠWhen', 'Ġeither', 'Ġskills', 'Ġso', 'Ġothers', 'Ġfigure', '?', 'Best', 'Ġalter', 'and', 'Ġof', 'ardless', 'Ġmy', 'Ġproblem', 'Ġnew', 'Ġafter', 'Ġit', 'Ġkind', 'Ġpleasure', 'Ġscripts', 'Ġpost', 'i', 'Ġfor', 'Ġgreat', 'Ġlot', 'ĠI', 'Ġusing', 'Ġfunction', 'Ġsuggestions'}\n",
      "Text 2: I'm in a long distance relationship as well. In my opinion there should always be a long term plan: when will you close the distance? What are you both doing, together and individually, to work on the problems that are keeping you apart? Who will move to where, is there family or careers to consider\n",
      "\t Tokenized no:I'mĠ inĠ aĠlongĠ distanceĠ relationshipĠ asĠwell .ĠInĠmyĠ opinion ĠthereĠ shouldĠ alwaysĠbeĠ aĠlong ĠtermĠ plan : Ġwhen ĠwillĠ youĠc loseĠtheĠ distance ?ĠWhatĠ areĠyouĠ bothĠ doing , ĠtogetherĠandĠ individ ually ,ĠtoĠ workĠonĠtheĠ problem sĠthatĠareĠ keepingĠ youĠ apart ?Ġ WhoĠ willĠ moveĠtoĠ where,Ġ isĠthereĠ familyĠ orĠc are ersĠtoĠ consider\n",
      "\t Tokenized wiki:I 'm Ġin Ġa Ġlong Ġdistance Ġrelationship Ġas Ġwell . ĠIn Ġmy Ġopinion Ġthere Ġshould Ġalways Ġbe Ġa Ġlong Ġterm Ġplan : Ġwhen Ġwill Ġyou Ġclose Ġthe Ġdistance ? ĠWhat Ġare Ġyou Ġboth Ġdoing , Ġtogether Ġand Ġindividually , Ġto Ġwork Ġon Ġthe Ġproblems Ġthat Ġare Ġkeeping Ġyou Ġapart ? ĠWho Ġwill Ġmove Ġto Ġwhere , Ġis Ġthere Ġfamily Ġor Ġcareers Ġto Ġconsider\n",
      "\t Unique Tokens no: {'orĠc', 'WhoĠ', 'youĠ', 'ĠwillĠ', 'evenĠ', 'where,Ġ', 'eitherĠ', 'ually', 'ĠtermĠ', 'dist', 'relationship', 'opinion', 'ĠtalkĠ', 'keepingĠ', ',ĠtoĠ', 'aĠlongĠ', 'doingĠ', '.ĠIfĠyouĠ', 'Dist', 'goodĠforĠ', 'ĠthereĠ', 'mightĠbeĠ', '?Ġ', 'apart', 'ifĠtheĠ', 'areĠyouĠ', 'ance,Ġ', 'isĠthereĠ', 'asĠwell', 'ĠisĠ', 'closeĠtheĠ', 'ersĠtoĠ', 'anceĠ', 'needĠtoĠ', 'familyĠ', 'inĠ', 'distance', 'plan', 'anythingĠtoĠ', 'Ġthis.', '?ĠWhatĠ', 'shouldĠ', 'sĠthatĠareĠ', 'distanceĠ', 'problem', 'eĠto', 'willĠ', 'youĠmightĠ', '-Ġ', 'loseĠtheĠ', 'individ', 'erĠandĠ', 'r/', 'aĠlong', \"I'mĠ\", 'youĠc', 'considerĠ', 'are', 'bothĠ', \"aren'tĠ\", 'alwaysĠbeĠ', 'soĠon', 'workĠonĠtheĠ', 'plac', 'helpfulĠ', 'consid', 'ofĠyou', '.ĠĠĠĠĠ', 'about', '.ĠInĠmyĠ', 'planningĠ', 'relationshipĠ', 'ĠtogetherĠandĠ', 'doing', 'moveĠtoĠ'}\n",
      "\t Unique Tokens wiki: {'ĠWhat', 'Ġwhere', 'Ġaren', 'Ġmight', 'Ġboth', 'Ġplan', 'Ġr', 'I', 'Ġrelationship', 'Ġor', 'Ġconsider', 'Ġthis', 'Ġlong', 'Ġtogether', 'Ġopinion', 'Ġcareers', 'Ġand', 'Ġa', 'Ġfamily', 'Ġbe', 'istance', 'Ġdistance', 'Ġif', 'Ġplace', 'Ġeither', 'Ġapart', 'Ġshould', 'Ġclose', 'Ġas', 'Ġwell', 'Ġyou', 'Ġmove', 'Ġso', \"'t\", 'Ġplanning', 'Ġgood', 'Ġabout', '?', 'Ġkeeping', 'Ġof', 'Ġ-', '.', 'Ġneed', 'Ġtalk', 'Ġmy', 'Ġis', 'Ġterm', 'Ġdoing', 'Ġanything', 'Ġhelpful', 'Ġthere', '/', 'Ġin', 'ĠIn', 'Ġ', 'Ġwill', 'ĠIf', 'Ġalways', 'Ġeven', 'Ġthe', 'Ġwork', 'Ġfor', 'Ġare', 'Ġto', 'Ġindividually', 'D', \"'m\", 'Ġthat', 'Ġon', 'ĠWho', 'Ġproblems'}\n",
      "==not Same Author==\n",
      "Text 1: Hi all, \n",
      "Please let me know ur comments / suggestion on the following. \n",
      "1. Is there any apache apr to have shared memory hash Table. I want a hash table with is shared and should be able to access and change in runtime.\n",
      "We can do work around with the apr_shm and apr_rmm to achieve this. but just wan\n",
      "\t Tokenized no:HiĠ all,Ġ Ċ PleaseĠ letĠmeĠknowĠ ur ĠcommentsĠ / Ġsuggest ionĠ onĠtheĠ following .ĠĊ 1 .ĠIsĠthereĠ anyĠ ap acheĠ ap r ĠtoĠhaveĠ sharedĠ memoryĠ hashĠ Table .ĠIĠ wantĠaĠ hash ĠtableĠ withĠ isĠ sharedĠ andĠ shouldĠbeĠableĠtoĠ acces sĠandĠ changeĠinĠ run tim e.Ċ WeĠ canĠdoĠ workĠ aroundĠ withĠtheĠ ap r_ sh mĠandĠ ap r _r mm ĠtoĠ achieveĠ this .Ġ butĠ justĠ wan\n",
      "\t Tokenized wiki:H i Ġall , Ġ Ċ Ple ase Ġlet Ġme Ġknow Ġur Ġcomments Ġ/ Ġsuggestion Ġon Ġthe Ġfollowing . Ġ Ċ 1 . ĠIs Ġthere Ġany Ġap ache Ġap r Ġto Ġhave Ġshared Ġmemory Ġhas h ĠTable . ĠI Ġwant Ġa Ġhas h Ġtable Ġwith Ġis Ġshared Ġand Ġshould Ġbe Ġable Ġto Ġaccess Ġand Ġchange Ġin Ġrun time . Ċ We Ġcan Ġdo Ġwork Ġaround Ġwith Ġthe Ġap r _ sh m Ġand Ġap r _ r mm Ġto Ġachieve Ġthis . Ġbut Ġjust Ġw an\n",
      "\t Unique Tokens no: {'WeĠ', 'ĠtoĠhaveĠ', 'justĠ', '<', 'aroundĠ', 'functionĠ', 'meĠ', 'shouldĠbeĠableĠtoĠ', 'workĠ', 'your', 'ĠtableĠ', 'achieveĠ', 'hashĠ', 'changeĠinĠ', 'which', 'acheĠ', 'ĠtoĠ', 'canĠdoĠ', 'ur', 'sĠinĠadv', 'isĠthereĠanyĠ', 'sĠandĠ', 's.Ġ', 'availableĠ', 'ance,Ġ', 'Pl', 'withĠ', 'ap', 'uit', '.ĠĊ', 'Ġcomment', '.ĠIsĠthereĠ', 'andĠ', 'Ċth', 'guideĠ', 'letĠmeĠknowĠ', 'run', 'knowĠ', 'ionĠ', 'withĠtheĠ', 'alreadyĠ', 'wantedĠtoĠ', 'acces', 'Ġsuggest', 'HiĠ', 'sharedĠ', '.ĠIĠ', 'ank', 'wantĠaĠ', 'isĠ', 'mĠandĠ', 'Ġs', 'memoryĠ', 'Table', 'following', '/', 'ĠcommentsĠ', 'all,Ġ', 'requirement', 'onĠtheĠ', 'r_', 'PleaseĠ', 'tim', 'e.Ċ', 'this', 'hash', 'anyĠ', 'butĠ', '_r', '.Ġ', 'sĠ', 'sĠthisĠ'}\n",
      "\t Unique Tokens wiki: {'Ġalready', 'Ġyour', 'H', 'Ġthis', ',', 'Ġbut', '_', 'Ġwhich', 'Ġtable', 'ase', 'Ġknow', 'Ġand', 'Ġa', 'Ġmemory', 'ache', 'Ġdo', 'Ġrun', 'Ġme', 'Ġbe', 'm', 'Ġable', 'Ple', 'We', 'Ġshould', 'h', 'Ġon', 'Ġ<', 'Ġrequirement', 'Ġ/', 'Ġadvance', 'ĠIs', 'ĠTable', 'Ġavailable', '.', 'Ġap', 'Ġfunction', 'Ġshared', 'Ġwant', 'Ġwanted', 'Ġfollowing', 'Ġis', 'Ġlet', 'anks', 'Ġachieve', 'Ġwith', 'Ġsuits', 'Ġhave', 'Ġjust', 'Ġguide', 'Ġthere', 'Ġall', 'ls', 'Ġcan', 'Ġin', 'Ġ', 'Ġur', 'i', 'Ġthe', 'Ġchange', 'time', 'Ġsuggestion', 'th', 'Ġwork', 'ĠI', 'Ġto', 'Ġcomments', 'Ġhas', 'Ġaccess', 'Ġaround', 'Ġany'}\n",
      "Text 2: Dear Users,\n",
      "\n",
      "I'm writing a plugin, which uses loci-tools.jar to open lsm image files. \n",
      "The lsm files (z-stacks) belong to a timeseries, where every timepoint \n",
      "is stored in a separate file. Additionally, every series has its own mdb \n",
      "file. Now I found the following strange behavior: If I open any lsm\n",
      "\t Tokenized no:DearĠ User s,ĊĊ I'm Ġwrit ingĠaĠ plugin ,ĠwhichĠ usesĠ loc i- tool s. jar ĠtoĠ openĠ ls mĠ imageĠ file s.Ġ ĊTheĠ ls mĠ filesĠ( z- stack s)Ġ belong ĠtoĠ a Ġtimes er ies,Ġ whereĠ every Ġtim ep o intĠ ĊisĠ stor edĠinĠaĠ separateĠ file .ĠAddition ally,Ġ everyĠ seriesĠ hasĠ itsĠown Ġm dbĠ Ċ file .ĠNowĠ IĠ foundĠtheĠ followingĠ strangeĠ behavior :Ġ IfĠIĠ openĠ anyĠ l sm\n",
      "\t Tokenized wiki:D ear ĠUs ers , Ċ Ċ I 'm Ġwriting Ġa Ġplug in , Ġwhich Ġuses Ġloc i - to ols . jar Ġto Ġopen Ġl sm Ġimage Ġfiles . Ġ Ċ The Ġl sm Ġfiles Ġ( z - st acks ) Ġbelong Ġto Ġa Ġtimes eries , Ġwhere Ġevery Ġtim ep oint Ġ Ċ is Ġstored Ġin Ġa Ġseparate Ġfile . ĠAdditionally , Ġevery Ġseries Ġhas Ġits Ġown Ġm d b Ġ Ċ file . ĠNow ĠI Ġfound Ġthe Ġfollowing Ġstrange Ġbehavior : ĠIf ĠI Ġopen Ġany Ġl sm\n",
      "\t Unique Tokens no: {'s.', 'ĊĊĊ', 'IfĠIĠ', ',ĠwhichĠ', 'work', 'strangeĠ', 'intĠ', 'Greet', 'Ġwrit', 'ĠtoĠ', '.ĠIfĠIĠ', 'itsĠown', 'viaĠtheĠ', 'User', 'stack', '(', 'belong', 'questionĠ', 'is:Ġ', 'Delet', 'dbĠ', '?ĊĊ', 'foundĠtheĠ', 'oudĠ', 'seriesĠ', 'plugin', 's)Ġ', 'codeĠ', 'isĠnotĠ', 's,ĊĊ', 'ĊTheĠ', 'ls', 'IĠ', 'willĠbeĠ', 'ally,Ġ', 'z-', 'sĠ(', 'behavior', 'ingĠaĠ', 'choo', 'deleteĠtheĠ', 'loc', 'ingĠtheĠ', 'a', 'first', 'separateĠ', 's.Ġ', 'ed.Ċ', 'hasĠ', 'canĠIĠ', 'everyĠ', 'e,ĠsoĠ', 'followingĠ', 'openĠ', 'md', 'aĠ', 'open', ',ĠtheĠ', 'anyĠ', 'seeĠ', 'series', 'stor', '.ĠAddition', 'bĠ', 'er', 'below', ':Ġ', 'myĠ', 'imageĠ', 'alwaysĠ', 'ĊĠĠĠ', 'tool', '.ĠNowĠ', 'ĊinĠtheĠ', 'methodĠofĠ', 'ĊisĠ', 'mĠ', 'edĠinĠaĠ', \"I'm\", 'usesĠ', 'every', 'fileĠ', 'basis', ')Ġ', 'DearĠ', 'onĠ', 'i-', 'pluginĠ', 'ies,Ġ', 'sĠtheĠ', 'sedĠ', 'o', 'choic', 'red', 'HowĠ', 'programĠ', 'whereĠ', 'F', 'itĠ', 'filesĠ('}\n",
      "\t Unique Tokens wiki: {'reet', 'Ġloc', 'ĠFred', 'Ġplug', 'Ġfiles', 'Ġa', 'Ġuses', 'lete', 'st', 'Ġseries', 'Ġits', 'oud', 'Ġfound', 'ear', 'Ġopened', 'Ġseparate', 'Ġ', 'z', 'Ġmethod', 'acks', 'Ġto', 'Ġimage', 'eries', 'Ġwhere', 'b', 'Ġwriting', 'code', ',', 'ers', 'Ġquestion', 'ols', 'Ġevery', 'Ġstored', 'Ġbelow', '.', 'Ġis', 'Ġfollowing', 'Ġfirst', 'Ġprogram', 'Ġwill', 'Ġown', 'is', 'Ġthe', 'Ġsee', 'via', 'Ġhas', 'D', 'Ġ(', \"'m\", 'Ġopen', 'I', 'Ġwhich', 'Ġde', 'Ġbe', 'G', 'Ġfile', 'Ġbasis', 'Ġstrange', 'ele', 'ting', 'Ġin', 'can', 'ĠIf', 'Ġchoice', 'ĠHow', 'Ġopens', '-', 'd', 'Ġnot', 'osed', 'The', 'Ġl', 'to', 'Ġbelong', 'Ġso', 'in', 'sm', 'oint', '?', 'ĠAdditionally', 'Ġof', 'Ġmy', 'ĠNow', 'Ġit', 'ĠUs', ':', 'Ġcho', 'i', 'Ġalways', 'Ġwork', 'ĠI', 'Ġany', 'on', 'Ġbehavior'}\n",
      "==Same Author==\n",
      "Text 1: Hi, Best wishes  from Marathi Language\n",
      " mr-wikipedia. We do have one article on Marathi Wikipedia about \n",
      "Shrilankan 'Anuradhapura Kingdom' with present title as mr:अनुराधापुर्‍याचे राज्य , presently style of its writing Devanagari script is being debated at Marathi Wikipedia village pump at mr:विकिप\n",
      "\t Tokenized no:Hi ,ĠB estĠ wish es ÂłĠ fromĠ Mar ath iĠ Language Ċ Ġm r - wik ip edia .ĠWeĠ doĠhaveĠ oneĠ articleĠ onĠ Mar ath iĠ Wikip ediaĠ aboutĠ Ċ Sh r il ank anĠ ' An ur ad ha pur aĠ Kingdom ' ĠwithĠ present ĠtitleĠ asĠ mr : à¤ ħ à¤ ¨ à¥ ģ à¤ ° à¤ ¾ à¤ § à¤ ¾ à¤ ª à¥ ģ à¤ ° à¥ į âĢ į à¤ ¯ à¤ ¾ à¤ ļ à¥ ĩĠ à¤ ° à¤ ¾ à¤ ľ à¥ į à¤ ¯ Ġ ,Ġ pres entlyĠ styleĠ ofĠitsĠ writingĠ Dev anag ari ĠscriptĠ isĠbeingĠ deb atedĠ atĠ Mar ath iĠ Wikip ediaĠ villageĠ pumpĠ atĠ mr : à¤ µ à¤ ¿ à¤ ķ à¤ ¿ à¤ ª\n",
      "\t Tokenized wiki:H i , ĠBest Ġwishes Âł Ġfrom ĠMarathi ĠLanguage Ċ Ġm r - wik ipedia . ĠWe Ġdo Ġhave Ġone Ġarticle Ġon ĠMarathi ĠWikipedia Ġabout Ġ Ċ Sh ril ank an Ġ' An ur adh ap ura ĠKingdom ' Ġwith Ġpresent Ġtitle Ġas Ġm r : à¤ ħ à¤ ¨ à¥ ģ à¤° à¤¾ à¤ § à¤¾ à¤ ª à¥ ģ à¤° à¥į âĢ į à¤ ¯ à¤¾ à¤ ļ à¥ ĩ Ġà¤ ° à¤¾ à¤ ľ à¥į à¤ ¯ Ġ, Ġpresently Ġstyle Ġof Ġits Ġwriting ĠDev an ag ari Ġscript Ġis Ġbeing Ġdebated Ġat ĠMarathi ĠWikipedia Ġvillage Ġpump Ġat Ġm r : à¤ µ à¤ ¿ à¤ ķ à¤ ¿ à¤ ª\n",
      "\t Unique Tokens no: {'WhichĠ', 'Dev', 'ĠwithĠ', 'ath', 'es', 'pur', 'ationĠofĠ', '¾Ġ', 'ad', 'aboutĠ', 'st', 'debateĠ', 'oneĠ', 'lyĠ', 'ĠtheseĠ', 'Ġ(Ġ', 'styleĠ', 'inhal', 'willĠbeĠ', 'anag', ')ĠorĠ', 'fromĠ', 'pumpĠ', 'mr', 'Thank', 'villageĠ', 'originalĠ', 'Hi', 'atedĠ', 'Language', 'deb', 'wish', 'ĠtitleĠ', 'pron', 'ĠscriptĠ', '&Ġ', 'correctĠ', 'il', 'estĠ', '.ĠWeĠ', 'aĠ', ',ĠB', '.Ċ', 'anĠ', 'knowledgeĠofĠ', 'whoĠhasĠ', 'writingĠ', 'RegardsĊ', 'issueĠ', '.ĠTheĠ', 'ha', 'isĠbeingĠ', 'fromĠsomeĠ', 'among', ',Ġ', 'doĠhaveĠ', 'entlyĠ', 'ĩĠ', 'isĠ', 'Mar', 'S', 'ediaĠ', 'ÂłĠ', 'someĠ', 'ofĠitsĠ', 'edia', 'appreciatedĠ', '.ĊĊ', 'present', '¾', ')Ġ', 'onĠ', 'sĠandĠ', 'wordĠ', 'oneĠisĠ', 'asĠ', 'iĠ', 'unc', 'inĠ', 'pres', 'atĠ', 'eseĠ', 'articleĠ', 'ip', 'Wikip', 'whatĠisĠ', 'Kingdom'}\n",
      "\t Unique Tokens wiki: {'ĩ', 'Ġoriginal', 'Ġfrom', ')', 'Ġand', 'Ġas', 'ĠThe', 'Ġpump', 'Ġits', 'ĠReg', 'ril', 'ĠBest', 'Ġarticle', 'ly', 'Ġwith', 'à¤¾', 'ĠWikipedia', 'à¥į', 'Ġdebated', 'Ġat', 'ĠMarathi', 'Ġbeing', 'ĠWh', 'Ġwriting', ',', \"Ġ'\", 'Ġdebate', 'Ġcorrect', 'ĠLanguage', 'Ġ&', 'h', 'Ġabout', 'Ġà¤', 'Ġthese', '.', 'Ġis', 'anks', 'Ġsome', 'Ġwill', 'Th', 'Ġhas', 'à¤°', 'Ġknowledge', 'Ġscript', 'apur', 'H', 'Ġor', 'ich', 'adh', 'Ġdo', 'Ġbe', 'm', 'ĠSin', 'ap', 'alese', 'Ġpresently', 'Ġtitle', 'Ġwhat', 'Ġone', 'Ġin', 'ĠKingdom', 'Ġpresent', 'Ġwho', 'Ġissue', 'ura', 'Ġappreciated', 'Ġon', 'ĠWe', 'Ġ,', 'Ġwishes', 'ĠDev', 'ag', 'Ġvillage', 'ĠAn', 'Ġof', 'Ġword', 'Ġhave', 'Ġamongst', 'Ġstyle', 'Âł', 'Ġ.', 'an', 'Ġpronunciation', 'ipedia', 'ards'}\n",
      "Text 2: Hi,\n",
      "Best wishes for 10th year celebration. If at all you happen to visit schools or \n",
      "any other place if one needs to make presentation or just wants other Marathi \n",
      "friend to know more about Marathi Wikipedia , we do have one online (google) \n",
      "powerpoint presentation .Any one can access and update it \n",
      "\t Tokenized no:Hi,Ċ BestĠ wish esĠforĠ 10 thĠ year Ġc elebr ation .ĠIfĠ atĠallĠ youĠ happenĠtoĠ visitĠ school sĠorĠ Ċ anyĠotherĠ placeĠ ifĠ oneĠ needsĠtoĠ makeĠ presentationĠ orĠjustĠ wantsĠ otherĠ Mar ath iĠĊ friend ĠtoĠ know ĠmoreĠ aboutĠ Mar ath iĠ Wikip ediaĠ ,ĠweĠ doĠhaveĠ oneĠ onlineĠ ( google )ĠĊ power pointĠ presentationĠ . AnyĠ oneĠcanĠ acces sĠandĠ updateĠ itĠ\n",
      "\t Tokenized wiki:H i , Ċ Best Ġwishes Ġfor Ġ10 th Ġyear Ġcelebration . ĠIf Ġat Ġall Ġyou Ġhappen Ġto Ġvisit Ġschools Ġor Ġ Ċ any Ġother Ġplace Ġif Ġone Ġneeds Ġto Ġmake Ġpresentation Ġor Ġjust Ġwants Ġother ĠMarathi Ġ Ċ friend Ġto Ġknow Ġmore Ġabout ĠMarathi ĠWikipedia Ġ, Ġwe Ġdo Ġhave Ġone Ġonline Ġ( go ogle ) Ġ Ċ power point Ġpresentation Ġ. Any Ġone Ġcan Ġaccess Ġand Ġupdate Ġit Ġ\n",
      "\t Unique Tokens no: {'pointĠtoĠ', 'happenĠtoĠ', 'ath', 'ĠtoĠ', 'at', 'Hi,Ċ', 'wantsĠ', '(', 'ation', 'canĠ', 'ĠĠĊ', 'aboutĠ', '.ĠLikeĠ', 'oneĠ', 'editĠ', 'cb', 'google.com/', 'sĠorĠ', 'ĊĊĊĊĊ', 'ĠtheĠsameĠ', 'atĠallĠ', 'youĠ', 'thĠ', 'sĊ', 'oneĠcanĠ', 'makeĠ', 'wish', 'AnyĠ', 'iĠĊ', 'BestĠ', 'v', 'year', 'needsĠtoĠ', 'makeĠaĠ', 'presentationĠ', 'visitĠ', 'placeĠ', 'ĠmoreĠ', 'anyĠ', 'google', 'ifĠ', 'anyĠtimeĠ', 'updateĠ', 'dt', 'pointĠ', 'elebr', 'anyĠotherĠ', '_1', 'https://', 'doĠhaveĠ', 'esĠforĠ', 'Mar', '?id=', 'school', 'ĠĠ', 'ediaĠ', 'onlineĠ', ',ĠweĠ', '.ĊĊ', 'otherĠ', 'know', 'sĠandĠ', ')ĠĊ', 'docs.', 'orĠjustĠ', 'wikiĠ', '10', 'iĠ', 'atĠ', 'acces', 'soĠ', 'Ġc', 'publicĠ', '.ĠIfĠ', 'Wikip', 'itĠ', 'beforeĠ', 'sf'}\n",
      "\t Unique Tokens wiki: {'ds', 'Ġknow', ')', 'Ġa', 'Ġand', 'https', 'Ġyou', 'Ġvisit', 'ogle', 'id', 'Ġall', 'Ġ', 'ĠWikipedia', 'bs', 'Ġat', 'ĠMarathi', 'th', 'Ġto', 'any', 'go', 'Ġtime', ',', 'Ġabout', 'point', '1', 'Ġedit', 'Ġonline', 'Ġschools', 'Ġupdate', 'ĠLike', 'Ġmore', 'iki', 'Ġjust', 'Ġcelebration', 'Ġthe', 'Ġyear', 'Any', 'Ġaccess', 'Ġ(', 'Ġpresentation', 'Ġcan', 'Ġother', 'H', 'Ġor', 'Ġdo', 'Ġpublic', 'Ġpoint', 'Ġif', 'Ġplace', 't', 'Ġwants', 'Ġ10', 'Ġhappen', 'Ġbefore', 'Ġone', 'can', 'ĠIf', 'c', 'doc', 'Ġmake', 'Ġ,', 'Ġwishes', '_', 'Ġwe', 'Ġneeds', 'Ġso', '?', 'Best', 'Ġsame', 'Ġw', '://', 'Ġhave', 'Ġit', '=', 'i', 's', 'Ġfor', 'Ġ.', 'vd', 'Ġany', 'com'}\n"
     ]
    }
   ],
   "source": [
    "compare_predictions(df_no_av, df_wikipedia_av, no_tok, tok_wikipedia, \"no\", \"wiki\", classification_at_1=\"Same Author\", text1=\"query_text\", text2=\"candidate_text\", n_examples=10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-09T13:32:52.418708Z",
     "start_time": "2025-02-09T13:32:52.344558Z"
    }
   },
   "id": "b62f0933493c1616"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==not Different Author==\n",
      "Text 1: Agreed. I guess Brazil still trips me up though because there’s no real reason for them not to take a stronger stance either. One Brazilian told me they couldn’t “risk trade being impacted with Russia”, but I don’t buy that given that 81% of Brazil’s trade is done with 25 nations, and Russia isn’t one of those. At best Russian goods constitute less than .8% of Brazil’s trade. And for countries that constantly deride colonialism and imperialism and the evils of it and how all of their problems are because of colonial invasions and imperialist meddling, it just strikes me that a literal imperialist nation invading a small, weaker one purely as a land grab is casted as “not our problem” or “a European issue”. Like how would any of these countries feel if they got invaded by the US or China and Europe/the US/the rest of the world said “that’s a Latin American problem”\n",
      "\t Tokenized llama3:Ag reed . ĠI Ġguess ĠBrazil Ġstill Ġtrips Ġme Ġup Ġthough Ġbecause Ġthere âĢĻs Ġno Ġreal Ġreason Ġfor Ġthem Ġnot Ġto Ġtake Ġa Ġstronger Ġstance Ġeither . ĠOne ĠBrazilian Ġtold Ġme Ġthey Ġcouldn âĢĻt ĠâĢľ r isk Ġtrade Ġbeing Ġimpacted Ġwith ĠRussia âĢĿ, Ġbut ĠI Ġdon âĢĻt Ġbuy Ġthat Ġgiven Ġthat Ġ 81 % Ġof ĠBrazil âĢĻs Ġtrade Ġis Ġdone Ġwith Ġ 25 Ġnations , Ġand ĠRussia Ġisn âĢĻt Ġone Ġof Ġthose . ĠAt Ġbest ĠRussian Ġgoods Ġconstitute Ġless Ġthan Ġ. 8 % Ġof ĠBrazil âĢĻs Ġtrade . ĠAnd Ġfor Ġcountries Ġthat Ġconstantly Ġder ide Ġcolonial ism Ġand Ġimperial ism Ġand Ġthe Ġev ils Ġof Ġit Ġand Ġhow Ġall Ġof Ġtheir Ġproblems Ġare Ġbecause Ġof Ġcolonial Ġinvas ions Ġand Ġimperial ist Ġmed d ling , Ġit Ġjust Ġstrikes Ġme Ġthat Ġa Ġliteral Ġimperial ist Ġnation Ġinv ading Ġa Ġsmall , Ġweaker Ġone Ġpurely Ġas Ġa Ġland Ġgrab Ġis Ġcast ed Ġas ĠâĢľ not Ġour Ġproblem âĢĿ Ġor ĠâĢľ a ĠEuropean Ġissue âĢĿ. ĠLike Ġhow Ġwould Ġany Ġof Ġthese Ġcountries Ġfeel Ġif Ġthey Ġgot Ġinv aded Ġby Ġthe ĠUS Ġor ĠChina Ġand ĠEurope /the ĠUS /the Ġrest Ġof Ġthe Ġworld Ġsaid ĠâĢľ that âĢĻs Ġa ĠLatin ĠAmerican Ġproblem âĢĿ\n",
      "\t Tokenized gpt2:Ag reed . ĠI Ġguess ĠBrazil Ġstill Ġtrips Ġme Ġup Ġthough Ġbecause Ġthere âĢĻ s Ġno Ġreal Ġreason Ġfor Ġthem Ġnot Ġto Ġtake Ġa Ġstronger Ġstance Ġeither . ĠOne ĠBrazilian Ġtold Ġme Ġthey Ġcouldn âĢĻ t ĠâĢľ risk Ġtrade Ġbeing Ġimpacted Ġwith ĠRussia âĢĿ, Ġbut ĠI Ġdon âĢĻ t Ġbuy Ġthat Ġgiven Ġthat Ġ81 % Ġof ĠBrazil âĢĻ s Ġtrade Ġis Ġdone Ġwith Ġ25 Ġnations , Ġand ĠRussia Ġisn âĢĻ t Ġone Ġof Ġthose . ĠAt Ġbest ĠRussian Ġgoods Ġconstitute Ġless Ġthan Ġ. 8 % Ġof ĠBrazil âĢĻ s Ġtrade . ĠAnd Ġfor Ġcountries Ġthat Ġconstantly Ġder ide Ġcolonial ism Ġand Ġimperial ism Ġand Ġthe Ġev ils Ġof Ġit Ġand Ġhow Ġall Ġof Ġtheir Ġproblems Ġare Ġbecause Ġof Ġcolonial Ġinvas ions Ġand Ġimperial ist Ġmed d ling , Ġit Ġjust Ġstrikes Ġme Ġthat Ġa Ġliteral Ġimperial ist Ġnation Ġinv ading Ġa Ġsmall , Ġweaker Ġone Ġpurely Ġas Ġa Ġland Ġgrab Ġis Ġcast ed Ġas ĠâĢľ not Ġour Ġproblem âĢĿ Ġor ĠâĢľ a ĠEuropean Ġissue âĢĿ. ĠLike Ġhow Ġwould Ġany Ġof Ġthese Ġcountries Ġfeel Ġif Ġthey Ġgot Ġinvaded Ġby Ġthe ĠUS Ġor ĠChina Ġand ĠEurope / the ĠUS / the Ġrest Ġof Ġthe Ġworld Ġsaid ĠâĢľ that âĢĻ s Ġa ĠLatin ĠAmerican Ġproblem âĢĿ\n",
      "\t Unique Tokens llama3: {'Ġ', 'r', '25', '81', '/the', 'isk', 'âĢĻs', 'aded', 'âĢĻt'}\n",
      "\t Unique Tokens gpt2: {'/', 'Ġ25', 'Ġinvaded', 's', 'Ġ81', 'âĢĻ', 'the', 'risk', 't'}\n",
      "Text 2: I’m not asking Brazil to go to war with Russia, or to even supply military aid. But at the very least, maybe your president shouldn’t be blaming Zelensky and refusing to so much as acknowledge that what Russia is doing is wrong. And sorry but I don’t think, even if they were your enemy, Russia is going to be crossing the earth to invade Brazil. But to answer your question, if they did, you can bet your ass the US would be fighting Russia and/or China if they tried invading a country in the western hemisphere. The US might not get involved with a border skirmish between your country and Venezuela, but if Russia tried to shit there I hope you know we’d be jumping at the excuse to kick their asses, regardless of your NATO status\n",
      "\t Tokenized llama3:I âĢĻm Ġnot Ġasking ĠBrazil Ġto Ġgo Ġto Ġwar Ġwith ĠRussia , Ġor Ġto Ġeven Ġsupply Ġmilitary Ġaid . ĠBut Ġat Ġthe Ġvery Ġleast , Ġmaybe Ġyour Ġpresident Ġshouldn âĢĻt Ġbe Ġblaming ĠZ el ens ky Ġand Ġrefusing Ġto Ġso Ġmuch Ġas Ġacknowledge Ġthat Ġwhat ĠRussia Ġis Ġdoing Ġis Ġwrong . ĠAnd Ġsorry Ġbut ĠI Ġdon âĢĻt Ġthink , Ġeven Ġif Ġthey Ġwere Ġyour Ġenemy , ĠRussia Ġis Ġgoing Ġto Ġbe Ġcrossing Ġthe Ġearth Ġto Ġinv ade ĠBrazil . ĠBut Ġto Ġanswer Ġyour Ġquestion , Ġif Ġthey Ġdid , Ġyou Ġcan Ġbet Ġyour Ġass Ġthe ĠUS Ġwould Ġbe Ġfighting ĠRussia Ġand /or ĠChina Ġif Ġthey Ġtried Ġinv ading Ġa Ġcountry Ġin Ġthe Ġwestern Ġhem isp here . ĠThe ĠUS Ġmight Ġnot Ġget Ġinvolved Ġwith Ġa Ġborder Ġsk irm ish Ġbetween Ġyour Ġcountry Ġand ĠVenezuela , Ġbut Ġif ĠRussia Ġtried Ġto Ġshit Ġthere ĠI Ġhope Ġyou Ġknow Ġwe âĢĻd Ġbe Ġjumping Ġat Ġthe Ġexcuse Ġto Ġkick Ġtheir Ġass es , Ġregardless Ġof Ġyour ĠNATO Ġstatus\n",
      "\t Tokenized gpt2:I âĢĻ m Ġnot Ġasking ĠBrazil Ġto Ġgo Ġto Ġwar Ġwith ĠRussia , Ġor Ġto Ġeven Ġsupply Ġmilitary Ġaid . ĠBut Ġat Ġthe Ġvery Ġleast , Ġmaybe Ġyour Ġpresident Ġshouldn âĢĻ t Ġbe Ġblaming ĠZ el ens ky Ġand Ġrefusing Ġto Ġso Ġmuch Ġas Ġacknowledge Ġthat Ġwhat ĠRussia Ġis Ġdoing Ġis Ġwrong . ĠAnd Ġsorry Ġbut ĠI Ġdon âĢĻ t Ġthink , Ġeven Ġif Ġthey Ġwere Ġyour Ġenemy , ĠRussia Ġis Ġgoing Ġto Ġbe Ġcrossing Ġthe Ġearth Ġto Ġinv ade ĠBrazil . ĠBut Ġto Ġanswer Ġyour Ġquestion , Ġif Ġthey Ġdid , Ġyou Ġcan Ġbet Ġyour Ġass Ġthe ĠUS Ġwould Ġbe Ġfighting ĠRussia Ġand / or ĠChina Ġif Ġthey Ġtried Ġinv ading Ġa Ġcountry Ġin Ġthe Ġwestern Ġhem isp here . ĠThe ĠUS Ġmight Ġnot Ġget Ġinvolved Ġwith Ġa Ġborder Ġsk irm ish Ġbetween Ġyour Ġcountry Ġand ĠVenezuela , Ġbut Ġif ĠRussia Ġtried Ġto Ġshit Ġthere ĠI Ġhope Ġyou Ġknow Ġwe âĢĻ d Ġbe Ġjumping Ġat Ġthe Ġexcuse Ġto Ġkick Ġtheir Ġass es , Ġregardless Ġof Ġyour ĠNATO Ġstatus\n",
      "\t Unique Tokens llama3: {'âĢĻt', 'âĢĻm', 'âĢĻd', '/or'}\n",
      "\t Unique Tokens gpt2: {'/', 'd', 'm', 'âĢĻ', 'or', 't'}\n",
      "==not Different Author==\n",
      "Text 1: The only ones they have thousands of are S300s, which have limited range (<120km) and are far older and less reliable\n",
      "\t Tokenized llama3:The Ġonly Ġones Ġthey Ġhave Ġthousands Ġof Ġare ĠS 300 s , Ġwhich Ġhave Ġlimited Ġrange Ġ(< 120 km ) Ġand Ġare Ġfar Ġolder Ġand Ġless Ġreliable\n",
      "\t Tokenized gpt2:The Ġonly Ġones Ġthey Ġhave Ġthousands Ġof Ġare ĠS 300 s , Ġwhich Ġhave Ġlimited Ġrange Ġ(< 120 km ) Ġand Ġare Ġfar Ġolder Ġand Ġless Ġreliable\n",
      "\t Unique Tokens llama3: set()\n",
      "\t Unique Tokens gpt2: set()\n",
      "Text 2: So the expectation is that Russia will continue to use cruise missiles and Iranian drones for long range strikes, and they are very limited in their stocks of those while their hit rates are getting worse and worse\n",
      "\t Tokenized llama3:So Ġthe Ġexpectation Ġis Ġthat ĠRussia Ġwill Ġcontinue Ġto Ġuse Ġcruise Ġmissiles Ġand ĠIranian Ġdr ones Ġfor Ġlong Ġrange Ġstrikes , Ġand Ġthey Ġare Ġvery Ġlimited Ġin Ġtheir Ġstocks Ġof Ġthose Ġwhile Ġtheir Ġhit Ġrates Ġare Ġgetting Ġworse Ġand Ġworse\n",
      "\t Tokenized gpt2:So Ġthe Ġexpectation Ġis Ġthat ĠRussia Ġwill Ġcontinue Ġto Ġuse Ġcruise Ġmissiles Ġand ĠIranian Ġdrones Ġfor Ġlong Ġrange Ġstrikes , Ġand Ġthey Ġare Ġvery Ġlimited Ġin Ġtheir Ġstocks Ġof Ġthose Ġwhile Ġtheir Ġhit Ġrates Ġare Ġgetting Ġworse Ġand Ġworse\n",
      "\t Unique Tokens llama3: {'ones', 'Ġdr'}\n",
      "\t Unique Tokens gpt2: {'Ġdrones'}\n",
      "==not Different Author==\n",
      "Text 1: Thing is, there's plenty of Russians that would answer the same. You don't have to sacrifice yourself and go to prison, you just need to be a decent person to understand what's right and what's wrong. And it doesn't make you a good Russian, it's just make you a compassionate human being. None of us is free of responsibility, besides kids who were to young in the times of Putin's raise to power. I was, and I still know that I have my share of responsibility\n",
      "\t Tokenized llama3:T hing Ġis , Ġthere 's Ġplenty Ġof ĠRussians Ġthat Ġwould Ġanswer Ġthe Ġsame . ĠYou Ġdon 't Ġhave Ġto Ġsacrifice Ġyourself Ġand Ġgo Ġto Ġprison , Ġyou Ġjust Ġneed Ġto Ġbe Ġa Ġdecent Ġperson Ġto Ġunderstand Ġwhat 's Ġright Ġand Ġwhat 's Ġwrong . ĠAnd Ġit Ġdoesn 't Ġmake Ġyou Ġa Ġgood ĠRussian , Ġit 's Ġjust Ġmake Ġyou Ġa Ġcompassion ate Ġhuman Ġbeing . ĠNone Ġof Ġus Ġis Ġfree Ġof Ġresponsibility , Ġbesides Ġkids Ġwho Ġwere Ġto Ġyoung Ġin Ġthe Ġtimes Ġof ĠPutin 's Ġraise Ġto Ġpower . ĠI Ġwas , Ġand ĠI Ġstill Ġknow Ġthat ĠI Ġhave Ġmy Ġshare Ġof Ġresponsibility\n",
      "\t Tokenized gpt2:T hing Ġis , Ġthere 's Ġplenty Ġof ĠRussians Ġthat Ġwould Ġanswer Ġthe Ġsame . ĠYou Ġdon 't Ġhave Ġto Ġsacrifice Ġyourself Ġand Ġgo Ġto Ġprison , Ġyou Ġjust Ġneed Ġto Ġbe Ġa Ġdecent Ġperson Ġto Ġunderstand Ġwhat 's Ġright Ġand Ġwhat 's Ġwrong . ĠAnd Ġit Ġdoesn 't Ġmake Ġyou Ġa Ġgood ĠRussian , Ġit 's Ġjust Ġmake Ġyou Ġa Ġcompassion ate Ġhuman Ġbeing . ĠNone Ġof Ġus Ġis Ġfree Ġof Ġresponsibility , Ġbesides Ġkids Ġwho Ġwere Ġto Ġyoung Ġin Ġthe Ġtimes Ġof ĠPutin 's Ġraise Ġto Ġpower . ĠI Ġwas , Ġand ĠI Ġstill Ġknow Ġthat ĠI Ġhave Ġmy Ġshare Ġof Ġresponsibility\n",
      "\t Unique Tokens llama3: set()\n",
      "\t Unique Tokens gpt2: set()\n",
      "Text 2: To be clear and honest, I'm half Russian and half Tatar person and I am strongly against the war. I'm not interested in being considered good. But I want to understand your logic\n",
      "\t Tokenized llama3:To Ġbe Ġclear Ġand Ġhonest , ĠI 'm Ġhalf ĠRussian Ġand Ġhalf ĠT atar Ġperson Ġand ĠI Ġam Ġstrongly Ġagainst Ġthe Ġwar . ĠI 'm Ġnot Ġinterested Ġin Ġbeing Ġconsidered Ġgood . ĠBut ĠI Ġwant Ġto Ġunderstand Ġyour Ġlogic\n",
      "\t Tokenized gpt2:To Ġbe Ġclear Ġand Ġhonest , ĠI 'm Ġhalf ĠRussian Ġand Ġhalf ĠT atar Ġperson Ġand ĠI Ġam Ġstrongly Ġagainst Ġthe Ġwar . ĠI 'm Ġnot Ġinterested Ġin Ġbeing Ġconsidered Ġgood . ĠBut ĠI Ġwant Ġto Ġunderstand Ġyour Ġlogic\n",
      "\t Unique Tokens llama3: set()\n",
      "\t Unique Tokens gpt2: set()\n",
      "==Different Author==\n",
      "Text 1: Two Jewish guys from Odesa meet up. One asks the other: ‘So what’s the situation? What are people saying?'”\n",
      "\t Tokenized llama3:Two ĠJewish Ġguys Ġfrom ĠO des a Ġmeet Ġup . ĠOne Ġasks Ġthe Ġother : ĠâĢĺ So Ġwhat âĢĻs Ġthe Ġsituation ? ĠWhat Ġare Ġpeople Ġsaying ?' âĢĿ\n",
      "\t Tokenized gpt2:Two ĠJewish Ġguys Ġfrom ĠO des a Ġmeet Ġup . ĠOne Ġasks Ġthe Ġother : ĠâĢĺ So Ġwhat âĢĻ s Ġthe Ġsituation ? ĠWhat Ġare Ġpeople Ġsaying ?' âĢĿ\n",
      "\t Unique Tokens llama3: {'âĢĻs'}\n",
      "\t Unique Tokens gpt2: {'âĢĻ', 's'}\n",
      "Text 2: Yep. Plus they repeated their own propaganda about Ukrainians so much that they started to believe in their superiority and underestimate Ukrainians. If someone is capable to stop Russian soldier than it can't be Ukrainian..\n",
      "\t Tokenized llama3:Yep . ĠPlus Ġthey Ġrepeated Ġtheir Ġown Ġpropaganda Ġabout ĠUkrain ians Ġso Ġmuch Ġthat Ġthey Ġstarted Ġto Ġbelieve Ġin Ġtheir Ġsuperior ity Ġand Ġunderest imate ĠUkrain ians . ĠIf Ġsomeone Ġis Ġcapable Ġto Ġstop ĠRussian Ġsoldier Ġthan Ġit Ġcan 't Ġbe ĠUkrain ian ..\n",
      "\t Tokenized gpt2:Yep . ĠPlus Ġthey Ġrepeated Ġtheir Ġown Ġpropaganda Ġabout ĠUkrain ians Ġso Ġmuch Ġthat Ġthey Ġstarted Ġto Ġbelieve Ġin Ġtheir Ġsuperior ity Ġand Ġunderest imate ĠUkrain ians . ĠIf Ġsomeone Ġis Ġcapable Ġto Ġstop ĠRussian Ġsoldier Ġthan Ġit Ġcan 't Ġbe ĠUkrain ian ..\n",
      "\t Unique Tokens llama3: set()\n",
      "\t Unique Tokens gpt2: set()\n",
      "==not Different Author==\n",
      "Text 1: The original post is absolutely true. Putin was a joke until he wasn’t. Sure we here in the US can laugh and call him a joke. I bet millions of Ukrainians don’t share that same sentiment\n",
      "\t Tokenized llama3:The Ġoriginal Ġpost Ġis Ġabsolutely Ġtrue . ĠPutin Ġwas Ġa Ġjoke Ġuntil Ġhe Ġwasn âĢĻt . ĠSure Ġwe Ġhere Ġin Ġthe ĠUS Ġcan Ġlaugh Ġand Ġcall Ġhim Ġa Ġjoke . ĠI Ġbet Ġmillions Ġof ĠUkrain ians Ġdon âĢĻt Ġshare Ġthat Ġsame Ġsentiment\n",
      "\t Tokenized gpt2:The Ġoriginal Ġpost Ġis Ġabsolutely Ġtrue . ĠPutin Ġwas Ġa Ġjoke Ġuntil Ġhe Ġwasn âĢĻ t . ĠSure Ġwe Ġhere Ġin Ġthe ĠUS Ġcan Ġlaugh Ġand Ġcall Ġhim Ġa Ġjoke . ĠI Ġbet Ġmillions Ġof ĠUkrain ians Ġdon âĢĻ t Ġshare Ġthat Ġsame Ġsentiment\n",
      "\t Unique Tokens llama3: {'âĢĻt'}\n",
      "\t Unique Tokens gpt2: {'t', 'âĢĻ'}\n",
      "Text 2: You’re missing the point. N Korea could still level S Korea if they wanted to. Sure, that’s about all they could do before we wiped them from the face of the earth. Nevertheless, they could still kill tens of thousands or even hundreds of thousands of S Koreans if they choose to\n",
      "\t Tokenized llama3:You âĢĻre Ġmissing Ġthe Ġpoint . ĠN ĠKorea Ġcould Ġstill Ġlevel ĠS ĠKorea Ġif Ġthey Ġwanted Ġto . ĠSure , Ġthat âĢĻs Ġabout Ġall Ġthey Ġcould Ġdo Ġbefore Ġwe Ġwiped Ġthem Ġfrom Ġthe Ġface Ġof Ġthe Ġearth . ĠNevertheless , Ġthey Ġcould Ġstill Ġkill Ġtens Ġof Ġthousands Ġor Ġeven Ġhundreds Ġof Ġthousands Ġof ĠS ĠKore ans Ġif Ġthey Ġchoose Ġto\n",
      "\t Tokenized gpt2:You âĢĻ re Ġmissing Ġthe Ġpoint . ĠN ĠKorea Ġcould Ġstill Ġlevel ĠS ĠKorea Ġif Ġthey Ġwanted Ġto . ĠSure , Ġthat âĢĻ s Ġabout Ġall Ġthey Ġcould Ġdo Ġbefore Ġwe Ġwiped Ġthem Ġfrom Ġthe Ġface Ġof Ġthe Ġearth . ĠNevertheless , Ġthey Ġcould Ġstill Ġkill Ġtens Ġof Ġthousands Ġor Ġeven Ġhundreds Ġof Ġthousands Ġof ĠS ĠKore ans Ġif Ġthey Ġchoose Ġto\n",
      "\t Unique Tokens llama3: {'âĢĻre', 'âĢĻs'}\n",
      "\t Unique Tokens gpt2: {'âĢĻ', 's', 're'}\n",
      "==Different Author==\n",
      "Text 1: Congressmen and Senators should be definitely banned from trading stocks, and the ban should be taken a step further and prohibit any family members from trading in stocks related to the committees of the members they're related too\n",
      "\t Tokenized llama3:Cong ress men Ġand ĠSen ators Ġshould Ġbe Ġdefinitely Ġbanned Ġfrom Ġtrading Ġstocks , Ġand Ġthe Ġban Ġshould Ġbe Ġtaken Ġa Ġstep Ġfurther Ġand Ġprohib it Ġany Ġfamily Ġmembers Ġfrom Ġtrading Ġin Ġstocks Ġrelated Ġto Ġthe Ġcommit tees Ġof Ġthe Ġmembers Ġthey 're Ġrelated Ġtoo\n",
      "\t Tokenized gpt2:Cong ress men Ġand ĠSen ators Ġshould Ġbe Ġdefinitely Ġbanned Ġfrom Ġtrading Ġstocks , Ġand Ġthe Ġban Ġshould Ġbe Ġtaken Ġa Ġstep Ġfurther Ġand Ġprohib it Ġany Ġfamily Ġmembers Ġfrom Ġtrading Ġin Ġstocks Ġrelated Ġto Ġthe Ġcommit tees Ġof Ġthe Ġmembers Ġthey 're Ġrelated Ġtoo\n",
      "\t Unique Tokens llama3: set()\n",
      "\t Unique Tokens gpt2: set()\n",
      "Text 2: I'm not defending it because I do think they should be banned from trading individual stocks (index funds I'm fine with and blind trusts should be mandatory), but so it's not like they're doing any of their stock investing in secret, it's quite easy to see what legislation is being debated and is going to come up for a vote, often months to years in advance of it happening. I also think the reason Congress tends to do better in the market then the average \"investor\" is the average investor isn't as clued into current economic trends, Congress has to be as it's part of their job. So it's not like most congresspeople are doing anything that is nefarious because it's all out in the open, more so than everyone else simply because they have more strict individual reporting requirements (well, not as strict as actual financial institutions)\n",
      "\t Tokenized llama3:I 'm Ġnot Ġdefending Ġit Ġbecause ĠI Ġdo Ġthink Ġthey Ġshould Ġbe Ġbanned Ġfrom Ġtrading Ġindividual Ġstocks Ġ( index Ġfunds ĠI 'm Ġfine Ġwith Ġand Ġblind Ġtrust s Ġshould Ġbe Ġmandatory ), Ġbut Ġso Ġit 's Ġnot Ġlike Ġthey 're Ġdoing Ġany Ġof Ġtheir Ġstock Ġinvesting Ġin Ġsecret , Ġit 's Ġquite Ġeasy Ġto Ġsee Ġwhat Ġlegislation Ġis Ġbeing Ġdeb ated Ġand Ġis Ġgoing Ġto Ġcome Ġup Ġfor Ġa Ġvote , Ġoften Ġmonths Ġto Ġyears Ġin Ġadvance Ġof Ġit Ġhappening . ĠI Ġalso Ġthink Ġthe Ġreason ĠCongress Ġtends Ġto Ġdo Ġbetter Ġin Ġthe Ġmarket Ġthen Ġthe Ġaverage Ġ\" invest or \" Ġis Ġthe Ġaverage Ġinvestor Ġisn 't Ġas Ġcl ued Ġinto Ġcurrent Ġeconomic Ġtrends , ĠCongress Ġhas Ġto Ġbe Ġas Ġit 's Ġpart Ġof Ġtheir Ġjob . ĠSo Ġit 's Ġnot Ġlike Ġmost Ġcongress people Ġare Ġdoing Ġanything Ġthat Ġis Ġne far ious Ġbecause Ġit 's Ġall Ġout Ġin Ġthe Ġopen , Ġmore Ġso Ġthan Ġeveryone Ġelse Ġsimply Ġbecause Ġthey Ġhave Ġmore Ġstrict Ġindividual Ġreporting Ġrequirements Ġ( well , Ġnot Ġas Ġstrict Ġas Ġactual Ġfinancial Ġinstitutions )\n",
      "\t Tokenized gpt2:I 'm Ġnot Ġdefending Ġit Ġbecause ĠI Ġdo Ġthink Ġthey Ġshould Ġbe Ġbanned Ġfrom Ġtrading Ġindividual Ġstocks Ġ( index Ġfunds ĠI 'm Ġfine Ġwith Ġand Ġblind Ġtrust s Ġshould Ġbe Ġmandatory ), Ġbut Ġso Ġit 's Ġnot Ġlike Ġthey 're Ġdoing Ġany Ġof Ġtheir Ġstock Ġinvesting Ġin Ġsecret , Ġit 's Ġquite Ġeasy Ġto Ġsee Ġwhat Ġlegislation Ġis Ġbeing Ġdeb ated Ġand Ġis Ġgoing Ġto Ġcome Ġup Ġfor Ġa Ġvote , Ġoften Ġmonths Ġto Ġyears Ġin Ġadvance Ġof Ġit Ġhappening . ĠI Ġalso Ġthink Ġthe Ġreason ĠCongress Ġtends Ġto Ġdo Ġbetter Ġin Ġthe Ġmarket Ġthen Ġthe Ġaverage Ġ\" invest or \" Ġis Ġthe Ġaverage Ġinvestor Ġisn 't Ġas Ġcl ued Ġinto Ġcurrent Ġeconomic Ġtrends , ĠCongress Ġhas Ġto Ġbe Ġas Ġit 's Ġpart Ġof Ġtheir Ġjob . ĠSo Ġit 's Ġnot Ġlike Ġmost Ġcongress people Ġare Ġdoing Ġanything Ġthat Ġis Ġne far ious Ġbecause Ġit 's Ġall Ġout Ġin Ġthe Ġopen , Ġmore Ġso Ġthan Ġeveryone Ġelse Ġsimply Ġbecause Ġthey Ġhave Ġmore Ġstrict Ġindividual Ġreporting Ġrequirements Ġ( well , Ġnot Ġas Ġstrict Ġas Ġactual Ġfinancial Ġinstitutions )\n",
      "\t Unique Tokens llama3: set()\n",
      "\t Unique Tokens gpt2: set()\n",
      "==not Different Author==\n",
      "Text 1: Yup. There is a huge hypocrisy standard all around. I recall when the Debra Lafave case happened way back along with the Mary Kay Letourneau case, and all I remember hearing from people like Bill Maher and so many others (mostly men) was that it what they did wasn't rape because the victims in the case must have obviously wanted it, due to them being pretty, I guess\n",
      "\t Tokenized llama3:Y up . ĠThere Ġis Ġa Ġhuge Ġhypocr isy Ġstandard Ġall Ġaround . ĠI Ġrecall Ġwhen Ġthe ĠDe bra ĠL af ave Ġcase Ġhappened Ġway Ġback Ġalong Ġwith Ġthe ĠMary ĠKay ĠLet our ne au Ġcase , Ġand Ġall ĠI Ġremember Ġhearing Ġfrom Ġpeople Ġlike ĠBill ĠMa her Ġand Ġso Ġmany Ġothers Ġ( mostly Ġmen ) Ġwas Ġthat Ġit Ġwhat Ġthey Ġdid Ġwasn 't Ġrape Ġbecause Ġthe Ġvictims Ġin Ġthe Ġcase Ġmust Ġhave Ġobviously Ġwanted Ġit , Ġdue Ġto Ġthem Ġbeing Ġpretty , ĠI Ġguess\n",
      "\t Tokenized gpt2:Y up . ĠThere Ġis Ġa Ġhuge Ġhypocr isy Ġstandard Ġall Ġaround . ĠI Ġrecall Ġwhen Ġthe ĠDe bra ĠL af ave Ġcase Ġhappened Ġway Ġback Ġalong Ġwith Ġthe ĠMary ĠKay ĠLet our ne au Ġcase , Ġand Ġall ĠI Ġremember Ġhearing Ġfrom Ġpeople Ġlike ĠBill ĠMa her Ġand Ġso Ġmany Ġothers Ġ( mostly Ġmen ) Ġwas Ġthat Ġit Ġwhat Ġthey Ġdid Ġwasn 't Ġrape Ġbecause Ġthe Ġvictims Ġin Ġthe Ġcase Ġmust Ġhave Ġobviously Ġwanted Ġit , Ġdue Ġto Ġthem Ġbeing Ġpretty , ĠI Ġguess\n",
      "\t Unique Tokens llama3: set()\n",
      "\t Unique Tokens gpt2: set()\n",
      "Text 2: We don't. We just point out the hypocrisy of Republicans, who claim anything LGBTQIA+ people do as wicked and evil, yet won't say anything about actual abuse. The Qanon bullshit actually hurt victims of sex abuse and sex trafficking, and despite being shown the evidence of it, they refuse to stop with their bigoted hate. It's ridiculous\n",
      "\t Tokenized llama3:We Ġdon 't . ĠWe Ġjust Ġpoint Ġout Ġthe Ġhypocr isy Ġof ĠRepublicans , Ġwho Ġclaim Ġanything ĠLGBT Q IA + Ġpeople Ġdo Ġas Ġwicked Ġand Ġevil , Ġyet Ġwon 't Ġsay Ġanything Ġabout Ġactual Ġabuse . ĠThe ĠQ anon Ġbullshit Ġactually Ġhurt Ġvictims Ġof Ġsex Ġabuse Ġand Ġsex Ġtrafficking , Ġand Ġdespite Ġbeing Ġshown Ġthe Ġevidence Ġof Ġit , Ġthey Ġrefuse Ġto Ġstop Ġwith Ġtheir Ġbig oted Ġhate . ĠIt 's Ġridiculous\n",
      "\t Tokenized gpt2:We Ġdon 't . ĠWe Ġjust Ġpoint Ġout Ġthe Ġhypocr isy Ġof ĠRepublicans , Ġwho Ġclaim Ġanything ĠLGBT Q IA + Ġpeople Ġdo Ġas Ġwicked Ġand Ġevil , Ġyet Ġwon 't Ġsay Ġanything Ġabout Ġactual Ġabuse . ĠThe ĠQ anon Ġbullshit Ġactually Ġhurt Ġvictims Ġof Ġsex Ġabuse Ġand Ġsex Ġtrafficking , Ġand Ġdespite Ġbeing Ġshown Ġthe Ġevidence Ġof Ġit , Ġthey Ġrefuse Ġto Ġstop Ġwith Ġtheir Ġbig oted Ġhate . ĠIt 's Ġridiculous\n",
      "\t Unique Tokens llama3: set()\n",
      "\t Unique Tokens gpt2: set()\n",
      "==not Different Author==\n",
      "Text 1: I agree they are sending some mixed messages and as a German it’s very annoying but as I voted for one of the coalition members I still have hope that they will do the sensible thing\n",
      "\t Tokenized llama3:I Ġagree Ġthey Ġare Ġsending Ġsome Ġmixed Ġmessages Ġand Ġas Ġa ĠGerman Ġit âĢĻs Ġvery Ġannoying Ġbut Ġas ĠI Ġvoted Ġfor Ġone Ġof Ġthe Ġcoalition Ġmembers ĠI Ġstill Ġhave Ġhope Ġthat Ġthey Ġwill Ġdo Ġthe Ġsensible Ġthing\n",
      "\t Tokenized gpt2:I Ġagree Ġthey Ġare Ġsending Ġsome Ġmixed Ġmessages Ġand Ġas Ġa ĠGerman Ġit âĢĻ s Ġvery Ġannoying Ġbut Ġas ĠI Ġvoted Ġfor Ġone Ġof Ġthe Ġcoalition Ġmembers ĠI Ġstill Ġhave Ġhope Ġthat Ġthey Ġwill Ġdo Ġthe Ġsensible Ġthing\n",
      "\t Unique Tokens llama3: {'âĢĻs'}\n",
      "\t Unique Tokens gpt2: {'âĢĻ', 's'}\n",
      "Text 2: I like how everybody is judging Olaf for this even tho it’s very normal for two countries to meet when they do so much business. The strategy for the future is changing but for now Germany still relies on China. This needs to change but until then this is what needs to be done. The coalition is Germany has more or less agreed to dealing with China differently\n",
      "\t Tokenized llama3:I Ġlike Ġhow Ġeverybody Ġis Ġjudging ĠOl af Ġfor Ġthis Ġeven Ġtho Ġit âĢĻs Ġvery Ġnormal Ġfor Ġtwo Ġcountries Ġto Ġmeet Ġwhen Ġthey Ġdo Ġso Ġmuch Ġbusiness . ĠThe Ġstrategy Ġfor Ġthe Ġfuture Ġis Ġchanging Ġbut Ġfor Ġnow ĠGermany Ġstill Ġrelies Ġon ĠChina . ĠThis Ġneeds Ġto Ġchange Ġbut Ġuntil Ġthen Ġthis Ġis Ġwhat Ġneeds Ġto Ġbe Ġdone . ĠThe Ġcoalition Ġis ĠGermany Ġhas Ġmore Ġor Ġless Ġagreed Ġto Ġdealing Ġwith ĠChina Ġdifferently\n",
      "\t Tokenized gpt2:I Ġlike Ġhow Ġeverybody Ġis Ġjudging ĠOl af Ġfor Ġthis Ġeven Ġtho Ġit âĢĻ s Ġvery Ġnormal Ġfor Ġtwo Ġcountries Ġto Ġmeet Ġwhen Ġthey Ġdo Ġso Ġmuch Ġbusiness . ĠThe Ġstrategy Ġfor Ġthe Ġfuture Ġis Ġchanging Ġbut Ġfor Ġnow ĠGermany Ġstill Ġrelies Ġon ĠChina . ĠThis Ġneeds Ġto Ġchange Ġbut Ġuntil Ġthen Ġthis Ġis Ġwhat Ġneeds Ġto Ġbe Ġdone . ĠThe Ġcoalition Ġis ĠGermany Ġhas Ġmore Ġor Ġless Ġagreed Ġto Ġdealing Ġwith ĠChina Ġdifferently\n",
      "\t Unique Tokens llama3: {'âĢĻs'}\n",
      "\t Unique Tokens gpt2: {'âĢĻ', 's'}\n",
      "==not Different Author==\n",
      "Text 1: Each line has 9 words, adding the extra word only adds clarity if you don't take the work as a whole, and the only sentences someone is likely to use independently are the first and last sentences, referencing the whole\n",
      "\t Tokenized llama3:Each Ġline Ġhas Ġ 9 Ġwords , Ġadding Ġthe Ġextra Ġword Ġonly Ġadds Ġclarity Ġif Ġyou Ġdon 't Ġtake Ġthe Ġwork Ġas Ġa Ġwhole , Ġand Ġthe Ġonly Ġsentences Ġsomeone Ġis Ġlikely Ġto Ġuse Ġindependently Ġare Ġthe Ġfirst Ġand Ġlast Ġsentences , Ġreferencing Ġthe Ġwhole\n",
      "\t Tokenized gpt2:Each Ġline Ġhas Ġ9 Ġwords , Ġadding Ġthe Ġextra Ġword Ġonly Ġadds Ġclarity Ġif Ġyou Ġdon 't Ġtake Ġthe Ġwork Ġas Ġa Ġwhole , Ġand Ġthe Ġonly Ġsentences Ġsomeone Ġis Ġlikely Ġto Ġuse Ġindependently Ġare Ġthe Ġfirst Ġand Ġlast Ġsentences , Ġreferencing Ġthe Ġwhole\n",
      "\t Unique Tokens llama3: {'Ġ', '9'}\n",
      "\t Unique Tokens gpt2: {'Ġ9'}\n",
      "Text 2: Is it the best, most clear sentence? No. Is it easy to interpret? Yes. Does the meter work better as written? Yes\n",
      "\t Tokenized llama3:Is Ġit Ġthe Ġbest , Ġmost Ġclear Ġsentence ? ĠNo . ĠIs Ġit Ġeasy Ġto Ġinterpret ? ĠYes . ĠDoes Ġthe Ġmeter Ġwork Ġbetter Ġas Ġwritten ? ĠYes\n",
      "\t Tokenized gpt2:Is Ġit Ġthe Ġbest , Ġmost Ġclear Ġsentence ? ĠNo . ĠIs Ġit Ġeasy Ġto Ġinterpret ? ĠYes . ĠDoes Ġthe Ġmeter Ġwork Ġbetter Ġas Ġwritten ? ĠYes\n",
      "\t Unique Tokens llama3: set()\n",
      "\t Unique Tokens gpt2: set()\n",
      "==not Different Author==\n",
      "Text 1: Well yeah it definitely doesn’t help them that she’s a 10 while the best they can do is a skanky escort and goonfaced KKKrossfit\n",
      "\t Tokenized llama3:Well Ġyeah Ġit Ġdefinitely Ġdoesn âĢĻt Ġhelp Ġthem Ġthat Ġshe âĢĻs Ġa Ġ 10 Ġwhile Ġthe Ġbest Ġthey Ġcan Ġdo Ġis Ġa Ġsk ank y Ġesc ort Ġand Ġgo on f aced ĠK KK ross fit\n",
      "\t Tokenized gpt2:Well Ġyeah Ġit Ġdefinitely Ġdoesn âĢĻ t Ġhelp Ġthem Ġthat Ġshe âĢĻ s Ġa Ġ10 Ġwhile Ġthe Ġbest Ġthey Ġcan Ġdo Ġis Ġa Ġsk ank y Ġescort Ġand Ġgo on faced ĠK KK ross fit\n",
      "\t Unique Tokens llama3: {'aced', 'Ġ', 'f', 'Ġesc', 'ort', 'âĢĻs', '10', 'âĢĻt'}\n",
      "\t Unique Tokens gpt2: {'Ġ10', 's', 'faced', 'âĢĻ', 't', 'Ġescort'}\n",
      "Text 2: AOC’s representation in the right wing media however is a sexist, racist distortion of someone who is actually competent. They’re afraid of her because of that competency. She’s progressive and popular and there isn’t one Republican in the House who can stand against her rhetorically. And I’m not talking about the average Fox News viewer. I’m talking about the establishment. They know she’s a threat to them so they have to smear her\n",
      "\t Tokenized llama3:A OC âĢĻs Ġrepresentation Ġin Ġthe Ġright Ġwing Ġmedia Ġhowever Ġis Ġa Ġsex ist , Ġracist Ġdist ortion Ġof Ġsomeone Ġwho Ġis Ġactually Ġcompetent . ĠThey âĢĻre Ġafraid Ġof Ġher Ġbecause Ġof Ġthat Ġcompet ency . ĠShe âĢĻs Ġprogressive Ġand Ġpopular Ġand Ġthere Ġisn âĢĻt Ġone ĠRepublican Ġin Ġthe ĠHouse Ġwho Ġcan Ġstand Ġagainst Ġher Ġrhet or ically . ĠAnd ĠI âĢĻm Ġnot Ġtalking Ġabout Ġthe Ġaverage ĠFox ĠNews Ġviewer . ĠI âĢĻm Ġtalking Ġabout Ġthe Ġestablishment . ĠThey Ġknow Ġshe âĢĻs Ġa Ġthreat Ġto Ġthem Ġso Ġthey Ġhave Ġto Ġsm ear Ġher\n",
      "\t Tokenized gpt2:A OC âĢĻ s Ġrepresentation Ġin Ġthe Ġright Ġwing Ġmedia Ġhowever Ġis Ġa Ġsex ist , Ġracist Ġdistortion Ġof Ġsomeone Ġwho Ġis Ġactually Ġcompetent . ĠThey âĢĻ re Ġafraid Ġof Ġher Ġbecause Ġof Ġthat Ġcompet ency . ĠShe âĢĻ s Ġprogressive Ġand Ġpopular Ġand Ġthere Ġisn âĢĻ t Ġone ĠRepublican Ġin Ġthe ĠHouse Ġwho Ġcan Ġstand Ġagainst Ġher Ġrhet or ically . ĠAnd ĠI âĢĻ m Ġnot Ġtalking Ġabout Ġthe Ġaverage ĠFox ĠNews Ġviewer . ĠI âĢĻ m Ġtalking Ġabout Ġthe Ġestablishment . ĠThey Ġknow Ġshe âĢĻ s Ġa Ġthreat Ġto Ġthem Ġso Ġthey Ġhave Ġto Ġsm ear Ġher\n",
      "\t Unique Tokens llama3: {'ortion', 'âĢĻm', 'Ġdist', 'âĢĻs', 'âĢĻt', 'âĢĻre'}\n",
      "\t Unique Tokens gpt2: {'s', 'm', 'Ġdistortion', 'âĢĻ', 're', 't'}\n",
      "==not Different Author==\n",
      "Text 1: That’s not at all a problem. And one thing, nerva isn’t a nuclear reactor it uses a small amount of fissile material to heat up liquid hydrogen into heated gas. And the small amount isn’t even much of an issue if it explodes because it’s so small it wouldn’t cause much fallout. All your thinking about is a bunch of anti nuclear propaganda pushed by coal and oil companies to keep peddling their shitty products\n",
      "\t Tokenized llama3:That âĢĻs Ġnot Ġat Ġall Ġa Ġproblem . ĠAnd Ġone Ġthing , Ġnerv a Ġisn âĢĻt Ġa Ġnuclear Ġreactor Ġit Ġuses Ġa Ġsmall Ġamount Ġof Ġf iss ile Ġmaterial Ġto Ġheat Ġup Ġliquid Ġhydrogen Ġinto Ġheated Ġgas . ĠAnd Ġthe Ġsmall Ġamount Ġisn âĢĻt Ġeven Ġmuch Ġof Ġan Ġissue Ġif Ġit Ġexpl odes Ġbecause Ġit âĢĻs Ġso Ġsmall Ġit Ġwouldn âĢĻt Ġcause Ġmuch Ġfall out . ĠAll Ġyour Ġthinking Ġabout Ġis Ġa Ġbunch Ġof Ġanti Ġnuclear Ġpropaganda Ġpushed Ġby Ġcoal Ġand Ġoil Ġcompanies Ġto Ġkeep Ġped d ling Ġtheir Ġshitty Ġproducts\n",
      "\t Tokenized gpt2:That âĢĻ s Ġnot Ġat Ġall Ġa Ġproblem . ĠAnd Ġone Ġthing , Ġnerv a Ġisn âĢĻ t Ġa Ġnuclear Ġreactor Ġit Ġuses Ġa Ġsmall Ġamount Ġof Ġf iss ile Ġmaterial Ġto Ġheat Ġup Ġliquid Ġhydrogen Ġinto Ġheated Ġgas . ĠAnd Ġthe Ġsmall Ġamount Ġisn âĢĻ t Ġeven Ġmuch Ġof Ġan Ġissue Ġif Ġit Ġexpl odes Ġbecause Ġit âĢĻ s Ġso Ġsmall Ġit Ġwouldn âĢĻ t Ġcause Ġmuch Ġfall out . ĠAll Ġyour Ġthinking Ġabout Ġis Ġa Ġbunch Ġof Ġanti Ġnuclear Ġpropaganda Ġpushed Ġby Ġcoal Ġand Ġoil Ġcompanies Ġto Ġkeep Ġped d ling Ġtheir Ġshitty Ġproducts\n",
      "\t Unique Tokens llama3: {'âĢĻt', 'âĢĻs'}\n",
      "\t Unique Tokens gpt2: {'âĢĻ', 't', 's'}\n",
      "Text 2: And in what way do we need to cart about fallout in space? The engine thrust isn’t enough to lift anything off the ground, it would be reserved for space activities only\n",
      "\t Tokenized llama3:And Ġin Ġwhat Ġway Ġdo Ġwe Ġneed Ġto Ġcart Ġabout Ġfall out Ġin Ġspace ? ĠThe Ġengine Ġthrust Ġisn âĢĻt Ġenough Ġto Ġlift Ġanything Ġoff Ġthe Ġground , Ġit Ġwould Ġbe Ġreserved Ġfor Ġspace Ġactivities Ġonly\n",
      "\t Tokenized gpt2:And Ġin Ġwhat Ġway Ġdo Ġwe Ġneed Ġto Ġcart Ġabout Ġfall out Ġin Ġspace ? ĠThe Ġengine Ġthrust Ġisn âĢĻ t Ġenough Ġto Ġlift Ġanything Ġoff Ġthe Ġground , Ġit Ġwould Ġbe Ġreserved Ġfor Ġspace Ġactivities Ġonly\n",
      "\t Unique Tokens llama3: {'âĢĻt'}\n",
      "\t Unique Tokens gpt2: {'t', 'âĢĻ'}\n"
     ]
    }
   ],
   "source": [
    "compare_predictions(df_llama3, df_gpt2, llama3_tok, gpt2_tok, \"llama3\", \"gpt2\", classification_at_1=\"Different Author\", text1=\"text 1\", text2=\"text 2\", n_examples=10, nbr_char_to_show=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-09T13:45:48.433676Z",
     "start_time": "2025-02-09T13:45:48.358137Z"
    }
   },
   "id": "fbe5d1e510428988"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==not Same Author==\n",
      "Text 1: close enough to see the coast. He didn't tell her that he'd quit the salt mines because, sea air notwithstanding, he could not bear to be so far underground, the white walls dwarfing him, his own shadow a stranger as he hunched over and shuttled carts of supplies to white men similar to how he imagi\n",
      "\t Tokenized 128k:close Ġenough Ġto Ġsee Ġthe Ġcoast . ĠHe Ġdidn 't Ġtell Ġher Ġthat Ġhe 'd Ġquit Ġthe Ġsalt Ġmines Ġbecause , Ġsea Ġair Ġnotwithstanding , Ġhe Ġcould Ġnot Ġbear Ġto Ġbe Ġso Ġfar Ġunderground , Ġthe Ġwhite Ġwalls Ġdwarf ing Ġhim , Ġhis Ġown Ġshadow Ġa Ġstranger Ġas Ġhe Ġhunched Ġover Ġand Ġshutt led Ġcarts Ġof Ġsupplies Ġto Ġwhite Ġmen Ġsimilar Ġto Ġhow Ġhe Ġimag i\n",
      "\t Tokenized 32k:close Ġenough Ġto Ġsee Ġthe Ġcoast . ĠHe Ġdidn 't Ġtell Ġher Ġthat Ġhe 'd Ġquit Ġthe Ġsalt Ġmines Ġbecause , Ġsea Ġair Ġnot with standing , Ġhe Ġcould Ġnot Ġbear Ġto Ġbe Ġso Ġfar Ġunderground , Ġthe Ġwhite Ġwalls Ġdwarf ing Ġhim , Ġhis Ġown Ġshadow Ġa Ġstranger Ġas Ġhe Ġhun ched Ġover Ġand Ġshut t led Ġc arts Ġof Ġsupplies Ġto Ġwhite Ġmen Ġsimilar Ġto Ġhow Ġhe Ġimag i\n",
      "\t Unique Tokens 128k: {'ĠWyoming', 'PERSON', 'ĠNegro', 'Ġthirties', 'Ġappealed', 'Ġshutt', 'ĠWax', 'Ġfoothold', 'Ġcarts', 'Ġbourbon', 'ĠMcGee', 'ĠSleeping', 'Ġpint', 'Ġstreetcar', 'Ġthermostat', 'Ġsummers', 'Ġradiator', 'Ġpredicated', 'starved', 'Ġhunched', 'Ġconcoction', 'Ġnotwithstanding', 'Ġaccordion', 'Ġversed', 'Ġquar', 'Ġivory', 'Fib', 'ĠExodus', 'Ġbanging', 'ĠMile', 'Ġhangovers', 'Ġsteamy', 'Ġadultery', 'Ġtwenties', 'Ġencroaching', 'Ġfondness', 'Ġgoings', 'Ġabsolve'}\n",
      "\t Unique Tokens 32k: {'with', 'int', 'ĠMcG', 'at', 'bon', 'ĠSleep', 'icated', 'Ġaccord', 'Ġgo', 'arts', 'ĠEx', 'PER', 'Ġhun', 'ax', 'od', 'Ġcon', 'y', 'ion', 'us', 'car', 'Ġb', 'Ġp', 'Ġenc', 'ator', 'overs', 'olve', 'co', 'aching', 'Ġstreet', 'star', 'ĠM', 'Ġth', 'ings', 'Ġadul', 'ro', 'ar', 'ction', 'tery', 'ved', 'aled', 'ee', 'old', 't', 'Ġther', 'Ġqu', 'Ġsum', 'gro', 'Ġtwent', 'Ġfond', 'ies', 'Ġf', 'Ġbour', 'ched', 'ĠWy', 'Ġradi', 'ooth', 'Ġvers', 'anging', 'Ġiv', 'ness', 'most', 'ed', 'Ġhang', 'standing', 'ĠNe', 'oming', 'ib', 'Ġsteam', 'Ġc', 'ile', 'ĠW', 'Ġshut', 'F', 'ory', 'irt', 'Ġappe', 'SON', 'mers', 'Ġpred', 'Ġabs'}\n",
      "Text 2: I think a lot about how different everything would be if just the smallest thing were changed, like how the whole world would be an entirely different place if it hadn't been for one crazy guy saying cats were familiars of the devil, or how my life might have been totally different if I had just bee\n",
      "\t Tokenized 128k:I Ġthink Ġa Ġlot Ġabout Ġhow Ġdifferent Ġeverything Ġwould Ġbe Ġif Ġjust Ġthe Ġsmallest Ġthing Ġwere Ġchanged , Ġlike Ġhow Ġthe Ġwhole Ġworld Ġwould Ġbe Ġan Ġentirely Ġdifferent Ġplace Ġif Ġit Ġhadn 't Ġbeen Ġfor Ġone Ġcrazy Ġguy Ġsaying Ġcats Ġwere Ġfamili ars Ġof Ġthe Ġdevil , Ġor Ġhow Ġmy Ġlife Ġmight Ġhave Ġbeen Ġtotally Ġdifferent Ġif ĠI Ġhad Ġjust Ġbee\n",
      "\t Tokenized 32k:I Ġthink Ġa Ġlot Ġabout Ġhow Ġdifferent Ġeverything Ġwould Ġbe Ġif Ġjust Ġthe Ġsmallest Ġthing Ġwere Ġchanged , Ġlike Ġhow Ġthe Ġwhole Ġworld Ġwould Ġbe Ġan Ġentirely Ġdifferent Ġplace Ġif Ġit Ġhadn 't Ġbeen Ġfor Ġone Ġcrazy Ġguy Ġsaying Ġcats Ġwere Ġfam ili ars Ġof Ġthe Ġdevil , Ġor Ġhow Ġmy Ġlife Ġmight Ġhave Ġbeen Ġtotally Ġdifferent Ġif ĠI Ġhad Ġjust Ġbee\n",
      "\t Unique Tokens 128k: {'Ġ[](/', 'Ġfamili', 'Ġregionally'}\n",
      "\t Unique Tokens 32k: {'Ġregion', 'ili', 'Ġfam', 'Ġ[', '](/', 'ally'}\n",
      "==not Same Author==\n",
      "Text 1: Sorry if this quesiton has been asked before, but I didn't see the answer in the huge MyFaces FAQ (..need to work on that FAQ!).\n",
      "\n",
      "So, can MyFaces be integrated to work with Java Studio Creator?\n",
      "\n",
      "I believe that MyFaces can co-exist with JSF RI as per the FAQ, but as far as \"integration\" with JSC wher\n",
      "\t Tokenized 128k:Sorry Ġif Ġthis Ġques iton Ġhas Ġbeen Ġasked Ġbefore , Ġbut ĠI Ġdidn 't Ġsee Ġthe Ġanswer Ġin Ġthe Ġhuge ĠMy Faces ĠFAQ Ġ( .. need Ġto Ġwork Ġon Ġthat ĠFAQ !). Ċ Ċ So , Ġcan ĠMy Faces Ġbe Ġintegrated Ġto Ġwork Ġwith ĠJava ĠStudio ĠCreator ? Ċ Ċ I Ġbelieve Ġthat ĠMy Faces Ġcan Ġco - exist Ġwith ĠJSF ĠRI Ġas Ġper Ġthe ĠFAQ , Ġbut Ġas Ġfar Ġas Ġ\" integration \" Ġwith ĠJSC Ġw her\n",
      "\t Tokenized 32k:Sorry Ġif Ġthis Ġqu es iton Ġhas Ġbeen Ġasked Ġbefore , Ġbut ĠI Ġdidn 't Ġsee Ġthe Ġanswer Ġin Ġthe Ġhuge ĠMy F aces ĠFAQ Ġ( .. need Ġto Ġwork Ġon Ġthat ĠFAQ !). Ċ Ċ So , Ġcan ĠMy F aces Ġbe Ġintegrated Ġto Ġwork Ġwith ĠJava ĠStudio ĠCreat or ? Ċ Ċ I Ġbelieve Ġthat ĠMy F aces Ġcan Ġco - exist Ġwith ĠJS F ĠR I Ġas Ġper Ġthe ĠFAQ , Ġbut Ġas Ġfar Ġas Ġ\" integration \" Ġwith ĠJ SC Ġw her\n",
      "\t Unique Tokens 128k: {'Ġtabbed', 'Faces', 'ĠJSF', 'Ġques', 'Ġpanes', 'ĠCreator', 'ĠRI', 'ĠJSC'}\n",
      "\t Unique Tokens 32k: {'Ġqu', 'ĠCreat', 'es', 'F', 'Ġtab', 'aces', 'Ġpan', 'ĠR', 'or', 'ĠJ', 'ĠJS', 'bed', 'SC'}\n",
      "Text 2: I thought I was the only person who preferred them! I think the iPhone ones look tacky, as another redditor put it, is because they are trying to cram too much detail into a tiny area. The default Android emojis, have a simpler design, but look much better IMO. Kinda of like the old iPhone 'liflike'\n",
      "\t Tokenized 128k:I Ġthought ĠI Ġwas Ġthe Ġonly Ġperson Ġwho Ġpreferred Ġthem ! ĠI Ġthink Ġthe ĠiPhone Ġones Ġlook Ġtacky , Ġas Ġanother Ġredditor Ġput Ġit , Ġis Ġbecause Ġthey Ġare Ġtrying Ġto Ġcram Ġtoo Ġmuch Ġdetail Ġinto Ġa Ġtiny Ġarea . ĠThe Ġdefault ĠAndroid Ġemojis , Ġhave Ġa Ġsimpler Ġdesign , Ġbut Ġlook Ġmuch Ġbetter ĠIMO . ĠKinda Ġof Ġlike Ġthe Ġold ĠiPhone Ġ' lif like '\n",
      "\t Tokenized 32k:I Ġthought ĠI Ġwas Ġthe Ġonly Ġperson Ġwho Ġpreferred Ġthem ! ĠI Ġthink Ġthe ĠiPhone Ġones Ġlook Ġtack y , Ġas Ġanother Ġredd itor Ġput Ġit , Ġis Ġbecause Ġthey Ġare Ġtrying Ġto Ġcr am Ġtoo Ġmuch Ġdetail Ġinto Ġa Ġtiny Ġarea . ĠThe Ġdefault ĠAndroid Ġem oj is , Ġhave Ġa Ġsimpler Ġdesign , Ġbut Ġlook Ġmuch Ġbetter ĠIMO . ĠKind a Ġof Ġlike Ġthe Ġold ĠiPhone Ġ' l if like '\n",
      "\t Unique Tokens 128k: {'ĠHolo', 'lif', 'ĠKinda', 'Ġtacky', 'Ġredditor', 'Ġcram', 'Ġemojis'}\n",
      "\t Unique Tokens 32k: {'o', 'l', 'ĠHol', 'Ġredd', 'if', 'oj', 'is', 'Ġem', 'Ġtack', 'ĠKind', 'Ġcr', 'itor', 'am', 'y', 'a'}\n",
      "==not Same Author==\n",
      "Text 1: Hi,\n",
      "\n",
      "it seams that there are no firewall hooks in pve-firewall is this correct?\n",
      "\n",
      "I would like to add my own action before, after the firewall \n",
      "configuration for a VM is stop,started or reloaded.\n",
      "\n",
      "My use case would be adding ARP filter and bridge filter rules, because \n",
      "at the moment each VM gets all \n",
      "\t Tokenized 128k:Hi , Ċ Ċ it Ġseams Ġthat Ġthere Ġare Ġno Ġfirewall Ġhooks Ġin Ġpve - firewall Ġis Ġthis Ġcorrect ? Ċ Ċ I Ġwould Ġlike Ġto Ġadd Ġmy Ġown Ġaction Ġbefore , Ġafter Ġthe Ġfirewall Ġ Ċ configuration Ġfor Ġa ĠVM Ġis Ġstop , started Ġor Ġreloaded . Ċ Ċ My Ġuse Ġcase Ġwould Ġbe Ġadding ĠARP Ġfilter Ġand Ġbridge Ġfilter Ġrules , Ġbecause Ġ Ċ at Ġthe Ġmoment Ġeach ĠVM Ġgets Ġall Ġ\n",
      "\t Tokenized 32k:Hi , Ċ Ċ it Ġse ams Ġthat Ġthere Ġare Ġno Ġfirewall Ġhooks Ġin Ġp ve - fire wall Ġis Ġthis Ġcorrect ? Ċ Ċ I Ġwould Ġlike Ġto Ġadd Ġmy Ġown Ġaction Ġbefore , Ġafter Ġthe Ġfirewall Ġ Ċ configuration Ġfor Ġa ĠVM Ġis Ġstop , started Ġor Ġreload ed . Ċ Ċ My Ġuse Ġcase Ġwould Ġbe Ġadding ĠAR P Ġfilter Ġand Ġbridge Ġfilter Ġrules , Ġbecause Ġ Ċ at Ġthe Ġmoment Ġeach ĠVM Ġgets Ġall Ġ\n",
      "\t Unique Tokens 128k: {'ARP', 'firewall', 'Ġmulticast', 'thx', 'Ġreloaded', 'directed', 'ĠSTP', 'ĠARP', 'Harald', 'Ġseams', 'Ġpve'}\n",
      "\t Unique Tokens 32k: {'Ġreload', 'fire', 'P', 'ald', 'AR', 'direct', 'Ġmultic', 'Ġse', 'ĠST', 'ed', 'Har', 'ĠAR', 'wall', 'ams', 've', 'th', 'ast', 'Ġp', 'x'}\n",
      "Text 2: Oh, I did manage to get this working. Thought I'd answer my own question. The key was to create a symlink for the missing libisl.10.dylib so that gcc 4.9.2 runs correctly, then recompile libtorrent, then reinstall rtorrent.\n",
      "Step 1: Downgrade gcc to 4.9.2.\n",
      "brew switch gcc 4.9.2\n",
      "\n",
      "Step 2: Ensure the sy\n",
      "\t Tokenized 128k:Oh , ĠI Ġdid Ġmanage Ġto Ġget Ġthis Ġworking . ĠThought ĠI 'd Ġanswer Ġmy Ġown Ġquestion . ĠThe Ġkey Ġwas Ġto Ġcreate Ġa Ġsymlink Ġfor Ġthe Ġmissing Ġlib isl . 10 . dylib Ġso Ġthat Ġgcc Ġ4 . 9 . 2 Ġruns Ġcorrectly , Ġthen Ġrecompile Ġlibt orrent , Ġthen Ġreinstall Ġr torrent . Ċ Step Ġ1 : ĠDown grade Ġgcc Ġto Ġ4 . 9 . 2 . Ċ brew Ġswitch Ġgcc Ġ4 . 9 . 2 Ċ Ċ Step Ġ2 : ĠEnsure Ġthe Ġsy\n",
      "\t Tokenized 32k:Oh , ĠI Ġdid Ġmanage Ġto Ġget Ġthis Ġworking . ĠThought ĠI 'd Ġanswer Ġmy Ġown Ġquestion . ĠThe Ġkey Ġwas Ġto Ġcreate Ġa Ġsy ml ink Ġfor Ġthe Ġmissing Ġlib isl . 10 . dy lib Ġso Ġthat Ġgcc Ġ4 . 9 . 2 Ġruns Ġcorrectly , Ġthen Ġre compile Ġli bt or rent , Ġthen Ġreinstall Ġr tor rent . Ċ Step Ġ1 : ĠDown grade Ġgcc Ġto Ġ4 . 9 . 2 . Ċ brew Ġswitch Ġgcc Ġ4 . 9 . 2 Ċ Ċ Step Ġ2 : ĠEn sure Ġthe Ġsy\n",
      "\t Unique Tokens 128k: {'torrent', 'Ġsymlink', 'orrent', 'dylib', 'Ġlibt', 'ĠEnsure', 'ĠUninstall', 'Ġrecompile'}\n",
      "\t Unique Tokens 32k: {'bt', 'dy', 'ml', 'compile', 'ĠEn', 'Ġsy', 'ĠUn', 'Ġli', 'tor', 'or', 'rent', 'Ġre', 'ink', 'sure'}\n",
      "==not Same Author==\n",
      "Text 1: Hi.\n",
      "I need a quick way to get the ip address of the esternal esmtp client that\n",
      "submit emails to my courier.\n",
      "\n",
      "Is there a way to force courier to generate a unique custom mail header\n",
      "with the ip address of the esmtp sender?\n",
      "\n",
      "I need to classify mails from maildrop filter rules by incoming mails ip\n",
      "addr\n",
      "\t Tokenized 128k:Hi . Ċ I Ġneed Ġa Ġquick Ġway Ġto Ġget Ġthe Ġip Ġaddress Ġof Ġthe Ġes ternal Ġes m tp Ġclient Ġthat Ċ submit Ġemails Ġto Ġmy Ġcourier . Ċ Ċ Is Ġthere Ġa Ġway Ġto Ġforce Ġcourier Ġto Ġgenerate Ġa Ġunique Ġcustom Ġmail Ġheader Ċ with Ġthe Ġip Ġaddress Ġof Ġthe Ġes m tp Ġsender ? Ċ Ċ I Ġneed Ġto Ġclassify Ġmails Ġfrom Ġmaild rop Ġfilter Ġrules Ġby Ġincoming Ġmails Ġip Ċ addr\n",
      "\t Tokenized 32k:Hi . Ċ I Ġneed Ġa Ġquick Ġway Ġto Ġget Ġthe Ġip Ġaddress Ġof Ġthe Ġes ternal Ġes m tp Ġclient Ġthat Ċ submit Ġemails Ġto Ġmy Ġcou rier . Ċ Ċ Is Ġthere Ġa Ġway Ġto Ġforce Ġcou rier Ġto Ġgenerate Ġa Ġunique Ġcustom Ġmail Ġheader Ċ with Ġthe Ġip Ġaddress Ġof Ġthe Ġes m tp Ġsender ? Ċ Ċ I Ġneed Ġto Ġclassify Ġmails Ġfrom Ġma ild rop Ġfilter Ġrules Ġby Ġincoming Ġmails Ġip Ċ addr\n",
      "\t Unique Tokens 128k: {'Ġmaild', 'Ġcourier'}\n",
      "\t Unique Tokens 32k: {'Ġcou', 'rier', 'Ġma', 'ild'}\n",
      "Text 2: You cannot send data received from your external application to a web page by trying to respond to the web page's GET request with request.body of POST request received from the external application. The simplest way to do what you're trying to achieve would be something like this.\n",
      "app.use(myParser.\n",
      "\t Tokenized 128k:You Ġcannot Ġsend Ġdata Ġreceived Ġfrom Ġyour Ġexternal Ġapplication Ġto Ġa Ġweb Ġpage Ġby Ġtrying Ġto Ġrespond Ġto Ġthe Ġweb Ġpage 's ĠGET Ġrequest Ġwith Ġrequest . body Ġof ĠPOST Ġrequest Ġreceived Ġfrom Ġthe Ġexternal Ġapplication . ĠThe Ġsimplest Ġway Ġto Ġdo Ġwhat Ġyou 're Ġtrying Ġto Ġachieve Ġwould Ġbe Ġsomething Ġlike Ġthis . Ċ app . use ( my Parser .\n",
      "\t Tokenized 32k:You Ġcannot Ġsend Ġdata Ġreceived Ġfrom Ġyour Ġexternal Ġapplication Ġto Ġa Ġweb Ġpage Ġby Ġtrying Ġto Ġrespond Ġto Ġthe Ġweb Ġpage 's ĠGET Ġrequest Ġwith Ġrequest . body Ġof ĠPOST Ġrequest Ġreceived Ġfrom Ġthe Ġexternal Ġapplication . ĠThe Ġsimplest Ġway Ġto Ġdo Ġwhat Ġyou 're Ġtrying Ġto Ġachieve Ġwould Ġbe Ġsomething Ġlike Ġthis . Ċ app . use ( my Parser .\n",
      "\t Unique Tokens 128k: {'}));', 'extended', 'getData', 'retrieve'}\n",
      "\t Unique Tokens 32k: {'ex', '));', 'rieve', 'tended', '}', 'ret'}\n",
      "==not Same Author==\n",
      "Text 1: Hello,\n",
      "\n",
      "I'm just testing Ivy, and I encountered a first problem.\n",
      "\n",
      "My corporate \"repository\" is shared through a network directory.\n",
      "\n",
      "So, I understand I have to customize Ivy settings to handle this.\n",
      "\n",
      "My problem is the \"ivy.shared.default.root\" is dynamic... It looks like:\n",
      "//builds/5_greatest/win64_x6\n",
      "\t Tokenized 128k:Hello , Ċ Ċ I 'm Ġjust Ġtesting ĠIvy , Ġand ĠI Ġencountered Ġa Ġfirst Ġproblem . Ċ Ċ My Ġcorporate Ġ\" repository \" Ġis Ġshared Ġthrough Ġa Ġnetwork Ġdirectory . Ċ Ċ So , ĠI Ġunderstand ĠI Ġhave Ġto Ġcustomize ĠIvy Ġsettings Ġto Ġhandle Ġthis . Ċ Ċ My Ġproblem Ġis Ġthe Ġ\" ivy . shared . default . root \" Ġis Ġdynamic ... ĠIt Ġlooks Ġlike : Ċ // builds / 5 _ greatest / win 64 _ x 6\n",
      "\t Tokenized 32k:Hello , Ċ Ċ I 'm Ġjust Ġtesting ĠIvy , Ġand ĠI Ġencountered Ġa Ġfirst Ġproblem . Ċ Ċ My Ġcorporate Ġ\" repository \" Ġis Ġshared Ġthrough Ġa Ġnetwork Ġdirectory . Ċ Ċ So , ĠI Ġunderstand ĠI Ġhave Ġto Ġcustomize ĠIvy Ġsettings Ġto Ġhandle Ġthis . Ċ Ċ My Ġproblem Ġis Ġthe Ġ\" iv y . shared . default . root \" Ġis Ġdynamic ... ĠIt Ġlooks Ġlike : Ċ // build s / 5 _ great est / win 64 _ x 6\n",
      "\t Unique Tokens 128k: {'Ġsubdirectory', 'Anthony', 'ivy', 'builds', 'greatest'}\n",
      "\t Unique Tokens 32k: {'est', 'great', 'build', 's', 'Ġsub', 'iv', 'directory', 'An', 'y', 'thony'}\n",
      "Text 2: Each and every project is different, however as a rule of thumb here are the core things that I try to have done prior to letting code go out to the wild.\n",
      "In no particular order:\n",
      "1) A version identification in place where it can be found by a user later, this must be unique to this release. (very ty\n",
      "\t Tokenized 128k:Each Ġand Ġevery Ġproject Ġis Ġdifferent , Ġhowever Ġas Ġa Ġrule Ġof Ġthumb Ġhere Ġare Ġthe Ġcore Ġthings Ġthat ĠI Ġtry Ġto Ġhave Ġdone Ġprior Ġto Ġletting Ġcode Ġgo Ġout Ġto Ġthe Ġwild . Ċ In Ġno Ġparticular Ġorder : Ċ 1 ) ĠA Ġversion Ġidentification Ġin Ġplace Ġwhere Ġit Ġcan Ġbe Ġfound Ġby Ġa Ġuser Ġlater , Ġthis Ġmust Ġbe Ġunique Ġto Ġthis Ġrelease . Ġ( very Ġty\n",
      "\t Tokenized 32k:Each Ġand Ġevery Ġproject Ġis Ġdifferent , Ġhowever Ġas Ġa Ġrule Ġof Ġthumb Ġhere Ġare Ġthe Ġcore Ġthings Ġthat ĠI Ġtry Ġto Ġhave Ġdone Ġprior Ġto Ġletting Ġcode Ġgo Ġout Ġto Ġthe Ġwild . Ċ In Ġno Ġparticular Ġorder : Ċ 1 ) ĠA Ġversion Ġidentification Ġin Ġplace Ġwhere Ġit Ġcan Ġbe Ġfound Ġby Ġa Ġuser Ġlater , Ġthis Ġmust Ġbe Ġunique Ġto Ġthis Ġrelease . Ġ( very Ġty\n",
      "\t Unique Tokens 128k: {'Ġdeferred', 'ĠCandidate', 'ĠDefect', 'ĠYMMV', 'Ġstorming', 'ĠSCM', 'Ġarchived', 'Ġflagged', 'Ġappending', 'Ġsprinkle', 'Ġinstallable'}\n",
      "\t Unique Tokens 32k: {'ged', 'Ġspr', 'Ġappend', 'Ġdef', 'ect', 'Ġflag', 'ĠCand', 'MM', 'V', 'Ġstorm', 'ĠS', 'erred', 'CM', 'idate', 'ĠDef', 'inkle', 'Ġarch', 'ĠY', 'ived', 'Ġinstall', 'ing'}\n",
      "==not Same Author==\n",
      "Text 1: Gread job so far, macromates!\n",
      "\n",
      "Here is stuff to think about:\n",
      "\n",
      "- AppleScriptability (pardon my ignorance, is it scriptable)?\n",
      "- Gutter: use color lines as clues for start and end of code blocks \n",
      "(steal that idea from jEdit :)\n",
      "- Preferences? Editting bundles is an unpleasant hybrid between mac and \n",
      "uni\n",
      "\t Tokenized 128k:G read Ġjob Ġso Ġfar , Ġmacrom ates ! Ċ Ċ Here Ġis Ġstuff Ġto Ġthink Ġabout : Ċ Ċ - ĠAppleScript ability Ġ( pardon Ġmy Ġignorance , Ġis Ġit Ġscript able )? Ċ - ĠG utter : Ġuse Ġcolor Ġlines Ġas Ġclues Ġfor Ġstart Ġand Ġend Ġof Ġcode Ġblocks Ġ Ċ ( steal Ġthat Ġidea Ġfrom ĠjEdit Ġ:) Ċ - ĠPreferences ? ĠEd itting Ġbundles Ġis Ġan Ġunpleasant Ġhybrid Ġbetween Ġmac Ġand Ġ Ċ uni\n",
      "\t Tokenized 32k:G read Ġjob Ġso Ġfar , Ġmac rom ates ! Ċ Ċ Here Ġis Ġstuff Ġto Ġthink Ġabout : Ċ Ċ - ĠApple Script ability Ġ( p ardon Ġmy Ġignorance , Ġis Ġit Ġscript able )? Ċ - ĠG utter : Ġuse Ġcolor Ġlines Ġas Ġclues Ġfor Ġstart Ġand Ġend Ġof Ġcode Ġblocks Ġ Ċ ( ste al Ġthat Ġidea Ġfrom Ġj Edit Ġ:) Ċ - ĠPre ferences ? ĠEd itting Ġbundles Ġis Ġan Ġunpleasant Ġhybrid Ġbetween Ġmac Ġand Ġ Ċ uni\n",
      "\t Unique Tokens 128k: {'steal', 'themes', 'ĠPreferences', 'Ġmacrom', 'ĠAppleScript', 'PERSON', 'unix', 'pardon', 'btw', 'ĠjEdit'}\n",
      "\t Unique Tokens 32k: {'ĠPre', 'al', 'bt', 'un', 'w', 'PER', 'Script', 'p', 'rom', 'ardon', 'SON', 'ferences', 'Ġj', 'ĠApple', 'mes', 'ix', 'ste', 'Edit'}\n",
      "Text 2: I think in a way a sci-fi writer dating himself makes for even more interesting reading- it is an important reminder, especially for some of the more esteemed authors, of just how much incredible speculation and imagination it took to craft a story that you, even in 2011, still picture as being set \n",
      "\t Tokenized 128k:I Ġthink Ġin Ġa Ġway Ġa Ġsci - fi Ġwriter Ġdating Ġhimself Ġmakes Ġfor Ġeven Ġmore Ġinteresting Ġreading - Ġit Ġis Ġan Ġimportant Ġreminder , Ġespecially Ġfor Ġsome Ġof Ġthe Ġmore Ġesteemed Ġauthors , Ġof Ġjust Ġhow Ġmuch Ġincredible Ġspeculation Ġand Ġimagination Ġit Ġtook Ġto Ġcraft Ġa Ġstory Ġthat Ġyou , Ġeven Ġin Ġ2011 , Ġstill Ġpicture Ġas Ġbeing Ġset Ġ\n",
      "\t Tokenized 32k:I Ġthink Ġin Ġa Ġway Ġa Ġsci - fi Ġwriter Ġdating Ġhimself Ġmakes Ġfor Ġeven Ġmore Ġinteresting Ġreading - Ġit Ġis Ġan Ġimportant Ġreminder , Ġespecially Ġfor Ġsome Ġof Ġthe Ġmore Ġeste emed Ġauthors , Ġof Ġjust Ġhow Ġmuch Ġincredible Ġspeculation Ġand Ġimagination Ġit Ġtook Ġto Ġcraft Ġa Ġstory Ġthat Ġyou , Ġeven Ġin Ġ2011 , Ġstill Ġpicture Ġas Ġbeing Ġset Ġ\n",
      "\t Unique Tokens 128k: {'Ġesteemed', 'PERSON', 'Ġforesee', 'ĠUb', 'Ġ1961'}\n",
      "\t Unique Tokens 32k: {'b', 'Ġfore', 'see', 'PER', 'emed', 'Ġeste', '61', 'Ġ19', 'SON', 'ĠU'}\n",
      "==not Same Author==\n",
      "Text 1: Hi.\n",
      "\n",
      "We are building our own linux distro for the pandaboard, what I'm\n",
      "wondering if there is somewhere public we could get the source for the\n",
      "pvr-omap4-dkms that ubuntu ships? And preferably also the pvr-omap4\n",
      "blobs. Right now we steal them from the ti-omap ppa, but several times\n",
      "we have seen that t\n",
      "\t Tokenized 128k:Hi . Ċ Ċ We Ġare Ġbuilding Ġour Ġown Ġlinux Ġdistro Ġfor Ġthe Ġpand aboard , Ġwhat ĠI 'm Ċ wondering Ġif Ġthere Ġis Ġsomewhere Ġpublic Ġwe Ġcould Ġget Ġthe Ġsource Ġfor Ġthe Ċ p vr - omap 4 - dk ms Ġthat Ġubuntu Ġships ? ĠAnd Ġpreferably Ġalso Ġthe Ġp vr - omap 4 Ċ blobs . ĠRight Ġnow Ġwe Ġsteal Ġthem Ġfrom Ġthe Ġti - omap Ġppa , Ġbut Ġseveral Ġtimes Ċ we Ġhave Ġseen Ġthat Ġt\n",
      "\t Tokenized 32k:Hi . Ċ Ċ We Ġare Ġbuilding Ġour Ġown Ġlinux Ġdistro Ġfor Ġthe Ġpand ab oard , Ġwhat ĠI 'm Ċ w ond ering Ġif Ġthere Ġis Ġsomewhere Ġpublic Ġwe Ġcould Ġget Ġthe Ġsource Ġfor Ġthe Ċ p vr - om ap 4 - dk ms Ġthat Ġubuntu Ġships ? ĠAnd Ġpreferably Ġalso Ġthe Ġp vr - om ap 4 Ċ bl obs . ĠRight Ġnow Ġwe Ġsteal Ġthem Ġfrom Ġthe Ġti - om ap Ġp pa , Ġbut Ġseveral Ġtimes Ċ we Ġhave Ġseen Ġthat Ġt\n",
      "\t Unique Tokens 128k: {'aboard', 'omap', 'wondering', 'Ġourself', 'PERSON', 'blobs', 'Ġppa'}\n",
      "\t Unique Tokens 32k: {'w', 'ab', 'om', 'pa', 'self', 'PER', 'obs', 'ond', 'SON', 'oard', 'bl', 'ap', 'ering'}\n",
      "Text 2: Life expectancy is higher, infant mortality is lower, and the likelihood of death between the ages of 15-54 is lower. Cuba has other problems, but their healthcare system is fantastic. Plus us Americans think of the embargo meaning that everything in Cuba has to be produced in Cuba, but it really ju\n",
      "\t Tokenized 128k:Life Ġexpectancy Ġis Ġhigher , Ġinfant Ġmortality Ġis Ġlower , Ġand Ġthe Ġlikelihood Ġof Ġdeath Ġbetween Ġthe Ġages Ġof Ġ15 - 54 Ġis Ġlower . ĠCuba Ġhas Ġother Ġproblems , Ġbut Ġtheir Ġhealthcare Ġsystem Ġis Ġfantastic . ĠPlus Ġus ĠAmericans Ġthink Ġof Ġthe Ġembargo Ġmeaning Ġthat Ġeverything Ġin ĠCuba Ġhas Ġto Ġbe Ġproduced Ġin ĠCuba , Ġbut Ġit Ġreally Ġju\n",
      "\t Tokenized 32k:Life Ġexpect ancy Ġis Ġhigher , Ġinfant Ġmortality Ġis Ġlower , Ġand Ġthe Ġlikelihood Ġof Ġdeath Ġbetween Ġthe Ġages Ġof Ġ15 - 54 Ġis Ġlower . ĠCuba Ġhas Ġother Ġproblems , Ġbut Ġtheir Ġhealthcare Ġsystem Ġis Ġfantastic . ĠPlus Ġus ĠAmericans Ġthink Ġof Ġthe Ġem bar go Ġmeaning Ġthat Ġeverything Ġin ĠCuba Ġhas Ġto Ġbe Ġproduced Ġin ĠCuba , Ġbut Ġit Ġreally Ġju\n",
      "\t Unique Tokens 128k: {'Ġexpectancy', 'Ġembargo', 'Ġcheaply'}\n",
      "\t Unique Tokens 32k: {'Ġem', 'Ġcheap', 'ly', 'bar', 'ancy', 'go', 'Ġexpect'}\n",
      "==Same Author==\n",
      "Text 1: Hello <PERSON>,\n",
      "\n",
      "Quite new to PMWicki, I already start to love it and am thankful to you, \n",
      "<PERSON> and all the other contributors and this very helpful community for \n",
      "their work.\n",
      "\n",
      "Glad to hear about your wife's good prognosis.\n",
      "\n",
      "I wish her and you all the best. Take all the time you need to comfort \n",
      "\t Tokenized 128k:Hello Ġ< PERSON >, Ċ Ċ Quite Ġnew Ġto ĠPM W icki , ĠI Ġalready Ġstart Ġto Ġlove Ġit Ġand Ġam Ġthankful Ġto Ġyou , Ġ Ċ < PERSON > Ġand Ġall Ġthe Ġother Ġcontributors Ġand Ġthis Ġvery Ġhelpful Ġcommunity Ġfor Ġ Ċ their Ġwork . Ċ Ċ Glad Ġto Ġhear Ġabout Ġyour Ġwife 's Ġgood Ġprognosis . Ċ Ċ I Ġwish Ġher Ġand Ġyou Ġall Ġthe Ġbest . ĠTake Ġall Ġthe Ġtime Ġyou Ġneed Ġto Ġcomfort Ġ\n",
      "\t Tokenized 32k:Hello Ġ< PER SON >, Ċ Ċ Qu ite Ġnew Ġto ĠPM W ick i , ĠI Ġalready Ġstart Ġto Ġlove Ġit Ġand Ġam Ġthankful Ġto Ġyou , Ġ Ċ < PER SON > Ġand Ġall Ġthe Ġother Ġcontributors Ġand Ġthis Ġvery Ġhelpful Ġcommunity Ġfor Ġ Ċ their Ġwork . Ċ Ċ Gl ad Ġto Ġhear Ġabout Ġyour Ġwife 's Ġgood Ġprognosis . Ċ Ċ I Ġwish Ġher Ġand Ġyou Ġall Ġthe Ġbest . ĠTake Ġall Ġthe Ġtime Ġyou Ġneed Ġto Ġcomfort Ġ\n",
      "\t Unique Tokens 128k: {'PERSON', 'Glad', 'icki', 'Quite'}\n",
      "\t Unique Tokens 32k: {'Qu', 'PER', 'i', 'Gl', 'ite', 'ick', 'SON', 'ad'}\n",
      "Text 2: Hello,\n",
      "\n",
      "I want to create a multilingual site using MultiLanguageViews, and I \n",
      "have editors from different countries. How can I set the wiki interface \n",
      "language via the XLPage for each editor?\n",
      "MultiLanguageViews proposes to do it according to the language view \n",
      "chosen by the user (editor), but I thin\n",
      "\t Tokenized 128k:Hello , Ċ Ċ I Ġwant Ġto Ġcreate Ġa Ġmultilingual Ġsite Ġusing ĠMulti Language Views , Ġand ĠI Ġ Ċ have Ġeditors Ġfrom Ġdifferent Ġcountries . ĠHow Ġcan ĠI Ġset Ġthe Ġwiki Ġinterface Ġ Ċ language Ġvia Ġthe ĠXL Page Ġfor Ġeach Ġeditor ? Ċ Multi Language Views Ġproposes Ġto Ġdo Ġit Ġaccording Ġto Ġthe Ġlanguage Ġview Ġ Ċ chosen Ġby Ġthe Ġuser Ġ( editor ), Ġbut ĠI Ġthin\n",
      "\t Tokenized 32k:Hello , Ċ Ċ I Ġwant Ġto Ġcreate Ġa Ġmult iling ual Ġsite Ġusing ĠMulti Language Views , Ġand ĠI Ġ Ċ have Ġeditors Ġfrom Ġdifferent Ġcountries . ĠHow Ġcan ĠI Ġset Ġthe Ġwiki Ġinterface Ġ Ċ language Ġvia Ġthe ĠXL Page Ġfor Ġeach Ġeditor ? Ċ Multi Language Views Ġpropos es Ġto Ġdo Ġit Ġaccording Ġto Ġthe Ġlanguage Ġview Ġ Ċ ch osen Ġby Ġthe Ġuser Ġ( editor ), Ġbut ĠI Ġthin\n",
      "\t Unique Tokens 128k: {'chosen', 'Ġmultilingual', 'Ġproposes'}\n",
      "\t Unique Tokens 32k: {'Ġmult', 'es', 'osen', 'ch', 'Ġpropos', 'iling', 'ual'}\n",
      "==not Same Author==\n",
      "Text 1: Hi all,\n",
      "\n",
      "I'd like to introduce myself to everyone on the Talk ZA list, my name\n",
      "is <PERSON> and have been a contributor to OSM for a few years\n",
      "now. I'm an expat, living in Dublin (Ireland) and thought I'd helpout\n",
      "with mapping ZA.\n",
      "\n",
      "I have been working on importing some of the eThekwini data (with <PER\n",
      "\t Tokenized 128k:Hi Ġall , Ċ Ċ I 'd Ġlike Ġto Ġintroduce Ġmyself Ġto Ġeveryone Ġon Ġthe ĠTalk ĠZ A Ġlist , Ġmy Ġname Ċ is Ġ< PERSON > Ġand Ġhave Ġbeen Ġa Ġcontributor Ġto ĠOSM Ġfor Ġa Ġfew Ġyears Ċ now . ĠI 'm Ġan Ġexpat , Ġliving Ġin ĠDublin Ġ( Ireland ) Ġand Ġthought ĠI 'd Ġhelp out Ċ with Ġmapping ĠZ A . Ċ Ċ I Ġhave Ġbeen Ġworking Ġon Ġimporting Ġsome Ġof Ġthe Ġe The k win i Ġdata Ġ( with Ġ< PER\n",
      "\t Tokenized 32k:Hi Ġall , Ċ Ċ I 'd Ġlike Ġto Ġintroduce Ġmyself Ġto Ġeveryone Ġon Ġthe ĠTalk ĠZ A Ġlist , Ġmy Ġname Ċ is Ġ< PER SON > Ġand Ġhave Ġbeen Ġa Ġcontributor Ġto ĠOS M Ġfor Ġa Ġfew Ġyears Ċ now . ĠI 'm Ġan Ġexp at , Ġliving Ġin ĠDublin Ġ( I re land ) Ġand Ġthought ĠI 'd Ġhelp out Ċ with Ġmapping ĠZ A . Ċ Ċ I Ġhave Ġbeen Ġworking Ġon Ġimporting Ġsome Ġof Ġthe Ġe The k win i Ġdata Ġ( with Ġ< PER\n",
      "\t Unique Tokens 128k: {'>).', 'themes', 'Firstly', 'PERSON', 'ĠPOI', 'Ġexpat', 'SAR', 'affairs', 'ĠDOT', 'ĠOSM', 'Secondly', 'Ġshapefile', 'ZN', 'Ġnearing', '669', 'ĠTourism', 'areas', 'Ġshapefiles', 'Ireland', 'Thirdly'}\n",
      "\t Unique Tokens 32k: {'aring', 'file', ').', 'ĠTour', 'Z', 'Ġshape', 'AR', 'mes', 'First', 'Second', 'Third', 'land', 'OT', 'ĠOS', 'N', 'Ġexp', 'M', 'Ġne', 'airs', 'PER', 'ly', 'files', 'S', 'aff', 'are', 'ĠD', 'ism', 're', 'SON', 'the', '9', '66', 'as', 'ĠPO'}\n",
      "Text 2: Yeah, they just dropped Dwolla, and their only other USD deposit is through mailing checks or money orders in.        As the other guy said, they will be offering deposits through ACH from your bank account on November 20th. I would assume that will move the price closer to the bigger guys. \n",
      "\t Tokenized 128k:Yeah , Ġthey Ġjust Ġdropped ĠDw olla , Ġand Ġtheir Ġonly Ġother ĠUSD Ġdeposit Ġis Ġthrough Ġmailing Ġchecks Ġor Ġmoney Ġorders Ġin . ĠĠĠĠĠĠĠ ĠAs Ġthe Ġother Ġguy Ġsaid , Ġthey Ġwill Ġbe Ġoffering Ġdeposits Ġthrough ĠACH Ġfrom Ġyour Ġbank Ġaccount Ġon ĠNovember Ġ20 th . ĠI Ġwould Ġassume Ġthat Ġwill Ġmove Ġthe Ġprice Ġcloser Ġto Ġthe Ġbigger Ġguys . Ġ\n",
      "\t Tokenized 32k:Yeah , Ġthey Ġjust Ġdropped ĠD w oll a , Ġand Ġtheir Ġonly Ġother ĠUSD Ġdeposit Ġis Ġthrough Ġmailing Ġchecks Ġor Ġmoney Ġorders Ġin . ĠĠĠĠĠĠĠ ĠAs Ġthe Ġother Ġguy Ġsaid , Ġthey Ġwill Ġbe Ġoffering Ġdeposits Ġthrough ĠA CH Ġfrom Ġyour Ġbank Ġaccount Ġon ĠNovember Ġ20 th . ĠI Ġwould Ġassume Ġthat Ġwill Ġmove Ġthe Ġprice Ġcloser Ġto Ġthe Ġbigger Ġguys . Ġ\n",
      "\t Unique Tokens 128k: {'ĠDw', 'olla', 'ĠACH'}\n",
      "\t Unique Tokens 32k: {'w', 'ĠD', 'oll', 'CH', 'ĠA', 'a'}\n",
      "==not Same Author==\n",
      "Text 1: Hello folks,\n",
      "\n",
      "Just wondering when you are developing flash site how many folks actually \n",
      "develop flash prototypes or fake quick mock up of how the site functional \n",
      "will feel...it seems like a waste of time to me especially with larger \n",
      "projects... but if someone didnt mind sharing their experiences \n",
      "\t Tokenized 128k:Hello Ġfolks , Ċ Ċ Just Ġwondering Ġwhen Ġyou Ġare Ġdeveloping Ġflash Ġsite Ġhow Ġmany Ġfolks Ġactually Ġ Ċ develop Ġflash Ġprototypes Ġor Ġfake Ġquick Ġmock Ġup Ġof Ġhow Ġthe Ġsite Ġfunctional Ġ Ċ will Ġfeel ... it Ġseems Ġlike Ġa Ġwaste Ġof Ġtime Ġto Ġme Ġespecially Ġwith Ġlarger Ġ Ċ projects ... Ġbut Ġif Ġsomeone Ġdidnt Ġmind Ġsharing Ġtheir Ġexperiences Ġ\n",
      "\t Tokenized 32k:Hello Ġfolks , Ċ Ċ Just Ġwondering Ġwhen Ġyou Ġare Ġdeveloping Ġflash Ġsite Ġhow Ġmany Ġfolks Ġactually Ġ Ċ develop Ġflash Ġprot otypes Ġor Ġfake Ġquick Ġmock Ġup Ġof Ġhow Ġthe Ġsite Ġfunctional Ġ Ċ will Ġfeel ... it Ġseems Ġlike Ġa Ġwaste Ġof Ġtime Ġto Ġme Ġespecially Ġwith Ġlarger Ġ Ċ projects ... Ġbut Ġif Ġsomeone Ġdidnt Ġmind Ġsharing Ġtheir Ġexperiences Ġ\n",
      "\t Unique Tokens 128k: {'Ġprototypes', 'PERSON'}\n",
      "\t Unique Tokens 32k: {'SON', 'Ġprot', 'otypes', 'PER'}\n",
      "Text 2: Why the fuck everyone here crave for so much attention with these essays based in absolutely nothing but empirical subjecive facts? Amazing you guys feel smart or important writting such bad essay, do yo even know how to do research to contrast your theories or facts?        Just be yourself, goes e\n",
      "\t Tokenized 128k:Why Ġthe Ġfuck Ġeveryone Ġhere Ġcrave Ġfor Ġso Ġmuch Ġattention Ġwith Ġthese Ġessays Ġbased Ġin Ġabsolutely Ġnothing Ġbut Ġempirical Ġsubj ec ive Ġfacts ? ĠAmazing Ġyou Ġguys Ġfeel Ġsmart Ġor Ġimportant Ġwritting Ġsuch Ġbad Ġessay , Ġdo Ġyo Ġeven Ġknow Ġhow Ġto Ġdo Ġresearch Ġto Ġcontrast Ġyour Ġtheories Ġor Ġfacts ? ĠĠĠĠĠĠĠ ĠJust Ġbe Ġyourself , Ġgoes Ġe\n",
      "\t Tokenized 32k:Why Ġthe Ġfuck Ġeveryone Ġhere Ġcra ve Ġfor Ġso Ġmuch Ġattention Ġwith Ġthese Ġessays Ġbased Ġin Ġabsolutely Ġnothing Ġbut Ġempirical Ġsub j ec ive Ġfacts ? ĠAmazing Ġyou Ġguys Ġfeel Ġsmart Ġor Ġimportant Ġwr itting Ġsuch Ġbad Ġessay , Ġdo Ġyo Ġeven Ġknow Ġhow Ġto Ġdo Ġresearch Ġto Ġcontrast Ġyour Ġtheories Ġor Ġfacts ? ĠĠĠĠĠĠĠ ĠJust Ġbe Ġyourself , Ġgoes Ġe\n",
      "\t Unique Tokens 128k: {'rut', 'Ġlmao', 'Ġpatrons', 'Ġsubj', 'Ġcrave', 'Ġwritting'}\n",
      "\t Unique Tokens 32k: {'r', 'Ġl', 'Ġcra', 'j', 'Ġpat', 'mao', 'ut', 'itting', 'Ġwr', 've', 'rons'}\n",
      "==not Same Author==\n",
      "Text 1: Just curious:\n",
      "When I do 'shorewall show <a chain>', some of the rules have 'ctorigdst' at\n",
      "the end, with an IP address. The IP is the original destination in a DNAT\n",
      "rule.\n",
      "Seems to be the same format as 'shorewall show nat' which gives the original\n",
      "dst as ...to:<original IP>, but without that cryptic \n",
      "\t Tokenized 128k:Just Ġcurious : Ċ When ĠI Ġdo Ġ' shore wall Ġshow Ġ< a Ġchain >', Ġsome Ġof Ġthe Ġrules Ġhave Ġ' ctor ig dst ' Ġat Ċ the Ġend , Ġwith Ġan ĠIP Ġaddress . ĠThe ĠIP Ġis Ġthe Ġoriginal Ġdestination Ġin Ġa ĠDN AT Ċ rule . Ċ Seems Ġto Ġbe Ġthe Ġsame Ġformat Ġas Ġ' shore wall Ġshow Ġnat ' Ġwhich Ġgives Ġthe Ġoriginal Ċ dst Ġas Ġ... to :< original ĠIP >, Ġbut Ġwithout Ġthat Ġcryptic Ġ\n",
      "\t Tokenized 32k:Just Ġcurious : Ċ When ĠI Ġdo Ġ' shore wall Ġshow Ġ< a Ġchain > ', Ġsome Ġof Ġthe Ġrules Ġhave Ġ' ctor ig dst ' Ġat Ċ the Ġend , Ġwith Ġan ĠIP Ġaddress . ĠThe ĠIP Ġis Ġthe Ġoriginal Ġdestination Ġin Ġa ĠD N AT Ċ rule . Ċ Seems Ġto Ġbe Ġthe Ġsame Ġformat Ġas Ġ' shore wall Ġshow Ġnat ' Ġwhich Ġgives Ġthe Ġoriginal Ċ dst Ġas Ġ... to : < original ĠIP >, Ġbut Ġwithout Ġthat Ġcrypt ic Ġ\n",
      "\t Unique Tokens 128k: {':<', 'ĠDN', \">',\", 'PERSON', 'Ġcryptic'}\n",
      "\t Unique Tokens 32k: {'ĠD', 'PER', 'ic', 'N', 'Ġcrypt', 'SON', \"',\"}\n",
      "Text 2: 10 |\n",
      "     | 2006-11-06 | Company Store |    -10 |            0 |\n",
      "     | 2006-11-07 | Company Store |    -10 |          -10 |\n",
      "     +------------+---------------+--------+--------------+\n",
      " #### Oracle\n",
      " Oracle has a neat extension to the windowing functions. You can use the ` OVER` clause in conjunction\n",
      "\t Tokenized 128k:10 Ġ| ĊĠĠĠĠ Ġ| Ġ2006 - 11 - 06 Ġ| ĠCompany ĠStore Ġ| ĠĠĠ Ġ- 10 Ġ| ĠĠĠĠĠĠĠĠĠĠĠ Ġ0 Ġ| ĊĠĠĠĠ Ġ| Ġ2006 - 11 - 07 Ġ| ĠCompany ĠStore Ġ| ĠĠĠ Ġ- 10 Ġ| ĠĠĠĠĠĠĠĠĠ Ġ- 10 Ġ| ĊĠĠĠĠ Ġ+ ------------ +---------------+ --------+ --------------+ Ċ Ġ#### ĠOracle Ċ ĠOracle Ġhas Ġa Ġneat Ġextension Ġto Ġthe Ġwindowing Ġfunctions . ĠYou Ġcan Ġuse Ġthe Ġ` ĠOVER ` Ġclause Ġin Ġconjunction\n",
      "\t Tokenized 32k:10 Ġ| ĊĠĠĠĠ Ġ| Ġ2006 - 11 - 06 Ġ| ĠCompany ĠStore Ġ| ĠĠĠ Ġ- 10 Ġ| ĠĠĠĠĠĠĠĠĠĠĠ Ġ0 Ġ| ĊĠĠĠĠ Ġ| Ġ2006 - 11 - 07 Ġ| ĠCompany ĠStore Ġ| ĠĠĠ Ġ- 10 Ġ| ĠĠĠĠĠĠĠĠĠ Ġ- 10 Ġ| ĊĠĠĠĠ Ġ+ ------------ +------------ ---+ -------- + -------------- + Ċ Ġ# ### ĠOracle Ċ ĠOracle Ġhas Ġa Ġneat Ġextension Ġto Ġthe Ġwind owing Ġfunctions . ĠYou Ġcan Ġuse Ġthe Ġ` ĠOVER ` Ġclause Ġin Ġconjunction\n",
      "\t Unique Tokens 128k: {'Ġ----------', '+---------------+', '--------+', 'Ġtransact', ';**', '--------------+', 'Ġ---------------', 'NOV', 'ĠRows', 'Hack', 'AMOUNT', 'ĠForgot', 'ĠRank', 'ĠAMOUNT', 'Ġtotals', 'OVER', 'Ġ####', 'Ġwindowing', 'Ġ###'}\n",
      "\t Unique Tokens 32k: {'H', 'ĠAM', 'ĠR', '##', 'Ġwind', '**', 'owing', '+------------', 'O', 'ows', 'V', 'VER', 'als', 'AM', 'orgot', '---', '----------', 'ack', 'OUNT', 'ĠF', 'act', 'NO', '--------------', '---+', 'Ġtrans', 'ank', 'Ġ', '###', '+', 'Ġtot', ';', '--------'}\n"
     ]
    }
   ],
   "source": [
    "compare_predictions(df_128k, df_gpt2_av, tok_128k, gpt2_tok, \"128k\", \"32k\", classification_at_1=\"Same Author\", text1=\"query_text\", text2=\"candidate_text\", n_examples=10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-09T13:51:24.634207Z",
     "start_time": "2025-02-09T13:51:24.555982Z"
    }
   },
   "id": "db331eeeee0373d7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "compare_predictions(df_wiki_NUCLE, df_mixed_NUCLE, tok_wikipedia, gpt2_tok, \"wiki\", \"mixed\", classification_at_1=\"Same Author\", text1=\"query_text\", text2=\"candidate_text\", n_examples=10)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4deabda7e4f4320a"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==Same Author==\n",
      "Text 1: . <PERSON> to <PERSON>, 26 June 1927, Phelps Stokes Papers, Yale.\n",
      " . <PERSON>, \"Impressions of the Port Elizabeth Conference,\" _Universitas_ 8:8 (August 1928).\n",
      " . <PERSON> to <PERSON>, 24 October 1928, NAACP Papers, Library of Congress.\n",
      " ### NOTES TO CHAPTER 4\n",
      " . \"<PERSON> of Africa Is Guest of the \n",
      "\t Tokenized GPT2:. Ġ< PER SON > Ġto Ġ< PER SON >, Ġ26 ĠJune Ġ19 27 , ĠP hel ps ĠSto kes ĠP apers , ĠY ale . Ċ Ġ. Ġ< PER SON >, Ġ\" Im press ions Ġof Ġthe ĠPort ĠElizabeth ĠConference ,\" Ġ_ Univers itas _ Ġ8 : 8 Ġ( August Ġ19 28 ). Ċ Ġ. Ġ< PER SON > Ġto Ġ< PER SON >, Ġ24 ĠOctober Ġ19 28 , ĠNA AC P ĠP apers , ĠLibrary Ġof ĠCongress . Ċ Ġ# ## ĠNOT ES ĠTO ĠCHAPTER Ġ4 Ċ Ġ. Ġ\"< PER SON > Ġof ĠAfrica ĠIs ĠGu est Ġof Ġthe Ġ\n",
      "\t Tokenized no:.Ġ < P ERS ON > ĠtoĠ < P ERS ON >,Ġ 26Ġ JuneĠ 19 27 ,ĠP helpsĠ St ok esĠ Pap ers,Ġ Y al e.Ċ Ġ .Ġ < P ERS ON > ,Ġ\" Im pression sĠofĠtheĠ PortĠ ElizabethĠ Con fer ence ,\"Ġ _ Universit as _Ġ 8: 8 Ġ( AugustĠ 19 28 ).Ċ Ġ .Ġ < P ERS ON > ĠtoĠ < P ERS ON >,Ġ 24Ġ OctoberĠ 19 28 ,Ġ NA AC PĠ Pap ers,Ġ Libr aryĠofĠ Congres s.Ċ Ġ ###Ġ NOTE SĠ TOĠ CHAPTERĠ 4 ĊĠ .Ġ\" < P ERS ON >Ġ ofĠ Afric aĠ IsĠ Gu estĠ ofĠtheĠ\n",
      "\t Unique Tokens GPT2: {'Ext', 'Ġciting', 'ography', 'ĠIndia', 'ar', 'ions', 'ĠRevolution', 'Ġby', 'ĠKat', 'ĠInst', \"'s\", ').', 'itut', 'ĠMax', 'ĠY', 'American', 'ĠTypes', 'ĠSto', 'my', 'ĠâĢĵ', 'ĠW', 'ĠOctober', 'Ġfrom', 'uma', 'ĠProject', 'ĠPol', 'ĠPr', '24', 'ĠCollection', 'ĠCommerce', 'op', 'older', 'apers', 'Ġnot', 'ĠAfrica', 'ĠNOT', 'Ġ_', 'Ġthe', 'ĠDiv', 'ĠHe', 'ĠCom', 'ĠIt', 'ĠSupport', 'Ġ9', 'Ġ1997', 'ĠCongress', 'Ċ', 'Ġfor', 'ĠCopy', 'ĠIm', 'Ġ31', 'ives', 'est', 'Ġcontribution', 'Ġ26', 'ĠNative', 'eton', 'ire', 'va', 'ors', 'ĠLondon', 'inity', 'Ġ88', 'Ġmade', '.,', 'Ġcourtesy', 'Ġ50', ':', 'ĠPort', '>,', 'orthy', 'mp', 'ĠStudent', 'ists', 'ĠIn', 'ual', 'ĠGu', 'ĠG', 'acts', 'ĠL', 'ĠPacific', 'Ġto', 'ĠRed', 'ĠGeneral', 'gan', 'Ġ\"', 'ism', 'ĠUniversity', 'ĠCorrespond', 'ĠTri', 'Ġed', 'ĠAfrican', 'ĠMay', 'ĠBas', 'ĠSp', 'ses', 'ĠLibrary', 'ĠNA', 'it', 'ĠToronto', 'A', 'ĠSe', 'itas', '.:', 'ĠJune', 'ĠCount', 'SON', 'ĠRelations', 'J', 'ĠCol', 'Ġ.', 'Comm', 'ving', 'Ġup', 'Ġvol', 'ĠYork', 'press', 'ĠIV', 'ld', 'Ġconference', 'ĠGene', 'ĠDivision', 'ĠMand', 'Ġ8', 'olars', 'ĠFar', 'ew', 'ĠC', 'on', 'i', 'Ġ(<', 'he', 'ĠWorld', 'ott', 'ĠMass', 'ons', 'Ġedited', 'Ġ5', 'Ġyielded', '5', 'th', 'attle', 'ĠSix', 'perial', 'Ġn', 'Report', 'Ġ13', ',_', 'Jim', 'astern', 'Ġsee', 'ĠT', 'ama', 'As', 'ĠD', 'Ġbook', 'ps', 'ĠNew', 'Ġat', 'our', 'Ġ19', 'Ġselected', 'ĠInstitute', '##', 'ĠCHAPTER', 'ĠWork', 'Ġthoroughly', 'ĠInternational', 'Ġdate', 'eds', 'ĠNe', 'Ġin', 'ĠContainer', 'ries', 'ĠApril', 'Recent', 'ĠSch', 'leveland', 'Ġand', 'ĠSchool', 'ĠSee', 'Ġ#', 'ĠElizabeth', 'vent', 'oke', 'mediate', 'Ġ84', 'ĠLord', 'er', 'Ġof', 'ĠAd', 'ĠWashington', 'r', 'ton', 'Ġcopy', 'ĠSem', 'ĠM', \"Ġ'\", 'Ġ\"<', 'ĠF', 'phas', 'Univers', 'ĠTown', 'ĠQuestion', 'ES', '164', 'Ġ<', 'Native', 'ĠVo', 'ale', 'ĠLife', '),', 'Ġ7', 'ĠRoom', 'Ġnotes', 'ĠP', 'itics', 'Ġ97', ',\"', 'ĠYear', 'ĠRe', 'intern', 'ĠBox', 'istant', 'un', 'ted', 'Ġa', 'ĠPress', 'ĠThe', 'ĠA', 'ĠBi', 'ĠArch', 'ĠIs', 'ĠOxford', 'Ġaddition', 'hel', 'inc', 'Pr', 'Ġ2', 'Ġ22', 'ment', 'ĠFrom', 'gram', 'asks', 'ĠN', '1', 'ĠProblem', 'Ġ24', 'E', 'ĠEnt', 'ĠO', 'ĠJanuary', 'ĠNews', 'Dear', 'uments', 'ĠEm', 'ĠMission', 'd', 'kes', 'Ġ4', 'ĠAmerican', 'August', 'ĠEduc', '-', 'ĠFriends', 'ĠAsian', 'ĠTO', 'Ġ1960', 'Ġla', 'ĠCable', 'ĠMove', 'ĠMeeting', 'ĠCommun', 'ands', \"'\", 'izes', 'ian', 'ĠMarch', 'ĠSouth', 'ĠPolicy', 'ĠChristian', 'PER', 'Ġwas', 'ape', 'ĠLoved', 'ĠConference', 'ĠDoc', 'h', 'Ġ3', 'Ġon', 'ĠOn', 'ĠSen'}\n",
      "\t Unique Tokens no: {'(L', 'ution', 'Ox', '.Ġ', 'onĠtheĠ', 'Polit', '),Ġ', 'LordĠ', 'InstituteĠofĠ', 'eĠofĠ', 'pression', '_Ġ', 'ement', ':Ġ', 's:Ġ', '.d', 'Collection', 'Tri', 'c', '>ĊĠ', 'apeĠ', 'AĠ', '.ĠSeeĠ', '3', 'UniversityĠofĠ', 'orsĠ', 'arĠ', 'ElizabethĠ', 'MarchĠ', 'mentĠofĠ', 'Colon', 'andĠ', 'edĠbyĠtheĠ', 'y,', 'olderĠ', 'Con', 'a:Ġ', 'Sem', 'Lov', '._Ġ', 'o:Ġ', 'erg', 'iv', 'Pap', 'court', 'JanuaryĠ', 'NewĠ', 's,\"Ġ', '.ĠOnĠ', 'AsianĠ', '16', '97', 'Kat', 'enceĠofĠ', 'yield', 'Question', '###Ġ', '27Ġ', '26Ġ', 'vol', 'sion', '.Ġ\"', 'Count', 'Sch', 'Educ', '88', ':ĠTheĠ', 'laĠ', 'level', 'a', 'dĠ', '_C', 'Libr', \"an'sĠ\", 'ix', 'siz', 'istĠ', 'Polic', 'al,Ġ', ',Ġ\"', 'v', '24Ġ', 'esĠ', 'WashingtonĠ', 'ic', 'sĠfromĠtheĠ', 'ofĠ', 'medi', 'um', 'Vo', 'MaxĠ', 'FĠ', 'e.Ċ', 'LifeĠ', 's.ĠTheĠ', 'PortĠ', 'ers,Ġ', 'PĠ', 'as', 'opyĠ', ',ĠandĠ', 'Intern', 'ha', 'yp', 'was', ',Ġ_', '(C', ',Ġ', '_D', 'Relation', '.A', \"'Ġ\", '1997', ',ĠF', 'ributionĠ', '5Ġ', 'IsĠ', 'keĠ', 'SouthĠAfric', '.ĠItĠ', '<', 'aryĠofĠ', 'Princ', 'University', 'Contain', 'Land', 'Universit', 'erĠ', 'Gen', ',ĠP', 'thĠ', '9Ġ', 'Instit', ',ĠM', 'Project', '_TheĠ', 'Correspond', 'ReportĠ', '>Ġ', 'Spo', 'onĠaĠ', 'InternationalĠ', 'SchoolĠ', '4', 'Afric', 'ation', 'ing', '4,Ġ', 'olar', 'Bio', 'ationalĠ', '50', '2,Ġ', '1,Ġ', 'itĠofĠ', '.ĊĠ', 'copyĠ', 'Dist', '8:', 'edit', 'yĠ', 'e,Ġ', 'Task', 'CHAPTERĠ', 'mer', 'sesĠ', 'on,Ġ', 'SĠ', '22Ġ', 'MayĠ', 'Ġ', 'W', 'select', ',\"Ġ', 'worthyĠ', 'ityĠ', 'edĠaĠ', 'UniversityĠ', 'JimmyĠ', 'ision', 'amaĠ', 'aĠ', 'leĠandĠ', 'ismĠandĠ', 'Rec', 'ok', 'the', 'Commun', 'IV', 'per', '(', 'a.', 'ateĠ', ',ĠD', 'cont', 'upĠtoĠ', 'helpsĠ', 'St', 'al', 'O', 'es,Ġ', 'WorkĠ', 'NativeĠ', 'graph', 'ĊĠ', 'ativeĠ', 'YearĠ', 'GeneralĠ', 'Ad', 'ĠthoroughlyĠ', 'sĠ', 'ottĠ', 'Mand', 'additionĠtoĠ', 'NewĠYork', 'ofĠtheĠ', '13', 'Room', 'WorldĠ', 'ĠtoĠ', '.Ġ(', 'eĠandĠ', 'i-', 'ualĠ', 's_', '7Ġ', 'able', 'aleĠ', 'an', 'G', 'Need', 'TOĠ', 'even', 'Bas', 'FromĠtheĠ', 'AmericanĠ', 'yĠinĠ', 'ence,Ġ', 'ourĠ', 'in', 'SupportĠ', 'Arch', 'Lif', '1960', 'Mov', 'edĠbyĠ', 'New', '19Ġ', \"'TheĠ\", 'JuneĠ', 'att', '.,Ġ', 's,', 'ev', 'ondon', 'es', 'edĠandĠ', 'India', 'antĠ', 'note', 'itingĠ', 'sĠofĠtheĠ', 'Stud', '.ĠM', 'Town', 'entĠ', 'Mis', 'ProblemĠ', 'ERS', 'fer', 'Document', 'ed.Ġ', 'InĠ', 'inter', ':5', 'NOTE', 'a,', '28Ġ', 'Pres', 'Gu', 'estĠ', 'NA', 'ĠandĠ', '2:', 'Sen', 'ĠtoĠ\"', 'ceĠ', 'book,Ġ', ',Ġ19', 'Ex', '.ĠC', 'inĠ', 's.Ċ', 'ON', 'AugustĠ', 'forĠtheĠ', 'et', ').Ċ', 'Div', 'notesĠ', 'Revolution', 'Friend', 'ton,Ġ', 'asternĠ', 'AprilĠ', 'Re', 'MC', 'date,Ġ', 'sĠandĠ', 'aryĠ', 'ementĠ', '_N', 'Congres', '31', 'fordĠ', 'ont', 'S', '>,Ġ', 'yĠofĠ', 'archĠ', 'ir', 's,Ġ', 'ireĠ', 'opt', 'yĠofĠtheĠ', 'ary,Ġ', ')', 'ialĠ', 'onĠ', 'nĠ', 'isionĠ', '.J', ',ĠW', 'Com', 'ChristianĠ', '.Ġ_', 'pon', 'Emp', 'BoxĠ', 'n', 'Nor', '.ĠT', 'TheĠ', '2', 'riesĠ', 'ĠmadeĠ', 'SouthĠ', 'DearĠ', 'Tor', ',ĠtoĠ', 'PacificĠ', 's', 'assĠ', 'Ent', 'atĠ', 'âĢĵĠ', 'conferenceĠ', 'Y', 'Hel', 'seeĠ', 'tract', 'OctoberĠ', 'AfricanĠ', 'Meet', 'gramĠ', 'RedĠ'}\n",
      "Text 2: \"We'd better go back before she teases him to death,\" <PERSON> said.\n",
      " \"He just needs to know how to defend himself,\" <PERSON> said. \"If she tries it again, <PERSON>, tickle her bottom.\"\n",
      " \"I don't have a bottom,\" the mermaid pointed out. \"Just a nice piece of tail.\"\n",
      " <PERSON> sighed. \"Then he must be\n",
      "\t Tokenized GPT2:\" We 'd Ġbetter Ġgo Ġback Ġbefore Ġshe Ġte ases Ġhim Ġto Ġdeath ,\" Ġ< PER SON > Ġsaid . Ċ Ġ\" He Ġjust Ġneeds Ġto Ġknow Ġhow Ġto Ġdefend Ġhimself ,\" Ġ< PER SON > Ġsaid . Ġ\" If Ġshe Ġtries Ġit Ġagain , Ġ< PER SON >, Ġtick le Ġher Ġbottom .\" Ċ Ġ\" I Ġdon 't Ġhave Ġa Ġbottom ,\" Ġthe Ġm erm aid Ġpointed Ġout . Ġ\" Just Ġa Ġnice Ġpiece Ġof Ġtail .\" Ċ Ġ< PER SON > Ġsighed . Ġ\" Then Ġhe Ġmust Ġbe\n",
      "\t Tokenized no:\" We 'dĠ betterĠ goĠbackĠ beforeĠsheĠ te asesĠ himĠtoĠ death ,\"Ġ < P ERS ON > Ġsaid .ĊĠ \" HeĠ justĠ needsĠtoĠ knowĠhowĠtoĠ defendĠ himself ,\"Ġ < P ERS ON > Ġsaid .Ġ\" If ĠsheĠ tri esĠ itĠ again,Ġ < P ERS ON > ,Ġt ick leĠ herĠ bottom .\" ĊĠ\" IĠdon'tĠ haveĠaĠ bottom ,\"ĠtheĠ mer ma idĠ pointedĠout .Ġ\" JustĠ aĠniceĠ pieceĠ ofĠt ail .\" ĊĠ < P ERS ON > Ġsigh ed.Ġ\" ThenĠ heĠ mustĠ be\n",
      "\t Unique Tokens GPT2: {'keys', 'Ġwith', 'erm', 'Ġend', 'Ġgiggle', 'Ġtold', 'ancel', 'Ġby', 'Ġaisle', 'Ġsadly', \"'s\", 'Ġagain', 'ica', 'Ġalready', 'Ġbride', 'Ġwhile', 'Ġthis', 'Ġthen', 'ops', 'Ġas', '.', 'ĠR', 'Ġbusy', 'Ġbefore', 'Ġhe', 'Ġpaused', 'Ġpee', 've', 'Ġinstead', 'Ġre', 'Ġstuck', 'Ġfrom', 'Ġbroke', 'al', 'Ġhow', 'Ġamazed', 'Ġenough', 'Ġbottom', 'Ġnot', 'ides', 'Ġchore', 'Ġfate', 'Ġnice', 'Ġshe', 'Ġthe', 'Ġm', 'Ġp', 'Ġshore', 'ĠHe', 'ak', 'Ġchair', 'ĠIt', 'Ġfor', 'Ġsame', 'id', 'Ċ', 'Ġbetter', 'Ġcastle', 'Ġthan', 'ĠBr', 'ĠHelp', 'Ġlaughed', 'Ġgo', 'Now', '>,', 'ĠThen', 'Ġfinally', 'Ġthrough', 'Ġhis', 'All', 'ĠG', 'Ġwill', 'Ġgot', 'Ġpr', 'Ġhers', 'Ġfabulous', 'Ġimpressed', 'Ġyou', 'Ġto', 'Ġseems', 'thm', 'Ġt', 'Ġpiece', 'Ġmight', 'Ġhim', 'Ġgirl', 'Ġtries', 'Ġrehears', 'Ġwho', 'Ġlined', 'Ġbrought', 'ĠPrincess', 'Ġwalked', 'Ġbe', 'Ġright', 'Then', 'ĠHonor', 'Ġdressed', 'Ġjob', \"'re\", 'ĠWe', 'Ġstep', 'Ġdeath', 'Ġquickly', 'Ġboat', 'Ġglanced', 'ĠCastle', 'Ġdown', 'Ġwater', 'aled', 'mony', 'Ġfetch', 'SON', 'Ġthere', 'Ġout', 'Ġup', 'Ġmore', 'ding', 'Ġus', 'Ġlooked', 'Ġpant', 'ases', 'cess', 'Ġsubstitute', 'Ġhimself', 'Ġenvy', 'Ġyoung', 'Ġdid', 'Ġits', 'Ġagreed', 'ĠCh', 'Ġmanaging', 'Ġclose', 'Ġwhispered', 'Ġbelow', 'Ġwind', 'Ġgovern', 'Ġdepart', 'Ġthey', 'Ġlost', 'Ġasked', 'Ġit', 'ĠShe', 'It', 'Ġte', 'Ġpointed', 'Ġher', ';', 'ĠWed', 'ared', 'Ġcorrectly', 'Ġdefend', 'Ġat', 'rice', 'Ġsong', 'Ġhere', 'ĠCap', 'ĠDell', 'Ġknow', 'ĠB', 'Ġbig', 'Ġcarefully', 'Ġblew', 'Ġignoring', 'Ġobey', 'ily', 'Just', 'Ġdry', 'aid', 'Ġsighed', 'Ġbut', 'ĠHar', 'Ġdon', 'ĠOnly', 'Ġin', 'Ġseeing', 'ĠEnd', 'Ġjust', 'Ġsit', 'Ġand', 'He', 'Ġsays', 'Ġhad', 'Ġdidn', '!', 'ĠBut', 'Ġfaced', 'Ġcruel', 'Ġgetting', 'Ġof', 'Ġplace', 'Ġback', 'Ġaround', 'ĠMel', 'quet', 'ĠM', 'ĠF', 'Ġ\"<', 'Whatever', 'Ġwedding', 'Ġplays', 'Ġdog', 'Ġfetched', 'Ġtail', 'Ġmine', 'Ġsoon', 'Ġrealize', 'Ġdo', 'Ġtravels', 'ĠToo', 'Ġway', 'Ġinto', ',\"', 'ĠThey', 'Ġneed', 'Ġnotice', '?\"', 'Ġall', 'Ġconcluded', 'Ġsign', 'You', 'Ġb', 'ĠSuddenly', 'Ġa', 'Ġought', 'Ġtonight', 'ĠSo', 'ĠThe', 'Ġcan', 'Win', 'Ġhave', 'Ġdeck', 'ĠI', 'Ġaccompanied', 'Pr', 'Ġset', 'Ġclimbed', 'Ġneeds', \"'t\", 'Ġwe', 'ody', 'ĠFortunately', 'Ġnow', 'Ġmust', 'Ġkiss', 'Ġover', 'Ġis', 'Ġthat', 'Ġwhats', 'Ġharmon', 'ĠNormally', 'iors', 'Ġappeared', 'Ġabout', 'Ġme', 'Best', \"'d\", 'Ġwent', 'Ġsmile', 'Ġfoundations', 'Ġfather', 'Ġyour', 'She', 'Ġwould', 'ĠMarch', 'Ġget', 'PER', ',', 'Ġcrossed', 'ĠMan', 'Ġbad', 'But', 'Ġsomeone', 'Ġflowers', 'Ġon', 'Ġw', 'Ġsmall'}\n",
      "\t Unique Tokens no: {'soĠ', 'etĠ', 'finallyĠ', 'Har', 'ĊĠ\"', 'alreadyĠ', 'aĠc', ',ĠwhileĠ', 'hereĠandĠ', 'impres', '.Ġ', 'wr', '?\"ĠheĠ', 'haveĠaĠ', 'maid', \"You'reĠ\", 'aĠbigĠ', 'needsĠtoĠ', 'ooĠ', 'himĠtoĠ', '.ĠWe', 'fetchĠ', 'allĠ', 'ais', 'Ġcon', 'riceĠ', 'Cap', 'heĠ', 'leĠ', 'death', 'fac', 'ought', 'MarchĠ', 'isĠnotĠ', 'March', 'andĠ', 'HeĠwasĠ', 'keysĠ', 'some', 'doĠthatĠ', 'in,Ġ', 'ausedĠ', \",Ġthat'sĠ\", 'eĠtoĠ', 'NowĠ', 'seeingĠ', 'ĠcanĠ', 'isĠtoĠ', 'BestĠ', 's,\"Ġ', 'aĠniceĠ', 'gleĠ', 'te', 'step', 'yĠandĠ', 'manag', '.Ġ\"', 'isĠtheĠ', 'below', 'flowersĠ', 's,ĠbutĠ', 'usĠ', 'sheĠ', 'defendĠ', 'kis', 'ĠtimeĠ', 'leĠt', 'here.Ġ', 'yourĠ', 'Ġsh', 'leĠthatĠ', 'ellĠ', 'sal', 'ĠtoĠtheĠ', 'we', 'agre', 'Brid', 'qu', '.ĠWeĠ', 'eĠthatĠtheĠ', 'esĠ', 'TheyĠ', 'hairĠ', 'sed', 'atelyĠ', 'ic', 'aidĠ', 'ble', 'ofĠ', 'ThenĠ', 'realizeĠthatĠ', 'Only', 'goĠbackĠ', 'asked,Ġ', 'edĠatĠ', 'Ġcorrect', 'sitĠ', 'edĠtheĠ', 'intoĠ', 'Ġtravel', ',ĠandĠtheĠ', 'ofĠt', 'wasĠ', 'od', 'ail', 'setĠ', 'needĠ', 'or', 're', 'was', 'Ġwhat', 'amaz', 'weĠ', ',Ġ', 'abul', 'ĠwithĠ', 'stuck', 'akĠ', 'fromĠtheĠ', 'Ġsigh', '.ĠH', 'herĠ', 'byĠ', 'HeĠ', 'himself', 'ĠthenĠ', 'knowĠ', 'govern', 'now', '.ĠItĠ', '<', 'one,Ġ', 'aĠsmile', 'gettingĠtoĠ', 'br', 'Princ', 'env', 'stepĠ', 'isĠthat', 'appearedĠ', 'boat', 'ĠtoĠhisĠ', 'ed.Ċ', 'anc', '.\"Ġ', 'edĠ', '>Ġ', 'had', 'th', 'R', 'ed.Ġ\"', 'found', 'accompani', 'mustĠbeĠ', 'm', 'ch', 'JustĠ', 'isĠ', 'ig', 'him', 'ort', 'enough', 'smile', 'wouldĠ', 'el', 'ousĠ', 'pant', '.ĊĠ', 'itsĠ', 'place', 'b', 'her', 'yĠ', 'Norm', 'ĠtoldĠ', 'SoĠ', \"He'sĠ\", 'instead', 'mer', 'e.', 'sesĠ', '!Ġ', 'ilyĠ', 'song', 'moreĠthanĠ', 'crossedĠ', 'edĠherĠ', 'o', 's!Ġ', 'hear', 'Ġ', 'ButĠ', 'W', ',\"Ġ', 'needĠto', \"'sĠ\", \"'llĠ\", 'udden', 'broughtĠ', 'Hon', 'ĠtheyĠ', 'aĠ', 'be', 'hereĠinĠ', \"you'reĠ\", \"IĠdon'tĠ\", 'notĠtoĠ', 'walkĠ', 'Br', 'jobĠ', 'signal', 'pr', ',Ġt', 'tri', 'whileĠtheĠ', 'p', 'laugh', 'ĊĠ', 'ĠsheĠ', 'might', 'M', 'needĠtoĠ', 'windĠ', 'Ġwhisper', 'sĠ', 'girl', 'it,Ġ', 'ideĠ', 'ĠthisĠwasĠ', 'idĠ', 'SheĠ', 'sheĠwasĠ', 'y', 'F', 'betterĠ', 'aĠg', 'depart', 'ĠtoĠ', 'it.', 'boatĠ', 'asĠ', 'ingĠup', 'doĠit', '.ĠBut', 'essĠ', 'elĠ', 'ĠtheĠsameĠ', 'needĠaĠ', 'backĠintoĠtheĠ', 'G', 'Ch', 'endĠtheĠ', 'w', 'lin', 'notic', 'fatherĠ', 'glanc', 'ingĠtheĠ', 'doĠ', 'pe', 'busyĠ', 'dog', 'aĠsmallĠ', \"it's\", 'Ġtime.', 'y.Ġ', \"youĠdidn'tĠ\", 'ĠthereĠ', 'ru', 'getĠ', 'Ġsay', 'oreĠ', 'right', 'seemsĠ', 'ĠsaysĠ', 'itĠwillĠ', ';Ġ', 'fromĠ', 'atĠtheĠ', 'whoĠ', 'wasĠtheĠ', 'un', \"we'reĠ\", 'dressedĠ', 'mon', \"YouĠdon'tĠ\", 'aroundĠ', 'bey', 'ItĠ', 'brokeĠ', 'quicklyĠ', 'clim', 'WhatĠ', 'ERS', 'ĠthemĠ', 'withĠ', ',\"ĠsheĠ', 'aĠf', 'YouĠcan', 'atĠherĠ', '.ĠB', 'getĠtheĠ', 'inĠtheĠ', 'ĠandĠ', 'got', 'eĠtimeĠtoĠ', 'again,Ġ', 'Mel', ',\"ĠtheĠ', ',ĠyourĠ', 'itĠ', 'knowĠhowĠtoĠ', 'sĠtoĠgetĠ', '.ĠThen', 'ingĠtoĠ', '?\"Ġ', 'dry', 'D', 'look', 'edĠthroughĠ', 'pau', '.\"ĠSheĠ', 'op', 'hadĠtoĠ', 'play', 'ON', 'about', 'mineĠ', 'overĠtoĠ', 'forĠtheĠ', 'ĠsaidĠ', 'substitut', '.ĠF', 'eveĠ', 'wedd', 'downĠtheĠ', 'AllĠright,Ġ', 'said,Ġ', 'eddingĠ', 'upĠtheĠ', 'ationsĠ', 'clud', 'ior', 'deck', 'bad', 'ore', 'it', 'said', 'lost', 'S', '>,Ġ', 'walk', 'justĠ', 'edĠup', 'fat', 'eĠthereĠ', 'carefullyĠ', 'on', '!ĠIĠ', 'sĠtheĠ', 'youngĠ', 'ick', 'ask', 'EndĠ', 'IĠwasĠjustĠ', 'Man', 'sad', 'didĠ', 'soonĠ', 'enoughĠ', 'water', 'Cast', 'onĠ', 'mĠ', '.ĠSheĠ', \"It'sĠaĠ\", 'IĠsaidĠ', 'lyĠtheĠ', 'aredĠ', 'bedĠ', 'ignor', 'edĠthem', '.ĠIt', 'aĠs', \"'dĠ\", 'backĠ', 'allyĠtheĠ', 's.ĠItĠ', 'onight', '.ĠT', 'pieceĠ', 'TheĠ', 'beforeĠsheĠ', 'wentĠ', 'P', 'weĠneedĠ', 'cast', 'don', 'way', 'Ġclos', 'bottom', 'harmon', 'asesĠ', 'ever', 'again', 'fetch', 'lookedĠ', 'pointedĠout'}\n",
      "==Same Author==\n",
      "Text 1: |\n",
      " **5)** | **_entail (to)_** |\n",
      " |\n",
      " \\- to include/involve\n",
      " |\n",
      " **6)** | **_pushing paper (to be)_** |\n",
      " |\n",
      " \\- to be doing routine office work\n",
      " |\n",
      " **7)** | **_navigate the system (to)_** |\n",
      " |\n",
      " \\- to know how to work within a system; to know the ropes\n",
      " |\n",
      " **8)** | **_go in one's stead (to)_** |\n",
      " |\n",
      " \\- t\n",
      "\t Tokenized GPT2:| Ċ Ġ** 5 )** Ġ| Ġ** _ ent ail Ġ( to )_ ** Ġ| Ċ Ġ| Ċ Ġ\\- Ġto Ġinclude / inv olve Ċ Ġ| Ċ Ġ** 6 )** Ġ| Ġ** _ p ushing Ġpaper Ġ( to Ġbe )_ ** Ġ| Ċ Ġ| Ċ Ġ\\- Ġto Ġbe Ġdoing Ġroutine Ġoffice Ġwork Ċ Ġ| Ċ Ġ** 7 )** Ġ| Ġ** _ n avig ate Ġthe Ġsystem Ġ( to )_ ** Ġ| Ċ Ġ| Ċ Ġ\\- Ġto Ġknow Ġhow Ġto Ġwork Ġwithin Ġa Ġsystem ; Ġto Ġknow Ġthe Ġro pes Ċ Ġ| Ċ Ġ** 8 )** Ġ| Ġ** _ go Ġin Ġone 's Ġstead Ġ( to )_ ** Ġ| Ċ Ġ| Ċ Ġ\\- Ġt\n",
      "\t Tokenized no:| ĊĠ ** 5) **Ġ|Ġ ** _ ent ail Ġ( to ) _ **Ġ | ĊĠ | ĊĠ \\ - ĠtoĠ include / inv ol ve ĊĠ | ĊĠ ** 6 ) **Ġ|Ġ ** _ pushingĠ pap erĠ( toĠ be ) _ **Ġ | ĊĠ | ĊĠ \\ - ĠtoĠbeĠ doingĠ routineĠ officeĠ work ĊĠ | ĊĠ ** 7 ) **Ġ|Ġ ** _ navig ateĠtheĠ system Ġ( to ) _ **Ġ | ĊĠ | ĊĠ \\ - ĠtoĠ knowĠhowĠtoĠ work ĠwithinĠ aĠsystem ; ĠtoĠ knowĠtheĠ rop es ĊĠ | ĊĠ ** 8 ) **Ġ|Ġ ** _ goĠ inĠ one'sĠ steadĠ ( to ) _ **Ġ | ĊĠ | ĊĠ \\ - Ġt\n",
      "\t Unique Tokens GPT2: {'Ġwin', 'C', 'Ġthen', '.', 'Ġsure', 'Ġcompassion', 'Ġboys', 'Ġmouth', 'Ġskill', 'Ġcompany', 'Ġshe', 'Ġsecond', 'Ġthan', 'Ġgo', 'Ġ,', 'Ġstead', '>,', 'Ġmusic', 'Ġ\"', ')**', 'Ġlearn', 'Ġright', 'ĠWhy', 'Ġquickly', 'Ġscratch', 'Ġface', 'Ġwithout', 'ĠVis', 'pes', 'Ġ5', 'Ġ11', 'oth', 'Ġdrill', 'Ġ13', 'Ġlegally', 'Ġclients', 'Ġ12', 'Ġ10', 'ues', 'Ġbuck', 'olve', 'Ġimmigration', 'Ġsaid', 'Ġdoing', 'Ġalways', 'Ġball', 'a', 'ride', 'Ġneed', 'all', 'Ġconversation', 'Ġcan', 'Ġwarning', 'Ġhave', 'Ġblood', 'Ġdeal', 'Ġset', \"'t\", 'Ġis', 'Ġpage', 'Ġsucceed', 'Ġ**', 'stone', 'Ġ4', 'Ġoffice', 'ĠHere', 'Ġgreen', 'Ġthink', 'Ġget', 'Ġroll', 'Ġ1', 'Ġcoworkers', '__.', 'ĠIf', 'Ġsomeone', 'Ġhorn', 'Ġthis', 'ut', 'Ġfrom', 'od', 'Ġ?', 'Ġrequired', 'Ġgrind', 'Ġ9', 'ĠImm', 'Ġsw', 'go', 'igr', 'Ġwho', 'Ġuncle', 'Ġlowest', 'Ġbe', 'ĠWe', 'ĠEvery', 'Ġusing', 'Ġup', 'Ġhands', 'Ġconference', 'ĠFill', 'Ġwords', 'ĠMy', 'Ġriding', 'Ġan', 'from', 'Ġhard', 'Ġare', 'ĠStory', 'ĠOne', 'Ġcrossing', 'Ġcomb', 'ing', 'Ġhasn', 'Ġold', 'Ġcalling', 'Ġmarch', 'Ġ18', 'Ġsays', 'Ġmorning', 'Ġsleeves', 'slave', 'Ġ15', 'Ġsuperior', '___', 'iversity', 'Ġcard', 'ĠRead', 'Ġst', 'Ġ7', 'Ġimpression', 'ĠI', 'Ġtime', 'Ġ2', 'Ġmake', 'Ġladder', 'earth', 'Ġclub', 'Ġbring', 'player', 'time', 'Ġ14', 'Ġbl', 'put', 'ump', 'PER', 'Ġwas', 'Ġmy', 'Ġfine', 'Ġprevious', '_**', 'Ġquite', 'Ġpractical', 'Ġabout', 'Ġidi', 'Ġby', 'ĠSome', 'ĠBecause', 'ails', 'Ġas', 'Ġwhen', 'Ġhe', 'Ġdrop', 'Ġlack', 'p', 'ĠU', 'Ġnot', 'Ġred', 'ĠIt', 'Ċ', 'Ġdict', 'Ġgives', 'haul', 'Ġro', 'Ġlevel', 'Ġhappy', 'Ġweek', 'Ġchance', 'Ġdone', 'Ġwill', 'Ġwork', 'ately', 'Ġgot', 'Ġto', 'Ġyou', 'Ġcomputer', 'ĠBefore', 'ĠNow', 'Ġjob', 'Ġdown', 'SON', 'Ġsubstitute', 'Ġrest', 'Ġsuch', 'Ġ\\\\-', 'Ġtook', 'Ġit', 'ĠD', \">'\", '##', 'tle', 'j', 'Ġin', 'Ġdoor', 'Ġnothing', 'Ġand', 'Ġability', 'Ġsystem', 'ant', 'Ġ#', 'Ġgoing', 'Ġof', 'ushing', 'Ġorders', 'ĠCan', 'Ġall', 'Ġsign', 'You', 'Ġa', 'ĠThe', 'ĠA', 'ĠAn', \"'m\", 'Ġnew', 'Ġpaper', 'oms', 'Ġconnections', 'ĠDon', 'ĠBoston', 'ers', 'Ġso', 'Ġ6', 'Ġinclude', 'Ġlot', 'Ġimportant', 'Ġ3', 'Ġon', 'Ġjump', 'ate', 'Ġwith', 'Ġco', 'ĠNot', 'Ġbefore', 'Ġdriver', 'Ġspending', 'Ġboss', 'avig', 'Ġone', 'Ġhow', 'ĠLet', 'Ġdot', 'ĠThis', 'Ġthe', 'ĠHe', 'Ġfor', 'Ġdays', 'Ġhis', 'Ġground', 'Ġt', 'Ġonce', 'Ġfoot', 'Ġwithin', 'Ġday', 'ĠPractice', 'Ġno', ')_', 'ĠYou', 'Ġout', 'S', 'Ġ.', 'Ġmore', 'Ġ8', 'Ġsc', 'Ġhistory', 'Ġcorporate', 'Ġblowing', 'Ġpersonal', 'taking', 'Ġknow', 'Ġshots', 'Ġbut', 'Ġdon', 'stand', 'Ġnose', 'n', 'Ġor', 'Ġour', 'Ġclean', 'Ġproperly', 'Ġroutine', 'eye', 'ĠP', 'gy', 'Ġpret', 'Ġ|', 'Ġclimbed', 'Ġmuch', 'Ġreally', 'Ġfirst', 'amped', 'Ġi', 'Ġover', 'Ġthat', 'ĠPersonally', 'Ġme', 'Ġconfront', 'Ġyour', 'ting', ',', 'Ġthese', 'anks', 'ĠContin', 'Ġwaiting'}\n",
      "\t Unique Tokens no: {\"one'sĠ\", '__Ġ', '.Ġ', 'toĠ', 'ĠwithinĠ', 'ĠtoĠbeĠ', '.ĠIfĠ', 'blow', 'says,Ġ', 'steadĠ', 'ed', '_Ġ', 'warning', '\"', 'doingĠ', 'daysĠ', 'onceĠ', 'rat', 'leĠ', 'happyĠ', 'lackĠ', 'include', 'fac', 'A-', 'sĠ(', 'ofĠmyĠ', 'ingĠinĠ', 'page', 'ĠtimeĠtoĠ', 'bloodĠ', '\\\\-Ġ', '.ĠYouĠ', 'e-', 'earthĠ', 'blank', 'eĠtoĠ', 'aboutĠtoĠ', '18.', 'standĠ', 'pap', '**Ġ|Ġ', 'lowestĠ', 'migr', 'iceĠ', 'pret', 'officeĠ', '###Ġ', 'conversation', 'An', 'isĠsoĠ', 'sheĠ', 'utt', 'orĠc', 'yourĠ', 'shot', \"'sĠtheĠ\", 'suchĠ', 'stoneĠ', 'jumpĠ', 'Ġcl', \"i'sĠ\", 'ingĠhisĠ', '.ĠWeĠ', 'doneĠ', 'greenĠ', 'withĠaĠ', '?Ġ', 'over', 'workĠ', '.C', 'ul', 'order', 'ingĠ', '.ĠNotĠ', 'edĠtheĠ', 'illĠ', 've', 'aĠst', 'BecauseĠ', 'firstĠ', 'singĠ', 'goingĠtoĠbeĠ', 'wasĠ', 'setĠ', 'ha', 'needĠ', \"Ġcan'tĠ\", 'morningĠ', ',Ġ', 'withĠc', 'cow', 'fromĠtheĠ', 'groundĠ', '5)', 'ersĠ', '.ĠH', 'byĠ', 'call', '<', 'youĠ', 'br', 'Vis', 'ooth', 'slaveĠ', 'sĠonĠtheĠ', 'footĠ', 'ersĠareĠ', 'ofĠtheseĠ', 'ubĠ', 'edĠ', '>Ġ', 'downĠandĠ', 'hisĠt', 's.Ġ', 'co', '4', 'ation', 'succe', 'isĠ', 'goĠ', 'Ġcom', 'anĠ', 'givesĠmeĠ', 'ol', 'youĠw', 'waitingĠforĠ', 'companyĠ', 'BostonĠ', 'ey', '.ĠBeforeĠ', '.ĊĠ', 'important', '\\\\', 'b', ',Ġthen', 'yĠ', 'restĠofĠtheĠ', 'navig', 'moreĠthanĠ', 'routineĠ', 'hisĠ', 'upĠ', 'buck', '.ĠWeĠhaveĠ', 'makeĠsureĠ', 'ityĠ', 'hard', 'chanceĠtoĠ', \"'sĠ\", 'ĠthatĠthisĠ', '1.ĠTheĠ', '-to-', 'gotĠ', \"hasn'tĠ\", 'myĠ', 'reallyĠ', 'ĠwithoutĠ', 'aĠ', 'be', 'dict', 'getĠaĠ', '.ĠWhyĠ', 'compass', 'overĠ', '(', 'oldĠ', 'adderĠ', 'jobĠ', 'ofĠanĠ', '(toĠ', 'U.S.Ġ', 'upĠtoĠ', 'mouth', 'boys', 'tt', 'new', 'sĠareĠ', 'sĠisĠ', 'sc', 'youĠgetĠ', 'ion', 'jump', 'ĊĠ', 'uncleĠ', 'needĠtoĠ', 'ingĠonĠ', 'YouĠ', \".ĠDon'tĠ\", 'sĠ', 'it,Ġ', 'makeĠaĠ', 'secondĠ', 'ourĠs', 'ork', 'ĠwhenĠsheĠ', 'levelĠ', 'F', 'ĠtoĠ', 'asĠ', ',ĠwhoĠ', 'properlyĠ', 'knowĠtheĠ', 'aĠsuper', 'march', 'corporateĠ', 'rollĠ', 'ĠtoĠworkĠ', 'w', 'howĠ', 'ateĠtheĠ', 'ball', 'ingĠtheĠ', 'beforeĠ', 'door', 'historyĠ', 'Read', \"I'mĠs\", 'pushingĠ', 'ĠwithĠnoĠ', 'om', 'abilityĠ', \"don'tĠ\", ';Ġ', 'putĠ', 'ev', 'someoneĠ', 'es', '.ĠNowĠ', 'youĠs', 'antĠ', '3.ĠLetĠ', 'tak', 'quicklyĠ', 'required', 'newĠ', 'ERS', 'StoryĠ', 'ĠcomputerĠ', 'outĠofĠmyĠ', 'deal', 'red', 'fin', 'Ġconnection', 'chĠ', 'previousĠ', 'inĠtheĠ', 'ĠandĠ', 'Person', 'Im', 'inĠaĠ', 'knowĠhowĠtoĠ', 'idi', 'D', '.ĠMyĠ', 'music', 'amp', 'lately', 'aĠlotĠ', 'inĠ', 'rightĠ', 's.Ċ', 'soĠmuch', 'hor', 'ON', 'personalĠ', 'aĠweek', 'IĠthinkĠ', 'ĠtheĠ', 'forĠtheĠ', 'ignĠ', 'rideĠ', 'nos', 'OneĠ', 'substitut', 'e/', 'ĠĠĠ', 'Pract', \"he'sĠ\", 'learnĠ', 'n,Ġ', 'climb', 'ior', 'sĠandĠ', 'allĠday', 'nothingĠ', 'me.Ġ', 'it', 'said', 'rop', 'allĠoverĠ', 'practical', 'eĠtoĠtheĠ', '>,Ġ', 'ros', 'bos', 'skillĠ', 'card', 'aĠsystem', 'work', 'timeĠ', 'ĠtookĠtheĠ', 'without', ')', '.ĠSomeĠ', 'do', 'faceĠtheĠ', 'ĠwillĠ', 'onĠ', 'hand', 'notĠ', 'Continu', 'driverĠ', 'spendingĠ', 'odg', 'quiteĠ', '.ĠThisĠ', '.ĠIt', 'buttĠ', 'drop', '.ĠCanĠ', 'isĠalwaysĠ', 'for', 'immigr', '2', 'TheĠ', 'ere', 'clean', '**Ġ', 'impression', 'system', 'P', 'legallyĠ', 'wordsĠ', 'ally,Ġ', '.ĠEveryĠ', '(a', 'grind', \"I'mĠ\", 'ĠtoĠthatĠ', 'esĠandĠ', 'rid', 'ivers', 'drill', 'usingĠtheĠ', 'meĠ', 'confront', 'lientsĠ', \".ĠHe'sĠ\", '.ĠYou', 'playerĠ', 'erĠ(', 'conferenceĠ'}\n",
      "Text 2: predictors only (G+2TiC=C), for example:\n",
      " Look at the following predictor thesis. Notice how each predictor becomes a topic sentence in each body paragraph. Notice also how **she** becomes the topic identifier **My mother** in each body paragraph topic sentence ( **T** iC).\n",
      " ## Predictor Thesis: _Us\n",
      "\t Tokenized GPT2:predict ors Ġonly Ġ( G + 2 Ti C = C ), Ġfor Ġexample : Ċ ĠLook Ġat Ġthe Ġfollowing Ġpredict or Ġthesis . ĠNotice Ġhow Ġeach Ġpredict or Ġbecomes Ġa Ġtopic Ġsentence Ġin Ġeach Ġbody Ġparagraph . ĠNotice Ġalso Ġhow Ġ** she ** Ġbecomes Ġthe Ġtopic Ġidentifier Ġ** My Ġmother ** Ġin Ġeach Ġbody Ġparagraph Ġtopic Ġsentence Ġ( Ġ** T ** Ġi C ). Ċ Ġ## ĠPred ictor ĠThe sis : Ġ_ Us\n",
      "\t Tokenized no:predict orsĠ onlyĠ (G +2 Ti C = C ),Ġ forĠexample :ĊĠ LookĠ atĠtheĠ followingĠ predict or Ġthes is .ĠNot iceĠ howĠ eachĠ predict orĠ becomesĠ aĠt op icĠs ent enceĠ inĠeachĠ bodyĠ paragraph .ĠNot iceĠ alsoĠ howĠ ** she **Ġ becom esĠtheĠ top icĠ identifi erĠ ** MyĠ mother **Ġ inĠeachĠ bodyĠ paragraph Ġtop icĠs ent enceĠ (Ġ ** T **Ġ i C ).Ċ Ġ ##Ġ Pred ict orĠ Thes is:Ġ _ Us\n",
      "\t Unique Tokens GPT2: {'Ġwith', 'Ġuse', 'Ġtold', 'Ġby', 'Ġyear', 'Ġco', 'Ġspoken', 'Ġitems', 'Ġthis', 'My', 'Ġas', '.', 'Also', 'Ġexperience', 'Ġresponse', 'Ġdemonstrate', 'Ġpurchased', 'ĠSt', 'Ġperspective', 'Ġinsp', 'Ġhow', 'Using', 'Ġverb', 'ĠThis', 'Ġ##', 'Ġ_', 'Ġshe', 'Ġthe', 'Ġfor', 'Ġdemonstrates', 'Ġmother', 'Ġbetter', 'ĠConnecticut', 'Ċ', 'ms', 'Ġwant', 'ĠWhen', 'Ġreasons', 'Ġif', 'Ġalso', 'Ġdays', 'Ġview', 'Ġbiggest', 'Ġbabys', 'Ġinfluence', 'Ġgreat', 'topic', 'Ġestimation', 'ĠE', ':', 'ĊĠĠ', 'Ġ=', 'Ġgrowing', 'Ġheart', 'ĠIn', 'ony', 'Ġwill', 'Ġwork', 'Ġstrange', 'Ġyou', 'Ġto', 'Ġfollowed', 'Ġdeliver', 'Ġ\"', 'Ġunity', 'Ġwho', 'ĠSyn', 'Ġworking', 'Ġrefund', 'Ġwithin', 'Ġbe', 'Ġfollowing', 'Ġright', 'Ġwhenever', 'ĠNotice', 'Ġpracticing', 'it', 'Ġ).', 'Ġeach', 'ĠPred', 'Ġdevelopment', 'Ġpair', 'Ġfeel', 'Ġspeaking', 'Ġfollow', 'Ġmothers', 'Ġup', 'DU', 'Ġtopic', 'Ġonly', 'ĠTo', 'Ġwords', 'ĠLook', 'Ġbecause', 'Ġreplace', 'herence', 'Ġ5', '+', 'Ġcontend', 'Ġpredict', 'Ġpost', 'Ġhas', 'Ġbecomes', 'Ġan', 'Ġthey', 'Ġit', 'ĠShe', 'Ġexample', 'ĠWords', 'Ġshould', 'G', 'Ġun', 'Ġat', 'Ġ(', 'Ġparagraph', 'ires', 'Ġproblems', 'Ġknow', 'Ġphrase', 'ĠNext', 'onymous', 'Ġprogression', 'Ġbut', 'ĠOnly', 'Ġhouse', 'Ġbeen', 'Ġthirty', 'Ġin', 'Ġremember', 'Ġsaid', 'ĠLiving', 'Ġfull', 'ĠReplace', 'Ġand', 'Ġjust', 'Ġdoing', 'Ġalways', 'ĠAt', 'Ġdidn', 'ĠBut', 'Ġwonderful', 'This', 'Ġau', 'Ġfamily', 'sis', 'Ġfriends', 'Ġwere', 'Ġdevelop', 'Ġleave', 'Ġgive', 'Personally', 'Ġhigh', 'Ġallowed', 'Ġlife', 'Ġlives', 'ulate', '),', 'Ġway', 'ĠAmerica', 'Ġall', 'Ġa', 'ite', 'Ġlanguage', 'ĠThe', 'Ġmain', 'ford', 'Ġthose', 'Ġcan', 'Ġhave', 'ĠI', 'Ġ|', 'Ġidentifier', 'ictor', 'Ġthesis', 'Ġ2', 'ĠAn', \"'m\", 'Ġsentence', 'Ġpos', 'ĠFrom', 'Ġmuch', \"'t\", 'itter', 'Ġnow', 'ĠEnglish', 'Ġbelieve', 'Ġfirst', 'Ġcome', '.**', 'ĠFor', 'Ġencourages', 'Ġi', 'Ġis', 'Ġreason', 'ĠPersonally', 'Ġlook', 'Ġorganization', 'Ġ**', 'Ġ4', 'ĠAmerican', 'Ġwanted', 'Ġthink', 'Ġme', 'Ġ185', 'Ġbody', 'Ġ6', 'Ġso', \"'\", 'Ġschool', 'Ġcustomers', 'Ġam', 'Ġwould', 'Ġlike', 'Ġsyn', 'Ġstrength', ',', 'Ġreturn', 'Ġ1', 'Ġmy', 'Ġwas', 'Ġthese', 'Ġ3'}\n",
      "\t Unique Tokens no: {'allyĠ', 'heart', 'wantĠtoĠ', 'Person', 'ĠtheseĠ', 'itĠwouldĠbeĠ', 'ĠthatĠ', 'iresĠ', '(', \"s'\", '.ĠAnĠ', 'wereĠ', 'wantedĠtoĠ', '.ĠIĠ', 'ut', 'cont', 'firstĠ', 'St', '),Ġ', 'responseĠ', 'aĠgreatĠ', 'itterĠ', 'op', 'IĠwasĠ', '##Ġ', 'ally', 'inĠ', 's.Ċ', 'iteĠ', 'forĠaĠ', 're', 'ollow', 'Ġm', 'posit', 's,ĠIĠ', 'day', 'workingĠ', 'irtyĠ', 'ages', 'plac', 'hasĠ', 'isĠlikeĠ', 'ys', 'phrase', ',Ġ', 'ĠwithĠ', 'onlyĠ', 'andĠtheyĠ', 'lookĠatĠtheĠ', 'atesĠ', 'Americ', 'ĊĠ', 'WhenĠ', 'comeĠtoĠ', ').Ċ', '.ĠFrom', 'is:Ġ', 'Ġthes', 'customersĠ', 'eĠthink', 'ĠthoseĠ', 'e,ĠIĠ', 'highĠschool', 'nowĠ', 'coher', 'remember', '.ĠIĠamĠ', 'ateĠthatĠ', 's:Ġ', 'icĠs', 'ĠĠĠ', 'OnlyĠ', 'forĠexample', 'paragraph', 'demonstrateĠ', 'ation,Ġ', 'develop', 'Re', 'byĠ', 'sĠinĠ', 'MyĠ', 'aĠfullĠ', 'ForĠexample,Ġ', 'UĠ', 'encour', 'practic', 'pairĠ', 'ingĠwithĠ', 'orĠ', 'returnĠ', 'strangeĠ', 'inĠeachĠ', 'othersĠ', 'allĠmyĠ', 'spoken', '.ĠL', 'paragraphĠ', 'LookĠ', 'orsĠ', 'y', 'doingĠthisĠ', 'bodyĠ', 'motherĠ', '=Ġ', 'example', 'yearĠ', 'erĠ', 'ifĠyouĠcanĠ', 'becomesĠ', 'Thes', 'believeĠthatĠ', '.ĠButĠ', 'youĠknowĠ', 'purchasedĠ', 'justĠ', 'ĊĠĠĠ', '(Ġ', '.ĠNot', 'ent', ',ĠforĠ', 'beenĠtheĠ', ',ĠIĠ', 'i', 'icĠ', 'UsingĠ', 'always', 'd,Ġ', 'alsoĠ', 'strength', 'familyĠ', '5', 'From', 'work', 's_', 'Pred', 'English', 'is', 'Connect', 'ence.', 'saidĠ', 'allowedĠtoĠ', 'wonderfulĠ', '.ĠThisĠwasĠ', 'speak', 'Ġth', 'iv', 'problem', 'howĠ', '4', 'ation', 'follow', 'iceĠ', 'friend', 'fundĠ', 'asĠanĠ', 'leaveĠ', 'isĠ', 'America,Ġ', '.ĠSheĠ', 'esĠtheĠ', 'anĠ', 'inĠmyĠ', 'Next', '.ĠThisĠwillĠ', 'ThisĠ', 'AmericanĠ', ',Ġun', 'ToĠ', 'better', 'organiz', 'giveĠmeĠ', 'influenceĠ', 'hasĠbeenĠaĠ', 'progression', 'demonstr', 'in', 'ĠthinkĠ', '.ĠPer', 'willĠ', 'meĠtoĠ', 'ym', 'aĠs', 'end', 'top', 'IĠhaveĠ', 'ityĠandĠ', 'IĠ', 'andĠmyĠ', 'ousĠ', 'wheneverĠ', '.ĊĠ', 'up,Ġ', 'Also,Ġ', 'sheĠ', 'lif', 'right', 'auĠ', 'deliverĠ', 'mother', 'identifi', 'livesĠ', 'for', 'verb', 'followingĠ', 'Word', 'ĠtoldĠ', 'me.', '3.Ġ', 'aĠbab', 'family', '**Ġ', 'atĠtheĠ', 'experienceĠandĠ', 'growingĠ', 'whoĠ', 'becom', 'For', 'perspect', 'un', 'AmericaĠ', 'enceĠ', 'edĠby', 'estim', 'wordsĠ', 'use,Ġ', 'ally,Ġ', 'shouldĠbeĠ', 'ĠmyĠ', ':ĊĠ', 'sĠforĠ', '185', 'agesĠ', ',ĠbutĠmyĠ', '1', 'replaceĠ', 'Ġ', 'ing,ĠIĠ', 'development', \"IĠdidn'tĠ\", 'E', 'mainĠ', 'soĠmuchĠ', \"I'mĠ\", 'eĠthatĠtheĠ', '|Ġ', 'Ġtop', 'ict', 'wasĠaĠ', 'with', 'post', 'pair', 'lifeĠ', 'LookĠatĠ', 'house', 'aĠandĠ', 'non', 'feelĠthatĠ', '6', '.ĠInĠmyĠ', 'experienceĠ', 'son', 'languageĠ', 'withĠ', 'IĠthinkĠthatĠ', 'ic', 'withĠaĠs', 'withĠanĠ', 'eachĠ', 'withĠaĠ', 'becauseĠ', 'reason', 'experienc', 'myĠ', 'wayĠtoĠ', 'view', '(G', 'Sy', 'EnglishĠ', 'aĠf', 'aĠt', 'item', 'aĠ', 'ul', '.ĠAtĠ', 'ive,Ġ', '+2', 'e.Ċ', '.ĠSheĠwasĠ', 'useĠ', 'biggestĠ', 'ingĠ', 'ĠandĠ'}\n",
      "==Different Author==\n",
      "Text 1: the template code through its paces. The `assert_select` assertion call confirms that the resulting page contains the expected number of `div` elements with a class of `story`:\n",
      "     assert_select 'div#content div.story', count: 1\n",
      " And that, dear reader, is the last test I'll make you write! Well, fo\n",
      "\t Tokenized GPT2:the Ġtemplate Ġcode Ġthrough Ġits Ġp aces . ĠThe Ġ` assert _ select ` Ġassertion Ġcall Ġconfirms Ġthat Ġthe Ġresulting Ġpage Ġcontains Ġthe Ġexpected Ġnumber Ġof Ġ` div ` Ġelements Ġwith Ġa Ġclass Ġof Ġ` story ` : ĊĠĠĠĠ Ġassert _ select Ġ' div # content Ġdiv . story ', Ġcount : Ġ1 Ċ ĠAnd Ġthat , Ġdear Ġreader , Ġis Ġthe Ġlast Ġtest ĠI 'll Ġmake Ġyou Ġwrite ! ĠWell , Ġfo\n",
      "\t Tokenized no:theĠt emplateĠ cod eĠthroughĠ itsĠ pac es.ĠTheĠ ` assert _ select `Ġ asser tion ĠcallĠ confirm sĠthatĠtheĠ resultingĠ pageĠ containsĠtheĠ expectedĠ numberĠofĠ ` div `Ġ elementsĠ withĠaĠ classĠofĠ ` story ` : ĊĠĠĠĠĠ assert _ selectĠ ' div # contentĠ div . story ',Ġ count :Ġ1 ĊĠ And Ġthat,Ġ dearĠ read er,Ġ isĠtheĠ last ĠtestĠ I'llĠ makeĠ youĠw rite !Ġ Well,Ġ fo\n",
      "\t Unique Tokens GPT2: {'ĠTest', 'ions', 'probably', 'aces', 'Ġlikes', 'Ġbuilding', 'Ġpat', 'ĠSummary', 'ed', 'Ġ49', 'Ġ20', 'Ġexceptions', 'ĠApplication', 'Ġyet', 'Ġdouble', 'Ġrein', '....', 'Ġresponsible', 'Ġenvironment', 'Ġhowever', 'Ġtools', 'Ġlearn', 'Ġarise', 'Ġinstalling', 'Ġwithout', 'Ġexpanded', 'Ġ--', 'Ġits', 'separ', 'seed', 'Ġthough', 'Ġtopics', 'Ġ11', 'Ġnecessary', 'is', 'Ġresults', 'ĊĠĠĠĠ', 'Ġones', 'Ġfailures', 'Ġcase', 'Ġalways', 'Ġrunning', 'Ġgone', '`.', 'Ġcomprehensive', 'ĠAfter', 'Ġelements', 'Ġgive', 'Ġ`', 'Ġsubmission', '),', 'Ġstory', 'Ġway', 'Ġneed', 'ĠTesting', \"',\", '53', 'Ġcan', 'Ġhave', 'Ġapplications', \"'t\", 'we', 'Ġexplore', 'Ġfinished', 'Ġdeveloper', 'Ġexisting', 'Ġis', 'Ġpage', 'Ġthings', 'Ġexpected', 'rake', 'ulatory', 'Ġget', 'Ġ1', 'Ġstill', 'ĠIf', 'Ġworked', \"'s\", 'Ġthis', 'icient', 'Ġdis', 'Ġstuck', 'Ġfocus', 'ĠBen', 'Ġdeveloping', 'Ġwhole', 'ĠAgain', 'Ġchapter', 'nn', 'Ġruns', 'Ġtalk', 'Ġfun', 'Ġguide', 'ĠYour', 'Ġbe', 'Ġ60', 'ĠDebug', 'Ġdevelopment', 'Ġusing', 'Ġhands', 'ated', 'Ġperfect', 'ĠSpring', 'Ġfunctionality', 'Ġline', 'Ġan', 'Ġerrors', 'Ġrun', 'Ġhard', 'Ġare', 'Ġ...', 'Ġbook', 'Ġnumber', 'Ġ90', 'ch', 'Ġdebugging', 'Ġapplying', 'content', 'let', 'Ġfix', 'Ġadd', 'Ġweb', 'Ġtag', 'Ġtests', 'Ġarchive', 'Ġseveral', 'Ġyourself', \"Ġ'\", 'Ġlesson', 'Ġplugin', 'Ġconsole', 'ished', 'ĠWelcome', 'ĠI', 'Ġdevoted', 'Ġtime', 'Ġgood', 'Ġmake', 'Ġown', 'Ġpassed', 'Ġcall', 'Ġsuite', 'Ġissues', 'Ġviews', 'Ġcourse', 'the', 'Ġwrong', 'ations', 'Ġabout', 'ĠChapter', 'Ġuse', 'Ġby', 'Ġhand', 'Ġagain', 'Ġas', 'Ġcongrat', 'Ġwhen', 'free', 'Ġdiv', '................................', 'Ġeasy', 'Ġanyway', 'ĠRunning', 'Ġnot', 'Ġconfiguring', 'Ġp', 'ĠIt', 'Ġwell', 'Ċ', 'ĠWhen', 'Ġbit', 'ĠThese', 'ĠIn', 'Ġdone', 'ĠOur', 'Ġwill', 'Ġwork', 'Ġyou', 'Ġto', 'Ġmight', 'loader', 'Ġmessages', 'Ġnext', 'Ġspeed', 'ĠRun', 'Ġintegration', 'Ġjob', 'Ġrails', 'Ġbugs', 'ĠWell', 'y', 'Ġdown', 'Ġlocate', 'Ġ45', 'ks', 'Ġmay', 'Ġform', 'Ġvery', 'ĠAnd', 'Ġit', 'Ġwheel', 'Ġconfirms', 'ĠOf', 'Ġat', 'Ġchapters', 'Ġlist', 'Ġapplication', 'Ġtruly', 'Ġin', 'Ġpretend', 'Ġflaws', 'Ġand', 'Ġcontains', 'Ġ#', 'Ġhad', 'Ġof', 'Ġrig', '05', '/', 'Ġback', 'Ġwhere', 'Ġall', 'check', 'Ġa', 'ĠThe', 'Ġ73', 'Ġcount', 'Ġany', 'Ġtags', 'Ġourselves', 'Ġagainst', 'Ġmigr', 'ĠOnce', 'Ġnew', 'ĠRails', 'ĠFin', '-', 'Ġextend', 'Ġso', 'Ġtake', 'Ġnobody', 'Ġahead', 'Ġlike', 'Ġdisplayed', 'ially', 'Ġbott', 'Ġon', 'Ġremaining', 'Ġjump', 'Ġwith', 'Ġdoesn', 'Ġtimes', 'Ġpre', 'Ġpass', 'Ġassure', 'Ġhow', 'Ġ##', 'Ġ_', 'Ġprof', 'Ġrich', 'Ġthe', 'Ġstop', 'Ġfor', 'Ġlast', 'Ġif', 'Ġbetter', 'Ġdata', 'Ġerror', 'Ġ0', 'Ġprofile', 'Ġoptions', 'Ġcomma', 'Ġthrough', 'ging', 'Ġprovided', 'Ġcode', 'ĠSuite', \"'re\", 'de', 'Ġthere', 'Ġlooked', 'Ġonly', 'ĠTo', 'Ġprocess', 'Ġtechnical', 'Ġsee', 'Ġsingle', 'Ġproduction', 'Ġ(', 'Ġreader', 'Ġbegin', 'Ġfind', 'ips', 'Ġclass', 'vent', 'Ġread', \"'ll\", 'Ġor', 'Ġour', 'Ġassertion', 'Ġdear', 'Ġassert', 'ere', 'Ġdisplay', 'Ġwrite', 'Ġhelp', '86', 'Ġcould', 'Ġslowing', 'certain', 'Ġfirst', 'Ġthat', 'Ġtemplate', 'Ġ$', 'Ġvia', \"'d\", 'Ġyour', ',', 'Ġthese', 'Ġresulting', 'Ġregardless', '218'}\n",
      "\t Unique Tokens no: {'sub', 'aĠc', 'dis', 'rail', 'passĠ', 'low', 'Run', 'pre', ',Ġthough', 'onĠtheĠ', 'toĠ', 'comm', 'rig', '),Ġ', \"you'dĠ\", 'aĠb', 'useĠofĠtheĠ', 'youĠareĠ', 'viaĠ', 'ag', 'pageĠ', '_Ġ', ':Ġ', 'mayĠ', 'hapterĠ', 's:Ġ', 'alwaysĠ', 'Ġcon', 'SpringĠ', 'read', 'containsĠtheĠ', 'atoryĠ', ',ĠtheĠfirstĠ', 'ations,Ġ', '...', 'forĠs', 'isĠnotĠ', 'ahead', 'ĠtrulyĠ', 'esĠthatĠ', 'edĠtoĠtheĠ', 'lik', 'wholeĠ', 'andĠ', 'edĠbyĠtheĠ', 'howĠtheĠ', 'aboutĠ', 'youĠcouldĠ', \"I'llĠ\", 'ientĠ', 'And', ',ĠweĠ', 'passed', 'Test', 'issu', 'migr', 'ĠtheĠt', 'usingĠanĠ', 'separatedĠ', 'eĠthroughĠ', 'stopĠ', 'RunĠ', 'yĠandĠ', '45Ġ', 'errorsĠ', 'willĠ', 'isĠtheĠ', 'anyway', ',ĠthereĠ', 'r', 'codeĠ', 'areĠ', 'form', '.ĠAfterĠ', 'richĠ', 'rite', 'ures,Ġ', 'anyĠ', 'yourĠ', 'ourselv', 'application', 'storyĠ', 'dearĠ', 'pluginĠ', 'es.Ċ', 'allĠtheĠtim', 'exploreĠ', 'jumpĠ', 'doneĠ', 'apply', 'enn', 'againĠ', 'ic', 'get', 'withĠaĠ', 'cert', 'view', 'lookedĠatĠ', 'yourselfĠ', 'er,Ġ', 'asĠwellĠasĠ', 'gratul', 'itĠtoĠ', 'ingĠ', 'goodĠatĠ', 'severalĠ', 'ofĠcourse', 'edĠtheĠ', 'bookĠ', 'erĠinĠ', 'withoutĠ', 'ofĠt', 'RunningĠ', \"you'llĠ\", 'whereĠ', 'iteĠ', 're', \"doesn'tĠ\", 'weĠ', 'ifĠyouĠ', ',Ġ', 'stuck', 'onlyĠ', 'ĠtoĠtalkĠ', 'WhenĠ', \"let'sĠ\", 'ed,Ġ', 'assureĠ', 'ingĠyourĠ', 'develop', 'configur', 'speed', 'load', 'addĠ', 'youĠ', 'es.ĠTheĠ', 'necessaryĠ', 'existingĠ', 'yet', 'expectedĠ', '`Ġ', 'ics,Ġ', 'willĠbeĠtheĠ', '0', 'giveĠ', 'ones', 'integration', '.ĠIf', 'edĠ', 'learn', 'eĠtimesĠ', 'chapter', 'R', '1.0', 'ksĠ', 'classĠofĠ', '4', 'isĠ', 'perfect', 'case,Ġ', 'contentĠ', 'readĠ', 'flaw', 'youĠmightĠ', 'tion', 'like', 'asser', 'pac', 'ToĠ', 'usingĠ', 'icsĠ', 'youĠw', \"we'llĠ\", 'developingĠ', 'TestĠ', 'application,Ġ', 'sĠonĠ', '60', '3Ġ', 'soleĠ', 'resultsĠinĠ', 's/', 'errorĠmess', '.ĊĠ', 'itsĠ', 'yĠ', 'e,Ġ', 'numberĠofĠ', '.ĠTheĠ', '!Ġ', 'comprehensiveĠ', '.ĊĠĠĠĠĠ', 'regardlessĠofĠ', 'agesĠ', 'devot', 'Ġthat,Ġ', 'onĠyourĠ', 'option', 'data', 'expand', '.ĠAgain', 'wrong', 'asĠaĠ', \"'sĠ\", 'nobodyĠ', 'display', 'haveĠ', 'aĠsingleĠ', 'buildingĠaĠ', 'aĠ', 'ĠcallĠ', 'focusĠ', \"you'reĠ\", '.ĠAndĠ', 'Runn', 'allĠofĠ', 'per', '(', \".ĠIt'sĠ\", '11', 'inĠandĠ', 'beĠthatĠ', 'ning', 'confirm', '##Ġ', 'new', 'p', 'ing,Ġ', 'ĊĠ', 'deĠ', 'forĠthisĠ', 'Su', '0Ġ', 'sĠthatĠtheĠ', 'beginĠ', 'exception', 'checkĠ', 'handĠtoĠ', 'ofĠtheĠ', 'codeĠtoĠ', 'De', '--', 'B', 'need', 'sĠtoĠ', 'developmentĠ', 'listĠ', 'applicationĠ', 'pretend', 'betterĠ', 'ĠtoĠ', 'your', 'asĠ', 'bugg', 'ĠtechnicalĠ', 'edĠinĠ', '.ĠTh', '7Ġ', 'ailsĠ', 'OnceĠ', 'ĠtoĠtheĠnextĠ', 'elementsĠ', 'cod', 'ĠtestsĠ', 'run', 'provid', 'howĠ', 'ateĠtheĠ', 'install', 'fix', 'beĠ', 'down', 'ingĠtheĠ', 'forĠ', 'finishedĠ', 'last', 'theĠt', '0.8', 'ourĠ', 'isĠthatĠ', 'job', 'er', 'help', 'Summary', 'con', 'resultingĠ', 'missionĠ', 'ifĠthereĠareĠ', ',ĠsoĠ', 's,ĠandĠ', 'es', 'yourĠcodeĠ', 'funĠ', 'double-', 'seedĠ', 'ĊĠĠĠĠĠ', '..', 'ingĠourĠ', 'ainedĠ', 'back', 'probablyĠtheĠ', ':Ġ1', 'eas', 'code', \"'\", '9.', 'howeverĠ', 'ChapterĠ', 'Our', ',ĠtheĠ', 'ĠtestĠ', 'guide', 'inĠtheĠ', 'count', 'forĠanyĠ', 'invent', 'debugg', 'Finish', 'loc', '56', 'ĠthatĠ', '#Ġ', 'whenĠyou', 'ip', 'InĠthisĠ', 'prof', 'bugsĠ', 'op', 'hadĠtoĠ', 'hardĠ', 's.Ċ', 'own', 'about', 'functionalityĠ', '-freeĠ', 'production', 'comeĠtoĠ', 'agĠ', 'plug', 'ing,ĠandĠ', 'learnĠ', 'processĠ', 'it!', 'orĠ', 'sĠandĠ', 'bug', 'wheel', 'OfĠcourse', 'responsibleĠ', 'a-', 'andĠtheĠ', 'Wel', 'stillĠhaveĠ', 'ĠthisĠisĠtheĠ', 'goneĠ', 'eĠt', 'archiveĠ', 'remainingĠ', 'work', 'iallyĠ', 'ingĠthingsĠ', 's,Ġ', 'IfĠ', 'againstĠtheĠ', '9', 'soĠthatĠ', 'youĠcanĠ', 'bott', 'ĠtoĠtakeĠaĠ', 'hand', 'runĠtheĠ', 'extendĠ', 'notĠ', 'error', 'runningĠ', '$Ġ', \"',Ġ\", \"'dĠ\", 'book', 'environmentĠ', 'Ġtool', 'Well,Ġ', 'selectĠ', 's.ĠItĠ', '18', 'lessonĠ', 'ake', '2', 'webĠ', 'ar', 'suiteĠ', 'ĠtoĠfindĠ', 'allĠofĠtheseĠ', 'plugin', 'YourĠ', 'fail', 'ench', 'way', 'atĠ', 'Ġtop', 'makeĠ', 's-', '65', 'seeĠ', 's.ĠTh', 'applicationsĠ', 'lineĠofĠ', '.ĠWhen', 'ĠworkĠ', 'very', 'iseĠ', 'Application', 'profileĠ', 'emplateĠ', 'ĠĠĠĠĠ'}\n",
      "Text 2: Hello,\n",
      "\n",
      "My comment was aimed to the fact of removing shared BP.\n",
      "\n",
      "If we have single BP per application then it's SMACK label is fixed and BP\n",
      "do not need to handle anything by itself.\n",
      "Privileges to files are enforced by platform.\n",
      "\n",
      "In actual situation, we have shared browser process. BP is needed to be\n",
      "\t Tokenized GPT2:Hello , Ċ Ċ My Ġcomment Ġwas Ġaimed Ġto Ġthe Ġfact Ġof Ġremoving Ġshared ĠBP . Ċ Ċ If Ġwe Ġhave Ġsingle ĠBP Ġper Ġapplication Ġthen Ġit 's ĠSM ACK Ġlabel Ġis Ġfixed Ġand ĠBP Ċ do Ġnot Ġneed Ġto Ġhandle Ġanything Ġby Ġitself . Ċ Pri v ile ges Ġto Ġfiles Ġare Ġenforced Ġby Ġplatform . Ċ Ċ In Ġactual Ġsituation , Ġwe Ġhave Ġshared Ġbrowser Ġprocess . ĠBP Ġis Ġneeded Ġto Ġbe\n",
      "\t Tokenized no:Hello,ĊĊ MyĠ commentĠ wasĠ aim edĠtoĠtheĠ fac tĠofĠ removingĠ sharedĠ BP .ĊĊ If ĠweĠhaveĠ singleĠ B PĠ perĠ application ĠthenĠ it'sĠ SM ACKĠ labelĠ isĠ fix edĠandĠ BP Ċ doĠnotĠ needĠtoĠ handleĠ anythingĠ byĠ itself .Ċ Pri vi leg esĠtoĠ filesĠareĠ enforc edĠbyĠ platform .ĊĊInĠ actual Ġsitu ation,Ġ weĠhaveĠ sharedĠ browserĠ process.Ġ B PĠ isĠ neededĠtoĠ be\n",
      "\t Unique Tokens GPT2: {'ĠSM', 'Ġuse', 'Ġby', \"'s\", 'Ġplatform', 'Ġthen', 'My', '.', 'Ġneeded', 'Ġone', 'Ġeasy', 'Ġactual', 'Ġnot', 'Ġsituation', 'ĠRP', 'Ġthe', 'Ġper', 'Ġissued', 'Ġfiles', 'Hello', 'Ġenforced', 'In', 'Ġto', 'Ġfixed', 'Ġcode', 'Ġlabel', 'Ġbrowser', 'Ġbe', 'Ġlimit', 'ACK', 'SON', 'ĊĊ', 'Ġusing', 'Ġmodels', 'ile', 'Ġfact', 'Ġprocess', 'Ġanything', 'Ġit', 'Ġare', 'Ġsingle', ')', 'do', 'Ġ(', 'Ġmaintain', 'I', 'Ġapplication', 'Ġwhich', 'Ġapp', 'Ġadditional', 'Ġin', 'Ġparallel', 'Ġand', 'Ġsecurity', 'Ġframework', 'Ġjust', 'Therefore', 'Ġhad', 'Ġcomment', 'ges', 'Ġof', 'Ġfile', 'which', 'so', 'Ġitself', 'Ġtwo', '>', 'Ġneed', 'Ġremoving', 'Ġthose', 'Ġhave', 'Ġwe', 'ĠBP', 'Ġenforce', 'Ġshared', 'v', 'Ġhandle', 'Ġis', 'Ġthat', 'aware', 'Ġthink', 'Ġaimed', 'PER', ',', 'Ġwas', 'Regards', 'Ġrequest'}\n",
      "\t Unique Tokens no: {'.ĊĊRegards,Ċ', 'parallel', 'hadĠ', 'PĠ', 'commentĠ', \"it'sĠ\", 'tĠofĠ', '.ĊĊInĠ', 'removingĠ', 'wasĠ', 'actual', 'neededĠtoĠ', 'ON', 'just', 'of', 'needĠtoĠ', 'process.Ġ', 'ation,Ġ', 'request', 'byĠ', 'sĠinĠ', 'MyĠ', 'ĠthenĠ', 'processĠ', 'sharedĠ', 'fileĠ', 'B', 'platform', 'fac', 'ĠweĠhaveĠ', 'ĠthoseĠt', 'labelĠ', 'edĠtoĠtheĠ', 'Hello,ĊĊ', 'wo', 'addition', 'enforc', 'edĠ', 'There', 'perĠ', 'ĠthoseĠtwoĠ', 'soĠthatĠ', 'beĊ', '.ĊĊIĠ', 'issu', 'itself', 'esĠtoĠ', 'foreĠ', '(whichĠ', 'maintain', 'fix', '.ĊĊ', 'securityĠ', 'isĠ', 'en', 'usingĠ', '.Ċ', 'Ġsitu', 'ĠthinkĠ', '>ĊĊ', 'vi', 'odeĠ', 'ACKĠ', 'edĠbyĠ', 'whichĠ', ')Ġ', 'awareĠofĠ', 'application', 'BP', 'easyĠtoĠ', 'aim', 'anythingĠ', 'weĠhaveĠ', 'P', 'edĠandĠ', 'singleĠ', 'forceĠ', 'alĠc', 'RP', 'lim', 'appĠ', 'browserĠ', 'justĠoneĠ', 'ERS', \"it'sĠnotĠ\", 'leg', 'SM', 'framework', 'doĠnotĠ', 'model', 'handleĠ', 'itĠtoĠ', 'useĠ', 'filesĠareĠ'}\n",
      "==Different Author==\n",
      "Text 1: burden it was.\n",
      " Yes, there had indeed been an \"incident,\" but nothing to do with gas leaks or an Airbnb, although <PERSON> still didn't know what the actual nature of it was. <PERSON> had finally resurfaced on the phone just after <PERSON> had picked the girls up at the airport. He hadn't sounded li\n",
      "\t Tokenized GPT2:bur den Ġit Ġwas . Ċ ĠYes , Ġthere Ġhad Ġindeed Ġbeen Ġan Ġ\" inc ident ,\" Ġbut Ġnothing Ġto Ġdo Ġwith Ġgas Ġleaks Ġor Ġan ĠAir bn b , Ġalthough Ġ< PER SON > Ġstill Ġdidn 't Ġknow Ġwhat Ġthe Ġactual Ġnature Ġof Ġit Ġwas . Ġ< PER SON > Ġhad Ġfinally Ġresur faced Ġon Ġthe Ġphone Ġjust Ġafter Ġ< PER SON > Ġhad Ġpicked Ġthe Ġgirls Ġup Ġat Ġthe Ġairport . ĠHe Ġhadn 't Ġsounded Ġli\n",
      "\t Tokenized no:burdenĠ itĠwas .ĊĠ Yes ,ĠthereĠ hadĠ indeedĠ beenĠ anĠ\" incident ,\"Ġ butĠ nothingĠtoĠdoĠwithĠ gasĠ leak sĠorĠ anĠ Air b nb ,ĠalthoughĠ < P ERS ON >Ġ stillĠ didn'tĠ know ĠwhatĠtheĠ actualĠ natureĠofĠ itĠwas .Ġ < P ERS ON >Ġ hadĠ finallyĠ resur fac edĠonĠtheĠ phoneĠ justĠ afterĠ < P ERS ON >Ġ hadĠ pick edĠtheĠ girl sĠupĠ atĠtheĠ airport .ĠHeĠ hadn'tĠ soundedĠ li\n",
      "\t Unique Tokens GPT2: {'Ġuntil', 'sing', '.', 'Ġage', 'Ġeight', 'Ġlooks', 'Ġattention', 'Ġch', 'Ġgreeted', 'ly', 'Ġshe', 'Ġhome', 'Ġthan', 'Ġgreat', 'Ġbehind', '>,', 'list', 'den', 'Ġmedi', 'Ġnoise', 'Ġright', 'au', 'Ġdeath', 'Ġeach', 'Ġrear', 'Ġpicture', 'Ġrev', 'It', 'Ġresur', 'Ġsight', 'Ġblind', 'Ġkeep', 'Ġstir', 'Ġdate', 'going', 'Ġfront', 'Ġcase', 'Ġheels', 'Ġplum', 'Ġsummer', 'Ġapparently', 'Ġusual', 'Ġdriving', 'Ġdo', '?', 'ĠThey', 'Ġtone', 'Ġr', 'Ġhave', 'season', 'Ġeven', 'Ġbag', 'Ġjealous', \"'t\", 'Ġfeeling', 'Ġar', 'Ġmust', 'Ġused', 'Ġbland', 'ĠWas', 'Ġthink', 'Ġdarling', 'ble', 'Ġsmile', 'chal', 'Ġstill', 'Not', 'Ġpanic', 'Ġsomeone', 'Ġend', 'Ġphone', \"'s\", 'Ġgar', 'Ġairport', 'ening', 'Ġwhat', 'Ġhead', 'Ġet', 'Ġal', 'Ġknown', 'ĠThere', 'ies', 'Ġtaller', 'ĠHis', 'Ġbe', 'ches', 'Ġair', 'Ġshow', 'Ġup', 'Ġhands', 'Ġsaw', 'faced', 'Ġafter', 'Ġsick', 'Ġnever', 'Ġsignal', 'ĠCh', 'ĠOr', 'Ġstretched', 'Ġan', 'Ġasked', 'Ġsomething', 'Ġswiftly', 'Why', 'Just', 'Ġunwilling', 'ĠYes', 'Ġjust', 'ĠShould', 'ĠChrist', 'which', 'Ġwings', 'Ġcar', 'Ġsaying', 'Ġ<', 'Ġpassion', 'Well', 'Ġtouching', ',\"', 'se', \"'ve\", 'ĠI', 'Ġtime', 'Ġkisses', 'Ġboth', 'Ġpassed', 'Ġmirror', 'Ġcall', 'Ġfelt', 'Ġbring', 'Ġfake', 'Can', 'Ġgrin', 'PER', 'Ġwas', 'Ġtheir', '.)', 'Ġmy', 'anel', 'Ġabout', 'Ġmagic', 'Ġby', 'Ġhand', 'Ġnon', 'Nothing', 'Ġas', 'Ġwhen', 'Ġhe', 'Nice', 'Ġeasy', 'Ġsake', 'Ġnature', 'Ġlifetime', 'Ġanyway', 'Ġnot', 'Ġspot', 'ĠIt', 'Ċ', 'ĠPalace', 'Ġwhatever', 'Ġshoulder', 'Ġfinally', 'Ġdone', 'Ġgot', 'Ġpr', 'Ġyou', 'Ġto', 'Ġfollowed', 'ocr', 'Ġmight', 'Everything', 'arily', 'A', 'Ġdown', 'SON', 'Ġasleep', 'Ġladies', 'Ġpicked', 'Ġphones', 'Ġdark', 'Ġbecause', 'Ġautomatically', 'Ġthey', 'Ġlost', 'Ġconcert', 'Ġit', 'Ġsmiling', 'Ġdepths', \">'\", 'Ġat', 'usted', 'Ġjealousy', 'Ġkind', 'Ġhadn', 'Ġbeen', 'Ġin', 'Ġstage', 'Ġnothing', 'Ġand', 'ĠAnyone', 'ant', 'Ġhad', 'Ġdidn', 'Ġgoing', 'Ġof', 'Ġrange', 'Ġback', 'What', 'Ġpump', 'Ġdaughters', 'Ġwhere', 'Ġall', 'bn', 'Ġa', 'Ġincident', 'Ġother', 'ĠThe', \"'m\", 'ĠFrom', 'Ġsounded', 'Ġnew', 'Ġsuspicious', 'Ġcaught', 'Ġself', 'ĠSilver', 'Ġownership', 'Ġindeed', 'ĠCong', 'Ġlike', 'Ġsuspected', 'Ġturn', 'Ġon', 'ĠBir', 'Ġwith', 'Ġturning', 'Ġgas', 'ĠNot', 'Ġless', 'Ġsilence', 'Ġago', 'D', 'Ġone', 'Ġactual', 'Ġ_', 'Ġthe', 'ĠHe', 'Ġfor', 'Ġif', 'bur', 'Ġthought', 'ĠAnother', 'Hello', 'bed', 'ĠPolish', 'Ġlooking', 'Ġholiday', 'Ġfallen', 'Ġhis', \"'re\", 'Ġglanced', 'Ġmoment', 'Ġout', 'Ġmore', 'Ġhimself', 'Ġdad', 'Ġpaying', 'Ġclose', 'oming', 'Ġwhite', 'sided', 'Ġsee', 'Ġlo', 'Ġher', 'ĠMr', 'Ġ(', 'Ġhere', 'Ġknow', 'Ġleaks', 'Ġbut', 'rical', 'Get', 'Ġfull', 'Ġknew', 'Ġloss', 'Ġrather', 'Ġsmaller', 'Ġwere', 'Ġor', 'ĠAir', 'ident', 'Ġworry', 'Ġcould', 'inc', 'ffe', 'Ġworried', 'Ġtrying', 'Ġgirls', 'Ġlovely', 'Ġover', 'Ġthat', 'Ġdriveway', 'Ġme', \"'d\", 'uring', 'Ġyour', 'Ġdragged', 'ting', 'Ġfurther', ',', 'Ġthese', 'Ġalthough'}\n",
      "\t Unique Tokens no: {'sâĢĶ', 'finallyĠ', 'looksĠ', 'sĠofĠ', 'ĊĠ\"', 'full', 'Ġtime,Ġ', 'ror', 'ratherĠthanĠ', 'greatĠ', 'ickĠ', 'Bir', '.Ġ', 'aĠh', 'onĠtheĠ', 'here.', \"Can'tĠ\", 'pas', 'inglyĠ', 'Ġm', 'ing_', 'caseĠtheĠ', 'ag', 'heĠwasĠ', 'verĠ', 'couldĠhaveĠ', 'allĠ', 'sid', 'onĠs', 'one', 'ight', 'eĠturn', 'heĠ', 'death', 'AĠ', 'heĠwas', 'ĠtoĠtheirĠ', 'fac', 'edĠthereĠ', 'airport', 'andĠ', 'oc', 'edĠbyĠtheĠ', 'whenĠheĠ', '-r', 'eĠtoĠ', 'Pal', 'is', 'because,Ġ', 't', 'rear', 'Ġcha', 'backĠofĠtheĠ', 'sightĠ', 'eway', 'noise', 'ifĠ', 'usualĠ', 'lyĠ', 'goingĠ', 'listen', \"he'dĠ\", 'gar', 'He', 'youĠtoĠ', 'AnyoneĠ', 'ShouldĠ', '.Ġ\"', 'anyway', 'fak', '.ĠMr', ',ĠthereĠ', 'usĠ', 'sheĠ', 'arilyĠ', 'ownershipĠ', 'ĠwasĠ', 'leak', 'Sil', 'untilĠsheĠ', 'indeedĠ', 'yourĠ', 'worryĠ', '.ĠThereĠwasĠ', 'carĠandĠ', 'concert', 'NotĠ', 'saw', 'phone', 'saying', 'attention', 'magic', 'fallenĠ', 'butĠ', 's.ĠHeĠ', 'es.Ċ', 'ĠtoĠtheĠ', 'downĠ', 'doneĠ', 'esĠ', 'goingĠtoĠ', 'rust', 'sĠwereĠ', 'wher', 'withĠtheseĠ', '?Ġ', 'ofĠ', 'medi', 'over', 'um', 'asĠtheyĠ', 'panic', 'evenĠ', 'apparent', 'ingĠ', 'passion', 'hadĠ', 'know', 'erĠofĠtheĠ', '.)Ġ', 'edĠtheĠ', 'jeal', 'grinĠ', 'alĠ', 'singĠ', 'wasĠ', 'thought', 'said.Ġ\"', 'Pol', 'herĠheadĠ', 'burdenĠ', ',Ġ', 'ĠwithĠ', 'AnotherĠ', 'should', 'outĠofĠ', 'dar', 'byĠ', 'himself', 'ed,', 'untilĠtheyĠ', '.ĠItĠ', 'smallerĠ', 'ingĠthemĠ', 'Dun', 'br', 'EverythingĠ', 'pumpĠ', 'endĠupĠ', '.ĠOrĠ', 'smileĠ', 'known', 'moreĠofĠaĠ', 'actualĠ', 'sĠorĠ', 'edĠ', 'eight', '>Ġ', 'dat', 'hisĠt', 'bleĠ', 'lessĠofĠ', 'oneĠ', 'me', 'ling', 's.Ġ', 'easy', 'asleep', 'hadĠbeenĠ', 'ing', 'been', 'ch', 'wereĠtheĠ', 'JustĠ', 'etimeĠ', 'Noth', 'ago', 'not', 'closeĠ', 'anĠ', 'ol', \"didn'tĠ\", 'sedĠtheĠ', 'summer', 'feltĠ', 'aĠsign', 'ousĠ', '.ĊĠ', 'will', 'sĠinĠtheĠ', 'home.Ġ', 'Hello,Ġ', 'e,Ġ', '.ĠTheĠ', 'could', 'inĠtoĠtheĠ', \"I'veĠgotĠ\", 'depth', 'somethingĠ', 'soundedĠ', 'age,Ġ', \"eĠthey'reĠ\", 'hisĠ', 'usedĠtoĠ', 'aĠsmileĠ', 'how', '.ĠHeĠhadĠ', ',\"Ġ', 'Ġthat,Ġ', 'spott', 'ĠandĠheĠ', 'non', 'saidĠtoĠ', \"'sĠ\", 'whateverĠ', 'lossĠofĠ', 'WasĠ', 'becauseĠ', 'myĠ', 'handsĠ', 'aĠ', 'ly,Ġ', 'ri', 'nb', 'pr', 'rangeĠofĠ', 'eĠtheĠ', 'al', 'you,Ġ', 'Yes', 'notĠtheĠ', 'phoneĠ', 'mustĠ', 'natureĠofĠ', 'daughter', 'suspect', 'lovelyĠ', ',ĠasĠ', 'ĊĠ', \"It'sĠ\", 'girl', \"hadn'tĠ\", 'whichĠwasĠ', 'incident', 'ĠtoĠ', 'eachĠotherĠ', 'deathĠ', '.Ġ(', 'behindĠ', 'iou', 'elĠ', 'heel', 'rev', 'an', 'edĠonĠtheĠ', 'Ch', 'w', 'sĠupĠ', 'wer', 'feelingĠofĠ', 'dragg', 'forĠ', '.\"ĠHeĠ', 'girlsĠ', 'caughtĠ', 'ous', \"I'm\", 'etĠal', 'NiceĠ', 'seeĠtheĠ', 'neverĠ', 'irĠ', 'keepĠ', 'ings,Ġ', 'ong', 'atĠtheĠ', 'wasĠtheĠ', 'someoneĠ', 'itĠwas', '.ĠTheyĠ', 'urnĠ', 'bothĠ', 'ingĠthem', 'ouch', 'antĠ', 'uf', 'moment', 'kind', 'newĠ', 'WhatĠ', 'ERS', 'st', 'ĠthemĠ', 'fe', 'afterĠ', 'withĠ', ',\"ĠsheĠ', 'ed.Ġ', \"'\", 'whiteĠ', 'furtherĠ', 'aĠf', 'aĠt', ',ĠtheĠ', 'incidentĠ', 'Christ', 'seas', 'inĠtheĠ', ',\"ĠtheĠ', ',ĠalthoughĠ', 'self', 'pick', 'itĠ', 'allerĠ', 'ĠthatĠ', 'ishĠ', 'inĠfrontĠofĠtheĠ', '.ĠItĠwasĠ', '?\"Ġ', 'silenceĠ', 'WhyĠ', 'haveĠbeenĠ', 'phones', '.ĠC', 'inĠ', 'rightĠ', 'ON', 'about', 'id', 'stillĠ', 'glancedĠ', 'GetĠ', 'ĠsaidĠ', 'you', 'callĠ', 'picture', 'at', 'aĠmom', '.ĠHeĠ', 'anĠ\"', 'ac', 'said', 'lost', 'andĠtheĠ', 'followedĠ', 'kissesĠ', '>,Ġ', 'here,Ġ', 'justĠ', 'bl', 'ĠwhatĠtheĠ', 'gasĠ', 'yĠofĠ', 'sayingĠ', 'ent,Ġ', 'ask', 'ir', 's,Ġ', 'pl', 'seĠtoĠ', 'ingĠforĠ', 'hand', 'driv', 'Air', 'automaticallyĠ', 'bedĠ', 'Ġwor', 'icalĠ', 'eĠtoĠbeĠ', 'ĠtryingĠtoĠ', 'ĠthinkĠ', 'iesĠthatĠ', 'aĠs', \"'dĠ\", 'stretch', 'ur', 'and', 'likeĠ', 'Well,Ġ', 'lif', '.ĠHisĠ', 'lookingĠ', 'iesĠ', 'loom', 'dad', 'ake', 'TheĠ', 'ar', 'P', 'e.Ġ', 'resur', 'edĠhisĠ', 'airĠ', 'edĠitĠ', 'dark', 'knew', 'atĠ', 'ingĠherĠ', 'beenĠ', 'ianĠ', 'lad', 'suspic', 'nothingĠtoĠdoĠwithĠ', 'ay', 'payingĠ', 'greet', 'sĠtoĠtheĠ', 'althoughĠ', 'FromĠ', 'ĠtheirĠ', 'swift', 'blind', 'smilingĠ'}\n",
      "Text 2: @White Buffalo ~ As a landscaper for twenty years I've watched both the flora and fauna on the move up here in the northern Adirondacks. Intrusions of so called invasive species has been doubling every few years. My opinion on this is that it's somewhat like the finger in the leaky dam, trying to ho\n",
      "\t Tokenized GPT2:@ White ĠBuffalo Ġ~ ĠAs Ġa Ġlandsc aper Ġfor Ġtwenty Ġyears ĠI 've Ġwatched Ġboth Ġthe Ġfl ora Ġand Ġfa una Ġon Ġthe Ġmove Ġup Ġhere Ġin Ġthe Ġnorthern ĠAd ir ond acks . ĠInt rus ions Ġof Ġso Ġcalled Ġinvasive Ġspecies Ġhas Ġbeen Ġdoub ling Ġevery Ġfew Ġyears . ĠMy Ġopinion Ġon Ġthis Ġis Ġthat Ġit 's Ġsomewhat Ġlike Ġthe Ġfinger Ġin Ġthe Ġle aky Ġdam , Ġtrying Ġto Ġho\n",
      "\t Tokenized no:@ WhiteĠ Buff al oĠ ~Ġ AsĠ aĠl and scap erĠ for ĠtwentyĠ yearsĠ I'veĠ watchedĠ bothĠtheĠ fl or aĠandĠ fa unaĠ onĠtheĠ moveĠ upĠ hereĠinĠtheĠ northernĠ Ad ir ond ack s.ĠIn tr usion sĠofĠ soĠ calledĠ invas iveĠ speciesĠ hasĠbeenĠ dou blingĠ everyĠ fewĠ years.Ġ MyĠ opinionĠ onĠthisĠ isĠthatĠ it'sĠ somewhatĠ likeĠtheĠ fing erĠinĠtheĠ leak yĠ dam ,ĠtryingĠtoĠ ho\n",
      "\t Unique Tokens GPT2: {'Ġfew', 'aper', 'Ġsomething', 'ar', 'Ġa', 'Ġto', 'Ġhere', 'ions', 'Ġspecies', \"'ve\", 'ĠThe', 'Ġwatched', 'Ġlandsc', \"'s\", 'ĠI', 'Ġthis', 'Ġless', 'Ġfinger', '.', 'Ġinvasive', 'acks', 'Ġtrying', 'Ġnorthern', 'Ġinevitable', 'Ġfl', 'Ġboth', 'Ġbeen', 'Ġin', 'Ġis', 'Ġopinion', 'Ġand', 'Ġoccasional', 'Ġyears', 'Ġthat', 'Ġtwenty', 'Ġbecoming', 'Ġup', 'Ġthe', 'ĠAs', 'Ġsomewhat', 'Ġof', 'Ġfor', 'ĠBuffalo', 'una', 'ĠAd', 'Ġcalled', 'ĠMy', 'Ġback', 'ĠC', 'Ġso', 'Ġmove', 'ĠInt', 'Ġevery', 'Ġle', 'Ġhold', 'Ġfa', 'Ġ~', 'ĠWolf', 'Ġlike', 'Ġor', 'White', 'Ġhas', ',', 'ora', 'Ġit', 'Ġdam', 'Ġare', 'ling', 'Ġdoub', 'aky', 'Ġon', 'rus'}\n",
      "\t Unique Tokens no: {'soĠ', 'MyĠ', 's.ĠIn', 'sĠofĠ', 'likeĠtheĠ', 'occasion', 'ableĠ', 'C', \"it'sĠ\", 'orĠ', 'everyĠ', 'somethingĠ', '~Ġ', \"I'veĠ\", 'dou', 'usion', 'speciesĠ', 'scap', 'calledĠ', 'upĠ', 'aĠl', 'tr', 'bothĠtheĠ', 'years.Ġ', 'arĠ', 'W', 'onĠtheĠ', 'yearsĠ', 'ĠtwentyĠ', 'opinionĠ', 'al', 'somewhatĠ', 'isĠthatĠ', 'erĠ', 'lessĠ', 'or', 'AsĠ', 'aĠandĠ', 'back', 'invas', 'hasĠbeenĠ', 'olfĠ', 'al.', 'fa', 'inevit', 'onĠthisĠ', 'erĠinĠtheĠ', 'watchedĠ', 'and', 'fing', 'WhiteĠ', 'iveĠ', 'becomingĠ', 'areĠ', 'for', 'leak', 'ack', ',ĠtryingĠtoĠ', 'yĠ', 'blingĠ', 'moveĠ', 'occasionalĠ', 'fewĠ', 'fl', 'Ad', 'hereĠinĠtheĠ', '.ĠTheĠ', 'unaĠ', 'Buff', 'oĠ', 'northernĠ', 'dam', 'holdĠ'}\n",
      "==Different Author==\n",
      "Text 1: railway bridges close to the river. Around these bridges were some desolate areas that had once contained small industrial units, now mostly empty. It was an ideal place for an attack. The hunters each had a gun concealed in a shoulder holster, loaded with silver bullets. Though the Guild preferred \n",
      "\t Tokenized GPT2:ra il way Ġbridges Ġclose Ġto Ġthe Ġriver . ĠAround Ġthese Ġbridges Ġwere Ġsome Ġdes olate Ġareas Ġthat Ġhad Ġonce Ġcontained Ġsmall Ġindustrial Ġunits , Ġnow Ġmostly Ġempty . ĠIt Ġwas Ġan Ġideal Ġplace Ġfor Ġan Ġattack . ĠThe Ġhunters Ġeach Ġhad Ġa Ġgun Ġconcealed Ġin Ġa Ġshoulder Ġhol ster , Ġloaded Ġwith Ġsilver Ġbullets . ĠThough Ġthe ĠG uild Ġpreferred Ġ\n",
      "\t Tokenized no:rail wayĠ bridg esĠ closeĠtoĠtheĠ river .ĠA round ĠtheseĠ bridg esĠwereĠ someĠ des ol ateĠ areas ĠthatĠhadĠ onceĠ containedĠ smallĠ industrialĠ unit s,Ġ now Ġm ostlyĠ empty .ĠItĠwasĠ anĠ idealĠ placeĠ forĠanĠ attack .ĠTheĠ hun tersĠ eachĠ hadĠaĠ gun Ġcon ce al edĠinĠaĠ shoulderĠ hol st er,Ġ loadedĠ withĠs il verĠ bullet s.ĠTh ough ĠtheĠ Gu ildĠ preferredĠ\n",
      "\t Unique Tokens GPT2: {'Ġmercy', 'Ġtoo', 'Ġmen', 'Ġkeen', 'Ġwith', 'Ġlanded', 'Ġby', 'Ġhand', 'Ġanyone', 'Ġagain', 'Ġscent', 'Ġwhile', 'Ġthen', 'Ġas', 'Ġseconds', '.', 'Ġhearing', 'Ġoff', 'Ġbefore', 'ĠAround', 'Ġstudied', 'Ġhe', 'Ġwall', 'Ġviolently', 'Ġremained', 'Ġfled', 'Ġone', 'Ġhuman', 'Ġfrom', 'Ġmouth', 'Ġbroke', 'Ġsenses', 'Ġescape', 'Ġdecided', 'ĠThis', 'Ġnot', 'olves', 'Ġg', 'Ġattacking', 'Ġkicked', 'Ġshe', 'Ġthe', 'Ġdread', 'Ġfood', 'ĠHe', 'Ġhead', 'ed', 'Ġthird', 'Ġconcealed', 'Ġalmost', 'Ġman', 'ĠIt', 'Ġsecond', 'Ġslipped', 'ĠThough', 'Ġdes', 'Ċ', 'Ġborn', 'ĠWhen', 'ful', 'Ġsmashed', 'ĠNo', 'Ġjack', 'Ġsprint', 'Ġshoulder', 'Ġareas', 'Ġbackwards', 'Ġhurt', 'Ġside', 'Ġpossibility', 'Ġprobably', 'Ġbridge', 'Ġnormally', 'Ġhis', 'ĠG', 'Ġkill', 'Ġgot', 'ung', 'Ġto', 'Ġquick', 'Ġground', 'ising', 'Ġgun', 'ĠBefore', 'Ġonce', 'Ġ\"', 'Ġhim', 'Ġgirl', 'Ġmight', 'Ġheel', 'Ġforwards', 'Ġbridges', 'Ġspeed', 'Ġwerewolf', 'Ġhunter', 'Ġinclined', 'Ġstrong', 'Ġfeet', 'Ġbe', 'Ġright', 'Ġorder', 'Ġpreferred', 'Ġwanting', 'Ġmaybe', 'Ġquickly', 'Ġtill', 'Ġeach', 'Ġno', 'Ġdown', 'alle', 'Ġthere', 'Ġdaughter', 'Ġshot', 'ang', 'Ġface', 'SON', 'Ġsome', '>.', 'Ġsaw', 'Ġhated', 'ated', 'Ġfire', 'Ġneck', 'ĠBlood', 'Ġafter', 'Ġonly', 'Ġbeneath', 'Ġwithout', 'Ġpull', 'Ġcracked', 'Ġsc', 'Ġyoung', 'Ġdid', 'Ġkick', 'Ġvanished', 'Ġgasp', 'Ġclose', 'Ġ-', 'Ġthey', 'Ġan', 'Ġmember', 'Ġtook', 'ĠShe', 'Ġrun', 'Ġit', 'Ġhard', 'ĠSat', 'Ġhol', 'ling', 'Ġindustrial', 'Look', 'Ġher', 'Ġamount', 'ĠOne', 'ĠBesides', \">'\", 'Ġwh', 'Ġmostly', 'Ġat', 'w', 'Ġdying', 'Ġsqu', 'Ġchest', 'Ġhunters', 'Ġhun', 'ied', 'Ġspr', 'Ġleft', 'Ġideal', 'Ġclosed', 'Ġmistake', 'Ġbroken', 'Ġpursu', 'Ġunits', 'Ġwhich', 'Ġkind', 'Ġempty', 'Ġbut', 'Ġsilver', 'Ġpolice', 'Ġbodies', 'Ġin', 'Ġfull', 'Ġfell', 'ram', 'Ġdead', 'Ġand', 'Ġsp', 'Ġshape', 'Ġsens', 'Ġsystem', 'Ġhad', 'Ġimmediately', 'ĠAt', 'Ġgoing', 'Ġfor', 'Ġtaking', 'Ġrunning', 'Ġleader', 'uild', 'Ġof', 'Ġplace', 'Ġcontained', 'Ġfamily', 'Ġbloody', 'Ġguns', 'Ġhalt', 'Ġwere', 'Ġabove', 'Ġneg', 'Ġfight', 'Ġruling', 'Ġor', 'isf', 'Ġwhether', 'Ġbullets', 'ts', 'Ġdisappeared', 'Ġst', 'Ġclear', 'ra', 'aining', 'Ġwhere', 'Ġcomplain', 'Ġtwo', 'Ġinto', 'ĠThey', 'sters', 'Ġall', 'Ġloaded', 'Ġcorner', 'Ġa', 'Ġattack', 'ted', 'Ġmaking', 'ĠThe', 'Ġother', 'ets', 'ĠHer', 'Ġnight', 'Ġcould', 'Ġthree', 'Ġany', 'Ġcoming', 'Ġready', 'Ġshadows', 'ĠReal', 'Ġnow', 'Ġkilled', 'Ġinside', 'Ġsl', 'Ġ', 'Ġtried', 's', 'Ġcl', 'Ġvom', 'iting', 'Ġribs', 'Ġfirst', 'Ġlet', 'Ġfist', 'Ġbegan', 'amped', 'Ġover', 'Ġonto', 'way', 'Ġrecover', 'Ġgave', 'Ġthat', 'Ġstr', 'Ġfelt', 'Ġtrace', 'Ġpulled', 'Ġbring', 'Ġstepped', 'Ġround', 'Ġarch', 'olate', 'Ġlast', 'Ġpreventing', 'Ġscaled', 'Ġfar', 'ble', 'Ġforce', 'Ġso', \"'d\", 'Ġwent', 'arely', 'Ġwould', 'ster', 'Ġanxiety', 'ot', 'Ġweakness', 'PER', 'Ġriver', 'Ġliked', 'Ġthese', 'umped', 'Ġunconscious', 'Ġflight', 'Ġimportant', 'Ġon', 'Ġtr', 'Ġsmall'}\n",
      "\t Unique Tokens no: {'soĠ', 'empty', 'quickĠ', 'bringĠtheĠ', 'letĠ', 'rail', 'edĠfromĠtheĠ', 'ledĠ', 'gun', '.Ġ', 'aĠh', 'e.ĠThisĠ', 'onĠtheĠ', 'consc', 'Ġm', 'ĠthatĠtheyĠ', 's.ĠSheĠ', ',ĠhisĠ', 'verĠ', 'began', 'edĠtoĠ', 'memberĠ', '\"', 'Ġcon', 'onceĠ', 'scal', 'placeĠ', 'importantĠtoĠ', 'wantingĠtoĠ', 'wentĠtoĠ', 'cy', 'heĠwas', 'letĠhimĠ', 'cornerĠ', 'ingĠtheir', 'LookĠ', 'andĠheĠ', 'bornĠ', 'edĠtoĠtheĠ', 'lik', 'andĠ', 'es.Ġ', 'killĠ', 'immedi', 'slipp', 'in,Ġ', 'atis', 'familyĠ', 'fellĠ', 'flightĠ', 'containedĠ', 'ing.Ġ', 'eĠtoĠ', 'makingĠ', 'ĠtriedĠtoĠ', 'Ġshould', 'whetherĠ', 'elyĠtoĠ', 'hunter', 'unit', 'fromĠherĠ', \"he'dĠ\", 'weak', 'Ġs', 'thirdĠ', 'hol', 'ĠwithĠtheĠ', 'bridgeĠ', 'ĠtooĠ', 'prevent', 'sheĠ', 'vom', 'onĠt', 'ĠwasĠ', 'ĠthatĠhadĠ', 'squ', 'brok', 'gasp', 'wh', 'shot', 'saw', 'vanish', 'olf', 'istĠ', 'withĠs', 'butĠ', 'ĠtoĠtheĠ', 'anxietyĠ', 'downĠ', 'ĠĠ', 'polic', 'esĠ', 'goingĠtoĠ', 'readyĠ', 'shoulderĠ', 'olfĠ', 'probablyĠ', 'sĠwereĠ', 'otherĠtwoĠ', 'againĠ', 'alt', 'des', 'withĠaĠ', 'ofĠ', 'asĠtheyĠ', 'wayĠ', 'er,Ġ', 'ĠtookĠ', 's.ĠTheĠ', 'smash', 'abov', 'hadĠ', 'noĠoneĠ', 'wallĠ', 'strongĠ', 'athĠtheĠ', 'rulingĠ', 'stamp', 'edĠtheĠ', ',ĠandĠ', 'firstĠ', 'cl', 'wasĠ', 'whereĠ', 'lastĠnight', 'BloodĠ', 'sense', 'steppedĠ', ',Ġ', 'isingĠ', 'ough', 'fromĠtheĠ', 'chestĠ', 'attack', 'ersĠ', 'humanĠ', 'herĠ', 'ed,Ġ', 'byĠ', 'now', 'other', '.ĠItĠ', '<', 'recover', 'idealĠ', 'leaderĠ', 'erĠ', 'ontoĠhisĠ', 'ensesĠ', 'esid', 'jacket', 'edĠ', '>Ġ', 'ostlyĠ', 'bullet', 'oneĠ', 'threeĠ', 's.Ġ', 'sĠthenĠ', 'terĠ', 'inclin', 'bodi', 'amountĠofĠ', 'pull', 'anĠ', 'ol', 'pulledĠ', 'let', 'lastĠ', 'sedĠtheĠ', 'handĠ', 'leader', ',ĠoneĠ', 'hun', 'wouldĠ', 'complain', 'hisĠheadĠ', '.ĠBeforeĠ', 'sen', '.ĊĠ', 'esĠwereĠ', 'orĠnot', 'almostĠ', 'es,', 'her', 'sp', 'e,Ġ', ',ĠtheĠonlyĠ', 'closeĠtoĠtheĠ', 'neg', 'second', '.ĠTheĠ', 'mer', '.ĠWhenĠ', 'ingĠandĠ', 'edĠherĠ', 'feet', 'systemĠandĠ', 'hear', 'hisĠ', 'forceĠ', 'asĠaĠ', 'hard', 'edĠhimĠ', 'fight', \"'sĠ\", 'whil', 'killed', '-Ġ', 'augh', 'pursu', 'fromĠs', 'gotĠ', 'industrialĠ', 'ĠwithoutĠ', 's.ĠS', 'ump', 'aĠ', 'menĠ', 'fireĠ', 'felt', 'ri', 'gaveĠ', 'allĠofĠ', 'faceĠandĠ', 'ĠtheseĠ', 'overĠ', 'viol', 'ateĠ', 'pr', 'eĠtheĠ', 'mouth', '.ĠAtĠtheĠ', 'possibility', 'bridg', 'al', 'lingĠ', 'eĠtheyĠwereĠ', 'intoĠtheĠ', 'f', 'beneathĠ', 'ingĠaĠ', 'girl', 'aĠd', 'ofĠtheĠ', 'maybeĠ', 'B', 'sheĠwasĠ', 'SheĠ', 'g', 'oneĠthatĠ', 'asĠ', 'ĠthreeĠ', 'ew', 'ness.Ġ', 'redĠtoĠ', 'iou', 'elĠ', 'feetĠ', 'ke', 'wer', 'foodĠ', 'down', 'forĠ', 'smallĠ', 'angĠ', 'normallyĠ', 'beforeĠ', 'Ġtak', 'sl', 'areas', 'mistak', 'ĠwasĠnotĠ', 'erĠandĠ', 'overĠtoĠtheĠ', 'olv', 'farĠtooĠ', '.ĠHerĠ', 'speedĠ', 'man', 'runĠ', 'ently', 'fromĠ', 'crack', '.ĠA', 's,', 'en,Ġ', 'un', 'scram', 'ce', 'kill', 'loadedĠ', '\"Ġ', 'land', 'strain', 'brokeĠ', 'dread', 'ingĠorĠ', 'enĠ', 'itingĠ', 'quicklyĠ', 'sĠofĠtheĠ', 'fi', 'ERS', 'st', 'kindĠ', 'afterĠ', 'illĠtheĠ', 'hunt', 'couldĠ', 'hap', 'Gu', 'NoĠ', 'bloodyĠ', 'inĠtheĠ', 'ĠthatĠ', 'remainedĠ', 'sprint', '.ĠItĠwasĠ', 'fromĠhisĠ', 'eĠtheirĠ', 'bene', 'her,Ġ', 'bsĠ', 'inĠ', 's.Ċ', 'rightĠ', 'escap', 'ON', 'hadĠaĠ', 'atedĠ', 'hurt', 'round', 'arelyĠ', 'rott', 'ĠtheĠ', 'her.ĠSheĠ', 'forĠtheĠ', ',ĠthenĠ', 'decidedĠtoĠ', 'ground', 'e.ĠTheyĠ', 'OneĠ', 'second,Ġ', 'comingĠtoĠ', ',ĠandĠnotĠ', 'prefer', 'at', 'forĠanĠ', 'anyoneĠ', 'faceĠ', 'Ġt', '.ĠHeĠ', 'ĠtoĠthem', 'shadow', 'andĠtheĠ', 'edĠinĠaĠ', 'orderĠ', 'backward', 'bl', 'd,Ġ', 'he', 'ĠthereĠwasĠnoĠ', 'archĠ', 'youngĠ', 's,Ġ', 'didĠ', 'ildĠ', 'onĠ', 'neckĠ', '.ĠWhenĠtheĠ', 'sĠfrom', 'forĠthem', '.ĠSheĠ', 'tersĠ', 'sideĠ', 'kick', 'runningĠ', 'ĠtheirĠs', 'lead', 'ĠtraceĠ', '.ĠS', 'river', 'ItĠwasĠ', 'mightĠbeĠ', 'whichĠ', 'any', 'off', 'closedĠ', 'leftĠ', 'wentĠ', 'dy', 'P', 'someĠ', 'e.Ġ', 'himĠ', 'studi', 'ĠthatĠhisĠ', 'Real', 'edĠitĠ', 'ingĠherĠ', 'atĠ', 'disappearedĠ', 'clearĠ', 'hatedĠ', 'girlĠ', 'offĠ', 'again', 's.ĠTh', 'eachĠ', 'insid', 'forward', 'fullĠ', 'cent', 'dead', 'fulĠ', 'ĠtheirĠ', 'ungĠ'}\n",
      "Text 2: \n",
      "Don't Make Me Say It,\n",
      "**Author's Note:**\n",
      "> I hope you enjoy....\n",
      ">\n",
      "> It's really cheesy I'm so sorry.\n",
      ">\n",
      "> [Don't repost my work]\n",
      "\"So~, Bakugou.\" Katsuki froze in his seat on the couch, a scowl crawling onto his face as he caught sight of Mina's smile. He resisted the urge to hike his shoulders up an\n",
      "\t Tokenized GPT2:Ċ Don 't ĠMake ĠMe ĠSay ĠIt , Ċ ** Author 's ĠNote :** Ċ > ĠI Ġhope Ġyou Ġenjoy .... Ċ > Ċ > ĠIt 's Ġreally Ġcheesy ĠI 'm Ġso Ġsorry . Ċ > Ċ > Ġ[ Don 't Ġrep ost Ġmy Ġwork ] Ċ \" So ~ , ĠBakugou .\" ĠKatsuki Ġfroze Ġin Ġhis Ġseat Ġon Ġthe Ġcouch , Ġa Ġsc owl Ġcrawling Ġonto Ġhis Ġface Ġas Ġhe Ġcaught Ġsight Ġof ĠM ina 's Ġsmile . ĠHe Ġres isted Ġthe Ġurge Ġto Ġhike Ġhis Ġshoulders Ġup Ġan\n",
      "\t Tokenized no:Ċ Don'tĠ MakeĠ MeĠ SayĠ It ,Ċ **Author'sĠNote:**Ċ>Ġ IĠhopeĠ youĠ enjoy ... .Ċ>Ċ>Ġ It'sĠ reallyĠ che es yĠ I'mĠsoĠ s orr y.Ċ >Ċ>Ġ [ Don'tĠ re postĠ myĠ work ]Ċ \" So ~ ,ĠB ak ug ou .\"Ġ Kat su kiĠ fro zeĠ inĠhisĠ seatĠ onĠtheĠ couch ,ĠaĠ sc ow lĠ craw lingĠ ontoĠ hisĠfaceĠ asĠheĠ caughtĠ sightĠ ofĠ Min a'sĠ smile .ĠHeĠ resist edĠtheĠ urg eĠtoĠ h ikeĠ hisĠ shoulder sĠupĠ an\n",
      "\t Unique Tokens GPT2: {'Ġlip', 'Ġaffection', 'Ġabsence', 'Ġdro', 'Ġuntil', 'Ġbrushed', 'Ġcalm', 'Ġseconds', 'Ġmass', '.', 'Ġsure', 'Ġcouch', 'Ġotherwise', 'Ġboys', 'Ġhuman', 'opping', 'Ġattention', 'Ġarms', 'Ġexactly', 'Ġf', 'Ġshe', 'ed', 'Ġcurl', 'aut', 'Ġag', 'Ġunw', 'Ġcoward', 'Ġtelling', 'ned', 'Ġgo', 'Ġgrowing', 'Ġcheek', 'Ġexplosion', '....', 'Ġnote', 'Come', 'ase', 'Ġ\"', 'Ġfinger', 'head', 'Ġright', 'Ġconfuse', 'Ġyellow', 'Ġcomparing', 'ĠWhy', 'Ġgrowled', 'Ġstep', 'Ġdeath', 'Ġmakes', 'Ġface', 'Ġgrip', 'Ġfast', 'ving', 'Ġspecifying', 'oses', 'Ġits', 'Ġevery', 'Ġwhispered', 'Ġprick', 'Ġword', 'Ġpalms', 'Ġtouch', 'erting', 'Ġsight', 'owl', 'Ġsweating', 'Ġsnorted', 'Ġclock', 'Ġsting', 'Ġfront', 'ĠSpark', 'Ġclenched', 'Ġsaid', 'Ġchuckled', 'Ġsp', 'He', 'Ġcouldn', 'Ġstare', 'Ġcatching', 'Ġgone', 'Ġgetting', 'itated', 'Ġapproaching', 'Ġscr', 'Ġattempt', 'Ġsummer', 'talk', 'Ġleaned', 'Ġdo', 'Ġtucked', '?', 'Ġins', 'ĠThey', 'Ġpurpose', ':**', 'Ġhave', 'Ġblood', 'Ġeven', 'Ġlittle', 'ĠMake', 'Ġset', \"'t\", 'Ġfeeling', 'Ġsounds', 'ably', 'Ġburst', 'Ġwasn', 'Ġstraight', 'Ġused', 'Ġstartled', 'Ġunder', 'Ġonto', 'utter', 'Ġpage', 'Ġdeprec', 'Ġlook', 'Ġfan', 'Ġthings', 'Ġstepped', 'Ġcheesy', 'Ġtick', 'Ġthink', 'ble', 'Ġheat', 'Ġche', 'Ġsmile', 'Ġentering', 'Ġwould', 'Ġ[', 'Ġget', 'Ġboil', 'Ġstill', 'Ġspark', 'Ġknocking', 'Ġend', 'Ġcrawl', 'itting', 'Ġrib', 'Ġwatched', 'Ġpeople', 'Ġthis', 'Ġglaring', 'Ġargument', 'Ġformed', 'Ġspoke', 'Ġdude', 'ative', 'Ġtense', 'Ġedge', 'Ġfrom', 'Ġarm', 'Ġvoice', 'Ġboy', 'Ġwhat', 'unk', 'Ġalmost', 'Ġsupposed', 'Ġgrumbled', 'Ġdifferent', 'Ġal', 'Ġlaughed', 'Ġurge', 'Ġerupt', 'ĠThere', 'Ġprey', 'Ġmade', 'Ġhike', 'Ġdisgust', 'ĠKatsuki', 'inge', 'ĠHis', 'Ġtent', 'Ġswallowed', 'Say', 'Ġhim', 'ari', 'Ġbe', 'Ġair', 'Ġimaginary', 'Ġplaced', 'Ġup', 'Ġhope', 'ĠNote', 'ened', 'Ġsoft', 'Ġj', 'Ġnever', 'aze', 'Ġfroze', 'ons', 'lic', 'isted', 'Ġloo', 'Ġan', 'Ġn', 'Ġenjoy', 'Ġhard', 'Ġare', 'ling', 'Ġdr', 'Ġbook', 'Ġun', 'Ġsomething', 'Ġshifting', 'Ġbrown', 'Ġmistake', 'Ġhelping', 'Ġmeant', 'Ġsliding', 'Ġcalling', 'Ġacross', 'Ġpalm', 'Ġthem', 'Ġjust', 'Ġmanage', 'Ġsays', 'Ġknee', 'Ġtrio', 'Ġsudden', 'Ġseat', 'sen', 'Ġflee', 'Ġbetween', 'Ġrep', 'age', 'Ġquiet', 'Ġsoon', ',\"', '?\"', 'iously', 'Ġh', 'Don', 'Ġminute', 'Ġshoulders', 'ĠI', ']', 'ĠK', 'Ġmake', 'Ġscow', 'Ġseem', 'Ġable', 'Ġblond', 'Ġnow', 'Ġburned', 'Ġcome', 'Ġskin', 'Ġpassed', 'Ġres', 'Ġfelt', 'flies', 'Ġanother', 'Ġwas', 'Ġtheir', 'Ġmy', 'Ġcrossed', 'Ġsoftly', 'Ġdanced', 'Ġturned', 'wed', 'Ġsmall', 'Ġabout', 'Ġadmit', 'Ġquirk', 'Ġpressing', 'Ġby', 'Ġhand', 'atsuki', 'Nothing', 'Ġas', 'Ġwhen', 'Ġoff', 'Ġhe', 'Ġquestioning', 'Ġpit', 'Ġsitting', 'Ġlack', 'Ġleaving', 'Ġnot', 'Ġsul', 'ides', 'ina', 'Ġred', 'ĠIt', 'Ġwell', 'Ġpersistent', 'Ġshoulder', 'Ġhappy', 'Ġspace', 'Ġgripped', 'Ġshallow', 'Ġteeth', 'Ġwhined', 'Ġfinally', 'Ġdone', 'Ġwork', 'Ġyou', 'ost', 'Ġto', 'Ġnext', 'Ġmight', 'Ġspat', 'ĠMe', 'Ġwrist', 'Ġlips', 'Ġdown', 'Ġfeel', 'Ġfire', 'Ġstarting', 'ena', 'Ġdid', 'hearted', 'Ġbreath', 'Ġtwisting', 'Ġslamming', 'Ġstood', 'Ġthey', 'Ġtook', 'Ġit', 'Ġsorry', 'Ġaway', 'Ġcrawling', 'Ġthrows', 'Ġbutter', 'Ġignore', 'Ġat', 'Ġmany', 'Ġstomach', 'Ġslide', 'Ġoverly', 'Ġeyes', 'Ġcre', 'Ġveins', 'Ġbeen', 'Ġin', 'Ġcompletely', 'Ġand', 'Ġability', 'Ġhad', 'Ġnormal', '**', 'Ġlight', 'The', 'Ġof', 'Ġjoking', 'led', 'Ġsay', 'Ġback', 'Ġplace', 'arks', 'What', 'Ġwarned', 'Ġabsent', 'Ġwhere', 'Ġitself', 'Ġinto', '>', 'Ġall', 'Ġcorner', 'Ġa', 'Ġstiff', 'Ġother', 'ĠThe', 'Ġleaning', '.\"', 'Ġleg', 'ĠA', 'Ġany', \"'m\", 'Ġagainst', 'Ġtried', 'Ġopen', 'Ġannoyance', 'Ġcaught', 'Ġself', 'Ġsnap', 'Ġheld', 'Ġsmirk', 'Ġrelaxed', 'Ġcurled', 'Ġso', 'Ġlike', 'Ġon', 'ate', 'Ġmassage', 'Ġwith', 'Ġco', 'Ġprepared', 'unch', 'Ġdirections', 'Ġfalling', 'Ġbefore', 'Ġresponse', 'Ġsilence', 'Ġdoesn', 'Ġatop', 'Ġuncomfort', 'Ġtimes', 'Ġone', 'itted', 'Ġscared', 'Ġdecided', 'Ġcheeks', 'Ġthe', 'ĠHe', 'ĠSay', 'ner', 'Ġfor', 'Being', 'Ġgame', 'Ġreign', 'Ġcontinued', 'Do', 'Ġheart', 'Ġhis', 'Ġreaction', 'Ġelse', 'izing', 'Ġholidays', 'Maybe', 'Ġpink', 'ac', \"'re\", 'Ġglanced', 'Ġmoment', 'Ġout', 'Ġgrow', 'Ġquestioned', 'Ġmore', 'cling', 'Ġlooked', 'Ġonly', 'Ġhimself', 'Ġgripping', 'Ġsc', 'Ġshr', 'Ġgr', 'Ġclose', 'kered', 'Sometimes', 'Ġher', 'Ġbitter', 'Ġtant', 'Ġaction', 'Ġfind', 'ĠSer', 'Ġbrushing', 'I', 'aped', 'Ġruby', 'ĠEveryone', 'Ġbut', 'ishima', 'Ġfiltered', 'Ġvideo', 'Ġpl', 'Ġhy', 'Ġhaving', 'ĠBut', 'Ġread', 'Ġaround', 'Ġfists', \"'ll\", 'ĠM', 'Ġwere', 'Ġfight', 'Ġleave', 'Ġfolded', 'ĠBakugou', 'Ġdart', 'aging', 'Ġchallenge', 'Ġhelp', 'Ġhalf', 'Ġcir', 'Ġmuscles', 'Ġd', 'Ġbrow', 'Ġcould', 'Author', 'Ġworried', 'Ġreally', 'Ġtrying', 'Ġstaring', 'Ġinside', 'Ġfl', 'Ġlet', 'Ġfamiliar', 'Ġover', 'king', 'Ġthat', 'Ġburning', 'Ġme', 'ĠKirishima', 'She', 'Ġtighter', 'Ġanxiety', 'Ġsat', 'Ġwaiting'}\n",
      "\t Unique Tokens no: {'satĠ', 'soĠ', 'dude', 'opĠ', 'sent', 'aĠc', 'every', 'letĠ', 'approach', 'postĠ', 'brown', 'aĠmist', 'shouldersĠ', 'ickĠ', 'ledĠ', \"wasn'tĠ\", 'pre', 'gl', 'onĠtheĠ', 'start', 'wr', 'purpose', 'inglyĠ', 'Kir', 'eĠofĠ', \"You'reĠ\", 'question', 'sound', 'Ġm', 'ense', 'yĠthatĠ', 'ak', 'asĠheĠ', 'ag', 'heĠwasĠ', ',ĠhisĠ', 'pageĠ', 'secondsĠ', 'es.', 'overĠaĠ', 'feltĠlikeĠ', 'slamm', 'ancedĠ', 'ell', 'craw', 'sul', 'placeĠ', 'eĠturn', 'supposedĠtoĠ', 'heĠ', 'ight', 'quirk', 'aliz', 'allow', 'stoodĠ', 'seatĠ', 'persistentĠ', 'happyĠ', 'exactlyĠ', 'allowĠ', 'findĠ', 'cornerĠ', 'ageĠ', 'fac', 'endĠ', 'absenceĠofĠ', '...', 'ingĠinĠ', 'bloodĠ', 'halfĠ', 'blon', 'arm', 'nerv', 'ofĠhisĠ', 'shiftingĠ', 'su', 'bo', 'gr', 'ent', 'whenĠheĠ', 'aboutĠ', 'ann', 'allĠtheĠ', 'uck', 'eĠtheyĠ', 'ayĠ', 'chuck', 'soft', 'eĠtoĠ', 'inĠanotherĠ', 'ontoĠ', 'is', 'passed', 't', 'saidĠ', 'BeingĠ', 'sightĠ', 'ĠthingsĠ', 'browĠ', 'manageĠ', 'en', 'ondĠ', 'goĠandĠ', 'lyĠ', 'Min', 'ear', 'mass', 'Kat', 'backĠtoĠhisĠ', 'anxiet', 'herĠeyesĠ', 'onĠherĠ', '.Ċ', 'his', 'Ġthere,Ġ', '.Ġ\"', 'burst', 'breathĠ', 'iouslyĠ', 'a', 'areĠ', 'clos', 'hisĠhandĠ', 'sn', 'fold', 'oĠ', 'ĠtoĠgetĠ', 'atchingĠ', 'istĠ', 'scared', 'aĠwasĠ', 'ou', 'heatĠ', 'Ġc', 'comeĠ', ',Ġ\"', 'msĠ', 'enjoy', 'es.Ċ', 'downĠonĠtheĠ', 'againstĠ', 'sayĠ', 'preparedĠ', \"i'sĠ\", 'enedĠ', ',ĠS', 'ingĠhisĠ', 'deprec', '>Ċ>Ġ', 'at,Ġ', 'edĠasĠaĠ', 'doneĠ', 'tingĠ', '.ĊItĠ', 'gettingĠ', 'whatĠheĠ', 'TheyĠ', 'esĠ', 'Ġthrow', 'shoulderĠ', 'videoĠ', 'ittedĠ', 'ikeĠ', 'nextĠtoĠtheĠ', 'edgeĠ', 'loos', 'ofĠ', 'over', '.Ċ\"', 'asĠtheyĠ', 'onsĠ', 'ĠtookĠ', 'itĠtoĠ', 'evenĠ', 'ingĠ', 'cheekĠ', 'aseĠ', 'lackĠofĠ', 'onĠhisĠ', 'leanedĠ', ',ĠbutĠheĠ', 'usedĠ', 'edĠtheĠ', 'tedĠ', 'burningĠ', 'intoĠ', 'stiff', ',ĠandĠ', 'utĠ', 'urlĠ', \"**Author'sĠNote:**Ċ>Ġ\", 'aĠl', 've', ',ĠandĠtheĠ', 'cl', 'butter', \"you'llĠ\", 'whereĠ', 'setĠ', 'quietĠ', 'underĠ', 'lĠ', 're', 'steppedĠ', 'startingĠtoĠ', 'palm', \"doesn'tĠ\", 'ĠsmallĠ', 'edĠasĠ', 'backĠtheĠ', '.ĊHeĠ', 'heldĠ', 'stood', 'makesĠyouĠ', 'fromĠtheĠ', 'overlyĠ', 'iĠw', 'ppingĠ', 'ingĠdownĠ', 'ĠwithĠherĠ', 'hisĠlip', 'nextĠtoĠ', '.ĠH', 'kneeĠ', 'ed,Ġ', 'onĠitsĠ', 'byĠ', 'doĠsomethingĠ', 'himself', 'che', 'ingĠthemĠ', 'youĠ', 'clockĠ', '[', \"I'mĠsoĠ\", 'stepĠ', 'ComeĠon,Ġ', 'eĠthinkĠ', 'iĠs', 'fastĠ', 'wereĠt', 'erĠ', 'no', 'whin', 'otherw', 'ated', '.ĠButĠ', 'still', 'ki', 'moreĠofĠaĠ', 'y,Ġ', ',ĠM', 'els', 'completely', 'meantĠ', '.\"Ġ', 'softlyĠ', 'edĠ', 'ĠtoĠme', '.Ċ>Ċ>Ġ', 'ab', 'th', 'bleĠ', 'oneĠ', 'me', 'pressingĠ', 's.Ġ', 'dro', 'itself', 'continuedĠtoĠ', 'ow', 'hadĠbeenĠ', 'armsĠ', ',ĠthisĠ', 'co', 'm', 'well', 'feelĠ', 'ch', 'leaveĠ', 'butĠthenĠ', 'Noth', 'readĠ', 'anĠ', 'him', 'ed,Ġ\"', 'ort', 'kiĠ', 'explosion', 'mightĠ', '.ĠK', 'handĠ', 'feltĠ', 'havingĠ', 'tionsĠ', 'ingĠt', 'smile', 'ouchĠ', 'ĠcouldĠ', 'dart', 'shoulder', 'IĠ', ']Ċ', 'Ser', 'HisĠ', 'abilityĠtoĠ', '.\"ĠTheĠ', 'almostĠ', 'watch', 'sp', 'yĠ', 'underĠtheĠ', 'e,Ġ', 'ing.Ċ\"', 'aĠsaidĠ', 'swe', 'clench', 'sesĠ', 'growingĠ', 'ingĠwithĠtheĠ', 'did', 'urg', 'sat', 'leav', 'crossedĠ', 'somethingĠ', 'kingĠ', 'hisĠ', 'enter', 'overĠtheĠ', 'worriedĠ', 'itt', 'upĠ', 'Ġ', 'imagin', 'suddenĠ', 'atingĠ', 'comfort', 'ifyingĠ', ',\"Ġ', 'Ġcall', 'ond', 'feltĠtheĠ', 'hard', 'MaybeĠ', 'ap', 'asĠsoonĠasĠ', 'ignoreĠ', 'eyesĠ', 'heartĠ', 'challeng', 'sittingĠ', 'fist', 'disgust', 'myĠ', 'reallyĠ', 'spaceĠ', 'argumentĠ', 'aĠ', 'be', 'annoy', 'attentionĠ', 'fireĠ', \"you'reĠ\", 'h', 'ly,Ġ', 'adm', 'a.Ċ', 'straightĠ', 'slid', 'overĠ', 'ed,ĠtheĠ', 'legĠ', 'sĠmeĠ', 'ateĠ', 'forĠaĠmoment', 'pr', 'whisper', 'himĠandĠ', 's.ĊTheĠ', 'skinĠ', 'umbledĠ', ',Ġt', 'tri', 'lingĠ', 'ard', 'sc', 'peopleĠ', 'eredĠ', 'laugh', 'ĠteethĠ', 'edĠatĠtheĠ', 'ariĠ', \"It'sĠ\", 'ativeĠ', 'eĠinĠ', 'feelingĠ', 'themĠ', 'norm', 'himĠtoĠtheĠ', 'intoĠtheĠ', 'oneĠtoĠ', 'f', 'e.Ċ\"', 'relax', 'parkingĠ', 'DoĠ', 'YouĠ', 'inn', '.\"Ċ', 'sure', 'wait', 'ofĠtheĠ', ',ĠheĠ', 'mus', 'SheĠ', 'game.Ġ', 'igh', 'fightĠ', 'aĠhadĠ', 'deathĠ', 'Ġconf', 'overĠhisĠ', 'cre', 'attempt', 'eĠto', 'outĠofĠhisĠ', 'burn', 'sĠnotĠ', 'stomachĠ', 'orr', 'wist', 'intoĠhisĠ', 'haveĠtoĠ', 'ug', 'backĠintoĠtheĠ', 'edĠonĠtheĠ', 'asĠtheĠ', 'run', 'ass', 'word,Ġ', 'heĠhadĠ', 'w', 'gripp', 'down', 'bitter', 'glanc', 'ingĠtheĠ', 'kno', '\"S', 'ThereĠ', 'forĠ', 'likeĠheĠ', '.\"ĠHeĠ', 'agingĠ', 'caughtĠ', '\"t', 'betweenĠ', 'up', 'Ġtim', 'in', 'ppedĠ', 'stomach', 'ant', 'overĠtoĠtheĠ', 'affection', 'irkĠ', 'lookĠ', 'MakeĠ', 'insideĠofĠ', 'out', 'er', 'ru', 'likeĠaĠ', 'help', 'right', 'IĠhopeĠ', 'Ġthem,Ġ', 'neverĠ', 'hisĠfaceĠ', 'fle', 'face.Ċ', 'formedĠ', 'ĠsaysĠ', 'off,Ġ', 'fromĠ', '.ĠA', 'allĠtoĠ', 'clesĠ', 'ckingĠ', 'atĠtheĠ', 'fall', 'un', 'stare', 'itĠwas', 'es', 'youĠs', 'aroundĠ', '\"Ġ', 'headĠ', 'pitĠ', 'ingĠfromĠ', 'untilĠ', 'softĠ', 'triedĠtoĠ', 'holid', 'moment', '?ĠWhyĠ', 'onlyĠtoĠ', 'st', 'openĠ', 'summerĠ', '?\"ĠHeĠ', 'filter', 'withĠ', 'pal', 'froz', 'oĠandĠ', 'couldĠ', 'doĠitĠ', 'red', 'cheek', 'reactionĠ', 'aĠt', 'kĠ', ',ĠtheĠ', 'inĠhisĠ', 'grac', 'Ġtimes,Ġ', '.ĠItĠjustĠ', 'inĠtheĠ', 'spok', 'spec', 'selfĠ', 'itĠ', 'inĠaĠ', '.ĠEveryoneĠ', ',ĠB', 'ingĠtoĠ', '?\"Ġ', 'silenceĠ', 'front', 'edĠoffĠ', 'asĠsheĠ', 'her,Ġ', 'direc', 'ingĠmeĠ', 'inĠ', 'rightĠ', 'onĠit', 'actionĠ', 'human', \"a'sĠ\", 'IĠthinkĠ', \"Don'tĠ\", 'overĠtoĠ', 'stillĠ', 'couch', 'boyĠ', 'forĠtheĠ', 'decidedĠtoĠ', 'plo', 'againstĠhisĠ', 'gri', 'nowĠ', 'iĠwasĠ', 'atĠanyĠ', 'arn', 'noteĠ', ',ĠK', 'utterĠ', 'at', 'aĠlittl', 'gripĠ', 'finally', '.ĠHeĠ', 'seemĠtoĠbeĠ', 'enseĠ', 'sĠandĠ', 'madeĠhisĠ', 'ĠtoĠthem', 'aryĠ', 'familiar', 'ign', 'sheĠhadĠ', 'ing.Ċ', 'it', 'atĠallĠ', 'e,Ġ\"', 'andĠtheĠ', 'SayĠ', 'calmĠ', 'alĠs', 'Ġcompar', 'aysĠ', 'ableĠtoĠ', 'goneĠ', 'justĠ', 'bl', 'k', 'anceĠ', 'grow', 'reaction', 'd,Ġ', 'scaredĠ', 'pink', 'he', 'placedĠ', 'staringĠ', 'ick', 'work', 'ĠturnedĠ', 'MeĠ', ',ĠtryingĠtoĠ', 'ilĠ', 'around', 's,Ġ', 'sur', 'againstĠtheĠ', 'makeĠyouĠ', 'az', \"What'sĠ\", 'eĠthatĠ', 'voiceĠ', 'resist', 'onĠ', 'hand', 'mĠ', 'dr', ',ĠtheirĠ', 'notĠ', 'sm', 'boy', 'many', 'jok', 'fro', 'aĠhy', 'yellowĠ', 'book', 'apĠ', 'park', 'eĠ', 'ed,ĠandĠ', 'sĠthatĠ', 'ably,Ġ', 'and', 'likeĠ', 'stareĠ', '.ĠHisĠ', 'eyes,Ġ', 'fli', 'responseĠtoĠtheĠ', 'ingĠitĠ', 'fl', 'TheĠ', 'alk', 'ar', 'moreĠt', 'brush', ',ĠaĠ', 'slideĠ', ',Ċ', 'y.Ċ', 'ingĠinĠtheĠ', 'awayĠ', 'upĠandĠ', 'acrossĠ', 'himĠ', 'e.Ġ', 'curledĠ', 'chew', 'SometimesĠ', \"I'mĠnotĠ\", 'rib', 'sh', 'edĠhisĠ', 'lipĠ', 'airĠ', 'beforeĠheĠ', 'iĠwouldĠ', 'lip', 'eyes', 'atĠ', 'onĠallĠ', 'leaningĠ', 'makeĠ', 'otherĠ', 'flick', 'd', \"couldn'tĠ\", 'offĠ', 'inĠdifferentĠ', ',ĠitĠ', 'jo', 'away', 'meĠ', 'insid', 'unchĠ', 'aĠminuteĠ', 'fingerĠ', 'ise,Ġ', 'enĠtheĠ', 'lookedĠ', 'useĠ', 'zeĠ'}\n",
      "==Different Author==\n",
      "Text 1: with tomato jelly\n",
      " This recipe was inspired by the tantalizing sashimi tomato jelly and langoustine dish created by <PERSON> that I enjoyed at Umu in London. If you have never eaten tomato jelly, it is a most extraordinary experience. The jelly is golden and crystal clear, bearing no physical resemb\n",
      "\t Tokenized GPT2:with Ġtomato Ġjelly Ċ ĠThis Ġrecipe Ġwas Ġinspired Ġby Ġthe Ġtant al izing Ġs ash imi Ġtomato Ġjelly Ġand Ġlang oust ine Ġdish Ġcreated Ġby Ġ< PER SON > Ġthat ĠI Ġenjoyed Ġat ĠU mu Ġin ĠLondon . ĠIf Ġyou Ġhave Ġnever Ġeaten Ġtomato Ġjelly , Ġit Ġis Ġa Ġmost Ġextraordinary Ġexperience . ĠThe Ġjelly Ġis Ġgolden Ġand Ġcrystal Ġclear , Ġbearing Ġno Ġphysical Ġresemb\n",
      "\t Tokenized no:withĠt omat oĠ j el ly ĊĠ ThisĠ recip eĠwasĠ inspiredĠ byĠtheĠ t ant al izingĠ s as him i Ġt omat oĠ j el lyĠandĠ lang oust ineĠ dish Ġcre atedĠbyĠ < P ERS ON > ĠthatĠIĠ enjoy edĠatĠ U mu Ġ inĠL ondon .ĠIfĠyouĠhaveĠ neverĠ eat en Ġt omat oĠ j el ly,Ġ itĠisĠ aĠm ostĠ extraordinaryĠ experienc e.ĠTheĠ j el lyĠ isĠ gold enĠandĠ cryst alĠc lear ,Ġ bear ingĠ noĠ physicalĠ resem b\n",
      "\t Unique Tokens GPT2: {'Ġfew', 'Ġuntil', 'Ġscent', 'Ġthen', '.', 'ĠAdd', 'Ġminutes', 'ible', 'ak', 'ed', 'Ġblack', ':', 'ĠCr', 'Ġserved', 'Ġcreated', 'Ġfrozen', 'Ġlang', 'Ġcloud', 'Ġsauce', 'Ġed', 'Ġce', 'Ġeach', 'ases', 'Ġlocal', 'Ġlemon', 'ĠC', 'Ġits', 'Ġsetting', 'Ġevery', 'Ġtract', 'Ġ5', 'Ġ750', 'Ġshould', 'Ġjelly', 'Ġ10', 'Ġleft', 'Ġstir', 'Ġpepper', 'Ġs', 'sea', 'Ġtables', 'Ġexcess', 'ipe', 'Ġcool', 'Ġclear', 'Ġdo', 'ĠAmerica', 'Ġneed', 'Ġr', 'Ġhave', 'Ġeven', 'Ġbag', 'Ġset', 'Ġis', 'with', 'ĠHalf', 'Ġheat', 'Ġ1', 'Ġboil', 'Ġcrystals', 'ĠIf', 'Ġadvis', 'Ġend', 'but', 'Ġexperience', 've', 'Ġaccount', 'Ġfrom', 'Ġreminis', 'Ġflav', 'Ġhours', 'iments', 'Ġfood', 'Ġserve', 'iche', 'Ġal', 'Ġalso', 'Ġfridge', 'asp', 'Ġliquid', 'Ġjuice', 'Ġruns', 'Ġextraordinary', 'blance', 'Ġbe', 'ash', 'Ġwater', 'Ġusing', 'Ġup', 'Ġhands', 'ened', 'Ġsoft', 'Ġafter', 'Ġnever', 'ired', 'ĠCh', 'Ġfish', 'Ġan', 'ves', 'Ġare', 'Ġtempted', 'our', 'Ġsheets', 'Ġfreshly', 'Ġcit', 'udo', 'Ġraw', 'Ġthem', 'Ġadd', 'Ġpowers', 'ĠEuropean', 'Ġbetween', 'b', 'Ġ<', 'ans', 'insp', 'ĠI', 'Ġcolour', 'Ġglass', 'Ġmake', 'Ġju', 'Ġinspired', 'Ġgel', 'Ġmost', 'ĠAmerican', 'PER', 'Ġwas', 'Ġquite', 'Ġsedu', 'the', 'Ġflowers', 'Ġsmall', 'Ġby', 'see', 'Ġas', 'Ġoff', 'Ġremove', 'ARI', 'ĠU', 'Ġnot', 'imi', 'ĠIt', 'Ġwell', 'Ċ', 'Ġtaste', 'ĠWhen', 'Ġgives', 'rant', 'ĠSc', 'pan', 'Ġcontainer', 'ful', 'Ġtomato', 'Ġcold', 'Ġshallow', 'Ġwill', 'Ġyou', 'Ġto', 'ĠUse', 'Ġput', 'Ġlarge', 'Ġtomatoes', 'Ġcrystal', 'Ġcover', 'Ġolive', 'lb', 'Ġcr', 'y', 'SON', 'Ġarray', 'Ġseason', 'Ġsheet', 'oop', 'Ġvery', 'able', 'Ġbriefly', 'Ġit', 'Ġte', 'Ġat', 'Ġtossed', 'inated', '##', 'Ġcarefully', 'Ġfrag', 'Ġdish', 'Ġoil', 'Ġin', 'Ġprocessor', 'Ġand', 'Ġ#', 'Ġbeef', 'Ġof', 'ĠTop', 'ached', 'Ġworks', 'Ġsalt', 'Ġmar', 'Ġinto', 'rus', 'Ġa', 'ĠThe', 'Ġother', 'es', 'ĠV', 'Ġany', 'ami', 'Ġrecipe', 'Ġphysical', 'Ġ6', 'Ġtake', 'Ġso', 'icy', 'Ġcut', 'Ġcub', 'Ġ3', 'Ġon', 'Ġremaining', 'Ġpale', 'Ġwith', 'ct', 'Ġgently', 'Ġwonderfully', 'Ġless', 'Ġbefore', 'Ġpreviously', 'Ġmind', 'ĠThis', 'Ġthe', 'Ġum', 'ĠBl', 'Ġstop', 'fill', 'ĠTrans', 'Ġfor', 'Ġshells', 'Ġsqueeze', 'ĠLondon', 'Ġeverybody', 'Ġgolden', 'Ġsubtle', 'ive', 'Ġground', 'Ġlightly', 'Ġonce', 'izing', 'Ġdivide', 'ĠPut', '31', 'Ġultra', 'Ġno', 'Ġthere', 'ĠYou', 'Ġout', 'Ġdishes', 'Ġmore', 'Ġcrust', 'Ġaccompan', 'Ġonly', 'Ġshr', 'Ġhung', 'Ġwhite', 'Ġdripping', 'oon', 'Ġeaten', 'ĠLeave', ';', ')', 'Ġdress', 'Ġ(', 'Ġtant', 'Ġhere', 'Ġlose', 'ml', 'Ġknow', 'ĠSer', 'Ġfruit', 'Ġrush', 'Ġbut', 'Ġfresh', 'Ġenjoyed', 'resh', 'Ġriot', 'Ġbud', 'Ġ100', 'Ġpo', 'Ġquantity', 'Ġleave', 'Ġshell', 'Ġor', 'ĠFish', 'fl', '2', 'Ġmelt', 'Ġbowl', 'Ġd', 'Ġready', 'Ġlow', 'Ġmeasure', 'Ġover', 'Ġthat', 'Ġresem', 'Ġpicking', 'Ġcup', 'ĠDo', 'Ġoz', 'Ġyour', 'ĠSouth', 'cent', ',', 'Ġbearing'}\n",
      "\t Unique Tokens no: {'beef', 'noĠ', 'etĠ', 'are,Ġ', 'opĠ', 'sĠofĠ', 'C', 'dis', 'cit', 'omat', 'ultr', 'bear', 'es)', '.Ġ', 'mar', 'toĠ', 'aĠb', 'ly', 'Ġshe', 'bĠ', 'Ġm', 'Hal', ':Ġ', 'edĠtoĠ', 'juiceĠ', 'ĠthatĠwillĠ', 'fresh', 'onĠs', 'ell', 'oodĠ', 'power', 'read', 'ase', 'allow', 'ea', 'endĠ', '3', 'allowĠ', 'fishĠ', 'wellĠ', 'spo', 'isĠnotĠ', 'sedĠ', 'lessĠ', 'act', 'andĠ', 'bo', 'ent', 'frid', 'lowĠ', 'ep', 'beĠt', 'soft', 'l', 'frag', 'eĠtoĠ', 'is', '6Ġ', 't', '10', 'sĠwithĠ', '(3', 'iv', 'owl', 'frozenĠ', 'black', 'worksĠ', 'ĠtheĠt', 'UseĠ', 'e.ĠTheĠ', 'en', 'stopĠ', 'lyĠ', 'handsĠandĠ', 's;Ġ', '###Ġ', 'rem', 'serv', 'brieflyĠ', 'water,Ġ', 'altĠ', 'usĠ', 'r', 'gĠ', ')Ġ', 'anim', 'onlyĠhaveĠ', 'oĠ', 'Ġsh', 'ou', 'heatĠ', 'season', 'Ġc', 'enjoy', 'enedĠ', '.ĠYouĠshouldĠ', 'IĠknowĠ', 'until', 'ice', 'ic', 'sembl', 'Bl', 'ofĠ', 'ich', 'edĠwithĠ', 'um', 'rip', '-f', 'eĠwasĠ', 'er,Ġ', 'e.Ċ', 'here', 'accountĠ', 'yĠt', 'ingĠ', 'ers,Ġ', 'edĠatĠ', 'as', 'intoĠ', 'ut', 'illĠ', 'aĠl', 'salt', 'cl', ')ĠandĠ', 'ĠtakeĠ', 'needĠ', 'edi', 'lĠ', 're', ',Ġ', 'ĠwithĠ', 'withĠc', 'achĠ', 'akĠ', 'WhenĠ', 'ine,Ġ', 'inspiredĠ', 'po', 'groundĠ', 'fromĠtheĠ', 'ppingĠ', 'aĠm', 'subtleĠ', 'ellsĠ', ')ĠofĠ', 'previouslyĠ', '.ĠItĠ', '<', 'lyĠandĠ', 'us', 'ans,Ġ', 'es.ĠTheĠ', 'readyĠtoĠ', 'amiĠ', 'rush', 'fullyĠ', 'ang', 'makeĠtheĠ', 'dish', 'Ġtable', 'i', 'anc', 'edĠ', 'alsoĠ', 'ant,Ġ', 'raw', 'onĠaĠ', 'extraordinaryĠ', 'bleĠ', 's.Ġ', 'ow', 'co', 'ing', 'vesĠ', 'leaveĠ', 'e,ĠbutĠ', 'isĠ', 'Ġtos', 'anĠ', 'him', '-the-', 'ol', 'butĠnotĠ', 'ingĠt', 'glassĠ', '100', 'izingĠ', 'el', 'Ser', 'pep', '.ĊĠ', 'itsĠ', ',Ġthen', 'Europe', 'e,Ġ', '.ĠTheĠ', 'freshĠ', 'se', 'Ġteas', 'hungĠ', 'o', 'ostĠ', 'Ġ', 'runsĠ', 'onĠtheĠt', 'lear', 'loseĠ', 'removeĠtheĠ', 'onĠyourĠ', 'rawĠ', 'veryĠ', 'evenĠwithĠ', '.ĠIfĠyouĠ', 'settingĠ', 'hour', 'experienc', 'handsĠ', 'empt', 'orĠotherĠ', 'ineĠandĠ', 'rud', 'icesĠ', 'ly,Ġ', 'ri', 'Ġcontain', 'processor', 'set', 'dres', 'overĠ', 'forĠeveryĠ', 'andĠt', 'oldĠ', 'AddĠ', 'aĠfewĠ', 'centĠ', 'esĠinĠ', 'eĠtheĠ', 'bud', 'sheet', 'withĠyourĠ', 'arrayĠofĠ', 'iceĠandĠ', 'flow', 'ĊĠ', 'withĠt', 'ityĠofĠ', ',ĠbutĠitĠisĠ', 'addĠtheĠ', 'lem', 'ers.Ġ', 'sĠ', 'it,Ġ', 'ofĠtheĠ', 'sĠbutĠ', 'esĠ(', 'veĠ', 'oil', '.ĠL', 'F', 'asĠ', 'paleĠ', 'atedĠinĠ', 'i-', 'anĠandĠ', 'mel', 'asteĠ', 'gentlyĠ', 'an', 'Ch', 'flav', 'bic', 'mind', 'dri', 'duct', 'America,Ġ', 'forĠ', 'Ġcre', 'ThisĠ', 'j', 'beforeĠ', 'aĠsmallĠ', 'excessĠ', 'AmericanĠ', 'betweenĠ', 'up', 'ourĠ', 'in', 'erĠandĠ', 'fruitĠ', 'each', 'ĠthatĠIĠ', 'lightlyĠ', 'ĠminutesĠ', 'neverĠ', 'ineĠ', 'meas', 'itĠwillĠ', ';Ġ', 'once', 'oesĠ', 'putĠ', 'fromĠ', 'udoĠ', 'atedĠbyĠ', 'colour', '.ĠDoĠ', 'ondon', 'U', 'ce', '750', 'untilĠ', '1Ġ', 'eat', 'antĠ', 'moreĠ', 'alĠc', 'WhenĠtheĠ', 'largeĠ', 'advis', 'lĠofĠ', 'ju', 'VAR', 'Trans', 'entĠ', 'heĠisĠ', 'ERS', 'ĠthemĠ', 'goldenĠ', 'afterĠ', 'mlĠ', 'withĠ', 'onĠanyĠ', 'whiteĠ', 'aĠf', 'doĠnotĠ', 'bagĠ', 'ThisĠisĠ', 'recip', '1/2', 'inĠtheĠ', 'ust', 'cub', 'stir', 'physicalĠ', 'pick', 'juic', 'ĠthatĠ', 'ishĠ', 'pp', 'squeez', 'willĠtakeĠ', 'sauc', 'otĠ', 'inĠL', 'cool', '.ĠC', 'inĠ', 'givesĠyouĠ', 's.Ċ', 'ON', 'atedĠ', 'cr', 'ĠtheĠ', 'et', 'bow', 'outĠtheĠ', ',ĠareĠ', 'gel', ')ĠofĠtheĠ', 'cutĠ', 'sĠandĠ', 'wonder', '.ĠIfĠyouĠhaveĠ', 'itĠisĠ', 'ing.Ċ', 'butĠthereĠ', 'ac', 'upĠwithĠ', 'a-', 'andĠtheĠ', 'notĠonlyĠ', 'eĠtoĠtheĠ', 'ableĠtoĠ', 'liquid', 'carefullyĠ', 'he', 'gent', 'remainingĠ', '5', 'coverĠ', 'everybody', 'ilĠ', 'cryst', 's,Ġ', 'offĠtheĠ', 'lang', 'l,Ġ', 'water', 'onĠ', 'I', 'notĠ', 'light', 'quiteĠ', 'youĠwillĠ', 'pepper', 'local', 'divideĠ', '.ĠThisĠ', '.ĠP', '.ĠS', 'aĠs', 'eĠ', 'sĠthatĠ', 'hes', 'byĠtheĠ', '.ĠT', 'so', 'leftĠ', 'SouthĠ', '(1', 'dy', 'ar', 'P', 'e.Ġ', '.ĠItĠisĠ', 'sh', 'gold', 'quant', 'comp', 'esĠandĠ', 'atĠ', 'ice,Ġ', 'enĠandĠ', '6', 'd', 'tĠ', 'seeĠ', 'isĠthenĠ', 'iveĠ', 'ureĠtheĠ', 'fulĠ', 'butĠnoĠ', 'ĠtoĠaĠ', 'y-', 'Ġtr'}\n",
      "Text 2: Depends on the scenario        If the rotation stops then the sea level at the poles will increase due to the earth being wider at the equator than going North South. So no, Antarctica would actually shrink as the water rises.        If the the rotation stops then the Earth will also bake on the sid\n",
      "\t Tokenized GPT2:Depend s Ġon Ġthe Ġscenario ĠĠĠĠĠĠĠ ĠIf Ġthe Ġrotation Ġstops Ġthen Ġthe Ġsea Ġlevel Ġat Ġthe Ġpoles Ġwill Ġincrease Ġdue Ġto Ġthe Ġearth Ġbeing Ġwider Ġat Ġthe Ġequ ator Ġthan Ġgoing ĠNorth ĠSouth . ĠSo Ġno , ĠAnt ar ct ica Ġwould Ġactually Ġshrink Ġas Ġthe Ġwater Ġrises . ĠĠĠĠĠĠĠ ĠIf Ġthe Ġthe Ġrotation Ġstops Ġthen Ġthe ĠEarth Ġwill Ġalso Ġbake Ġon Ġthe Ġsid\n",
      "\t Tokenized no:Depend sĠonĠtheĠ scenario ĠĠĠĠĠĠĠĠ IfĠtheĠ rot ationĠ stop s ĠthenĠtheĠ seaĠ levelĠ atĠtheĠ po lesĠ willĠ increaseĠ dueĠtoĠtheĠ earthĠ beingĠ widerĠ atĠtheĠ equ ator ĠthanĠ goingĠ NorthĠ South .ĠSoĠ no,Ġ Ant ar ct icaĠ wouldĠ actuallyĠ shr inkĠ asĠtheĠ waterĠ ri ses .ĠĠĠĠĠĠĠĠ IfĠtheĠ theĠ rot ationĠ stop s ĠthenĠtheĠ Earth ĠwillĠ alsoĠ b akeĠ onĠtheĠ sid\n",
      "\t Unique Tokens GPT2: {'Ġby', 'ica', 'Ġagain', \"'s\", 'Ġfaces', 'Ġthen', 'Ġas', 'Ġchanges', 'Ġless', 'Ġbefore', 'Ġeverything', 'Ġpoles', 'ĠEarth', 'ocate', 'Ġmel', 'Ġfrom', 'Ġhow', 'Ġ_', 'Ġthe', 'Ġhead', 'Ġduring', 'Ġif', 'ĠDun', 'ĠOcean', 'Ġthan', 'ĠĠĠĠĠĠĠ', 'Ġalso', 'ĠAgain', 'Ġlevel', 'ĠAnt', 'Ġcold', 'Ġbit', 'Ġside', 'Ġsuff', 'Ġwill', 'Ġyou', 'Ġto', 'Ġelse', 'Ġrises', 'Ġnear', 'Ġbe', 'Ġrise', 'ĠEve', 'Ġfreezing', 'Ġactually', 'Ġrelative', 'Ġno', 'Ġdeath', 'Ġwater', 'Ġrotation', 'Ġmore', 'Ġice', 'Ġcontinent', 'Ġslowly', 'Ġgain', 'Ġvery', 'ĠAnd', 'Ġit', 'Ġaway', 'Ġat', 'Ġdue', 'Ġmoves', 'Ġfind', 'Ġfacing', 'Ġbut', 'Ġstops', 'Ġremain', 'Ġin', 'Ġsun', 'Ġbeing', 'Ġand', 'Ġgoing', 'Ġremoved', 'Ġof', 'Ġaltitude', 'Ġyourself', 'Ġscenario', 'getting', 'ĠM', \"Ġ'\", 'Ġor', 'Ġweight', 'ts', 'Ġmelt', '?', 'Ġall', 'se', 'all', 'Ġa', 'ĠSo', 'ĠThe', 'Ġother', 'Ġequivalent', 'Ġsea', 'Ġincrease', 'Ġhave', 'Ġbake', 'Ġmuch', 'rest', 'ĠFor', 'Ġtop', 'Ġfactor', 'Ġthat', 'ĠNorth', 'Ġearth', 'Ġso', 'Ġyour', 'ĠSouth', 'Ġwould', ',', 'Ġshrink', 'Ġwider', 'Ġequ', 'ĠIf', 'asons', 'Ġzone', 'Ġon', 'ĠLOT'}\n",
      "\t Unique Tokens no: {'soĠ', 'beingĠ', 'uff', 'near', 'itĠ', 'remov', 'seaĠ', 'Again', 'ateĠ', 'edĠfromĠtheĠ', 'LO', 'onĠtheĠ', 'dueĠtoĠtheĠ', 'asĠyourĠ', 'ĠtheĠ', ',Ġ', 'aĠbitĠofĠ', 'remainĠ', 'facingĠ', 'est', 'andĠotherĠ', 'po', 'theĠ', 'ĠandĠ', 'weightĠ', '?ĠAndĠ', 'Earth', 'aĠfac', 'byĠ', '.ĠO', 'ofĠtheĠ', 'stop', 'shr', 'ideĠ', 'sĠandĠ', 'orĠ', 'Dun', 'youĠ', 'zoneĠ', 'ofĠM', 'ĠthanĠ', 'coldĠ', 'fac', 'ses', 'rise', 'entĠandĠ', 'findĠ', 'levelĠ', 'duringĠtheĠ', 'wouldĠbeĠ', 'deathĠ', 'sĠonĠtheĠ', 'oc', 'rot', 'ĠĠĠĠĠĠĠĠ', \"andĠthat'sĠ\", 'howĠmuchĠ', 'alsoĠ', 'ifĠtheĠ', 'scenario', 'mel', 'earthĠ', 'akeĠ', 'ĠthanĠtheĠ', \"s'Ġ\", 'water', 'asĠtheĠ', 'ĠwillĠ', 'allĠthatĠ', 's,ĠthenĠ', '.ĠĠĠĠĠĠĠĠ', 'iceĠ', 'beĠ', 'itĠwouldĠ', 'esĠtheĠ', 'waterĠ', 'ingĠtheĠ', 'ĠthenĠtheĠ', 'actuallyĠ', 'sideĠ', 'sun', 'anĠ', 'ĠtheĠtopĠ', 'goingĠ', 'sideĠofĠ', 'wouldĠhaveĠ', 'beforeĠ', 'in', 'willĠ', 'would', 'IfĠtheĠ', 'wouldĠ', 'itudeĠ', '.ĠSoĠ', 'else,Ġ', 'South', 'level', 'ĠwouldĠ', 'equ', 'b', 'freez', 'all_', 'contin', '.ĠTheĠ', 'widerĠ', 'T,Ġ', 'SouthĠ', 'Ever', 'atĠtheĠ', 'For', 'increaseĠ', 'awayĠfromĠtheĠ', 'gainĠ', 'relativeĠ', 'se,Ġ', 'ce', 'awayĠ', 'no,Ġ', '.ĠĠĠĠĠĠĠĠSoĠ', 'factorĠ', 'youĠs', 'ationĠ', 'season', 'butĠ', 'headĠ', 'slowlyĠ', 'Ant', 'inkĠ', 'atorĠ', 'moreĠ', 'asĠaĠs', 'youĠhaveĠtoĠ', 'esĠandĠ', 'lesĠ', 'entĠ', 'EarthĠ', 'movesĠ', 'gettingĠ', 'veryĠ', 'chang', \"'sĠ\", 'tĠ', 'ic', 'icaĠ', 'again', 'away', 'alt', '?Ġ', 'less', 'yourselfĠ', 'aĠ', 'NorthĠ', 'everythingĠ', 'equivalentĠ', 'ingĠ', 'ri'}\n",
      "==Same Author==\n",
      "Text 1: you came every summer to Bergerac.\n",
      " <PERSON> You used to fashion lances out of reeds ...\n",
      " <PERSON> The silk of the tasselled corn furnished hair for your doll...\n",
      " <PERSON> It was the time of long delightful games ...\n",
      " <PERSON> And somewhat sour berries ...\n",
      " <PERSON> The time when you did everything \n",
      "\t Tokenized GPT2:you Ġcame Ġevery Ġsummer Ġto ĠBer ger ac . Ċ Ġ< PER SON > ĠYou Ġused Ġto Ġfashion Ġl ances Ġout Ġof Ġre eds Ġ... Ċ Ġ< PER SON > ĠThe Ġsilk Ġof Ġthe Ġt asse lled Ġcorn Ġfurnished Ġhair Ġfor Ġyour Ġdoll ... Ċ Ġ< PER SON > ĠIt Ġwas Ġthe Ġtime Ġof Ġlong Ġdelightful Ġgames Ġ... Ċ Ġ< PER SON > ĠAnd Ġsomewhat Ġsour Ġber ries Ġ... Ċ Ġ< PER SON > ĠThe Ġtime Ġwhen Ġyou Ġdid Ġeverything Ġ\n",
      "\t Tokenized no:youĠc ameĠ everyĠ summer ĠtoĠ Ber ger ac .ĊĠ < P ERS ON >Ġ YouĠ usedĠtoĠ fashionĠ l ancesĠ outĠofĠ re ed sĠ ...Ċ Ġ< P ERS ON >Ġ TheĠ sil kĠ ofĠtheĠt as sel ledĠ cor nĠ furn ishedĠ hairĠ forĠyourĠ doll ...Ċ Ġ< P ERS ON >Ġ ItĠ wasĠtheĠ timeĠofĠ longĠ delight fulĠ gamesĠ ...Ċ Ġ< P ERS ON >Ġ AndĠ somewhatĠ s ourĠ ber riesĠ ...Ċ Ġ< P ERS ON >Ġ TheĠ timeĠ whenĠyouĠ didĠ everythingĠ\n",
      "\t Unique Tokens GPT2: {'Ġpale', 'ĠHow', 'Ġwith', 'Ġuntil', 'Ġband', 'itting', 'Ġhand', 'Ġagain', 'Ġplay', 'Ġspoken', 'ome', 'Ġwhile', 'Ġfighting', 'Ġthis', 'Ġwhen', 'Ġill', 'Ġbefore', 'Ġas', 'Ġeverything', 'Ġreg', 'Ġhe', 'ple', 'Ġbears', 'Ġdi', 'Ġre', 'Ġlong', 'Ġfrom', 'Ġmis', 'ĠAh', 'cks', 'Ġmaternal', 'Ġhow', 'Ġfro', 'Ġvoice', 'ĠLet', 'Ġcorn', 'Ġamazed', 'Ġexactly', 'Ġnot', 'Ġ_', 'Ġcompany', 'Ġboy', 'ly', 'Ġthe', 'Ġm', 'Ġwhat', 'Ġcame', 'Ġcheering', 'ĠHe', 'Ġfor', 'ĠIt', 'Ġnaught', 'Ġserve', 'Ġif', 'Ċ', 'Ġsame', 'et', 'com', 'ĠH', 'Ġdays', 'ĠNo', 'Ġknown', 'Ġgreat', 'ĠThere', 'aug', 'Ġyet', 'Ġreleasing', 'Ġbreathed', 'ĠOh', 'ĠPort', 'Ġhurt', '>,', 'ĠThen', 'Ġheart', 'ĠIn', 'ings', 'Ġdone', 'Ġhis', '....', 'Ġyou', 'Ġfurnished', 'Ġto', 'Ġt', 'Ġstrange', 'ising', 'Ġseen', 'Ġwhom', 'Ġ\"', 'Ġhim', 'Ġhowever', 'Ġfun', 'Ġnear', 'Ġwho', 'Ġdare', 'Ġsmart', 'Ġtell', 'asse', 'ĠWhat', 'Ġashamed', 'Ġeach', 'Ġdown', 'ances', 'Ġwater', 'looking', 'SON', 'ĠYou', 'Ġout', 'Ġvent', 'Ġsour', 'y', 'Ġface', 'Ġnoble', 'lled', '>.', 'Ġspeak', 'ume', 'Ġclimb', 'Ġwithout', 'Ġtable', 'pping', 'Ġnever', 'Ġdistance', 'Ġdid', 'Ġcourage', 'Ġyoung', 'Ġhot', 'Ġevery', 'Ġcoincidence', 'Ġhas', 'Ġhair', 'ĠAnd', 'Ġit', 'Ġhard', 'Ġ...', 'Ġaway', 'Ġher', 'without', 'Ġshould', 'Ġsmiling', 'Ġplain', 'Ġfashion', 'Ġat', 'Ġmany', 'Ġwit', 'ĠTell', 'Ġpoor', 'Ġknow', 'Ġkeep', 'Ġcousin', 'that', 'Ġl', 'ily', 'Ġeyes', 'en', 'eds', 'ĠSometimes', 'ĠOnly', 'ĠBer', 'Ġstops', 'Ġpocket', 'Ġhandsome', 'Ġin', 'Ġ!', 'ries', 'Ġbeing', 'Ġstamp', 'Ġwearing', 'Ġtim', 'Ġand', 'Ġdraw', 'ĠYes', 'Ġgoing', 'ĠBut', 'Ġrunning', 'Ġsomewhat', 'Ġof', 'ĠNothing', 'Ġsay', 'Ġsummer', 'Ġread', 'Ġback', 'Ġcad', 'r', 'Ġsilk', 'Ġyourself', 'Ġdaring', 'Ġwere', 'Will', 'Ġsave', 'Ġleave', 'Ġ]', 'Ġcool', 'Ġgenius', 'Ġour', 'Ġpretty', 'Ġtakes', 'Ġpast', 'Ġsoon', 'Ġforehead', 'Ġhappen', 'ĠAren', 'Ġdo', 'aging', 'Ġwhere', 'owing', 'Ġinto', 'Ġb', '?\"', 'Ġmatter', 'Ġa', 'ĠSo', 'ĠThe', 'ĠA', 'Ġother', 'Ġthose', 'Ġpret', 'Ġperf', 'Ġber', 'Ġhave', 'ĠI', 'Ġblood', 'ĠWith', 'Ġtime', 'Ġlittle', 'amma', 'Ġglass', 'Ġmuch', 'Ġde', 'Ġloved', \"'t\", 'ĠN', 'Ġplaying', 'Ġnow', 'Ġdelightful', 'Ġcome', 'Sh', 'Ġproud', 'Ġmust', 'Ġused', 'Ġbleeding', 'Ġis', 'Ġthat', 'Ġlook', 'Ġyes', 'ade', 'Ġwash', 'Ġdoes', 'Ġbrave', '.]', 'ĠWas', 'ĠHere', 'ĠAs', 'hing', 'Ġhundred', 'Ġabout', 'Ġso', 'ands', 'Ġyour', 'She', 'Ġwould', 'Ġdoll', 'Ġ[', 'Ġlike', 'Ġgames', 'ting', 'PER', 'Ġwas', ',', 'Ġmy', 'Ġlove', 'Ġquite', 'Ġsomeone', 'Ġon', 'Ġsmall'}\n",
      "\t Unique Tokens no: {'soĠ', 'are', 'beingĠ', 'are,Ġ', 'past', 'sĠofĠ', 'akesĠ', '.ĠThereĠisĠ', 'aĠc', 'ill', 'greatĠ', 'ledĠ', 'bear', 'ingĠofĠ', 'sil', 'soon', 'inglyĠ', 'ber', 'himĠs', 'forĠyourĠ', 'forehead', 'ing_', 'ag', 'ed', 'day', 'hasĠ', '_Ġ', 'withĠourĠ', '\"', 'N', 'Hand', 'OnlyĠ', 'lovedĠ', 'Ber', 'In', 'heĠ', 'stopsĠ', 'didĠthisĠ', 'AĠ', 'pocket', 'exactlyĠ', 'ageĠ', 'WillĠ', '.ĠHeĠisĠ', 'bloodĠ', 'nearĠtheĠ', 'itĠis', 'tyĠ', 'hisĠhand', 'andĠ', ',ĠIĠ', 'some', 'in,Ġ', 'loveĠ', '[Ġ', 'And', 'hot', 'ing.Ġ', 'AndĠ', 'speak', 'ameĠ', 'shouldĠ', 'going', 'rock', 'willĠ', '.ĠIĠhaveĠ', 'happen', 'No', 'yourself', '...Ġ', 'yourĠ', 'hisĠhandĠ', 'love', 'complet', 'dist', 'ou', 'comeĠ', 'es.Ċ', 'sayĠ', 'timeĠofĠ', 'asham', 'ingĠhisĠ', 'didĠnotĠ', 'doneĠ', 'ance,Ġ', 'hairĠ', 'cour', 'whatĠisĠtheĠ', '?Ġ', 'ofĠ', 'ThenĠ', ',ĠthatĠ', 'knownĠasĠ', 'youĠhaveĠ', 'onĠhisĠ', 'hĠ', 'aboutĠit', 'ern', 'as', 'fashionĠ', 'wereĠ', '.ĠIĠ', ',ĠandĠ', 'withoutĠ', 'ble,Ġ', 'wasĠ', 're', 'amaz', 'dareĠ', 'plainĠ', ',Ġ', 'whomĠ', 'outĠofĠ', 'YouĠwereĠ', 'reg', 'thatĠ', 'furn', 'dar', 'drawĠ', 'chiefĠ', 'HeĠ', 'IĠwas', 'knowĠ', 'now', '.ĠItĠ', 'youĠ', 'br', 'it!Ġ', 'serveĠ', 'befor', 'no', 'ancesĠ', 'AsĠ', 'glas', 'ĠtellĠ', '.ĠButĠ', 'ad', 'ampĠ', \"en'tĠ\", '>Ġ', 'ĠtoĠtellĠ', 'aveĠ', 'hand,Ġ', 'esĠtoĠ', ',ĠthisĠ', 'co', 'ing', 'm', 'leaveĠ', 'Noth', 'isĠ', 'ĠtooĠmuch', 'him', 'prettyĠ', 'summer', 'handĠ', ']Ġ', 'withĠherĠ', 'ingsĠ', 'sel', 'doll', 'ĠthisĠ', 'IĠhaveĠ', 'ey', 'umeĠ', 'IĠ', 'young', '.ĊĠ', 'mis', 'isĠitĠ', 'Oh,Ġ', 'notĠaĠ', 'bleed', 'b', 'e,Ġ', 'SoĠ', '>Ġ<', 'smart', '!Ġ', 'Port', 'everyĠ', 'neverĠseenĠ', 'usedĠtoĠ', 'ButĠ', 'how', 'iusĠ', 'W', 'somewhatĠ', 'smil', 'ence', 'hard', 'fight', 'doesĠitĠ', 'sittingĠ', 'augh', 'myĠ', 'haveĠ', 'etim', 'ishedĠ', 'h', 'youĠneverĠ', 'ly,Ġ', 'ri', 'matter', 'pr', 'however', 'al', 'you,Ġ', 'Yes', 'es,Ġ', 'delight', 'ing,Ġ', 'howĠmanyĠ', 'laugh', 'withĠtheĠ', 'ĊĠ', 'deĠ', 'longĠ', 'f', 'whenĠyouĠ', 'YouĠ', 'sĠ', 'it,Ġ', '.ĠO', 'doesĠnotĠ', 'ofĠtheĠ', 'hisĠheartĠ', 'inĠthoseĠ', 'SheĠ', 'am', 'incid', 'ing!Ġ', 'A', 'ĠtoĠ', 'me,Ġ', 'edĠinĠ', 'wearingĠ', 'gamesĠ', '.ĠHowĠ', 'T', 'w', 'whileĠ', 'ris', 'eachĠother', 'down', 'cous', 'ofĠtheĠt', 'ingĠtheĠ', 'inĠyourĠ', 'ma', 'smallĠ', 'must', 'asĠIĠ', 'di', 'ourĠ', 'releas', '.ĠAr', 'ifĠheĠ', 'Som', 'cheer', 'neverĠ', 'face.Ċ', 'h,Ġ', 'keepĠ', 'atĠtheĠ', 'wasĠtheĠ', '...Ċ', 'whoĠ', 'inĠtheĠsameĠ', 'You', 'someoneĠ', 'funĠ', 'ableĠandĠ', 'yes,Ġ', 'ItĠ', 'untilĠ', 'aĠhund', 'ed?Ġ', '_t', 'enĠ', '..', 'lookĠatĠ', ',ĠIĠwouldĠ', '.ĠWhatĠ', 'WhatĠ', 'ERS', 'st', '_S', 'shortĠ', 'withĠ', 'pal', 'cor', 'red', 'kĠ', 'aĠt', 'inĠhisĠ', 'everythingĠ', 'runn', 'ily,Ġ', 'itĠ', 's?', 'adeĠ', 'pp', 'ingĠtoĠ', '?\"Ġ', 'look', 'mat', 'play', 'ON', 'boyĠ', 'ĠtheĠ', 'breath', 'nowĠ', 'No,Ġ', 'climb', 'intoĠaĠ', 'company', 'strangeĠ', 'itĠisĠ', 'poorĠ', 'spoken', 'ing.Ċ', 'it', 'IĠdoĠ', '>,Ġ', 'd,Ġ', 'yetĠ', 'prett', 'coolĠ', 'timeĠ', 's,Ġ', 'didĠ', 'fromĠaĠ', 'eĠtim', 'water', 'voiceĠ', 'ellĠmeĠ', 'aĠlittleĠ', 'nĠ', 'notĠ', 'downĠatĠ', 'quiteĠ', 'itĠagain', 'Ah', '.ĠHereĠ', 'With', 'vent', 'gen', 'backĠ', 'eĠ', 'likeĠ', 'av', 'youĠc', 'ĠwouldĠ', 'n', 'et,Ġ', 'TheĠ', 'band', 'short', 'riesĠ', 'ed_', 'whatĠ', 'P', 'someĠ', 'awayĠ', '.ĠItĠisĠ', 'omeĠ', 'LetĠmeĠ', 'youĠwereĠ', 'IĠhaveĠreadĠ', 'e,ĠandĠ', 'atĠ', 'HereĠ', 'ingĠherĠ', 'playingĠ', 'perf', 'd', 'yourĠm', 'And,Ġ', 'meĠ', 'YouĠknow', '.ĠYou', 'hurtĠ', 'fulĠ', 'where'}\n",
      "Text 2: lui, c'est plus _select_ !\n",
      " <PERSON>\n",
      " Comment ?\n",
      " LE MERLE\n",
      " Comment ?Oui, tu comprends, il maintient à distance\n",
      " Tous les petits oiseaux dénués d'importance.\n",
      " Les parents pauvres, ça fait mal dans un salon.\n",
      " CHANTECLER\n",
      " Le jour de la Pintade !\n",
      " PATOU, flegmatique.\n",
      " Le jour de la Pintade !Une bulle !\n",
      "\n",
      "\t Tokenized GPT2:l ui , Ġc ' est Ġplus Ġ_ select _ Ġ! Ċ Ġ< PER SON > Ċ ĠComment Ġ? Ċ ĠLE ĠMER LE Ċ ĠComment Ġ? O ui , Ġtu Ġcomp rend s , Ġil Ġmain t ient ĠÃł Ġdistance Ċ ĠT ous Ġles Ġpet its Ġo ise aux ĠdÃ© nu Ã©s Ġd ' import ance . Ċ ĠLes Ġparents Ġp au v res , ĠÃ § a Ġf ait Ġmal Ġd ans Ġun Ġsal on . Ċ ĠCH ANT E CL ER Ċ ĠLe Ġjour Ġde Ġla ĠP int ade Ġ! Ċ ĠPAT OU , Ġf leg mat ique . Ċ ĠLe Ġjour Ġde Ġla ĠP int ade Ġ! U ne Ġbul le Ġ! Ċ\n",
      "\t Tokenized no:lu i,Ġ c ' estĠ plusĠ _ select _Ġ ! ĊĠ < P ERS ON >ĊĠ Com mentĠ ? ĊĠ LEĠ MER LE ĊĠ Com mentĠ ? O ui ,Ġt u Ġcom pr end s,Ġ il Ġm aint ientĠ ÃłĠ distance ĊĠ T ousĠ lesĠ pet itsĠ o ise au xĠ d Ã© nu Ã© sĠ d' import anc e.Ċ Ġ L esĠ parentsĠ pau v res,Ġ Ã§ aĠf aitĠ malĠ d ansĠ un Ġsal on .ĊĠ CH AN TE CL ER ĊĠ L eĠ j ourĠ deĠlaĠ P int adeĠ ! ĊĠ P AT O U ,Ġ f leg mat iqu e.Ċ Ġ L eĠ j ourĠ deĠlaĠ P int adeĠ ! Un eĠ bul leĠ !Ċ\n",
      "\t Unique Tokens GPT2: {'Ġfur', 'ens', 'ds', 'as', 'Ġcontinu', '.', 'ĠLE', 'Ġmast', 'ĠCH', 'Ġdis', 'its', 'ĠIl', 'Par', 'Le', 'ĠMer', 'Ġcomm', 'ĠAh', 'Ġch', 'Que', 'Ġmis', 'Ġv', 'ĠComment', 'ĠPAT', 'Ġpou', 'Ġcoup', 'Ġver', 'Ġ_', 'Ġprof', 'Ġf', 'ains', 'Ġper', 'Ġ?', 'Ġp', 'Ġdes', 'Ċ', 'Ġet', 'une', 'Ġbec', 'Ġil', 'est', 'ĠÃł', 'M', 'ors', 'oin', 'Ġfe', 'Ġse', 'OU', '§', 'ĠOh', '>,', 'Ġaut', 'at', '....', 'Ġpr', 'Ġpas', 'Ġto', 'Ġacc', 'Ġpet', 'Ġmet', 'Ġparents', 'Ġplus', 'Ġren', 'Ġque', 'asse', 'Ġcr', 'Ġce', 'it', 'Ġ:', 'y', 'Ġvide', 'SON', 'ĠCe', 'ien', '>.', 'Ġbrill', 'Ġregard', 'ĠÂ»', 'ile', 'Ġj', 'Ġaff', 'ĠC', 'Ġdistance', 'ĠEt', 'ge', 'ir', 'Ġnar', 'Ġune', 'ANT', 'Ġn', 'ĠT', 'Ġdu', 'ance', 'ĠÃ', 'Ġ...', 'ĠD', 'Ġte', 'Ġun', 'ient', 'Ġdress', 'our', 'tre', 'ris', 'ied', 'Ġbul', 'und', 'ime', 'Ġgl', 'Ġl', 'ure', 'ier', 'ĠAl', 'Ġpour', 'Ġles', 'Ġmo', 'Ġsurprise', 'ĠUn', 'Ġcomp', 'Ġdon', 'Ġgrand', 'te', 'Ġfall', 'Ã©s', 'q', 'ine', 'Ġfive', 'To', 'Ġo', 'ait', 'Ġ!', 'que', 'tend', 'ue', 'ind', 'Ġqu', 'rend', 'Ġye', 'Ġball', 'ĠÂ«', 'Comment', 'Ġdom', 'Ġim', 'ress', 'ique', 'out', 'Ġau', 'and', 'acon', 'rix', 'quet', 'Ġdire', 'Ġje', 'Ġcol', 'ĠQue', 'Ã´', 'Ġen', 'Ġtu', 'Tu', 'Ġvo', 'ĠLes', 'Ġest', 'Ġmar', 'Ġmat', 'Ġfol', 'ĠPe', 'Ġfort', 'Non', 'ĠP', 'Ġb', 'Ġall', 'ĠTu', 'Ã©e', 'ĠÃ©', 'tu', 'Ġd', 'Ġa', 'ite', 'Ġr', '«', 'Ġmain', 'Ġy', 'Ġde', 'Ġon', 'ffe', 'Ġdess', 'ĠN', 'Ġmal', 'Ġcl', 'Ġvas', 'ette', 'ĠO', 'nes', 'ĠCo', 'ade', 'ĠdÃ©', 'Il', 'Ġsix', 'Ġent', 'On', 'Ġla', 'Ġbout', 'Ġme', 'Ġpend', 'Ġle', 'Ġso', 'Ġche', 'ĠTi', 'Ġmes', 'Ġfa', 'ĠMER', 'Ġqui', 'Ġva', 'Ġbl', 'ĠLe', 'PER', ',', 'Ġjour', 'Â', 'ille', 'Ġton', 'oi', 'ois', 'ĠOn', 'Ġadmire'}\n",
      "\t Unique Tokens no: {'bul', 'etĠ', 'OnĠ', 'vo', 'Ti', 'hĠ', \"d'\", 'C', 'dres', 'per', 'dis', 'adeĠ', 'ceĠ', 'fortĠ', 'mu', 'fiveĠ', 'pr', 'surprise', 'uxĠ', 'desĠ', 'gl', 'cl', 'pau', 'mesĠ', 'ail', 'aĠb', ',Ġt', 'p', 'inĠ', 'uneĠ', ',ĊĠ', 'iteĠ', 'lĠ', ',Ġn', 'ON', 'xĠ', 'ors,Ġ', 'mast', 'pĠ', 'Ã§', 'pend', 'u,Ġ', 'lu', 'aut', 'diĠ', ',Ġ', 'verĠ', 'sa', 'ĊĠ', 'deĠ', 'cr', 'le,Ġ', 'malĠ', 'attend', 'continu', '_Ġ', 'com', ':Ġ', 'ire', 'dsĠ', 'MER', 'f', 'fur', 'jour', 'N', 'in.', 'surpriseĠ', 'olĠ', 'regard', 'aĠm', '.ĠAl', 'sĠ', 'pi', 'p,Ġ', 'pass', 'i,Ġ', 'c', 'aint', 'etteĠ', 'ye', 'uĠ', 'iqu', '>ĊĠ', 'leĠ', '.L', 'AN', 'queĠ', 'distance', 'br', 'dom', 'acc', 'endĠ', 'it-', 'imeĠ', 'LEĠ', 'TE', 'arĠ', 'A', 'seĠ', 'erĠ', 'asĠ', 'met', '>,Ġ', 'ierĠ', 'andĠ', 'bl', 'fa', 'ad', 'anceĠ', 'ueĠ', 'ent', 'ren', 'Ã©', 'er.', 'anc', 'eĠto', 'ientĠ', 'edĠ', '>Ġ', 'atis', 'ant,Ġ', 'pour', 'Â«', 'ilĠ', 'onĠaĠ', 'parentsĠ', 'fre', 's,Ġ', 'imit', \"o'\", 'arch', 'vasĠ', 'res,Ġ', 'ass', 'T', 'vaĠ', 'onĠ', 'dri', 'ball', 'reĠ', 'nĠ', 'mentĠ', 'isĠ', 'ixĠ', 'I', '-t', 'Ġcom', 'aĠdeĠ', '.ĠOn', '´', 'mo', 'OhĠ', 'j', 'bec', 'Com', 'Pe', 'di', 'lun', 'Ġs', 'ourĠ', 'vi', 'zĠ', '!', 'pet', 'ff', 'eĠ', 'illeĠ', 'lo', 'grandĠ', 'ousĠ', '.ĊĠ', 'r', 'oul', 'qĠ', 'itsĠ', 'laĠ', 'ri', 'dess', 'ouĠ', 'geĠ', 'auĠ', '...Ġ', 'b', 'irĠ', 'ineĠ', 'n', 'yĠ', 'CH', 'iĠ', 'e,Ġ', 'so', 'jeĠ', 'cha', 'con', 'aitĠ', 'outĠ', 'plusĠ', '!Ġ', 'Co', '?', '...Ċ', 'L', 'ar', 'all', 'fall', 'on,Ġ', 'Q', 'P', '-l', 'AT', 'Â»', 'o', 'cin', 'deĠlaĠ', ':ĊĠ', 'antĠ', 'Ġ', 'enĠ', 'ensĠ', 'tr', 'don', 'ain', \"l'\", 'lesĠ', 'unĠ', 'ÃłĠ', 'esĠ', 'fol', 'ERS', 'fe', 'ureĠ', 'tĠ', 'Et', '.To', 'meĠ', 'Ã©t', 'aĠf', 'duĠ', 'aĠt', 'Mo', 'aĠ', 'estĠ', 'Mer', 'e.Ċ', 'vid', 'profit', 's...', 'af', 'ansĠ', 'adm'}\n",
      "==Same Author==\n",
      "Text 1: Jubal [...] past, MS; _AM_ does not indent line  \n",
      " 64c | beside?] beside – _MS_  \n",
      " 64d | god –] god, _MS, AM, MM_  \n",
      " 64e | skies –] skies, _MS_  \n",
      " 64f | aught else for] as other _MS, AM, MM_  \n",
      " 64g | _her]_ her _MS_ , _AM_ , _MM_  \n",
      " 64h | Buried within] And buried in _MS_  \n",
      " 64i | may sleep] that li\n",
      "\t Tokenized GPT2:J ub al Ġ[...] Ġpast , ĠMS ; Ġ_ AM _ Ġdoes Ġnot Ġindent Ġline ĠĠĊ Ġ64 c Ġ| Ġbeside ? ] Ġbeside ĠâĢĵ Ġ_ MS _ ĠĠĊ Ġ64 d Ġ| Ġgod ĠâĢĵ ] Ġgod , Ġ_ MS , ĠAM , ĠMM _ ĠĠĊ Ġ64 e Ġ| Ġskies ĠâĢĵ ] Ġskies , Ġ_ MS _ ĠĠĊ Ġ64 f Ġ| Ġaug ht Ġelse Ġfor ] Ġas Ġother Ġ_ MS , ĠAM , ĠMM _ ĠĠĊ Ġ64 g Ġ| Ġ_ her ] _ Ġher Ġ_ MS _ Ġ, Ġ_ AM _ Ġ, Ġ_ MM _ ĠĠĊ Ġ64 h Ġ| ĠBur ied Ġwithin ] ĠAnd Ġburied Ġin Ġ_ MS _ ĠĠĊ Ġ64 i Ġ| Ġmay Ġsleep ] Ġthat Ġli\n",
      "\t Tokenized no:J ub alĠ [... ]Ġ past ,ĠM S ;Ġ _ AM _Ġ doesĠnotĠ ind entĠ lineĠ Ġ ĊĠ 64 cĠ |Ġ bes ide ? ]Ġ besideĠ âĢĵĠ _ MS _ ĠĠ ĊĠ 64 dĠ |Ġ godĠ âĢĵ ]Ġ god ,Ġ_ MS ,Ġ AM ,ĠM M_ ĠĠ ĊĠ 64 eĠ | Ġsk iesĠ âĢĵ ] Ġsk ies,Ġ _ MS _ ĠĠ ĊĠ 64 fĠ |Ġ aughtĠ elseĠ for ]Ġ asĠ otherĠ _ MS ,Ġ AM ,ĠM M_ ĠĠ ĊĠ 64 gĠ |Ġ _ her ] _Ġ herĠ _ MS _Ġ ,Ġ_ AM _Ġ ,Ġ_ MM _ ĠĠ ĊĠ 64 hĠ |Ġ Bur i edĠwith in ]Ġ AndĠ buri edĠinĠ _ MS _ ĠĠ ĊĠ 64 iĠ | ĠmayĠ sleep ] ĠthatĠ li\n",
      "\t Unique Tokens GPT2: {'Ġslope', 'uty', 'Ġmen', 'ĠFrance', 'ct', 'Ġcorrected', 'Ġduty', 'Ġ[...]', 'Ġ65', 'Ġby', 'France', 'Ġas', '.', 'tree', 'ĠMaria', 'ip', 'ĠâĢĵ', 'ĠJ', 'ĠHoly', 'al', 'Ġhollow', 'ĠThis', 'Ġnot', 'Ġ_', 'Ġboy', 're', 'ĠCross', 'ĠMS', 'Ġthe', 'Ġeditions', 'Ġhead', 'Ġfor', 'Ġ{', 'ĠMonth', 'Ċ', 'anta', 'Ġdetails', 'f', 'Ġreferred', 'Ġ75', 'Ġ,', '>,', 'ĠBoy', 'eps', 'ĠOld', 'Ġto', 'c', 'Ġelse', 'Ġsouls', 'ĠBur', 'Ġskies', 'Ġcows', 'Ġwithin', 'Ġsouth', 'B', 'Ġright', 'Ġno', 'SON', 'Ġrespectively', 'Ġindent', 'Ġâ', ':]', 'ged', 'ened', 'Ġmelody', 'Ġafter', 'ks', 'jar', 'Ġstorm', 'Ġ67', 'Ã¤r', 'Ġmay', 'ent', 'ĠWorld', 'world', 'Ġline', 'lic', 'â', 'Ġbelow', 'Ġ64', 'Ġshows', 'th', 'Ġhas', 'Ġaug', 'l', 'means', 'ĠAnd', ';]', 'Ġcompared', 'both', 'Ġare', 'ĠRh', 'ĠD', 'Ġher', 'Ġ;', 'Ġhear', \">'\", ')', 'ied', 'm', 'ht', 'Ġold', 'Ġfruit', 'that', 'Ġeyes', 'Ġobvious', 'ĠOpen', 'Of', 'Ġmistake', 'Ġste', 'Ġlie', 'ĠThat', 'j', 'Ġbeen', 'Ġin', 'Ġpl', 'Ġ/', '71', 'Ġburied', 'Ġ18', 'Ġsoul', 'Ġand', 'ĠSee', 'December', 'Ġ#', 'antic', 'ĠMrs', 'text', 'rose', 'Ġpoverty', 'Ġof', 'Ġcopy', 'ĠMel', 'ĠMary', 'Ġabove', 'ĠM', 'At', \"Ġ'\", 'b', 'ĠF', 'Ġrose', 'Ġgive', '78', 'ĠLove', 'Ġ<', 'Ġpast', 'Ĩ', 'Ġgate', 'Ġnotes', 'e', 'ĠP', 'ĠAM', '_.', '}', 'Ġh', 'Ġa', 'Ġother', 'ĠThe', 'Ġedition', '.\"', 'Ġ73', 'Ġ|', 'Ġown', 's', 'Ġgirls', 'note', 'Ġlow', 'Ġis', 'Ġgave', 'Ġthat', 'ĠJanuary', 'ĠSan', 'agues', 'Ġbeside', 'Ġdoes', 'd', 'Ġsleep', '.]', 'That', 'Ġgrave', 'ched', 'Ġhill', 'Ġwoods', 'Ġ---', 'ĠĠĊ', 'Ġgod', 'ĠMM', 'Ġomn', 'Ġ[', 'ot', 'PER', 'Ġlove', 'Ġcottage', 'Ġtree', 'Ġon', 'Ġ74'}\n",
      "\t Unique Tokens no: {'âĨ', 'gaveĠ', 'ate', 'past', 'Mel', 'Mr', 'hĠ', '-h', 'lieĠ', 'Ġco', 'Ġcorrect', 'atch', 'ĠthatĠ', '-w', 'taĠ', '}Ġ', '#Ġ', '8Ġ', 'low', 'giv', '64', 'OfĠ', \"'ĠĠ\", '.Ġ', 'FranceĠ', 'outh', 'alĠ', 'buri', 'toĠ', 'Mari', 'onĠtheĠ', 'ĠwithinĠ', 'her,Ġ', 'extĠ', 'ĠmayĠ', 'tt', 'th,Ġ', 'inĠ', 'odyĠ', 'own', 'meansĠ', 'lĠ', 'es,Ġ', 'ON', ',Ġ_', 'bĠ', 'Ġm', 'edition', 'ag', 'Bur', 'withĠtheĠ', 'boyĠ', 'ĠtheĠ', 'e,', ',Ġ', 'ĊĠ', 'notesĠ', 'est', '_Ġ', 'M_', 's]Ġ', 'aughtĠ', 'edĠtoĠ', 'M', 'ĠwhichĠ', 'cĠ', 'Pin', 'sleep', \"'Ġ\", ':', 'herĠ', 'sĠ', 'potent', 'stormĠ', 'dut', ',ĠC', 'Month', 'girl', 'elseĠ', 'ste', 'it,Ġ', 'noteĠ', 'doesĠnotĠ', ']ĠĠ', '>ĊĠ', 'comparedĠ', 'ĠshowsĠ', '.ĠSeeĠ', 'head', 'ide', '[...', '[', 'ageĠ', 'Atl', 'mistakeĠ', 'LoveĠ', 'Dut', 'y', 'andĠtheĠ', 'F', 'SantaĠ', 'loveĠandĠ', 'detail', 'asĠ', '67', '>,Ġ', 'eĠt', 'andĠ', 'ros', 'thĠ', \"y'sĠ\", 'y,', 'rel', 'soul', 'giveĠ', '9Ġ', ',ĠM', 'e-', 'everĠ', 'edĠinĠ', 'in,Ġ', 'loveĠ', '.\"Ġ', 'anc', 'edĠ', '>Ġ', 'ep', 'ni', 'editionĠ', 's_', 'World', 's,Ġ', 'pl', 'pot', 'AndĠ', 'aboveĠ', '186', 'anticĠ', 'ksĠ', 'ow', 'ues', 'JanuaryĠ', 'mĠ', 'referredĠtoĠ', 'isĠ', '-t', '75', 'fru', 'Ã¤', 'MaryĠ', 'pe', 'forĠ', 'hasĠnoĠ', 'girlsĠ', 'ThisĠ', 'DecemberĠ', '.Ġ_', 'ind', ']Ġ', '7', 'Ġs', 'wardĠ', 'OldĠ', 'worldĠ', 'hol', 'agu', \"en'sĠ\", 'in', 'edg', 'y.Ġ', 'hasĠbeenĠ', 'eĠ', 'povert', 'old', 'el', 'lineĠ', 'genĠ', 'belowĠ', 'reeĠ', '.ĊĠ', 'gĠ', 'r', 'cott', 'right', 'HolyĠ', 'ownĠ', 'dĠ', 'se-', 'om', 'ies,Ġ', 'Bo', 'age', 'ineĠ', 'for', 'iesĠ', 'iĠ', ';Ġ', 'byĠtheĠ', 'eyes,Ġ', 'slo', 'love', 'Rh', 'aj', 'TheĠ', 'storm', 'e.', 's,', 'copy', 'fruit', 'se', 'ar', '-th', '73', 'hungĠ', 'P', 'Ben', 'e.Ġ', 'Ġc', 'age,Ġ', 'bes', 'hill', 'bothĠ', 'hear', '/Ġ', 'Ġ', 'ĠĠ', 'ThatĠ', 'Ġsk', 'sĠofĠtheĠ', 'eyes', '|Ġ', 'âĢĵĠ', 'entĠ', 'jĠ', 's]', 'otherĠ', '1)Ġ', 'ERS', '65', \"'sĠ\", 'ree', 'al_', '187', 'whichĠareĠ', 'afterĠ', 'tĠ', 'ic', 'besideĠ', 'respectivelyĠ', 'godĠ', 'hillĠ', 'edĠwith', '4Ġ', 'kĠ', ',ĠtheĠ', 'a,', 'obviousĠ', 'aĠ', 'ro', 'grav', 'god', 'fĠ', 'ĠtheirĠ', 'inĠtheĠ', 'ĠandĠ'}\n",
      "Text 2: fields,\n",
      " Instead of sipping at the heart of flowers,\n",
      " Poising in sunshine, fluttering towards its bride,\n",
      " Should fast and speculate, considering\n",
      " What were if it were not? or what now is\n",
      " Instead of that which seems to be itself?\n",
      " Its deepest wisdom surely were to be\n",
      " A sipping, marrying, blue-winge\n",
      "\t Tokenized GPT2:fields , Ċ ĠInstead Ġof Ġs ipping Ġat Ġthe Ġheart Ġof Ġflowers , Ċ ĠPo ising Ġin Ġsunshine , Ġfl uttering Ġtowards Ġits Ġbride , Ċ ĠShould Ġfast Ġand Ġspec ulate , Ġconsidering Ċ ĠWhat Ġwere Ġif Ġit Ġwere Ġnot ? Ġor Ġwhat Ġnow Ġis Ċ ĠInstead Ġof Ġthat Ġwhich Ġseems Ġto Ġbe Ġitself ? Ċ ĠIts Ġdeepest Ġwisdom Ġsurely Ġwere Ġto Ġbe Ċ ĠA Ġs ipping , Ġmar rying , Ġblue - wing e\n",
      "\t Tokenized no:field s, ĊĠ In steadĠofĠ sip pingĠ atĠtheĠ heartĠofĠ flow ers ,ĊĠ Po is ingĠinĠ sun sh ine,Ġ flutter ingĠtowardsĠ itsĠ br ide ,ĊĠ ShouldĠ fastĠandĠ specul ate,Ġ consider ing ĊĠ WhatĠ wereĠ ifĠitĠ wereĠ not ?Ġ orĠwhatĠ nowĠ is ĊĠ In steadĠ ofĠthatĠ whichĠ seemsĠtoĠbeĠ itself ? ĊĠ ItsĠ deep estĠ wis dom Ġs urelyĠ wereĠtoĠ be ĊĠ AĠs ipp ing,Ġ m arry ing,Ġ blue -w ing e\n",
      "\t Unique Tokens GPT2: {'og', 'Ġfigured', 'Ġbride', 'ĠWere', 'Ġreach', 'ĠHas', '.', 'ara', 'Ġpattern', 'Ġsunshine', 'Ġhuman', 'Ġphilos', 'ipping', 'Ġdance', 'ly', 'ĠPhys', 'ed', 'Ġmother', 'Ġpity', 'Ġhold', 'fields', 'Ġworthy', 'Ġmusic', 'ĠG', 'Ġnote', 'Ġ\"', 'ĠWhat', 'Ġhint', 'Ġright', 'ĠMaking', 'ĠWhy', 'Ġrelative', 'ĠPers', 'Ġfast', 'Ġwithout', 'Ġits', 'Ġresist', 'Ġword', 'ĠBeing', 'Ġsharp', 'akes', 'ocus', 'ĠThat', 'Ġpun', 'isting', 'rem', 'Ġsp', 'Ġlaw', 'Ġliving', 'Ġsort', 'Ġprotest', 'Ġneed', 'Ġconversation', 'ĠPo', 'ĠAnalysis', 'wise', 'Ġthree', 'Ġneeds', 'Ġmust', 'Ġspec', 'ĠFor', 'Ġintense', 'ĠWr', 'inate', 'Ġis', 'ĠWould', 'ade', 'hip', 'Ġrooted', 'Ġproperty', 'ĠReason', 'Ġprejudice', 'Ġwould', 'Ġlove', 'ĠIf', 'Ġwisdom', 'Ġspark', 'Ġthinking', 'Ġfigure', 'Ġdis', 'Ġfrom', 'Ġpassing', 'ĠWho', 'Ġache', 'Ġfood', 'Ġman', 'Ġgrasp', 'Ġsupposed', 'Ġanalysis', 'ĠSince', 'Ġregul', 'ĠNo', 'Ġknown', 'ors', 'Ġbackward', 'ĠIts', 'bling', 'Ġexistence', 'Ġwho', 'ĠOb', 'Ġbe', 'Ġnegative', 'ĠWe', 'Ġtowards', 'enum', 'Ġequivalence', 'istically', 'Ġshow', 'no', 'ĠSaid', 'Ġmeanwhile', 'Ġwants', 'Ġmolecules', 'Ġdoctrine', 'Ġdecimal', 'Ġname', 'Ġan', 'Ġjoy', 'Ġn', 'Ġfilled', 'Ġun', 'Ġworse', 'Ġphrase', 'ĠWill', 'ig', 'Ġcalling', 'Ġbreast', 'Ġjust', 'ĠShould', 'Ġadd', 'ĠTe', 'Ġut', 'most', 'against', 'Ġbasis', 'Ġ<', 'ĠWhatever', 'Ġh', 'Ġstrict', 'ĠI', 'Ġprayer', 'Ġmake', 'Ġnow', 'Ġcome', 'borne', 'Ġrhetoric', 'Ġfelt', 'Ġbl', 'less', 'PER', 'Ġtheir', 'Ġtrue', 'Ġmy', 'Ġfine', 'Ġflowers', 'Ġobjects', 'othy', 'Ġuse', 'ĠWhich', 'Ġby', 'Ġas', 'Ġhe', 'Ġeasy', 'ning', 'Ġnot', 'Ġshown', 'Ġmetaph', 'Ċ', 'Ġthin', 'Ġpersistence', 'Ġstand', 'ĠIn', 'ĠOur', 'Ġwill', 'Ġyou', 'Ġto', 'clusions', 'ux', 'Ġput', 'Ġmight', 'ĠBur', 'uttering', 'cing', 'aching', 'Ġstart', 'Ġlips', 'Ġdown', 'SON', 'Ġabsolute', 'ĠGood', 'Ġdark', 'ental', 'ying', 'Ġinfl', 'Ġearn', 'ĠPri', 'Ġhas', 'ĠAnd', 'Ġit', 'Ġsyll', 'Ġreached', 'ĠOf', 'Ġbutter', 'Ġinsight', 'Ġat', 'Ġsurely', 'Ġgrace', 'Ġfr', 'Ġwhich', 'Ġin', 'Ġand', 'Ġspare', 'ishing', 'rying', 'The', 'Ġof', 'Ġsay', 'led', 'Ġchar', 'Ġcarries', 'Ġweight', 'ates', 'ulate', 'Ġmar', 'Ġstays', 'Ġitself', 'Ġwhere', 'hers', 'Ġinto', '>', 'Ġall', 'Ġsign', 'Ġanalog', 'Ġa', 'Ġother', 'ĠA', 'ĠThe', 'Ġthose', 'ĠFinal', 'umps', 'Ġrelation', 'ĠWith', 'Ġagree', 'Ġany', 'ĠAn', 'ĠFrom', 'Ġwe', 'Ġbubble', 'Ġimages', 'ĠNor', '-', 'hing', \"'\", 'Ġdelay', 'Ġshrink', 'Ġocean', 'Ġcluster', 'Ġblown', 'Ġon', 'ĠOn', 'ĠBy', 'Ġconsidering', 'Ġwith', 'Ġband', 'Ġbru', 'ĠCon', 'Ġblue', 'ĠNot', 'Ġflux', 'Ġstandard', 'Ġill', 'ip', 'ĠAll', 'ĠâĢĵ', 'Ġone', 'ĠLa', 'Ġdemands', 'Ġalphabet', 'Ġdot', 'ĠThis', 'ĠFed', 'Ġthe', 'Ġfor', 'Ġphysic', 'ovable', 'Ġif', 'Ġsees', 'ives', 'Ġdays', 'Ġdimin', 'Ġdeepest', 'Ġstanding', 'ĠE', 'Ġlooking', 'Ġacted', 'Ġheart', 'Ġconclusions', 'Ġthrough', 'Ġpain', 'Ġseems', 'ogene', 'ising', 'Ġhom', 'Ġchoice', 'Ġroom', 'ĠPut', 'Ġwithin', 'Ġmeet', 'ĠMay', 'ac', 'Ġno', 'Ġsucker', 'eless', 'ĠChurch', 'Ġmore', 'Ġir', 'Ġbubbles', 'There', 'ĠInstead', 'ĠT', 'Ġher', ';', ')', 'Ġ(', 'Ġfind', 'Ġknow', 'Ġbrings', 'I', 'Ġfellows', 'Ġbut', 'Ġbar', 'Ġbent', 'Ġpl', 'Ġqu', 'Ġbounds', 'Ġmissing', 'Ġtaking', 'Ġlate', 'ĠBut', 'Ġcrow', 'Ġstrengthe', 'Ġsac', 'Ġwere', 'Ġdeep', 'Ġsave', 'Ġcontent', 'Ġor', 'Ġarguments', 'Ġ125', 'Ġloaded', 'wing', 'Ġstruggle', 'Ġdifference', 'Ġfl', 'Ġfirst', 'Ġthat', 'Ġscale', 'Ġmetaphor', 'Ġcarry', 'ĠSuch', 'Ġspeculation', 'Ġyour', 'Ġsolely', 'Ġletters', 'ĠIll'}\n",
      "\t Unique Tokens no: {'noĠ', 'etĠ', 'letter', 'figureĠ', 'aĠc', 'dis', '-w', 'worth', 'ill', 'ofĠthatĠ', 'ledĠ', 'log', 'consider', 'ingĠofĠ', 'pre', 'toĠ', 'inglyĠ', 'ĠtoĠbeĠ', 'icallyĠ', 'Po', 'bĠ', 'steadĠ', 'day', 'Bur', 'hasĠ', 'ofĠyourĠ', 'phrase', 'suck', 'WhoĠ', ':Ġ', 'liv', 'allĠ', 'cĠ', 'Mak', '\"', 'N', 'flu', 'Ġcon', 'les', 'rhetor', 'ereĠ', 'In', 'erĠofĠ', 'gras', 'shownĠ', 'shr', 'spark', 'wis', 'AĠ', 'room', 'ingĠinĠ', 'forĠmyĠ', 'lessĠ', 'Wh', 'entalĠ', 'oc', 'andĠ', 'enumĠ', 'ItsĠ', 'gr', 'joy', 'ent', ',ĠIĠ', 'strength', 'jud', 'eĠtoĠ', 'is', 'AndĠ', 'sharpĠ', 'standĠ', 'bb', 'BeingĠ', 'phys', 'Pri', 'sun', 'analysis', 'If', 'find', 'wardĠ', 'conversation', 'sĠyouĠ', ';ĊĠ', 'sip', 'ShouldĠ', 'us-', 'yingĠ', 'filledĠ', 'urelyĠ', 'dĠ', 'say,Ġ', 'useĠ', 'isĠheĠ', 'yourĠ', 'anyĠ', 'love', 'NotĠ', 'For', 'all', 'differenceĠ', 'comeĠ', 'butĠ', ',Ġ\"', 'againstĠ', 'space', 'qu', 'ThatĠ', 'with', 'esĠ', 'ice', 'Analys', 'Ġthink', 'bet', '?Ġ', 'dimin', 'ofĠ', 'field', 'weight', ',ĠthatĠ', 'AĠc', 'ingĠthroughĠ', 'ule', 'hint', 'ingĠ', 'exist', 'specul', 'OnĠ', 'cu', 'ableĠ', 'edĠatĠ', 'showĠ', 'ationĠonĠ', 'as', '125', 'wereĠ', 'La', 'intoĠ', 'ut', 'withoutĠ', 'ofĠtheirĠ', 'firstĠ', 'flyĠ', 'butter', 'insightĠ', 'regul', ',ĊĠ', 'oceanĠ', 'graceĠ', 'weĠ', ',Ġ', 'isingĠ', 'orĠanyĠ', 'ine,Ġ', 'po', 'fromĠtheĠ', 'weightĠ', 'humanĠ', 'aĠm', 'word', 'IĠwillĠ', 'byĠ', 'All', 'knowĠ', 'other', 'while', 'ifĠitĠ', '<', 'youĠ', 'basisĠ', 'br', 'ide', 'solelyĠ', 'x', 'ivesĠ', 'spareĠ', 'alpha', 'known', 'erĠ', 'AnĠ', 'âĢĵ', 'musicĠ', 'ad', 'y,Ġ', 'icĠ', 'justĠthatĠ', 'edĠ', '>Ġ', '.h', 'th', 'ist', 'oneĠ', 's.Ġ', 'itself', 'atesĠtheĠ', 'equival', 'm', 'Ill', 'WithĠ', 'par', 'isĠ', 'not', 'relativ', 'struggleĠ', 'imag', 'aid', 'or,Ġ', 'content', 'law', 'mightĠ', 'ingĠtowardsĠ', 'yourĠs', 'We', 'philos', 'ĠthisĠ', 'This', 'IĠ', 'clust', '.ĊĠ', 'itsĠ', 'es,', 'her', 'ise', 'steadĠofĠ', 'carryĠ', 'judic', 'propert', 'se', 'Phys', 'pray', 'uni', 'standard', 'MayĠ', 'negativ', 'Ġ', 'ButĠ', 'barĠ', 'E', 'bu', 'W', 'unĠ', 'argument', 'asĠaĠ', 'mostĠ', \"'sĠ\", 'That', 'startĠ', 't,Ġ', 'blue', 'decimalĠ', 'put', 'ump', 'go,Ġ', 'aĠ', 'be', 'trueĠ', 'orĠotherĠ', 'ish', 'GoodĠ', 'ly,Ġ', 'Ġsup', 'remov', 'WouldĠ', 'objectsĠ', 'ace', 'seemsĠtoĠbeĠ', 'agree', 'delay', 'es,Ġ', 'sĠIĠ', 'ing,Ġ', 'flow', ',ĠisĠ', 'of', 'e,', 'ĊĠ', 'orĠwhatĠ', 'Wr', 'ingĠaĠ', 'aĠmanĠ', 'sĠ', 'it,Ġ', 'e,ĠaĠ', 'easyĠ', 'pass', 'ĠtakingĠ', 'protest', 'eningĠ', 'e)', 'edĠinĠtheĠ', 'igh', 'need', 'sĠtoĠ', 'y', 'F', 'ĠtoĠ', 'your', 'sĠwhereĠ', ',ĠwhoĠ', 'doctr', 'ĠthreeĠ', 'pattern', 'eĠandĠ', 'lateĠ', 'arry', 'posed', 'earnĠ', 'analog', 'ipp', 'ance', 'relationĠ', 'G', 'asĠtheĠ', 'T', 'Which', 'missingĠ', 'syl', 'beĠ', 'down', 'Be', 'Tak', 'esĠtheĠ', 'ingĠtheĠ', 'inĠyourĠ', 'ThereĠ', 'ine', 'fastĠandĠ', 'sĠofĠaĠ', 'PutĠ', 'strictĠ', 'ence,Ġ', 'in', 'demand', 'absolut', 'each', 'persist', 'ingĠtheirĠ', 'er', 'ByĠ', 'sign', 'fellow', 'Will', 'sac', 'stĠ', 'pain', 'homogen', 'What', 'byĠaĠ', 'youĠtheĠ', ';Ġ', ':ĠaĠ', 'putĠ', 'Ġcarri', 'atĠtheĠ', 's,', 'painĠ', 'un', 's)', 'SuchĠ', 'stay', 'loadedĠ', ':ĊĠ', 'more,Ġ', '(noĠ', 'icĠandĠ', 'sĠofĠtheĠ', 'flutter', 'WhatĠ', 'ERS', 'back', 'root', 'withĠ', 'ers', 'reach', '\"IĠ', 'InĠ', 'ship', 'want', ',ĠtheĠ', 'dotĠ', 'estĠ', 'SinceĠ', 'NoĠ', 'ink', 'inĠtheĠ', 'WhateverĠ', 'bru', 'itĠ', 'deep', 'ĠthatĠ', 'wereĠtoĠ', 'char', 'figuredĠ', '(TheĠ', ',ĠanĠ', 'OfĠ', 'sole', 'ingĠmeĠ', 'inĠ', 'bubble', 'own', 'rightĠ', 'needsĠ', 'ON', 'bound', 'ChurchĠ', 'ĠtheĠ', 'forĠtheĠ', 'saveĠ', 'metaph', 'blingĠ', 'nowĠ', 'actedĠ', 'choic', 'Re', 'pingĠ', '.ĠThatĠ', 'by', 'orĠ', 'ingĠtoĠbeĠ', 'add', 's,ĠtheĠ', 'dom', 'hold', 'pit', 'influ', 'FinalĠ', 'andĠtheĠ', 'S', 'isĠnot', 'ĠmustĠ', 'bl', 'fel', 'yĠofĠ', 'aĠth', 'ir', 'standingĠ', 'Conclusion', 'pl', 'nameĠofĠ', 'ence.', 'name', 'bringsĠ', 'Forc', \"s'Ġ\", 'eĠthisĠ', 'eĠthatĠ', 'ĠwillĠ', 'intenseĠ', 'resist', 'Ob', 'onĠ', 'cal', 'notĠ', 'ĠthoseĠwhoĠ', 'mor', 'breast', 'HasĠ', 'oughtĠ', 'withinĠtheĠ', 'fro', 'AĠs', 'food', 'forĠher', 'heartĠofĠ', 'whichĠ', 'row', 'ĠwouldĠ', 'lookingĠ', 'n', 'band', 'TheĠ', 'mean', 'ach', ',ĠtoĠ', 'enceĠ', 'P', 'y.Ċ', 'clusion', 'forĠherĠ', 'Pers', 'ate,Ġ', 'isĠnoĠ', 'reachedĠ', 'sh', 'dark', 'lip', 'âĢĵĠ', 'makeĠ', 'OurĠ', '.g', 'borneĠ', 'molec', 'worseĠ', 'sortĠ', 'meĠ', 'ĊĠTheĠ', 'fineĠ', 'FromĠ', 'fĠ', 'ĠtheirĠ', 'youĠcallĠ'}\n",
      "==Different Author==\n",
      "Text 1: Hi,\n",
      "     I am using fligthgear 0.9.4. I am having three queries.\n",
      "1.  For scenery database  I have downloaded flightgear world scenery\n",
      "which is in *.btg file format.I would like to know why *.btg files are\n",
      "called within *.stg file.\n",
      "2.  Can I use Multi Gen creator which gives *.flt files to model the\n",
      "\n",
      "\t Tokenized GPT2:Hi , ĊĠĠĠĠ ĠI Ġam Ġusing Ġfl ig th gear Ġ0 . 9 . 4 . ĠI Ġam Ġhaving Ġthree Ġqueries . Ċ 1 . Ġ ĠFor Ġscenery Ġdatabase Ġ ĠI Ġhave Ġdownloaded Ġflight gear Ġworld Ġscenery Ċ which Ġis Ġin Ġ*. bt g Ġfile Ġformat . I Ġwould Ġlike Ġto Ġknow Ġwhy Ġ*. bt g Ġfiles Ġare Ċ called Ġwithin Ġ*. st g Ġfile . Ċ 2 . Ġ ĠCan ĠI Ġuse ĠMulti ĠGen Ġcreator Ġwhich Ġgives Ġ*. fl t Ġfiles Ġto Ġmodel Ġthe Ċ\n",
      "\t Tokenized no:Hi ,ĊĠĠĠĠĠ IĠamĠusingĠ fl ig th gearĠ 0. 9. 4 .ĠIĠamĠ having ĠthreeĠ queries .Ċ 1 .ĠĠ For Ġsc ener yĠ databaseĠ Ġ IĠhaveĠ downloadedĠ flight gear ĠworldĠ sc ener yĊ whichĠisĠ inĠ *. bt gĠ fileĠ format . IĠwouldĠlikeĠtoĠ knowĠwhyĠ *. bt gĠ filesĠ areĊ calledĠ withinĠ *. st gĠ file .Ċ2 .ĠĠ CanĠ IĠuseĠ Mult iĠ G enĠ cre ator ĠwhichĠ givesĠ *. fl tĠ file sĠtoĠ model ĠtheĊ\n",
      "\t Unique Tokens GPT2: {'Ġobjects', 'Ġqueries', 'ĊĠĠĠĠ', 'Ġto', 'Ġuse', 'Ġby', 'Ġknow', 'Ġcan', 'Ġhave', 'ĠI', 'Ġdatabase', 'Ġthree', 'I', 'Ġformat', 'Ġworld', 'Whether', 'ĠCan', 'Ġwhich', 'Ġwithin', 'ĠWhat', 'Ġformats', 'Ġscenery', 'Ġfl', 'g', 'Ġin', 'ĠMulti', 'ĠFor', 'Ġsupported', 'rain', 'called', 'Ġis', 'SON', 'Ġand', 'Ġimport', 'Ġusing', 'Ġmodel', 'Ġwhy', 'Ġhaving', 'Ġthe', 'Ġcreator', 'Ċ', 'Ġfile', 'which', 'Ġgives', 'ĠGen', 'Ġ0', 'Ġ*.', 'Ġam', 'Ġwould', 'Ġlike', 'Ġfiles', 'Ġdownloaded', 'PER', ',', '9', '2', 'Ġare', 't', 'regards', 'Ġflight', '?'}\n",
      "\t Unique Tokens no: {'For', 'yĊ', 'P', '0.', '?Ċ', 'IĠuseĠ', '.ĊĊ', 'flight', 'format', 'G', 'files', '.ĠĠ', 'importĠ', 'fileĠ', 'calledĠ', 'CanĠ', 'whichĠisĠ', 'supportedĠbyĠ', 'enĠ', 'object', 'IĠwouldĠlikeĠtoĠ', 'givesĠ', 'sĠtoĠ', '.Ċ', 'withinĠ', 'ĠtheĊ', '.ĠWhatĠ', 's.', 'knowĠwhyĠ', 'inĠ', 'ERS', 'rainĠ', '.Ċ2', 'andĠ', ',ĊĠĠĠĠĠ', 'ON', 'ĠthreeĠ', 'sc', 'gearĠ', 's,Ċ', 'IĠhaveĠ', 'areĊ', 'tĠ', 'Ġsc', 'cre', 'areĠtheĠ', 'ener', 'IĠamĠusingĠ', 'gĠ', 'having', '9.', 'Mult', 'model', 'databaseĠ', 'downloadedĠ', 'ĠwhichĠ', 'yĠ', 'ator', 'iĠ', 'WhetherĠ', '.ĠIĠamĠ', 'queries', 'filesĠ', 'ĠworldĠ', 'regard', 'sĠ', 'file', 'ĠcanĠ', '*.'}\n",
      "Text 2: Hi,\n",
      "\n",
      "There is a problem only in GeoPackage layers.\n",
      "\n",
      "I have realized that when you put any expression in Provider feature filter\n",
      "(like \"id\" > 1), Layer is loaded successfully according to filter.\n",
      "\n",
      "On the other hand, identifying any feature on this layer, gives wrong\n",
      "results. There is a sequence shift\n",
      "\t Tokenized GPT2:Hi , Ċ Ċ There Ġis Ġa Ġproblem Ġonly Ġin ĠGeo Package Ġlayers . Ċ Ċ I Ġhave Ġrealized Ġthat Ġwhen Ġyou Ġput Ġany Ġexpression Ġin ĠPro vider Ġfeature Ġfilter Ċ ( like Ġ\" id \" Ġ> Ġ1 ), ĠLay er Ġis Ġloaded Ġsuccessfully Ġaccording Ġto Ġfilter . Ċ Ċ On Ġthe Ġother Ġhand , Ġidentifying Ġany Ġfeature Ġon Ġthis Ġlayer , Ġgives Ġwrong Ċ results . ĠThere Ġis Ġa Ġsequence Ġshift\n",
      "\t Tokenized no:Hi,ĊĊ ThereĠisĠaĠ problemĠ onlyĠinĠ Geo Pack ageĠ lay ers .ĊĊIĠhaveĠ realizedĠthatĠ whenĠyouĠ putĠ anyĠ expressionĠ inĠ Provid erĠ featureĠ filter Ċ( likeĠ\" id \"Ġ >Ġ 1),Ġ Lay erĠisĠ loadedĠ successfullyĠ accordingĠtoĠ filter .ĊĊ OnĠtheĠotherĠ hand,Ġ ident ifyingĠ anyĠ featureĠ onĠthisĠ lay er,Ġ givesĠ wrong Ċ result s.Ġ ThereĠisĠaĠ sequenceĠ shift\n",
      "\t Unique Tokens GPT2: {'Ġloaded', 'ĠG', 'Ġa', 'Ġyou', 'Ġto', 'results', 'Ġother', 'In', 'iron', 'Ġversion', 'Ġput', 'Ġhand', 'ĠGeo', 'Ġhave', 'Ġ\"', 'Ġlayers', 'I', 'Ġany', '(', 'Ġthis', 'Ġwhen', '.', 'Ġfilter', 'Ġexpression', 'like', 'Ġ', 'Ġin', 'Ġno', 'Package', 'Ġsequence', 'ĠOK', 'Ġis', 'Ġthere', 'ĊĊ', 'Ġthat', 'ĠPro', 'ĠThis', 'Ġ>', 'Ġshift', 'Without', 'Ġwrong', 'Hi', 'Ġocc', 'Ġafter', 'Ġonly', 'Ġthe', 'Ġproblem', 'er', 'On', 'vider', 'Ġgives', 'a', 'Ġsuccessfully', 'uring', 'ĠLay', 'Ġidentifying', 'There', 'Ġfeature', ',', 'Ġ1', 'ĠThere', '),', '\"', 'Regards', 'Ġresults', 'Ġlayer', 'Ġare', 'Ġaccording', 'Ġrealized', 'ĠBon', 'Ġon'}\n",
      "\t Unique Tokens no: {'ĠthereĠisĠnoĠ', 'expressionĠ', 'problem', 'likeĠ\"', 'ident', 'onlyĠinĠ', '.ĊĊ', 'Bon', 'problemĠ', '.ĠĠThisĠ', '\"Ġ', 'loadedĠ', 'filterĠ', 'OK', 'featureĠ', 'Pack', 'Regards,ĊĊ', 'erĠisĠ', 'ageĠ', '.ĊĊIĠhaveĠ', 'resultsĠareĠ', 'givesĠ', '.ĊĊInĠ', '1),Ġ', 'Geo', 'onĠtheĠ', 'accordingĠtoĠ', 'Hi,ĊĊ', 'ifyingĠ', 'erĠ', 'Gir', 'wrong', 'inĠ', 'sequenceĠ', 'WithoutĠ', 'problemĠisĠ', 'Ċ(', 'afterĠ', 'filter', 'onĠthisĠ', 'Lay', 'on', 'shift', 'ers', 'occur', 'Provid', 'ThereĠisĠaĠ', '>Ġ', 'OnĠtheĠotherĠ', 'aĠ', 'whenĠyouĠ', 'er,Ġ', 'hand,Ġ', 'anyĠ', 'putĠ', 'result', 'successfullyĠ', 'lay', 's.Ġ', 'ingĠ', 'realizedĠthatĠ'}\n",
      "==Different Author==\n",
      "Text 1: Thanks <PERSON>, and the others on this list that raised the issue.  I\n",
      "managed to nuke the MBL database here before any customers complained :)\n",
      "\n",
      "One thing I'd recommend to others though is to check your logs to see how\n",
      "many emails get caught by MBL (other than MBL_144360) - in my case there\n",
      "were non\n",
      "\t Tokenized GPT2:Thanks Ġ< PER SON >, Ġand Ġthe Ġothers Ġon Ġthis Ġlist Ġthat Ġraised Ġthe Ġissue . Ġ ĠI Ċ managed Ġto Ġn uke Ġthe ĠM BL Ġdatabase Ġhere Ġbefore Ġany Ġcustomers Ġcomplained Ġ:) Ċ Ċ One Ġthing ĠI 'd Ġrecommend Ġto Ġothers Ġthough Ġis Ġto Ġcheck Ġyour Ġlogs Ġto Ġsee Ġhow Ċ many Ġemails Ġget Ġcaught Ġby ĠM BL Ġ( other Ġthan ĠM BL _ 14 43 60 ) Ġ- Ġin Ġmy Ġcase Ġthere Ċ were Ġnon\n",
      "\t Tokenized no:ThanksĠ < P ERS ON > ,ĠandĠtheĠ othersĠ onĠthisĠ list ĠthatĠ ra i sedĠtheĠ issu e.ĠĠ IĊ managedĠtoĠ n uk eĠtheĠ MB LĠ databaseĠ hereĠ beforeĠ anyĠ customersĠ compl ainedĠ :)ĊĊ On eĠthingĠ I'dĠ recommend ĠtoĠ others ĠthoughĠ isĠtoĠ checkĠ yourĠ log sĠtoĠ seeĠ how Ċ manyĠ email sĠ getĠ caughtĠ byĠ MB L Ġ( otherĠthanĠ MB L_ 14 43 60 )Ġ-Ġ inĠmyĠ cas eĠthere Ċ wereĠ non\n",
      "\t Unique Tokens GPT2: {'Ġothers', 'Ġby', 'Ġagain', 'Ġthis', 'Ġbefore', '.', 'Ġhow', 'Ġlogs', 'Ġthe', 'Ġtherefore', 'Thanks', 'Ġfor', 'Ġlast', 'Ġthan', 'were', 'Ġemails', '>,', 'Ġreliable', 'Ġto', 'other', 'Ġonce', 'Ġdatabase', \"'re\", 'Ġmonth', 'Ġthere', 'SON', 'ĊĊ', 'Ġmore', 'Ġmay', 'pro', 'And', 'Ġthough', 'Ġ-', 'Ġthey', 'Ġn', 'Ġsee', 'Ġalternatives', 'Ġnone', ')', 'Ġhere', 'Ġfind', 'Ġlist', 'Ġin', 'many', 'Ġcase', 'Ġuseful', 'Ġcompletely', 'Ġand', 'Ġthem', 'Ġsystem', 'managed', 'Ġrunning', 'Ġissue', 'Ġremoved', 'One', 'security', 'Ġ:)', 'ĠM', 'Ġcheck', 'Ġrecommend', 'Ġ<', 'Ġsan', \"'ve\", 'ĠI', 'Ġany', 'Ġnow', 'Ġ', 's', 'Ġthing', 'Ġraised', 'Ġcomplained', 'Ġis', '_', 'Ġcaught', 'Ġthanks', 'uke', \"'d\", 'BL', 'Ġyour', 'Ġcustomers', 'Ġget', 'PER', ',', 'Ġmy', 'Other', 'ven', 'Ġon'}\n",
      "\t Unique Tokens no: {'hereĠ', 'remov', 'ĠthatĠ', 'wereĠ', 'log', 'eĠtheĠ', ',ĠandĠtheĠ', 'O', 'ON', 'e.ĠĠ', 'reli', 'onĠthisĠ', 'completelyĠ', 'managedĠtoĠ', 'databaseĠ', 'mayĠ', ')Ġ-Ġ', 'customersĠ', 'sĠ', 'uk', 'byĠ', 'list', 'checkĠ', 'abl', 'now', 'MB', 'inĠtheĠlastĠ', \"Ġthey'reĠ\", 'eĠthere', 'othersĠ', 'sĠtoĠ', 'ĠtoĠ', 'L_', ':)ĊĊ', 'systemĠ', 'cas', 'i', 'month', '>Ġ', 'compl', 'onceĠagain', 'issu', 'manyĠ', 'isĠtoĠ', 'email', 'foreĠ', 'securityĠ', '.ĊĊAndĠ', 'san', 'recommend', 'ThanksĠ', 'inĠmyĠ', 'forĠ', 'LĠ', 'beforeĠ', 'caughtĠ', 'find', 'sedĠtheĠ', \"I'dĠ\", 'edĠthem', 'useful', 'eĠthingĠ', 'getĠ', 'altern', 'n', 'there', 'yourĠ', 'anyĠ', 'ra', 'others', 'IĊ', 'L', 'P', 'runningĠtheĠ', ',Ġthank', \"I'veĠ\", 'provenĠ', 'ĠthoughĠ', '.ĠĊ', 'moreĠ', 'Ċ<', 'otherĠthanĠ', 'how', 'eĠthanĠtheĠ', 'ainedĠ', 'ERS', 'thersĠ', 'noneĠ', 'seeĠ', 'On', '-Ġ', 'ativesĠ', 'ĠandĠ'}\n",
      "Text 2: I read that Amarok 2.3 has dynamic collections back in, which excites me\n",
      "because I've been waiting for this feature to return!  I can't get it to\n",
      "work however.  I'm running off Git, and what I'm observing is that if I plug\n",
      "in an external drive, do a full scan, then unmount the drive, the songs are\n",
      "s\n",
      "\t Tokenized GPT2:I Ġread Ġthat ĠAm ar ok Ġ2 . 3 Ġhas Ġdynamic Ġcollections Ġback Ġin , Ġwhich Ġexc ites Ġme Ċ because ĠI 've Ġbeen Ġwaiting Ġfor Ġthis Ġfeature Ġto Ġreturn ! Ġ ĠI Ġcan 't Ġget Ġit Ġto Ċ work Ġhowever . Ġ ĠI 'm Ġrunning Ġoff ĠGit , Ġand Ġwhat ĠI 'm Ġobserving Ġis Ġthat Ġif ĠI Ġplug Ċ in Ġan Ġexternal Ġdrive , Ġdo Ġa Ġfull Ġscan , Ġthen Ġun mount Ġthe Ġdrive , Ġthe Ġsongs Ġare Ċ s\n",
      "\t Tokenized no:IĠ readĠthatĠ Am ar okĠ 2. 3Ġ hasĠ dynamic Ġcollection sĠ backĠ in ,ĠwhichĠ excit esĠ me Ċ becauseĠ I'veĠbeen ĠwaitingĠ forĠthisĠ feat ureĠtoĠ return !ĠĠ IĠcan'tĠ getĠit ĠtoĊ workĠ however .ĠĠI'mĠ runningĠ offĠ G it,ĠandĠ whatĠ I'mĠ observ ingĠ isĠthatĠ ifĠIĠ plug Ċ inĠanĠ externalĠ driv e,Ġ doĠ aĠfullĠ scan ,ĠthenĠ un mount ĠtheĠ drive ,ĠtheĠ song sĠ areĊ s\n",
      "\t Unique Tokens GPT2: {'Ġident', 'Ġwith', 'ĠUSB', 'Ġvisible', 'ĠAlso', 'Ġagain', 'sing', \"'s\", 'Ġsomehow', 'Ġthis', 'Ġas', 'Ġmass', '.', 'Ġoff', 'Ġdoesn', 'mar', 'Ġre', 'Ġrestart', 'or', 'Ġthe', 'Ġwhat', 'Thanks', 'Ġfor', 'ĠMight', 'Ġif', 'ites', 'Ġmiss', 'Ġalso', 'So', 'because', 'can', 'Ġto', 'Ġonce', 'Ġhowever', 'Ġdatabase', 'Ġrelevant', 'Ġbe', '3', 'Ġkicking', 'Ġdisappear', 'A', 'Ġthere', 'SON', 'ĊĊ', 'Ġexc', 'Ġexternal', 'Ġup', 'still', 'work', 'Ġ-', 'Ġshows', 'Ġhas', 'Ġan', 'Ġthey', 'Ġit', 'le', 'Ġare', 'query', 'Ġun', 'Ġcollections', ')', 'Ġsomething', 'Ġobserving', 'Ġat', 'Ġ(', 'I', 'Ġwhich', 'Ġbut', 'Ġdon', 'If', 'Ġbeen', 'Ġstorage', 'Ġin', 'Ġfull', 'Ġmount', 'Ġand', 'Ġseparate', 'ĠAm', '!', 'Ġrunning', 'Ġdynamic', 'Ġbehavior', 'Ġread', 'Ġback', 'Ġdevice', 'Ġinteracting', 'Ġcheck', 'Ġscan', 'Ġdo', 'Ġmysq', 'appear', 'Ġway', '?', '>', 'all', 'Ġa', 'ifiers', \"'ve\", 'Ġcan', 'ĠI', 'ĠGit', 'Ġeven', 'Ġ2', 'Ġgood', \"'m\", 'Ġseem', \"'t\", 'Ġ', 'Ġdrive', 'Ġres', 'Ġis', 'Ġplug', 'Ġthat', 'Ġsongs', 'Ġme', 'Ġso', 'Ġam', 'Ġlike', 'Ġanother', 'Ġfeature', 'Ġget', 'PER', ',', 'Ġreturn', 'Ġconnect', 'ĠIf', 'Ġwaiting'}\n",
      "\t Unique Tokens no: {'.ĠĠIfĠ', 'aĠgoodĠ', 'dynamic', \"it'sĠ\", 'excit', '?ĊĊThank', 'ureĠtoĠ', 'So,Ġ', 'however', 'singĠ', 'atĊ', 'readĠthatĠ', 'soĠIĠcanĠ', 're', 'ON', 'Ġm', 'stillĠ', 'hasĠ', \"IĠcan'tĠ\", 'ĠtheĠ', 'withĠtheĠ', 'externalĠ', ',ĠthenĠ', 'forĠthisĠ', 'allĠ', 'plug', 'databaseĠ', 'M', 'ifĠIĠ', 'onceĠ', 'sĠ', '(orĠ', 'aĠfullĠ', 'andĊ', 'ight', 'likeĠtheĠ', 'checkĠtheĠ', 'leĠ', 'okĠ', 'connectĠtoĠ', 'evenĠ', \"ĠtheyĠdon'tĠ\", 'ingĠinĠ', 'Ġsome', 'IĠdoĠ', '!ĠĠ', 'areĊ', ',Ġbut', 'showsĠ', 'sĊ', 'ĠtoĊ', 'alsoĠ', 'separateĠ', 'IfĠ', 'me', 'G', ',ĠthisĠ', \"itĠdoesn'tĠ\", 'USB', 'driv', '.ĊĊ', 'beĠ', 'isĠ', '?ĊĊ', 'doĠ', 'scan', 'anotherĠ', 'kick', '.ĠĠ', 'seemĠ', 'runningĠ', 'ĠwaitingĠ', 'canĠ', 'deviceĠ', 'storage', 'disappear', 'yĠtheĠ', 'mysq', 'getĠit', 'ĠtheĊ', 'isĠthatĠ', '>ĊĊ', 'ĠthisĠ', 'backĠ', 'IĠ', '2.', '3Ġ', 'feat', 'Also,Ġ', 'actingĠ', 'Ġcol', \"I'veĠbeen\", 'identifi', 'e,Ġ', 'visibleĠ', 'it,ĠandĠ', 'something', 'whatĠ', 'un', 'driveĠ', 'P', 'song', 'restart', 'assĠ', 'upĠ', \"I'mĠ\", 'how', 'Am', 'observ', 'isĠthereĠ', 'inĠanĠ', ')ĠtheĠ', ',ĠwhichĠ', 'res', 'esĠ', 'asĠaĠ', 'ERS', 'offĠ', \".ĠĠI'mĠ\", 'behaviorĠ', 'again', 'ers', '-Ġ', 'miss', 'amĠIĠ', 'becauseĠ', 'inter', 'lectionĠ', 'wayĠtoĠ', 'workĠ', 'return', ',ĠtheĠ', 'appearĠ', '?ĠĠ', 'relevant', 'quer', 'ingĠ', 'inĠtheĠ'}\n",
      "==Different Author==\n",
      "Text 1: Hi guys, this is my first post here, so let me start by saying it's a real pleasure to be a part of this community. I'm a newbie Mac developer and am looking forward to learning a lot and (hopefully) helping others along the way.\n",
      "\n",
      "I'm adding AquaticPrime to my first app, and setting up the PHP scrip\n",
      "\t Tokenized GPT2:Hi Ġguys , Ġthis Ġis Ġmy Ġfirst Ġpost Ġhere , Ġso Ġlet Ġme Ġstart Ġby Ġsaying Ġit 's Ġa Ġreal Ġpleasure Ġto Ġbe Ġa Ġpart Ġof Ġthis Ġcommunity . ĠI 'm Ġa Ġnewbie ĠMac Ġdeveloper Ġand Ġam Ġlooking Ġforward Ġto Ġlearning Ġa Ġlot Ġand Ġ( hopefully ) Ġhelping Ġothers Ġalong Ġthe Ġway . Ċ Ċ I 'm Ġadding ĠAqu atic Pr ime Ġto Ġmy Ġfirst Ġapp , Ġand Ġsetting Ġup Ġthe ĠPHP Ġscri p\n",
      "\t Tokenized no:HiĠ guys ,ĠthisĠisĠ myĠfirstĠ postĠ here,Ġ soĠ letĠmeĠ startĠ byĠ sayingĠ it'sĠaĠ realĠ pleas ureĠtoĠ beĠaĠ partĠofĠthisĠ community .ĠI'mĠ aĠnew bieĠ MacĠ develop erĠandĠ amĠ lookingĠforwardĠtoĠ learn ingĠaĠ lo tĠandĠ ( hopefully )Ġ helpingĠ othersĠ alongĠtheĠ way .ĊĊI'mĠ addingĠ Aqu atic Prim eĠtoĠ myĠfirstĠ app ,ĠandĠ settingĠ upĠtheĠ PHP Ġs crip\n",
      "\t Unique Tokens GPT2: {'Ġtoo', 'Ġothers', 'Ġwith', 'Ġby', 'Ġanyone', \"'s\", 'Signature', 'Ġthis', 'My', 'Ġas', '.', 'Ġargument', 'Ġfigure', 'Ġasks', 'Ġone', 'Ġpassing', 'Ġgenerated', 'Ġhow', 'Ġnot', 'Ġpointers', 'Hi', 'Ġthe', 'Ġfor', 'Ġpoint', 'Ċ', 'ĠWhen', 'Ġthan', 'Ġerror', 'Ġgreat', 'Ġalter', 'Ġlooking', 'Ġscripts', 'Ġbit', 'Ġto', 'Ġforward', 'Ġfunction', 'Ġcommunity', 'Ġtesting', 'Ġbe', 'Ġlicense', 'Ġstart', 'Ġno', 'Ġreal', 'Ġdown', 'Ġthere', 'Ġeither', 'Ġout', 'SON', 'Ġsome', 'Ġusing', 'Ġup', 'Ġarray', 'Ġafter', 'Ġproblem', 'ĠPerhaps', 'Ġnewbie', 'Ġsetting', 'Ġskills', 'Ġthough', 'Ġ-', 'Ġpost', 'Ġan', 'Ġit', 'ĠThrough', 'Ġare', 'Ġactivation', 'Ġat', 'Ġ(', 'Ġhere', 'ing', 'ime', 'Ġknow', 'I', 'Ġpurchase', 'Ġalong', 'Ġhelping', 'Ġkind', 'Ġdon', 'Ġapp', 'Ġadding', 'Ġbackend', 'Ġin', ')?', 'Ġbeing', 'Ġand', 'Ġjust', 'Ġof', 'Ġtracked', 'rather', 'and', 'Ġfile', 'Ġsaying', 'Ġcreate', 'Ġor', 'Ġwhere', 'Ġway', '?', 'Ġhelp', 'Ġa', \"'ve\", 'ĠI', 'Pr', \"'m\", 'Ġsince', \"'t\", 'Ġsuggestions', 'Ġ', 'Ġfirst', 'Ġlet', 'Ġtrial', 'Ġdeveloper', 'Ġlearning', 'Ġoccurring', 'Ġis', 'ĠAny', 'ĠPHP', 'Ġgenerate', 'Ġgoal', 'Ġdoes', 'ager', 'Ġme', 'Ġso', 'Best', 'ĠAqu', 'Ġam', 'Ġwould', 'ĠRegardless', 'Ġget', 'PER', ',', 'ĠMac', 'Ġmy', 'Ġguys', 'Ġpleasure', 'Ġlot', 'Ġquestion', 'Ġon', 'Ġpart', 'Ġintend'}\n",
      "\t Unique Tokens no: {'soĠ', 'noĠ', \"IĠdon'tĠ\", \"it'sĠaĠ\", '()Ġ', 'whereĠtheĠ', '(', 'ureĠtoĠ', 'postĠ', 'knowĠhowĠtoĠ', 'ratherĠthanĠ', 'greatĠ', 'activ', ',ĠandĠ', 'aĠl', 'sĠbyĠ', 'suggestion', 'downĠtoĠtheĠ', 'questionĠ', 'Ġtri', 'Ġtest', 'passingĠ', 'Ġskill', 'is,Ġ', 'sĠareĠ', 'ON', 'ag', \".ĊĊI'mĠ\", 'erĠtoĠ', 'partĠofĠthisĠ', 'est', 'oneĠtoĠ', 'letĠmeĠ', 'ersĠ', 'ingĠaĠ', 'functionĠ', 'les', 'it.Ġ', 'tĠandĠ', 'it,Ġ', 'develop', 'byĠ', 'Ġscript', 'HiĠ', 'anyoneĠ', 'upĠtheĠ', 'atĠthisĠpointĠ', 'purchase', 'enseĠ', 'Sign', 'fileĠ', 'othersĠ', 'errorĠ', 'endĠ', 'int', 'Regard', 'aĠbit', 'sĠtoĠ', 'app', 'aĠnew', 'AnyĠ', 'doesĠ', 'ingĠinĠ', 'ĠcodeĠ', 'wouldĠbeĠ', ',ĊĊ', 'realĠ', 'alĠandĠ', 'here,Ġ', 'sayingĠ', ',ĠthisĠisĠ', 'helpingĠ', 'ĠsinceĠ', 'PHPĠ', 'sĠorĠ', '.ĠTh', 'learn', 'ask', 'someĠkindĠofĠ', 'eĠtoĠ', 'me', ',ĠnotĠ', 'Perhap', 'guys', 'isĠbeingĠ', 'alongĠtheĠ', 'ation', 'onĠ', 'asĠanĠ', 'Aqu', 'ingĠtheĠ', 'ature', 'createĠ', 'anĠ', 'problemĠis', 'usingĠ', 'pleas', 'point', 'edĠthisĠ', 'erĠandĠ', 'track', 'generateĠ', 'myĠfirstĠ', 'figureĠoutĠ', 'generated', 'ĠtooĠ', 'lo', 'andĠmyĠ', 'inĠsomeĠ', 'help', '.ĊĊB', 'function', ')Ġ', 'Prim', 'backendĠ', 'beĠaĠ', 'justĠanĠ', 'MacĠ', 'file', 's,', 'P', 'amĠ', 'addingĠ', \"I'veĠ\", 'alter', \".ĠI'mĠ\", '.ĊĊMyĠ', 'bieĠ', 'ĠthoughĠ', 'either', 'community', \"I'mĠ\", 'ĠĠ', 'way', 'arrayĠ', 'rough', 'ERS', 'afterĠ', 'startĠ', '-', 'withĠ', 'ic', 'get', 'lookingĠforwardĠtoĠ', '.ĠWhen', 'settingĠ', '?Ġ', 'ofĠ', 'PHP', 'argumentĠ', \"Ġthere'sĠ\", 'PrimeĠ', 'occurr', 'sĠforĠ', 'goalĠisĠtoĠ'}\n",
      "Text 2: I'm in a long distance relationship as well. In my opinion there should always be a long term plan: when will you close the distance? What are you both doing, together and individually, to work on the problems that are keeping you apart? Who will move to where, is there family or careers to consider\n",
      "\t Tokenized GPT2:I 'm Ġin Ġa Ġlong Ġdistance Ġrelationship Ġas Ġwell . ĠIn Ġmy Ġopinion Ġthere Ġshould Ġalways Ġbe Ġa Ġlong Ġterm Ġplan : Ġwhen Ġwill Ġyou Ġclose Ġthe Ġdistance ? ĠWhat Ġare Ġyou Ġboth Ġdoing , Ġtogether Ġand Ġindividually , Ġto Ġwork Ġon Ġthe Ġproblems Ġthat Ġare Ġkeeping Ġyou Ġapart ? ĠWho Ġwill Ġmove Ġto Ġwhere , Ġis Ġthere Ġfamily Ġor Ġcareers Ġto Ġconsider\n",
      "\t Tokenized no:I'mĠ inĠ aĠlongĠ distanceĠ relationshipĠ asĠwell .ĠInĠmyĠ opinion ĠthereĠ shouldĠ alwaysĠbeĠ aĠlong ĠtermĠ plan : Ġwhen ĠwillĠ youĠc loseĠtheĠ distance ?ĠWhatĠ areĠyouĠ bothĠ doing , ĠtogetherĠandĠ individ ually ,ĠtoĠ workĠonĠtheĠ problem sĠthatĠareĠ keepingĠ youĠ apart ?Ġ WhoĠ willĠ moveĠtoĠ where,Ġ isĠthereĠ familyĠ orĠc are ersĠtoĠ consider\n",
      "\t Unique Tokens GPT2: {'Ġneed', 'Ġwill', 'Ġwork', 'Ġa', 'Ġyou', 'Ġto', 'Ġr', 'Ġproblems', 'Distance', 'Ġmight', 'Ġtalk', 'Ġeven', 'I', 'Ġgood', 'Ġthis', \"'m\", 'Ġas', '.', \"'t\", 'Ġcareers', 'ĠWhat', 'Ġbe', 'ĠĠĠĠ', 'Ġrelationship', 'Ġboth', 'Ġin', 'Ġplanning', 'Ġlong', 'Ġhelpful', 'Ġthere', 'Ġis', 'Ġopinion', 'Ġand', 'Ġaren', 'Ġplan', 'Ġdoing', 'Ġalways', 'Ġthat', 'ĠWho', 'Ġeither', 'Ġapart', 'Ġconsider', 'Ġthe', 'Ġof', 'Ġfor', 'Ġwell', 'Ġdistance', '/', 'Ġfamily', 'Ġif', 'Ġplace', 'ĠIn', 'Ġso', 'Ġmove', 'Ġclose', 'Ġ-', 'Ġor', 'Ġanything', 'Ġtogether', 'Ġmy', 'Ġshould', 'Ġkeeping', 'Ġterm', 'Ġare', 'ĠIf', 'Ġwhere', 'Ġindividually', 'Ġon', '?', 'Ġabout'}\n",
      "\t Unique Tokens no: {'are', 'problem', 'ĠwillĠ', 'goodĠforĠ', ',ĠtoĠ', 'loseĠtheĠ', 'sĠthatĠareĠ', 'dist', 'distanceĠ', 'apart', 'soĠon', 'individ', 'doing', 'r/', 'youĠ', 'Ġthis.', 'distance', 'bothĠ', 'shouldĠ', 'ĠtalkĠ', '.ĠĠĠĠĠ', 'youĠmightĠ', 'relationshipĠ', 'opinion', 'relationship', \"I'mĠ\", 'evenĠ', 'consid', 'isĠthereĠ', 'planningĠ', 'ĠisĠ', 'ersĠtoĠ', 'erĠandĠ', 'considerĠ', 'inĠ', 'willĠ', 'aĠlong', '.ĠInĠmyĠ', 'areĠyouĠ', 'ĠthereĠ', 'ance,Ġ', 'about', 'anceĠ', '.ĠIfĠyouĠ', 'keepingĠ', 'moveĠtoĠ', 'plac', '-Ġ', 'ually', 'youĠc', 'mightĠbeĠ', 'eĠto', 'ĠtogetherĠandĠ', 'WhoĠ', 'Dist', '?Ġ', 'ifĠtheĠ', 'familyĠ', 'alwaysĠbeĠ', 'workĠonĠtheĠ', 'ĠtermĠ', \"aren'tĠ\", 'needĠtoĠ', 'ofĠyou', 'orĠc', 'closeĠtheĠ', 'helpfulĠ', '?ĠWhatĠ', 'anythingĠtoĠ', 'asĠwell', 'doingĠ', 'plan', 'eitherĠ', 'where,Ġ', 'aĠlongĠ'}\n"
     ]
    }
   ],
   "source": [
    "# note that 0 for PAN corresponds to same author or no change, see https://pan.webis.de/clef24/pan24-web/style-change-detection.html\n",
    "i = 0\n",
    "first_tok = gpt2_tok\n",
    "first_name = \"GPT2\"\n",
    "second_tok = no_tok\n",
    "second_name = \"no\"\n",
    "text1 = \"query_text\"\n",
    "text2 = \"candidate_text\"\n",
    "for index, row in df_no_correct_gpt2_wrong_AV.iterrows():\n",
    "    print(\"==Same Author==\" if row[\"predictions\"] == 1 else \"==Different Author==\")\n",
    "    print(f\"Text 1: {row[text1][:300]}\")\n",
    "    print(f\"\\t Tokenized {first_name}:{' '.join(first_tok.tokenize(row[text1][:300]))}\")\n",
    "    print(f\"\\t Tokenized {second_name}:{' '.join(second_tok.tokenize(row[text1][:300]))}\")\n",
    "    print(f\"\\t Unique Tokens {first_name}: {set(first_tok.tokenize(row[text1])) - set(second_tok.tokenize(row[text1]))}\")\n",
    "    print(f\"\\t Unique Tokens {second_name}: {set(second_tok.tokenize(row[text1])) - set(first_tok.tokenize(row[text1]))}\")\n",
    "    print(f\"Text 2: {row[text2][:300]}\")   \n",
    "    print(f\"\\t Tokenized {first_name}:{' '.join(first_tok.tokenize(row[text2][:300]))}\")\n",
    "    print(f\"\\t Tokenized {second_name}:{' '.join(second_tok.tokenize(row[text2][:300]))}\")\n",
    "    print(f\"\\t Unique Tokens {first_name}: {set(first_tok.tokenize(row[text2])) - set(second_tok.tokenize(row[text2]))}\")\n",
    "    print(f\"\\t Unique Tokens {second_name}: {set(second_tok.tokenize(row[text2])) - set(first_tok.tokenize(row[text2]))}\")\n",
    "    i += 1\n",
    "    \n",
    "    if i > 10: break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-07T14:52:49.775781Z",
     "start_time": "2025-02-07T14:52:49.719853Z"
    }
   },
   "id": "bf6fe8c5dc6b5b3f"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "531\n",
      "434\n",
      "447\n"
     ]
    }
   ],
   "source": [
    "print(len(df_llama3_correct_gpt2_wrong))\n",
    "print(len(df_wb_gpt2_correct_llama3_wrong))\n",
    "print(len(df_ws_correct_gpt2_wrong))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T16:35:33.265965Z",
     "start_time": "2025-02-06T16:35:33.263046Z"
    }
   },
   "id": "2d3b9c1dd8bf229d"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_ws_correct_gpt2_wrong' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[49], line 9\u001B[0m\n\u001B[1;32m      6\u001B[0m second_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLLAMA3\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# second_tok = ws_tok\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# second_name = \"ws\"\u001B[39;00m\n\u001B[0;32m----> 9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m index, row \u001B[38;5;129;01min\u001B[39;00m df_ws_correct_gpt2_wrong\u001B[38;5;241m.\u001B[39miterrows():\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m==Same Author==\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpredictions\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m==Different Author==\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mText 1: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrow[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext 1\u001B[39m\u001B[38;5;124m'\u001B[39m][:\u001B[38;5;241m300\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'df_ws_correct_gpt2_wrong' is not defined"
     ]
    }
   ],
   "source": [
    "# note that 0 for PAN corresponds to same author or no change, see https://pan.webis.de/clef24/pan24-web/style-change-detection.html\n",
    "i = 0\n",
    "first_tok = gpt2_tok\n",
    "first_name = \"GPT2\"\n",
    "second_tok = llama3_tok\n",
    "second_name = \"LLAMA3\"\n",
    "# second_tok = ws_tok\n",
    "# second_name = \"ws\"\n",
    "for index, row in df_ws_correct_gpt2_wrong.iterrows():\n",
    "    print(\"==Same Author==\" if row[\"predictions\"] == 0 else \"==Different Author==\")\n",
    "    print(f\"Text 1: {row['text 1'][:300]}\")\n",
    "    print(f\"\\t Tokenized {first_name}:{' '.join(first_tok.tokenize(row['text 1'][:300]))}\")\n",
    "    print(f\"\\t Tokenized {second_name}:{' '.join(second_tok.tokenize(row['text 1'][:300]))}\")\n",
    "    print(f\"\\t Unique Tokens {first_name}: {set(first_tok.tokenize(row['text 1'])) - set(second_tok.tokenize(row['text 1']))}\")\n",
    "    print(f\"\\t Unique Tokens {second_name}: {set(second_tok.tokenize(row['text 1'])) - set(first_tok.tokenize(row['text 1']))}\")\n",
    "    print(f\"Text 2: {row['text 2'][:300]}\")   \n",
    "    print(f\"\\t Tokenized {first_name}:{' '.join(first_tok.tokenize(row['text 2'][:300]))}\")\n",
    "    print(f\"\\t Tokenized {second_name}:{' '.join(second_tok.tokenize(row['text 2'][:300]))}\")\n",
    "    print(f\"\\t Unique Tokens {first_name}: {set(first_tok.tokenize(row['text 2'])) - set(second_tok.tokenize(row['text 2']))}\")\n",
    "    print(f\"\\t Unique Tokens {second_name}: {set(second_tok.tokenize(row['text 2'])) - set(first_tok.tokenize(row['text 2']))}\")\n",
    "    i += 1\n",
    "    \n",
    "    if i > 10: break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-09T13:43:40.217986Z",
     "start_time": "2025-02-09T13:43:40.147444Z"
    }
   },
   "id": "320d65f4176d6363"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "# load webbook corpus\n",
    "path_webbook = os.path.join(local_finder_addition,\n",
    "                             \"TOKENIZER/data/train-corpora/webbook\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T16:43:09.463129Z",
     "start_time": "2025-02-06T16:43:09.451573Z"
    }
   },
   "id": "368414537d2cbe27"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading dataset from disk:   0%|          | 0/40 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "728bb1a8a439473c861a351e5f99aa5d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "webbook = load_from_disk(path_webbook)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T16:47:47.019130Z",
     "start_time": "2025-02-06T16:43:31.352705Z"
    }
   },
   "id": "c49ce1204cda4d8"
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excerpt 0:  visit and they all swore on a second blood oath that it wasn't them.\n",
      "\n",
      "\"What about Phantom?\" Hell Girl asked. \"Maybe Phantom talked to Campus News.\"\n",
      "\n",
      "Frankenstein shook his head. \"Why? Skeletor has pictures of Phantom like he has of the rest of us.\"\n",
      "\n",
      "Ghost Face looked thoughtful. \"Maybe Phantom want\n",
      "Excerpt 1: The future of SA-affiliated club sports, a cappella, and Greek groups is uncertain after the All-Campus Judicial Council ruled Friday that they may not use gendered language in their constitutions, advertisements, and names, and that they may not participate in gender-exclusive competitions.\n",
      "\n",
      "The ru\n",
      "Excerpt 2:  to support it; then a chunk of Iceland spar was discovered among navigational instruments on an Elizabethan ship that sank in 1592. This wasn't a Viking ship, but it existed almost two hundred years before scientists even understood the concept of using a crystal to determine direction at dawn and \n",
      "Excerpt 3:  in his Trojan maiden. Statius's Antigone, like all the young women of the _Thebaid_ , is distinguished by her sexual purity as well as by the nobility of character which appears in her _pietas_ toward the body of Polynices. We see her first just before the initial joining of battle, looking out fro\n",
      "Excerpt 4:  **Felted Adult Version A:** Cascade 128 Tweed (90% Peruvian wool, 10% Donegal tweed; 128 yd [117 m]/100 g): #7602 dark olive, 256 yd (234 m). Crystal Palace Splash (100% polyester; 85 yd [78 m]/100 g) #7181 jaguar, 85 yd (78 m).\n",
      "\n",
      "**Unfelted Adult Version B:** Cascade 128 Tweed (90% Peruvian wool, 1\n",
      "Excerpt 5: The Kaimukī Shire is a section of urban Honolulu that you can’t necessarily find on a map of O‘ahu, but is a gathering place for creatives, surfers, hippies, designers, and tourists. Although the moniker, coined by younger residents of Kaimukī, conjures images of hipster hobbits, puffing vape pens, \n",
      "Excerpt 6: Looking for a fun way to enjoy books and meet new people? Start a book club! But there is more to starting a book club than you might think. To help you make sure your book club is on point, here is a short guide about how to start a book club to help you make yours perfect.\n",
      "\n",
      "1. Figure Out Who You’r\n",
      "Excerpt 7: “Ohhhhhh! Who lives in a pineapple under the sea? SpongeBob SquarePants! Absorbent and yellow and porous is he! SpongeBob SquarePants!”\n",
      "\n",
      "Sometimes you don’t ask why someone wants a certain case mod, you just take what they ask for and do your best. This kind of happened with a request from EVGA when\n",
      "Excerpt 8:  down to the local hardware store for a replacement part. It takes all the ingenuity, patience, and sense of humor a scientist can muster to cope with the challenges. But it's happening, and it's thrilling.\n",
      "\n",
      "Rick Potts and his team drilled two boreholes at Olorgesailie in 2012. The cores they lifted\n",
      "Excerpt 9:  the WBHG, and the householder identity it represented, in Shanghai's specific cultural context in ways that would have been clear to those in attendance. President Shi's speech, which had in fact been jointly written with Wang Yiting and other WBHG leaders,33 pointed directly to the fundamental set\n"
     ]
    }
   ],
   "source": [
    "# for the dev set print the first 10 texts\n",
    "for i in range(10):\n",
    "    print(f\"Excerpt {i}: {webbook['train'][i]['text'][:300]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T16:49:34.226647Z",
     "start_time": "2025-02-06T16:49:34.219832Z"
    }
   },
   "id": "8116068a6dd35c98"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "path_gpt2_mnli = os.path.join(local_finder_addition, \"TOKENIZER/output/GLUE/train-mixed/base-BERT/mixed-gpt2-32000/749M/steps-45000/seed-42/42/mnli\")\n",
    "path_llama3_mnli = os.path.join(local_finder_addition, \"TOKENIZER/output/GLUE/train-mixed/base-BERT/mixed-llama3-32000/749M/steps-45000/seed-42/42/mnli\")\n",
    "df_mnli_gpt2 = pd.read_csv(os.path.join(path_gpt2_mnli, \"2025-02-01_eval_dataset_mnli.tsv\"), sep=\"\\t\")\n",
    "df_mnli_llama3 = pd.read_csv(os.path.join(path_llama3_mnli, \"2025-01-22_eval_dataset_mnli.tsv\"), sep=\"\\t\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T17:02:44.461842Z",
     "start_time": "2025-02-06T17:02:32.947396Z"
    }
   },
   "id": "55d998d7d656a788"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "df_mnli_gpt2_correct_llama3_wrong = df_mnli_gpt2[(df_mnli_gpt2[\"label\"] == df_mnli_gpt2[\"predictions\"]) & (df_mnli_llama3[\"label\"] != df_mnli_llama3[\"predictions\"])]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T17:03:18.778247Z",
     "start_time": "2025-02-06T17:03:18.759647Z"
    }
   },
   "id": "9d116acb341e7fd3"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "619\n"
     ]
    }
   ],
   "source": [
    "print(len(df_mnli_gpt2_correct_llama3_wrong))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T17:03:21.144882Z",
     "start_time": "2025-02-06T17:03:21.135930Z"
    }
   },
   "id": "b4f8eac9ad99bda4"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==entailment==\n",
      "Text 1: uh i don't know i i have mixed emotions about him uh sometimes i like him but at the same times i love to see somebody beat him\n",
      "\t Tokenized GPT2:uh Ġi Ġdon 't Ġknow Ġi Ġi Ġhave Ġmixed Ġemotions Ġabout Ġhim Ġuh Ġsometimes Ġi Ġlike Ġhim Ġbut Ġat Ġthe Ġsame Ġtimes Ġi Ġlove Ġto Ġsee Ġsomebody Ġbeat Ġhim\n",
      "\t Tokenized LLAMA3:uh Ġi Ġdon 't Ġknow Ġi Ġi Ġhave Ġmixed Ġemotions Ġabout Ġhim Ġuh Ġsometimes Ġi Ġlike Ġhim Ġbut Ġat Ġthe Ġsame Ġtimes Ġi Ġlove Ġto Ġsee Ġsomebody Ġbeat Ġhim\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I like him for the most part, but would still enjoy seeing someone beat him.\n",
      "\t Tokenized GPT2:I Ġlike Ġhim Ġfor Ġthe Ġmost Ġpart , Ġbut Ġwould Ġstill Ġenjoy Ġseeing Ġsomeone Ġbeat Ġhim .\n",
      "\t Tokenized LLAMA3:I Ġlike Ġhim Ġfor Ġthe Ġmost Ġpart , Ġbut Ġwould Ġstill Ġenjoy Ġseeing Ġsomeone Ġbeat Ġhim .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: yeah i i think my favorite restaurant is always been the one closest  you know the closest as long as it's it meets the minimum criteria you know of good food\n",
      "\t Tokenized GPT2:yeah Ġi Ġi Ġthink Ġmy Ġfavorite Ġrestaurant Ġis Ġalways Ġbeen Ġthe Ġone Ġclosest Ġ Ġyou Ġknow Ġthe Ġclosest Ġas Ġlong Ġas Ġit 's Ġit Ġmeets Ġthe Ġminimum Ġcriteria Ġyou Ġknow Ġof Ġgood Ġfood\n",
      "\t Tokenized LLAMA3:yeah Ġi Ġi Ġthink Ġmy Ġfavorite Ġrestaurant Ġis Ġalways Ġbeen Ġthe Ġone Ġclosest Ġ Ġyou Ġknow Ġthe Ġclosest Ġas Ġlong Ġas Ġit 's Ġit Ġmeets Ġthe Ġminimum Ġcriteria Ġyou Ġknow Ġof Ġgood Ġfood\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: My favorite restaurants are always at least a hundred miles away from my house. \n",
      "\t Tokenized GPT2:My Ġfavorite Ġrestaurants Ġare Ġalways Ġat Ġleast Ġa Ġhundred Ġmiles Ġaway Ġfrom Ġmy Ġhouse . Ġ\n",
      "\t Tokenized LLAMA3:My Ġfavorite Ġrestaurants Ġare Ġalways Ġat Ġleast Ġa Ġhundred Ġmiles Ġaway Ġfrom Ġmy Ġhouse . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Calcutta seems to be the only other production center having any pretensions to artistic creativity at all, but ironically you're actually more likely to see the works of Satyajit Ray or Mrinal Sen shown in Europe or North America than in India itself.\n",
      "\t Tokenized GPT2:Cal cut ta Ġseems Ġto Ġbe Ġthe Ġonly Ġother Ġproduction Ġcenter Ġhaving Ġany Ġpret ensions Ġto Ġartistic Ġcreativity Ġat Ġall , Ġbut Ġiron ically Ġyou 're Ġactually Ġmore Ġlikely Ġto Ġsee Ġthe Ġworks Ġof ĠSat y aj it ĠRay Ġor ĠMr inal ĠSen Ġshown Ġin ĠEurope Ġor ĠNorth ĠAmerica Ġthan Ġin ĠIndia Ġitself .\n",
      "\t Tokenized LLAMA3:Cal cut ta Ġseems Ġto Ġbe Ġthe Ġonly Ġother Ġproduction Ġcenter Ġhaving Ġany Ġpret ensions Ġto Ġartistic Ġcreativity Ġat Ġall , Ġbut Ġiron ically Ġyou 're Ġactually Ġmore Ġlikely Ġto Ġsee Ġthe Ġworks Ġof ĠSat y aj it ĠRay Ġor ĠMr inal ĠSen Ġshown Ġin ĠEurope Ġor ĠNorth ĠAmerica Ġthan Ġin ĠIndia Ġitself .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Most of Mrinal Sen's work can be found in European collections.\n",
      "\t Tokenized GPT2:Most Ġof ĠMr inal ĠSen 's Ġwork Ġcan Ġbe Ġfound Ġin ĠEuropean Ġcollections .\n",
      "\t Tokenized LLAMA3:Most Ġof ĠMr inal ĠSen 's Ġwork Ġcan Ġbe Ġfound Ġin ĠEuropean Ġcollections .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: i'm not sure what the overnight low was\n",
      "\t Tokenized GPT2:i 'm Ġnot Ġsure Ġwhat Ġthe Ġovernight Ġlow Ġwas\n",
      "\t Tokenized LLAMA3:i 'm Ġnot Ġsure Ġwhat Ġthe Ġovernight Ġlow Ġwas\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I don't know how cold it got last night.\n",
      "\t Tokenized GPT2:I Ġdon 't Ġknow Ġhow Ġcold Ġit Ġgot Ġlast Ġnight .\n",
      "\t Tokenized LLAMA3:I Ġdon 't Ġknow Ġhow Ġcold Ġit Ġgot Ġlast Ġnight .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: He hadn't seen even pictures of such things since the few silent movies run in some of the little art theaters.\n",
      "\t Tokenized GPT2:He Ġhadn 't Ġseen Ġeven Ġpictures Ġof Ġsuch Ġthings Ġsince Ġthe Ġfew Ġsilent Ġmovies Ġrun Ġin Ġsome Ġof Ġthe Ġlittle Ġart Ġtheaters .\n",
      "\t Tokenized LLAMA3:He Ġhadn 't Ġseen Ġeven Ġpictures Ġof Ġsuch Ġthings Ġsince Ġthe Ġfew Ġsilent Ġmovies Ġrun Ġin Ġsome Ġof Ġthe Ġlittle Ġart Ġtheaters .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He had recently seen pictures depicting those things.\n",
      "\t Tokenized GPT2:He Ġhad Ġrecently Ġseen Ġpictures Ġdep icting Ġthose Ġthings .\n",
      "\t Tokenized LLAMA3:He Ġhad Ġrecently Ġseen Ġpictures Ġdep icting Ġthose Ġthings .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Look, there's a legend here.\n",
      "\t Tokenized GPT2:Look , Ġthere 's Ġa Ġlegend Ġhere .\n",
      "\t Tokenized LLAMA3:Look , Ġthere 's Ġa Ġlegend Ġhere .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: See, there is a well known hero here.\n",
      "\t Tokenized GPT2:See , Ġthere Ġis Ġa Ġwell Ġknown Ġhero Ġhere .\n",
      "\t Tokenized LLAMA3:See , Ġthere Ġis Ġa Ġwell Ġknown Ġhero Ġhere .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: There are no shares of a stock that might someday come back, just piles of options as worthless as those shares of Cook's American Business Alliance.\n",
      "\t Tokenized GPT2:There Ġare Ġno Ġshares Ġof Ġa Ġstock Ġthat Ġmight Ġsomeday Ġcome Ġback , Ġjust Ġp iles Ġof Ġoptions Ġas Ġworthless Ġas Ġthose Ġshares Ġof ĠCook 's ĠAmerican ĠBusiness ĠAlliance .\n",
      "\t Tokenized LLAMA3:There Ġare Ġno Ġshares Ġof Ġa Ġstock Ġthat Ġmight Ġsomeday Ġcome Ġback , Ġjust Ġp iles Ġof Ġoptions Ġas Ġworthless Ġas Ġthose Ġshares Ġof ĠCook 's ĠAmerican ĠBusiness ĠAlliance .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2:  Cook's American Business Alliance caused shares of stock to come back.\n",
      "\t Tokenized GPT2:ĠCook 's ĠAmerican ĠBusiness ĠAlliance Ġcaused Ġshares Ġof Ġstock Ġto Ġcome Ġback .\n",
      "\t Tokenized LLAMA3:ĠCook 's ĠAmerican ĠBusiness ĠAlliance Ġcaused Ġshares Ġof Ġstock Ġto Ġcome Ġback .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: yeah really no kidding\n",
      "\t Tokenized GPT2:yeah Ġreally Ġno Ġkidding\n",
      "\t Tokenized LLAMA3:yeah Ġreally Ġno Ġkidding\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Really? No kidding! \n",
      "\t Tokenized GPT2:Really ? ĠNo Ġkidding ! Ġ\n",
      "\t Tokenized LLAMA3:Really ? ĠNo Ġkidding ! Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Visit at sundown or out of season to get the full flavor of the setting.\n",
      "\t Tokenized GPT2:Vis it Ġat Ġsund own Ġor Ġout Ġof Ġseason Ġto Ġget Ġthe Ġfull Ġflavor Ġof Ġthe Ġsetting .\n",
      "\t Tokenized LLAMA3:Vis it Ġat Ġsund own Ġor Ġout Ġof Ġseason Ġto Ġget Ġthe Ġfull Ġflavor Ġof Ġthe Ġsetting .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The setting is better to visit at sundown or during low season.\n",
      "\t Tokenized GPT2:The Ġsetting Ġis Ġbetter Ġto Ġvisit Ġat Ġsund own Ġor Ġduring Ġlow Ġseason .\n",
      "\t Tokenized LLAMA3:The Ġsetting Ġis Ġbetter Ġto Ġvisit Ġat Ġsund own Ġor Ġduring Ġlow Ġseason .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: While parents may pick up this gay semaphore, kids aren't likely to.\n",
      "\t Tokenized GPT2:While Ġparents Ġmay Ġpick Ġup Ġthis Ġgay Ġsem aph ore , Ġkids Ġaren 't Ġlikely Ġto .\n",
      "\t Tokenized LLAMA3:While Ġparents Ġmay Ġpick Ġup Ġthis Ġgay Ġsem aph ore , Ġkids Ġaren 't Ġlikely Ġto .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Some kids do understand gay signals.\n",
      "\t Tokenized GPT2:Some Ġkids Ġdo Ġunderstand Ġgay Ġsignals .\n",
      "\t Tokenized LLAMA3:Some Ġkids Ġdo Ġunderstand Ġgay Ġsignals .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: As a basic guide, the symbols below have been used to indicate high-season rates in Hong Kong dollars, based on double occupancy, with bath or shower.\n",
      "\t Tokenized GPT2:As Ġa Ġbasic Ġguide , Ġthe Ġsymbols Ġbelow Ġhave Ġbeen Ġused Ġto Ġindicate Ġhigh - season Ġrates Ġin ĠHong ĠKong Ġdollars , Ġbased Ġon Ġdouble Ġoccup ancy , Ġwith Ġbath Ġor Ġshower .\n",
      "\t Tokenized LLAMA3:As Ġa Ġbasic Ġguide , Ġthe Ġsymbols Ġbelow Ġhave Ġbeen Ġused Ġto Ġindicate Ġhigh -season Ġrates Ġin ĠHong ĠKong Ġdollars , Ġbased Ġon Ġdouble Ġoccup ancy , Ġwith Ġbath Ġor Ġshower .\n",
      "\t Unique Tokens GPT2: {'-', 'season'}\n",
      "\t Unique Tokens LLAMA3: {'-season'}\n",
      "Text 2: As you can see, the symbols are of dolphins and octopuses.\n",
      "\t Tokenized GPT2:As Ġyou Ġcan Ġsee , Ġthe Ġsymbols Ġare Ġof Ġd olphins Ġand Ġoct op uses .\n",
      "\t Tokenized LLAMA3:As Ġyou Ġcan Ġsee , Ġthe Ġsymbols Ġare Ġof Ġd olph ins Ġand Ġoct op uses .\n",
      "\t Unique Tokens GPT2: {'olphins'}\n",
      "\t Unique Tokens LLAMA3: {'olph', 'ins'}\n",
      "==entailment==\n",
      "Text 1: This having come to his stepmother's ears, she taxed him with it on the afternoon before her death, and a quarrel ensued, part of which was overheard. \n",
      "\t Tokenized GPT2:This Ġhaving Ġcome Ġto Ġhis Ġstep mother 's Ġears , Ġshe Ġtax ed Ġhim Ġwith Ġit Ġon Ġthe Ġafternoon Ġbefore Ġher Ġdeath , Ġand Ġa Ġqu ar rel Ġens ued , Ġpart Ġof Ġwhich Ġwas Ġover heard . Ġ\n",
      "\t Tokenized LLAMA3:This Ġhaving Ġcome Ġto Ġhis Ġstep mother 's Ġears , Ġshe Ġtax ed Ġhim Ġwith Ġit Ġon Ġthe Ġafternoon Ġbefore Ġher Ġdeath , Ġand Ġa Ġqu ar rel Ġens ued , Ġpart Ġof Ġwhich Ġwas Ġover heard . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: A fight broke out between the stepmother and the man before her death.\n",
      "\t Tokenized GPT2:A Ġfight Ġbroke Ġout Ġbetween Ġthe Ġstep mother Ġand Ġthe Ġman Ġbefore Ġher Ġdeath .\n",
      "\t Tokenized LLAMA3:A Ġfight Ġbroke Ġout Ġbetween Ġthe Ġstep mother Ġand Ġthe Ġman Ġbefore Ġher Ġdeath .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The tomb guardian will unlock the gate to the tunnel and give you a candle to explore the small circular catacomb, but for what little you can see, it is hardly worth the effort.\n",
      "\t Tokenized GPT2:The Ġtomb Ġguardian Ġwill Ġunlock Ġthe Ġgate Ġto Ġthe Ġtunnel Ġand Ġgive Ġyou Ġa Ġcandle Ġto Ġexplore Ġthe Ġsmall Ġcircular Ġcat ac omb , Ġbut Ġfor Ġwhat Ġlittle Ġyou Ġcan Ġsee , Ġit Ġis Ġhardly Ġworth Ġthe Ġeffort .\n",
      "\t Tokenized LLAMA3:The Ġtomb Ġguardian Ġwill Ġunlock Ġthe Ġgate Ġto Ġthe Ġtunnel Ġand Ġgive Ġyou Ġa Ġcandle Ġto Ġexplore Ġthe Ġsmall Ġcircular Ġcat ac omb , Ġbut Ġfor Ġwhat Ġlittle Ġyou Ġcan Ġsee , Ġit Ġis Ġhardly Ġworth Ġthe Ġeffort .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The tomb garden can give you a thorough tour of the catacombs.\n",
      "\t Tokenized GPT2:The Ġtomb Ġgarden Ġcan Ġgive Ġyou Ġa Ġthorough Ġtour Ġof Ġthe Ġcat ac om bs .\n",
      "\t Tokenized LLAMA3:The Ġtomb Ġgarden Ġcan Ġgive Ġyou Ġa Ġthorough Ġtour Ġof Ġthe Ġcat ac om bs .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: at least i'm going to give it a try cause you can see i mean the oil filters i mean you can touch it it's right there\n",
      "\t Tokenized GPT2:at Ġleast Ġi 'm Ġgoing Ġto Ġgive Ġit Ġa Ġtry Ġcause Ġyou Ġcan Ġsee Ġi Ġmean Ġthe Ġoil Ġfilters Ġi Ġmean Ġyou Ġcan Ġtouch Ġit Ġit 's Ġright Ġthere\n",
      "\t Tokenized LLAMA3:at Ġleast Ġi 'm Ġgoing Ġto Ġgive Ġit Ġa Ġtry Ġcause Ġyou Ġcan Ġsee Ġi Ġmean Ġthe Ġoil Ġfilters Ġi Ġmean Ġyou Ġcan Ġtouch Ġit Ġit 's Ġright Ġthere\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It seems like it's worth trying to get the oil filter out.\n",
      "\t Tokenized GPT2:It Ġseems Ġlike Ġit 's Ġworth Ġtrying Ġto Ġget Ġthe Ġoil Ġfilter Ġout .\n",
      "\t Tokenized LLAMA3:It Ġseems Ġlike Ġit 's Ġworth Ġtrying Ġto Ġget Ġthe Ġoil Ġfilter Ġout .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Even the lower limit of that differential compounds to a hefty sum over time.\n",
      "\t Tokenized GPT2:Even Ġthe Ġlower Ġlimit Ġof Ġthat Ġdifferential Ġcompounds Ġto Ġa Ġhe fty Ġsum Ġover Ġtime .\n",
      "\t Tokenized LLAMA3:Even Ġthe Ġlower Ġlimit Ġof Ġthat Ġdifferential Ġcompounds Ġto Ġa Ġhe fty Ġsum Ġover Ġtime .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The differential will not grow.\n",
      "\t Tokenized GPT2:The Ġdifferential Ġwill Ġnot Ġgrow .\n",
      "\t Tokenized LLAMA3:The Ġdifferential Ġwill Ġnot Ġgrow .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Reportedly the biggest payment made in such a case, it is hardly a nick in Texaco's annual revenue of more than $30 billion.\n",
      "\t Tokenized GPT2:Rep orted ly Ġthe Ġbiggest Ġpayment Ġmade Ġin Ġsuch Ġa Ġcase , Ġit Ġis Ġhardly Ġa Ġnick Ġin ĠTex aco 's Ġannual Ġrevenue Ġof Ġmore Ġthan Ġ$ 30 Ġbillion .\n",
      "\t Tokenized LLAMA3:Rep orted ly Ġthe Ġbiggest Ġpayment Ġmade Ġin Ġsuch Ġa Ġcase , Ġit Ġis Ġhardly Ġa Ġnick Ġin ĠTex aco 's Ġannual Ġrevenue Ġof Ġmore Ġthan Ġ$ 30 Ġbillion .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The biggest payment they made barely hurt their profits.\n",
      "\t Tokenized GPT2:The Ġbiggest Ġpayment Ġthey Ġmade Ġbarely Ġhurt Ġtheir Ġprofits .\n",
      "\t Tokenized LLAMA3:The Ġbiggest Ġpayment Ġthey Ġmade Ġbarely Ġhurt Ġtheir Ġprofits .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: They are levied through the power of the Government to compel payment, and the person or entity that pays these fees does not receive anything of value from the Government in exchange.\n",
      "\t Tokenized GPT2:They Ġare Ġle v ied Ġthrough Ġthe Ġpower Ġof Ġthe ĠGovernment Ġto Ġcom pel Ġpayment , Ġand Ġthe Ġperson Ġor Ġentity Ġthat Ġpays Ġthese Ġfees Ġdoes Ġnot Ġreceive Ġanything Ġof Ġvalue Ġfrom Ġthe ĠGovernment Ġin Ġexchange .\n",
      "\t Tokenized LLAMA3:They Ġare Ġle v ied Ġthrough Ġthe Ġpower Ġof Ġthe ĠGovernment Ġto Ġcom pel Ġpayment , Ġand Ġthe Ġperson Ġor Ġentity Ġthat Ġpays Ġthese Ġfees Ġdoes Ġnot Ġreceive Ġanything Ġof Ġvalue Ġfrom Ġthe ĠGovernment Ġin Ġexchange .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: They are not levied through the power of the Government to compel payment.\n",
      "\t Tokenized GPT2:They Ġare Ġnot Ġle v ied Ġthrough Ġthe Ġpower Ġof Ġthe ĠGovernment Ġto Ġcom pel Ġpayment .\n",
      "\t Tokenized LLAMA3:They Ġare Ġnot Ġle v ied Ġthrough Ġthe Ġpower Ġof Ġthe ĠGovernment Ġto Ġcom pel Ġpayment .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: It was replaced in 1910 by the famous old pontoon bridge with its seafood restaurants, which served until the present bridge was opened in 1992.\n",
      "\t Tokenized GPT2:It Ġwas Ġreplaced Ġin Ġ19 10 Ġby Ġthe Ġfamous Ġold Ġp onto on Ġbridge Ġwith Ġits Ġse af ood Ġrestaurants , Ġwhich Ġserved Ġuntil Ġthe Ġpresent Ġbridge Ġwas Ġopened Ġin Ġ1992 .\n",
      "\t Tokenized LLAMA3:It Ġwas Ġreplaced Ġin Ġ 191 0 Ġby Ġthe Ġfamous Ġold Ġp onto on Ġbridge Ġwith Ġits Ġse af ood Ġrestaurants , Ġwhich Ġserved Ġuntil Ġthe Ġpresent Ġbridge Ġwas Ġopened Ġin Ġ 199 2 .\n",
      "\t Unique Tokens GPT2: {'Ġ1992', '10', 'Ġ19'}\n",
      "\t Unique Tokens LLAMA3: {'199', '191', 'Ġ', '0', '2'}\n",
      "Text 2: The pontoon bridge had shops as well as restaurants.\n",
      "\t Tokenized GPT2:The Ġp onto on Ġbridge Ġhad Ġshops Ġas Ġwell Ġas Ġrestaurants .\n",
      "\t Tokenized LLAMA3:The Ġp onto on Ġbridge Ġhad Ġshops Ġas Ġwell Ġas Ġrestaurants .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: it would probably be a lot more work and probably not turn out as good\n",
      "\t Tokenized GPT2:it Ġwould Ġprobably Ġbe Ġa Ġlot Ġmore Ġwork Ġand Ġprobably Ġnot Ġturn Ġout Ġas Ġgood\n",
      "\t Tokenized LLAMA3:it Ġwould Ġprobably Ġbe Ġa Ġlot Ġmore Ġwork Ġand Ġprobably Ġnot Ġturn Ġout Ġas Ġgood\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Oh that way sounds great, it could turn out even better\n",
      "\t Tokenized GPT2:Oh Ġthat Ġway Ġsounds Ġgreat , Ġit Ġcould Ġturn Ġout Ġeven Ġbetter\n",
      "\t Tokenized LLAMA3:Oh Ġthat Ġway Ġsounds Ġgreat , Ġit Ġcould Ġturn Ġout Ġeven Ġbetter\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: I put it to you that, wearing a suit of Mr. Inglethorp's clothes, with a black beard trimmed to resemble his, you were there ”and signed the register in his name!\n",
      "\t Tokenized GPT2:I Ġput Ġit Ġto Ġyou Ġthat , Ġwearing Ġa Ġsuit Ġof ĠMr . ĠIn gle th orp 's Ġclothes , Ġwith Ġa Ġblack Ġbeard Ġtrim med Ġto Ġresem ble Ġhis , Ġyou Ġwere Ġthere ĠâĢĿ and Ġsigned Ġthe Ġregister Ġin Ġhis Ġname !\n",
      "\t Tokenized LLAMA3:I Ġput Ġit Ġto Ġyou Ġthat , Ġwearing Ġa Ġsuit Ġof ĠMr . ĠIn gle th orp 's Ġclothes , Ġwith Ġa Ġblack Ġbeard Ġtrim med Ġto Ġresem ble Ġhis , Ġyou Ġwere Ġthere ĠâĢĿ and Ġsigned Ġthe Ġregister Ġin Ġhis Ġname !\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The green suit that he wore was actually Mr. Inglethorp's which he stole from his closet a few days ago. \n",
      "\t Tokenized GPT2:The Ġgreen Ġsuit Ġthat Ġhe Ġwore Ġwas Ġactually ĠMr . ĠIn gle th orp 's Ġwhich Ġhe Ġstole Ġfrom Ġhis Ġcloset Ġa Ġfew Ġdays Ġago . Ġ\n",
      "\t Tokenized LLAMA3:The Ġgreen Ġsuit Ġthat Ġhe Ġwore Ġwas Ġactually ĠMr . ĠIn gle th orp 's Ġwhich Ġhe Ġstole Ġfrom Ġhis Ġcloset Ġa Ġfew Ġdays Ġago . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The activities included in the Unified Agenda are, in general, those expected to have a regulatory action within the next 12 months, although agencies may include activities with an even longer time frame.\n",
      "\t Tokenized GPT2:The Ġactivities Ġincluded Ġin Ġthe ĠUn ified ĠA gend a Ġare , Ġin Ġgeneral , Ġthose Ġexpected Ġto Ġhave Ġa Ġregulatory Ġaction Ġwithin Ġthe Ġnext Ġ12 Ġmonths , Ġalthough Ġagencies Ġmay Ġinclude Ġactivities Ġwith Ġan Ġeven Ġlonger Ġtime Ġframe .\n",
      "\t Tokenized LLAMA3:The Ġactivities Ġincluded Ġin Ġthe ĠUn ified ĠA gend a Ġare , Ġin Ġgeneral , Ġthose Ġexpected Ġto Ġhave Ġa Ġregulatory Ġaction Ġwithin Ġthe Ġnext Ġ 12 Ġmonths , Ġalthough Ġagencies Ġmay Ġinclude Ġactivities Ġwith Ġan Ġeven Ġlonger Ġtime Ġframe .\n",
      "\t Unique Tokens GPT2: {'Ġ12'}\n",
      "\t Unique Tokens LLAMA3: {'12', 'Ġ'}\n",
      "Text 2: Most of the activities taken under the regulatory actions have been longer that 12 months.\n",
      "\t Tokenized GPT2:Most Ġof Ġthe Ġactivities Ġtaken Ġunder Ġthe Ġregulatory Ġactions Ġhave Ġbeen Ġlonger Ġthat Ġ12 Ġmonths .\n",
      "\t Tokenized LLAMA3:Most Ġof Ġthe Ġactivities Ġtaken Ġunder Ġthe Ġregulatory Ġactions Ġhave Ġbeen Ġlonger Ġthat Ġ 12 Ġmonths .\n",
      "\t Unique Tokens GPT2: {'Ġ12'}\n",
      "\t Unique Tokens LLAMA3: {'Ġ', '12'}\n",
      "==entailment==\n",
      "Text 1: Black professionals braid their hair to display their ethnic pride.\n",
      "\t Tokenized GPT2:Black Ġprofessionals Ġb raid Ġtheir Ġhair Ġto Ġdisplay Ġtheir Ġethnic Ġpride .\n",
      "\t Tokenized LLAMA3:Black Ġprofessionals Ġb raid Ġtheir Ġhair Ġto Ġdisplay Ġtheir Ġethnic Ġpride .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Blacks proudly braid their hair.\n",
      "\t Tokenized GPT2:Bl acks Ġproudly Ġb raid Ġtheir Ġhair .\n",
      "\t Tokenized LLAMA3:Bl acks Ġproudly Ġb raid Ġtheir Ġhair .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: This testing of the marketplace may range from written or telephone contacts with knowledgeable federal and non-federal experts regarding similar or duplicate requirements and the results of any market test recently undertaken, to the more formal sources-sought announcements in pertinent publication\n",
      "\t Tokenized GPT2:This Ġtesting Ġof Ġthe Ġmarketplace Ġmay Ġrange Ġfrom Ġwritten Ġor Ġtelephone Ġcontacts Ġwith Ġknowledgeable Ġfederal Ġand Ġnon - fed eral Ġexperts Ġregarding Ġsimilar Ġor Ġduplicate Ġrequirements Ġand Ġthe Ġresults Ġof Ġany Ġmarket Ġtest Ġrecently Ġundertaken , Ġto Ġthe Ġmore Ġformal Ġsources - s ought Ġannouncements Ġin Ġpert inent Ġpublication\n",
      "\t Tokenized LLAMA3:This Ġtesting Ġof Ġthe Ġmarketplace Ġmay Ġrange Ġfrom Ġwritten Ġor Ġtelephone Ġcontacts Ġwith Ġknowledgeable Ġfederal Ġand Ġnon -f ed eral Ġexperts Ġregarding Ġsimilar Ġor Ġduplicate Ġrequirements Ġand Ġthe Ġresults Ġof Ġany Ġmarket Ġtest Ġrecently Ġundertaken , Ġto Ġthe Ġmore Ġformal Ġsources -s ought Ġannounce ments Ġin Ġpert inent Ġpublication\n",
      "\t Unique Tokens GPT2: {'fed', 's', 'Ġannouncements', '-', 'g'}\n",
      "\t Unique Tokens LLAMA3: {'Ġannounce', '-s', 'ed', '-f', 'ments', '.g'}\n",
      "Text 2: This marketplace testing ranges from informal to formal surveys.\n",
      "\t Tokenized GPT2:This Ġmarketplace Ġtesting Ġranges Ġfrom Ġinformal Ġto Ġformal Ġsurveys .\n",
      "\t Tokenized LLAMA3:This Ġmarketplace Ġtesting Ġranges Ġfrom Ġinformal Ġto Ġformal Ġsurveys .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: They made little effort, despite the Jesuit presence in Asia, to convert local inhabitants to Christianity or to expand their territory into the interior.\n",
      "\t Tokenized GPT2:They Ġmade Ġlittle Ġeffort , Ġdespite Ġthe ĠJ es uit Ġpresence Ġin ĠAsia , Ġto Ġconvert Ġlocal Ġinhabitants Ġto ĠChristianity Ġor Ġto Ġexpand Ġtheir Ġterritory Ġinto Ġthe Ġinterior .\n",
      "\t Tokenized LLAMA3:They Ġmade Ġlittle Ġeffort , Ġdespite Ġthe ĠJ es uit Ġpresence Ġin ĠAsia , Ġto Ġconvert Ġlocal Ġinhabitants Ġto ĠChristianity Ġor Ġto Ġexpand Ġtheir Ġterritory Ġinto Ġthe Ġinterior .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The Jesuit thought that by converting the Asian people to Christianity, it would help them to expand their territory. \n",
      "\t Tokenized GPT2:The ĠJ es uit Ġthought Ġthat Ġby Ġconverting Ġthe ĠAsian Ġpeople Ġto ĠChristianity , Ġit Ġwould Ġhelp Ġthem Ġto Ġexpand Ġtheir Ġterritory . Ġ\n",
      "\t Tokenized LLAMA3:The ĠJ es uit Ġthought Ġthat Ġby Ġconverting Ġthe ĠAsian Ġpeople Ġto ĠChristianity , Ġit Ġwould Ġhelp Ġthem Ġto Ġexpand Ġtheir Ġterritory . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: True to his word to his faithful mare, Ca'daan left Whitebelly in Fena Dim and borrowed Gray Cloud from his uncle.\n",
      "\t Tokenized GPT2:True Ġto Ġhis Ġword Ġto Ġhis Ġfaithful Ġm are , ĠCa 'd aan Ġleft ĠWhite b elly Ġin ĠF ena ĠDim Ġand Ġborrowed ĠGray ĠCloud Ġfrom Ġhis Ġuncle .\n",
      "\t Tokenized LLAMA3:True Ġto Ġhis Ġword Ġto Ġhis Ġfaithful Ġm are , ĠCa 'd aan Ġleft ĠWhite b elly Ġin ĠF ena ĠDim Ġand Ġborrowed ĠGray ĠCloud Ġfrom Ġhis Ġuncle .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Ca'daan kept his word to Gray Cloud and borrowed Whitebelly from his uncle. \n",
      "\t Tokenized GPT2:Ca 'd aan Ġkept Ġhis Ġword Ġto ĠGray ĠCloud Ġand Ġborrowed ĠWhite b elly Ġfrom Ġhis Ġuncle . Ġ\n",
      "\t Tokenized LLAMA3:Ca 'd aan Ġkept Ġhis Ġword Ġto ĠGray ĠCloud Ġand Ġborrowed ĠWhite b elly Ġfrom Ġhis Ġuncle . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: She gave the girl clothes and gifts and took her to her Connecticut estate for weekend pony rides, according to the Star . How was I supposed to compete with that?\n",
      "\t Tokenized GPT2:She Ġgave Ġthe Ġgirl Ġclothes Ġand Ġgifts Ġand Ġtook Ġher Ġto Ġher ĠConnecticut Ġestate Ġfor Ġweekend Ġpony Ġrides , Ġaccording Ġto Ġthe ĠStar Ġ. ĠHow Ġwas ĠI Ġsupposed Ġto Ġcompete Ġwith Ġthat ?\n",
      "\t Tokenized LLAMA3:She Ġgave Ġthe Ġgirl Ġclothes Ġand Ġgifts Ġand Ġtook Ġher Ġto Ġher ĠConnecticut Ġestate Ġfor Ġweekend Ġpony Ġrides , Ġaccording Ġto Ġthe ĠStar Ġ. ĠHow Ġwas ĠI Ġsupposed Ġto Ġcompete Ġwith Ġthat ?\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: She gave the boy clothes, gifts and pony rides. That's hard to compete with.\n",
      "\t Tokenized GPT2:She Ġgave Ġthe Ġboy Ġclothes , Ġgifts Ġand Ġpony Ġrides . ĠThat 's Ġhard Ġto Ġcompete Ġwith .\n",
      "\t Tokenized LLAMA3:She Ġgave Ġthe Ġboy Ġclothes , Ġgifts Ġand Ġpony Ġrides . ĠThat 's Ġhard Ġto Ġcompete Ġwith .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Decline and Decadence\n",
      "\t Tokenized GPT2:Decl ine Ġand ĠDec ad ence\n",
      "\t Tokenized LLAMA3:Decl ine Ġand ĠDec ad ence\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Decline and decadence have a direct correlation. \n",
      "\t Tokenized GPT2:Decl ine Ġand Ġdec ad ence Ġhave Ġa Ġdirect Ġcorrelation . Ġ\n",
      "\t Tokenized LLAMA3:Decl ine Ġand Ġdec ad ence Ġhave Ġa Ġdirect Ġcorrelation . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: and they're more independent and there's things to do then it's good for them to go to different i mean it he goes to a a mother's day out program now once a week both of my kids do\n",
      "\t Tokenized GPT2:and Ġthey 're Ġmore Ġindependent Ġand Ġthere 's Ġthings Ġto Ġdo Ġthen Ġit 's Ġgood Ġfor Ġthem Ġto Ġgo Ġto Ġdifferent Ġi Ġmean Ġit Ġhe Ġgoes Ġto Ġa Ġa Ġmother 's Ġday Ġout Ġprogram Ġnow Ġonce Ġa Ġweek Ġboth Ġof Ġmy Ġkids Ġdo\n",
      "\t Tokenized LLAMA3:and Ġthey 're Ġmore Ġindependent Ġand Ġthere 's Ġthings Ġto Ġdo Ġthen Ġit 's Ġgood Ġfor Ġthem Ġto Ġgo Ġto Ġdifferent Ġi Ġmean Ġit Ġhe Ġgoes Ġto Ġa Ġa Ġmother 's Ġday Ġout Ġprogram Ġnow Ġonce Ġa Ġweek Ġboth Ġof Ġmy Ġkids Ġdo\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Independence does not grant anymore options for them.\n",
      "\t Tokenized GPT2:Ind ependence Ġdoes Ġnot Ġgrant Ġanymore Ġoptions Ġfor Ġthem .\n",
      "\t Tokenized LLAMA3:Ind ependence Ġdoes Ġnot Ġgrant Ġanymore Ġoptions Ġfor Ġthem .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: but but it is peaceful i mean it is relaxing to do once you find the time to do it\n",
      "\t Tokenized GPT2:but Ġbut Ġit Ġis Ġpeaceful Ġi Ġmean Ġit Ġis Ġrelaxing Ġto Ġdo Ġonce Ġyou Ġfind Ġthe Ġtime Ġto Ġdo Ġit\n",
      "\t Tokenized LLAMA3:but Ġbut Ġit Ġis Ġpeaceful Ġi Ġmean Ġit Ġis Ġrelaxing Ġto Ġdo Ġonce Ġyou Ġfind Ġthe Ġtime Ġto Ġdo Ġit\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The time it takes is not very much.\n",
      "\t Tokenized GPT2:The Ġtime Ġit Ġtakes Ġis Ġnot Ġvery Ġmuch .\n",
      "\t Tokenized LLAMA3:The Ġtime Ġit Ġtakes Ġis Ġnot Ġvery Ġmuch .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: so i really i really don't have heart burn at all with doing it myself over four nights tie i tied the car up if four days but we're fortunate we didn't need it\n",
      "\t Tokenized GPT2:so Ġi Ġreally Ġi Ġreally Ġdon 't Ġhave Ġheart Ġburn Ġat Ġall Ġwith Ġdoing Ġit Ġmyself Ġover Ġfour Ġnights Ġtie Ġi Ġtied Ġthe Ġcar Ġup Ġif Ġfour Ġdays Ġbut Ġwe 're Ġfortunate Ġwe Ġdidn 't Ġneed Ġit\n",
      "\t Tokenized LLAMA3:so Ġi Ġreally Ġi Ġreally Ġdon 't Ġhave Ġheart Ġburn Ġat Ġall Ġwith Ġdoing Ġit Ġmyself Ġover Ġfour Ġnights Ġtie Ġi Ġtied Ġthe Ġcar Ġup Ġif Ġfour Ġdays Ġbut Ġwe 're Ġfortunate Ġwe Ġdidn 't Ġneed Ġit\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I tied the car up for four nights but I had heartburn the entire time.\n",
      "\t Tokenized GPT2:I Ġtied Ġthe Ġcar Ġup Ġfor Ġfour Ġnights Ġbut ĠI Ġhad Ġheart burn Ġthe Ġentire Ġtime .\n",
      "\t Tokenized LLAMA3:I Ġtied Ġthe Ġcar Ġup Ġfor Ġfour Ġnights Ġbut ĠI Ġhad Ġheart burn Ġthe Ġentire Ġtime .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Every August young women convene to light joss sticks and some even climb the nine-meter (30-ft) rock to pray for good husbands.\n",
      "\t Tokenized GPT2:Every ĠAugust Ġyoung Ġwomen Ġconven e Ġto Ġlight Ġj oss Ġsticks Ġand Ġsome Ġeven Ġclimb Ġthe Ġnine - meter Ġ( 30 - ft ) Ġrock Ġto Ġpray Ġfor Ġgood Ġhus bands .\n",
      "\t Tokenized LLAMA3:Every ĠAugust Ġyoung Ġwomen Ġconven e Ġto Ġlight Ġj oss Ġsticks Ġand Ġsome Ġeven Ġclimb Ġthe Ġnine -m eter Ġ( 30 - ft ) Ġrock Ġto Ġpray Ġfor Ġgood Ġhus bands .\n",
      "\t Unique Tokens GPT2: {'meter'}\n",
      "\t Unique Tokens LLAMA3: {'eter', '-m'}\n",
      "Text 2: Women converge on this place to light joss sticks and climb the rock.  \n",
      "\t Tokenized GPT2:Women Ġconver ge Ġon Ġthis Ġplace Ġto Ġlight Ġj oss Ġsticks Ġand Ġclimb Ġthe Ġrock . ĠĠ\n",
      "\t Tokenized LLAMA3:W omen Ġconver ge Ġon Ġthis Ġplace Ġto Ġlight Ġj oss Ġsticks Ġand Ġclimb Ġthe Ġrock . ĠĠ\n",
      "\t Unique Tokens GPT2: {'Women'}\n",
      "\t Unique Tokens LLAMA3: {'omen', 'W'}\n",
      "==entailment==\n",
      "Text 1: In Japan, Mainichi Shimbun criticized the new Liberal Democratic Party leader Keizo Obuchi for being devoid of fresh ideas for reviving the Japanese economy.\n",
      "\t Tokenized GPT2:In ĠJapan , ĠMain ichi ĠSh imb un Ġcriticized Ġthe Ġnew ĠLiberal ĠDemocratic ĠParty Ġleader ĠKe iz o ĠOb uchi Ġfor Ġbeing Ġdev oid Ġof Ġfresh Ġideas Ġfor Ġrev iving Ġthe ĠJapanese Ġeconomy .\n",
      "\t Tokenized LLAMA3:In ĠJapan , ĠMain ichi ĠSh imb un Ġcriticized Ġthe Ġnew ĠLiberal ĠDemocratic ĠParty Ġleader ĠKe iz o ĠOb uchi Ġfor Ġbeing Ġdev oid Ġof Ġfresh Ġideas Ġfor Ġrev iving Ġthe ĠJapanese Ġeconomy .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Mainichi Shimbun was critical of Keizo Obuchi, the new Liberal Democratic Party Leader.\n",
      "\t Tokenized GPT2:Main ichi ĠSh imb un Ġwas Ġcritical Ġof ĠKe iz o ĠOb uchi , Ġthe Ġnew ĠLiberal ĠDemocratic ĠParty ĠLeader .\n",
      "\t Tokenized LLAMA3:Main ichi ĠSh imb un Ġwas Ġcritical Ġof ĠKe iz o ĠOb uchi , Ġthe Ġnew ĠLiberal ĠDemocratic ĠParty ĠLeader .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: One bakes Flipper.\n",
      "\t Tokenized GPT2:One Ġb akes ĠF li pper .\n",
      "\t Tokenized LLAMA3:One Ġb akes ĠF li pper .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The flipper was here.\n",
      "\t Tokenized GPT2:The Ġfli pper Ġwas Ġhere .\n",
      "\t Tokenized LLAMA3:The Ġfli pper Ġwas Ġhere .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The purpose of the Diwan-i-Khas is hotly disputed; it is not necessarily the hall of private audience that its name implies.\n",
      "\t Tokenized GPT2:The Ġpurpose Ġof Ġthe ĠDi wan - i - K has Ġis Ġhot ly Ġdisput ed ; Ġit Ġis Ġnot Ġnecessarily Ġthe Ġhall Ġof Ġprivate Ġaudience Ġthat Ġits Ġname Ġimplies .\n",
      "\t Tokenized LLAMA3:The Ġpurpose Ġof Ġthe ĠDi wan -i -K has Ġis Ġhot ly Ġdis puted ; Ġit Ġis Ġnot Ġnecessarily Ġthe Ġhall Ġof Ġprivate Ġaudience Ġthat Ġits Ġname Ġimplies .\n",
      "\t Unique Tokens GPT2: {'Ġdisput', 'ed', 'K', '-', 'i'}\n",
      "\t Unique Tokens LLAMA3: {'Ġdis', '-i', '-K', 'puted'}\n",
      "Text 2: The name suggest that it is open to the public.\n",
      "\t Tokenized GPT2:The Ġname Ġsuggest Ġthat Ġit Ġis Ġopen Ġto Ġthe Ġpublic .\n",
      "\t Tokenized LLAMA3:The Ġname Ġsuggest Ġthat Ġit Ġis Ġopen Ġto Ġthe Ġpublic .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: and ancient coins\n",
      "\t Tokenized GPT2:and Ġancient Ġcoins\n",
      "\t Tokenized LLAMA3:and Ġancient Ġcoins\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: And really old coins.\n",
      "\t Tokenized GPT2:And Ġreally Ġold Ġcoins .\n",
      "\t Tokenized LLAMA3:And Ġreally Ġold Ġcoins .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The Kal tangled both of Adrin's arms, keeping the blades far away.\n",
      "\t Tokenized GPT2:The ĠKal Ġtangled Ġboth Ġof ĠAd rin 's Ġarms , Ġkeeping Ġthe Ġblades Ġfar Ġaway .\n",
      "\t Tokenized LLAMA3:The ĠKal Ġtangled Ġboth Ġof ĠAd rin 's Ġarms , Ġkeeping Ġthe Ġblades Ġfar Ġaway .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Adrin's arms were tangled, keeping his rusty blades away from Kal.\n",
      "\t Tokenized GPT2:Ad rin 's Ġarms Ġwere Ġtangled , Ġkeeping Ġhis Ġrust y Ġblades Ġaway Ġfrom ĠKal .\n",
      "\t Tokenized LLAMA3:Ad rin 's Ġarms Ġwere Ġtangled , Ġkeeping Ġhis Ġrust y Ġblades Ġaway Ġfrom ĠKal .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The burden of his spiritual functions as high priest of Shinto and the tasks of administration led the emperor to welcome an early abdication, frequently to retire to a life of Buddhist meditation and scholarship.\n",
      "\t Tokenized GPT2:The Ġburden Ġof Ġhis Ġspiritual Ġfunctions Ġas Ġhigh Ġpriest Ġof ĠSh into Ġand Ġthe Ġtasks Ġof Ġadministration Ġled Ġthe Ġem peror Ġto Ġwelcome Ġan Ġearly Ġab d ication , Ġfrequently Ġto Ġretire Ġto Ġa Ġlife Ġof ĠBudd hist Ġmeditation Ġand Ġscholarship .\n",
      "\t Tokenized LLAMA3:The Ġburden Ġof Ġhis Ġspiritual Ġfunctions Ġas Ġhigh Ġpriest Ġof ĠSh into Ġand Ġthe Ġtasks Ġof Ġadministration Ġled Ġthe Ġem peror Ġto Ġwelcome Ġan Ġearly Ġab d ication , Ġfrequently Ġto Ġretire Ġto Ġa Ġlife Ġof ĠBudd hist Ġmeditation Ġand Ġscholarship .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: People looked down on the emperor for abandoning his duties and abdicating.\n",
      "\t Tokenized GPT2:People Ġlooked Ġdown Ġon Ġthe Ġem peror Ġfor Ġabandon ing Ġhis Ġduties Ġand Ġab d icating .\n",
      "\t Tokenized LLAMA3:People Ġlooked Ġdown Ġon Ġthe Ġem peror Ġfor Ġabandon ing Ġhis Ġduties Ġand Ġab d icating .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Managing better requires that agencies have, and rely upon, sound financial and program information.\n",
      "\t Tokenized GPT2:Man aging Ġbetter Ġrequires Ġthat Ġagencies Ġhave , Ġand Ġrely Ġupon , Ġsound Ġfinancial Ġand Ġprogram Ġinformation .\n",
      "\t Tokenized LLAMA3:Man aging Ġbetter Ġrequires Ġthat Ġagencies Ġhave , Ġand Ġrely Ġupon , Ġsound Ġfinancial Ġand Ġprogram Ġinformation .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Agencies need sound financial and program information for good management.\n",
      "\t Tokenized GPT2:Ag encies Ġneed Ġsound Ġfinancial Ġand Ġprogram Ġinformation Ġfor Ġgood Ġmanagement .\n",
      "\t Tokenized LLAMA3:Ag encies Ġneed Ġsound Ġfinancial Ġand Ġprogram Ġinformation Ġfor Ġgood Ġmanagement .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Always check with drivers and hotel employees to determine if road conditions are good before you depart.\n",
      "\t Tokenized GPT2:Always Ġcheck Ġwith Ġdrivers Ġand Ġhotel Ġemployees Ġto Ġdetermine Ġif Ġroad Ġconditions Ġare Ġgood Ġbefore Ġyou Ġdepart .\n",
      "\t Tokenized LLAMA3:Always Ġcheck Ġwith Ġdrivers Ġand Ġhotel Ġemployees Ġto Ġdetermine Ġif Ġroad Ġconditions Ġare Ġgood Ġbefore Ġyou Ġdepart .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: If the roads are not in good condition, you will be provided an extra night's stay for free.\n",
      "\t Tokenized GPT2:If Ġthe Ġroads Ġare Ġnot Ġin Ġgood Ġcondition , Ġyou Ġwill Ġbe Ġprovided Ġan Ġextra Ġnight 's Ġstay Ġfor Ġfree .\n",
      "\t Tokenized LLAMA3:If Ġthe Ġroads Ġare Ġnot Ġin Ġgood Ġcondition , Ġyou Ġwill Ġbe Ġprovided Ġan Ġextra Ġnight 's Ġstay Ġfor Ġfree .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: His arm came up over his eyes, cutting off the glare.\n",
      "\t Tokenized GPT2:His Ġarm Ġcame Ġup Ġover Ġhis Ġeyes , Ġcutting Ġoff Ġthe Ġglare .\n",
      "\t Tokenized LLAMA3:His Ġarm Ġcame Ġup Ġover Ġhis Ġeyes , Ġcutting Ġoff Ġthe Ġglare .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Everything was dark, and he couldn't see a thing.\n",
      "\t Tokenized GPT2:Everything Ġwas Ġdark , Ġand Ġhe Ġcouldn 't Ġsee Ġa Ġthing .\n",
      "\t Tokenized LLAMA3:Everything Ġwas Ġdark , Ġand Ġhe Ġcouldn 't Ġsee Ġa Ġthing .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: A piece describes the Learning Channel's new women-targeted reality TV  A Wedding Story , A Baby Story , and A Dating Story , featuring real-life marriages, babies, and dates.\n",
      "\t Tokenized GPT2:A Ġpiece Ġdescribes Ġthe ĠLearning ĠChannel 's Ġnew Ġwomen - target ed Ġreality ĠTV Ġ ĠA ĠWed ding ĠStory Ġ, ĠA ĠBaby ĠStory Ġ, Ġand ĠA ĠD ating ĠStory Ġ, Ġfeaturing Ġreal - life Ġmar riages , Ġbabies , Ġand Ġdates .\n",
      "\t Tokenized LLAMA3:A Ġpiece Ġdescribes Ġthe ĠLearning ĠChannel 's Ġnew Ġwomen -target ed Ġreality ĠTV Ġ ĠA ĠWed ding ĠStory Ġ, ĠA ĠBaby ĠStory Ġ, Ġand ĠA ĠD ating ĠStory Ġ, Ġfeaturing Ġreal -life Ġmar riages , Ġbabies , Ġand Ġdates .\n",
      "\t Unique Tokens GPT2: {'-', 'life', 'target'}\n",
      "\t Unique Tokens LLAMA3: {'-target', '-life'}\n",
      "Text 2: The LEarning Channel focuses on the male audience.\n",
      "\t Tokenized GPT2:The ĠLE ar ning ĠChannel Ġfocuses Ġon Ġthe Ġmale Ġaudience .\n",
      "\t Tokenized LLAMA3:The ĠLE ar ning ĠChannel Ġfocuses Ġon Ġthe Ġmale Ġaudience .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Beatrice and Grace made out OK legally, but some of us will never use their products again without thinking about Travolta losing his shirt in the name of those wasted-away little kids.\n",
      "\t Tokenized GPT2:Be at rice Ġand ĠGrace Ġmade Ġout ĠOK Ġlegally , Ġbut Ġsome Ġof Ġus Ġwill Ġnever Ġuse Ġtheir Ġproducts Ġagain Ġwithout Ġthinking Ġabout ĠTr av ol ta Ġlosing Ġhis Ġshirt Ġin Ġthe Ġname Ġof Ġthose Ġwasted - away Ġlittle Ġkids .\n",
      "\t Tokenized LLAMA3:Be at rice Ġand ĠGrace Ġmade Ġout ĠOK Ġlegally , Ġbut Ġsome Ġof Ġus Ġwill Ġnever Ġuse Ġtheir Ġproducts Ġagain Ġwithout Ġthinking Ġabout ĠTr av ol ta Ġlosing Ġhis Ġshirt Ġin Ġthe Ġname Ġof Ġthose Ġwasted -a way Ġlittle Ġkids .\n",
      "\t Unique Tokens GPT2: {'-', 'away'}\n",
      "\t Unique Tokens LLAMA3: {'-a', 'way'}\n",
      "Text 2: Beatrice and Grace ended up in prison at the end.\n",
      "\t Tokenized GPT2:Be at rice Ġand ĠGrace Ġended Ġup Ġin Ġprison Ġat Ġthe Ġend .\n",
      "\t Tokenized LLAMA3:Be at rice Ġand ĠGrace Ġended Ġup Ġin Ġprison Ġat Ġthe Ġend .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: eThe number of deletions was negligible.\n",
      "\t Tokenized GPT2:e The Ġnumber Ġof Ġdelet ions Ġwas Ġneglig ible .\n",
      "\t Tokenized LLAMA3:e The Ġnumber Ġof Ġdelet ions Ġwas Ġneglig ible .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The precise number of deletions was 71.\n",
      "\t Tokenized GPT2:The Ġprecise Ġnumber Ġof Ġdelet ions Ġwas Ġ71 .\n",
      "\t Tokenized LLAMA3:The Ġprecise Ġnumber Ġof Ġdelet ions Ġwas Ġ 71 .\n",
      "\t Unique Tokens GPT2: {'Ġ71'}\n",
      "\t Unique Tokens LLAMA3: {'71', 'Ġ'}\n",
      "==entailment==\n",
      "Text 1: It takes a deeper fire than most salamanders can stir, Ser Perth.\n",
      "\t Tokenized GPT2:It Ġtakes Ġa Ġdeeper Ġfire Ġthan Ġmost Ġsal am and ers Ġcan Ġstir , ĠSer ĠPer th .\n",
      "\t Tokenized LLAMA3:It Ġtakes Ġa Ġdeeper Ġfire Ġthan Ġmost Ġsal am and ers Ġcan Ġstir , ĠSer ĠPer th .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Most salamanders can't stir a fire that deep.\n",
      "\t Tokenized GPT2:Most Ġsal am and ers Ġcan 't Ġstir Ġa Ġfire Ġthat Ġdeep .\n",
      "\t Tokenized LLAMA3:Most Ġsal am and ers Ġcan 't Ġstir Ġa Ġfire Ġthat Ġdeep .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: We need your help with another new feature that starts next week.\n",
      "\t Tokenized GPT2:We Ġneed Ġyour Ġhelp Ġwith Ġanother Ġnew Ġfeature Ġthat Ġstarts Ġnext Ġweek .\n",
      "\t Tokenized LLAMA3:We Ġneed Ġyour Ġhelp Ġwith Ġanother Ġnew Ġfeature Ġthat Ġstarts Ġnext Ġweek .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: We are able to work with the new feature on our own.\n",
      "\t Tokenized GPT2:We Ġare Ġable Ġto Ġwork Ġwith Ġthe Ġnew Ġfeature Ġon Ġour Ġown .\n",
      "\t Tokenized LLAMA3:We Ġare Ġable Ġto Ġwork Ġwith Ġthe Ġnew Ġfeature Ġon Ġour Ġown .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: She was quite young, not more than eighteen.\n",
      "\t Tokenized GPT2:She Ġwas Ġquite Ġyoung , Ġnot Ġmore Ġthan Ġeighteen .\n",
      "\t Tokenized LLAMA3:She Ġwas Ġquite Ġyoung , Ġnot Ġmore Ġthan Ġeight een .\n",
      "\t Unique Tokens GPT2: {'Ġeighteen'}\n",
      "\t Unique Tokens LLAMA3: {'Ġeight', 'een'}\n",
      "Text 2: The girls was at least eighteen years old, but not much older. \n",
      "\t Tokenized GPT2:The Ġgirls Ġwas Ġat Ġleast Ġeighteen Ġyears Ġold , Ġbut Ġnot Ġmuch Ġolder . Ġ\n",
      "\t Tokenized LLAMA3:The Ġgirls Ġwas Ġat Ġleast Ġeight een Ġyears Ġold , Ġbut Ġnot Ġmuch Ġolder . Ġ\n",
      "\t Unique Tokens GPT2: {'Ġeighteen'}\n",
      "\t Unique Tokens LLAMA3: {'Ġeight', 'een'}\n",
      "==entailment==\n",
      "Text 1: no not it not no it's a it's not something\n",
      "\t Tokenized GPT2:no Ġnot Ġit Ġnot Ġno Ġit 's Ġa Ġit 's Ġnot Ġsomething\n",
      "\t Tokenized LLAMA3:no Ġnot Ġit Ġnot Ġno Ġit 's Ġa Ġit 's Ġnot Ġsomething\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It's not anything\n",
      "\t Tokenized GPT2:It 's Ġnot Ġanything\n",
      "\t Tokenized LLAMA3:It 's Ġnot Ġanything\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: (Read Slate 's  on how Bush flaunts the courage of his cliches.\n",
      "\t Tokenized GPT2:( Read ĠSl ate Ġ' s Ġ Ġon Ġhow ĠBush Ġfl a unts Ġthe Ġcourage Ġof Ġhis Ġcl iches .\n",
      "\t Tokenized LLAMA3:( Read ĠSl ate Ġ' s Ġ Ġon Ġhow ĠBush Ġfl aun ts Ġthe Ġcourage Ġof Ġhis Ġcl iches .\n",
      "\t Unique Tokens GPT2: {'unts', 'a'}\n",
      "\t Unique Tokens LLAMA3: {'ts', 'aun'}\n",
      "Text 2: Slate talks about how Bush is ashamed of his cliches.\n",
      "\t Tokenized GPT2:Sl ate Ġtalks Ġabout Ġhow ĠBush Ġis Ġashamed Ġof Ġhis Ġcl iches .\n",
      "\t Tokenized LLAMA3:Sl ate Ġtalks Ġabout Ġhow ĠBush Ġis Ġashamed Ġof Ġhis Ġcl iches .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: The last 12 years of his life are a blank.\n",
      "\t Tokenized GPT2:The Ġlast Ġ12 Ġyears Ġof Ġhis Ġlife Ġare Ġa Ġblank .\n",
      "\t Tokenized LLAMA3:The Ġlast Ġ 12 Ġyears Ġof Ġhis Ġlife Ġare Ġa Ġblank .\n",
      "\t Unique Tokens GPT2: {'Ġ12'}\n",
      "\t Unique Tokens LLAMA3: {'Ġ', '12'}\n",
      "Text 2: He can't remember the last 12 years of his life\n",
      "\t Tokenized GPT2:He Ġcan 't Ġremember Ġthe Ġlast Ġ12 Ġyears Ġof Ġhis Ġlife\n",
      "\t Tokenized LLAMA3:He Ġcan 't Ġremember Ġthe Ġlast Ġ 12 Ġyears Ġof Ġhis Ġlife\n",
      "\t Unique Tokens GPT2: {'Ġ12'}\n",
      "\t Unique Tokens LLAMA3: {'Ġ', '12'}\n",
      "==neutral==\n",
      "Text 1: they don't i don't i don't work at TI\n",
      "\t Tokenized GPT2:they Ġdon 't Ġi Ġdon 't Ġi Ġdon 't Ġwork Ġat ĠTI\n",
      "\t Tokenized LLAMA3:they Ġdon 't Ġi Ġdon 't Ġi Ġdon 't Ġwork Ġat ĠTI\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I work at Boeing instead of TI.\n",
      "\t Tokenized GPT2:I Ġwork Ġat ĠBoe ing Ġinstead Ġof ĠTI .\n",
      "\t Tokenized LLAMA3:I Ġwork Ġat ĠBoe ing Ġinstead Ġof ĠTI .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: He saw Stark buried under the earth, screaming for a mercy or death that would never come and crawling out of the rock decades later.\n",
      "\t Tokenized GPT2:He Ġsaw ĠStark Ġburied Ġunder Ġthe Ġearth , Ġscreaming Ġfor Ġa Ġmercy Ġor Ġdeath Ġthat Ġwould Ġnever Ġcome Ġand Ġcrawling Ġout Ġof Ġthe Ġrock Ġdecades Ġlater .\n",
      "\t Tokenized LLAMA3:He Ġsaw ĠStark Ġburied Ġunder Ġthe Ġearth , Ġscreaming Ġfor Ġa Ġmercy Ġor Ġdeath Ġthat Ġwould Ġnever Ġcome Ġand Ġcrawling Ġout Ġof Ġthe Ġrock Ġdecades Ġlater .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Stark got buried in a big hole.\n",
      "\t Tokenized GPT2:St ark Ġgot Ġburied Ġin Ġa Ġbig Ġhole .\n",
      "\t Tokenized LLAMA3:St ark Ġgot Ġburied Ġin Ġa Ġbig Ġhole .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: A profile crowns Chris Rock The Funniest Man in America.\n",
      "\t Tokenized GPT2:A Ġprofile Ġcrown s ĠChris ĠRock ĠThe ĠFun n iest ĠMan Ġin ĠAmerica .\n",
      "\t Tokenized LLAMA3:A Ġprofile Ġcrown s ĠChris ĠRock ĠThe ĠFun n iest ĠMan Ġin ĠAmerica .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: A profile crowns Chris Rock the Funniest Man in America, but many disagree. \n",
      "\t Tokenized GPT2:A Ġprofile Ġcrown s ĠChris ĠRock Ġthe ĠFun n iest ĠMan Ġin ĠAmerica , Ġbut Ġmany Ġdisagree . Ġ\n",
      "\t Tokenized LLAMA3:A Ġprofile Ġcrown s ĠChris ĠRock Ġthe ĠFun n iest ĠMan Ġin ĠAmerica , Ġbut Ġmany Ġdisagree . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: A re-created street of colonial Macau is lined with traditional Chinese shops.\n",
      "\t Tokenized GPT2:A Ġre - created Ġstreet Ġof Ġcolonial ĠMac au Ġis Ġlined Ġwith Ġtraditional ĠChinese Ġshops .\n",
      "\t Tokenized LLAMA3:A Ġre - created Ġstreet Ġof Ġcolonial ĠMac au Ġis Ġlined Ġwith Ġtraditional ĠChinese Ġshops .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: You'll find plenty of authentic, old-world restaurants on that street.\n",
      "\t Tokenized GPT2:You 'll Ġfind Ġplenty Ġof Ġauthentic , Ġold - world Ġrestaurants Ġon Ġthat Ġstreet .\n",
      "\t Tokenized LLAMA3:You 'll Ġfind Ġplenty Ġof Ġauthentic , Ġold -world Ġrestaurants Ġon Ġthat Ġstreet .\n",
      "\t Unique Tokens GPT2: {'world', '-'}\n",
      "\t Unique Tokens LLAMA3: {'-world'}\n",
      "==contradiction==\n",
      "Text 1: Similarly, OIM revised the electronic Grant Renewal Application to accommodate new information sought by LSC and to ensure greater ease for users.\n",
      "\t Tokenized GPT2:Similar ly , ĠO IM Ġrevised Ġthe Ġelectronic ĠGrant ĠRen ew al ĠApplication Ġto Ġaccommodate Ġnew Ġinformation Ġsought Ġby ĠL SC Ġand Ġto Ġensure Ġgreater Ġease Ġfor Ġusers .\n",
      "\t Tokenized LLAMA3:Similar ly , ĠO IM Ġrevised Ġthe Ġelectronic ĠGrant ĠRen ew al ĠApplication Ġto Ġaccommodate Ġnew Ġinformation Ġsought Ġby ĠL SC Ġand Ġto Ġensure Ġgreater Ġease Ġfor Ġusers .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The OIM is hoping to revise the Grant Renewal Application to reduce the LSC's ability to request information.\n",
      "\t Tokenized GPT2:The ĠO IM Ġis Ġhoping Ġto Ġrev ise Ġthe ĠGrant ĠRen ew al ĠApplication Ġto Ġreduce Ġthe ĠL SC 's Ġability Ġto Ġrequest Ġinformation .\n",
      "\t Tokenized LLAMA3:The ĠO IM Ġis Ġhoping Ġto Ġrev ise Ġthe ĠGrant ĠRen ew al ĠApplication Ġto Ġreduce Ġthe ĠL SC 's Ġability Ġto Ġrequest Ġinformation .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: guess it didn't last too long at the box office but i thought it was pretty good\n",
      "\t Tokenized GPT2:guess Ġit Ġdidn 't Ġlast Ġtoo Ġlong Ġat Ġthe Ġbox Ġoffice Ġbut Ġi Ġthought Ġit Ġwas Ġpretty Ġgood\n",
      "\t Tokenized LLAMA3:gu ess Ġit Ġdidn 't Ġlast Ġtoo Ġlong Ġat Ġthe Ġbox Ġoffice Ġbut Ġi Ġthought Ġit Ġwas Ġpretty Ġgood\n",
      "\t Unique Tokens GPT2: {'guess'}\n",
      "\t Unique Tokens LLAMA3: {'gu', 'ess'}\n",
      "Text 2: Wow, it lasted in the box office for so long, I'm surprised.\n",
      "\t Tokenized GPT2:Wow , Ġit Ġlasted Ġin Ġthe Ġbox Ġoffice Ġfor Ġso Ġlong , ĠI 'm Ġsurprised .\n",
      "\t Tokenized LLAMA3:Wow , Ġit Ġlasted Ġin Ġthe Ġbox Ġoffice Ġfor Ġso Ġlong , ĠI 'm Ġsurprised .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Built in a.d. 715 to help measure the peak and trough of the Nile flood.\n",
      "\t Tokenized GPT2:Bu ilt Ġin Ġa . d . Ġ7 15 Ġto Ġhelp Ġmeasure Ġthe Ġpeak Ġand Ġtr ough Ġof Ġthe ĠN ile Ġflood .\n",
      "\t Tokenized LLAMA3:B uilt Ġin Ġa .d . Ġ 715 Ġto Ġhelp Ġmeasure Ġthe Ġpeak Ġand Ġtr ough Ġof Ġthe ĠN ile Ġflood .\n",
      "\t Unique Tokens GPT2: {'ilt', '15', 'Ġ7', 'Bu', 'd'}\n",
      "\t Unique Tokens LLAMA3: {'uilt', 'Ġ', 'B', '.d', '715'}\n",
      "Text 2: It said the Nile flood was 12 feet deep.\n",
      "\t Tokenized GPT2:It Ġsaid Ġthe ĠN ile Ġflood Ġwas Ġ12 Ġfeet Ġdeep .\n",
      "\t Tokenized LLAMA3:It Ġsaid Ġthe ĠN ile Ġflood Ġwas Ġ 12 Ġfeet Ġdeep .\n",
      "\t Unique Tokens GPT2: {'Ġ12'}\n",
      "\t Unique Tokens LLAMA3: {'Ġ', '12'}\n",
      "==contradiction==\n",
      "Text 1: All of a sudden I sat down on the edge of the table, and put my face in my hands, sobbing out a 'Mon Dieu! \n",
      "\t Tokenized GPT2:All Ġof Ġa Ġsudden ĠI Ġsat Ġdown Ġon Ġthe Ġedge Ġof Ġthe Ġtable , Ġand Ġput Ġmy Ġface Ġin Ġmy Ġhands , Ġsobbing Ġout Ġa Ġ' Mon ĠDie u ! Ġ\n",
      "\t Tokenized LLAMA3:All Ġof Ġa Ġsudden ĠI Ġsat Ġdown Ġon Ġthe Ġedge Ġof Ġthe Ġtable , Ġand Ġput Ġmy Ġface Ġin Ġmy Ġhands , Ġsobbing Ġout Ġa Ġ' Mon ĠDie u ! Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I sat down on the table and started laughing out loudly.\n",
      "\t Tokenized GPT2:I Ġsat Ġdown Ġon Ġthe Ġtable Ġand Ġstarted Ġlaughing Ġout Ġloudly .\n",
      "\t Tokenized LLAMA3:I Ġsat Ġdown Ġon Ġthe Ġtable Ġand Ġstarted Ġlaughing Ġout Ġloudly .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Very simply. \n",
      "\t Tokenized GPT2:Very Ġsimply . Ġ\n",
      "\t Tokenized LLAMA3:Very Ġsimply . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Only a little explanation was needed.\n",
      "\t Tokenized GPT2:Only Ġa Ġlittle Ġexplanation Ġwas Ġneeded .\n",
      "\t Tokenized LLAMA3:Only Ġa Ġlittle Ġexplanation Ġwas Ġneeded .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: In a new retrospective, the Vienna modernist (1890-1918) wins critics' grudging respect.\n",
      "\t Tokenized GPT2:In Ġa Ġnew Ġretrospective , Ġthe ĠVienna Ġmodern ist Ġ( 18 90 - 19 18 ) Ġwins Ġcritics ' Ġgr udging Ġrespect .\n",
      "\t Tokenized LLAMA3:In Ġa Ġnew Ġretrospective , Ġthe ĠV ienna Ġmodern ist Ġ( 189 0 - 191 8 ) Ġwins Ġcritics ' Ġgr udging Ġrespect .\n",
      "\t Unique Tokens GPT2: {'ĠVienna', '90', '19', '18'}\n",
      "\t Unique Tokens LLAMA3: {'191', 'ĠV', 'ienna', '189', '0', '8'}\n",
      "Text 2: Critics are reluctant but ultimately they are forced to respect the Vienna Modernist. \n",
      "\t Tokenized GPT2:Cr itics Ġare Ġreluctant Ġbut Ġultimately Ġthey Ġare Ġforced Ġto Ġrespect Ġthe ĠVienna ĠModern ist . Ġ\n",
      "\t Tokenized LLAMA3:Cr itics Ġare Ġreluctant Ġbut Ġultimately Ġthey Ġare Ġforced Ġto Ġrespect Ġthe ĠV ienna ĠModern ist . Ġ\n",
      "\t Unique Tokens GPT2: {'ĠVienna'}\n",
      "\t Unique Tokens LLAMA3: {'ienna', 'ĠV'}\n",
      "==entailment==\n",
      "Text 1: The 37 hectares (91 acres) of garden are set on lands above the Wag Wag River, which twists through a steep and narrow valley.\n",
      "\t Tokenized GPT2:The Ġ37 Ġhe ct ares Ġ( 91 Ġacres ) Ġof Ġgarden Ġare Ġset Ġon Ġlands Ġabove Ġthe ĠW ag ĠW ag ĠRiver , Ġwhich Ġtwists Ġthrough Ġa Ġsteep Ġand Ġnarrow Ġvalley .\n",
      "\t Tokenized LLAMA3:The Ġ 37 Ġhe ct ares Ġ( 91 Ġacres ) Ġof Ġgarden Ġare Ġset Ġon Ġlands Ġabove Ġthe ĠW ag ĠW ag ĠRiver , Ġwhich Ġtwists Ġthrough Ġa Ġsteep Ġand Ġnarrow Ġvalley .\n",
      "\t Unique Tokens GPT2: {'Ġ37'}\n",
      "\t Unique Tokens LLAMA3: {'37', 'Ġ'}\n",
      "Text 2: 37 hectares is equivalent to just over 90 acres, and is the size of the gardens above the Wag Wag River.\n",
      "\t Tokenized GPT2:37 Ġhe ct ares Ġis Ġequivalent Ġto Ġjust Ġover Ġ90 Ġacres , Ġand Ġis Ġthe Ġsize Ġof Ġthe Ġgardens Ġabove Ġthe ĠW ag ĠW ag ĠRiver .\n",
      "\t Tokenized LLAMA3:37 Ġhe ct ares Ġis Ġequivalent Ġto Ġjust Ġover Ġ 90 Ġacres , Ġand Ġis Ġthe Ġsize Ġof Ġthe Ġgardens Ġabove Ġthe ĠW ag ĠW ag ĠRiver .\n",
      "\t Unique Tokens GPT2: {'Ġ90'}\n",
      "\t Unique Tokens LLAMA3: {'90', 'Ġ'}\n",
      "==entailment==\n",
      "Text 1: Ocho Rios is Spanish for  eight rivers,  but this name is not descriptive of the area.\n",
      "\t Tokenized GPT2:O cho ĠR ios Ġis ĠSpanish Ġfor Ġ Ġeight Ġrivers , Ġ Ġbut Ġthis Ġname Ġis Ġnot Ġdescriptive Ġof Ġthe Ġarea .\n",
      "\t Tokenized LLAMA3:O cho ĠR ios Ġis ĠSpanish Ġfor Ġ Ġeight Ġrivers , Ġ Ġbut Ġthis Ġname Ġis Ġnot Ġdescriptive Ġof Ġthe Ġarea .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: \"Ocho Rios\" means eight rivers in Spain's national language.\n",
      "\t Tokenized GPT2:\" O cho ĠR ios \" Ġmeans Ġeight Ġrivers Ġin ĠSpain 's Ġnational Ġlanguage .\n",
      "\t Tokenized LLAMA3:\" O cho ĠR ios \" Ġmeans Ġeight Ġrivers Ġin ĠSpain 's Ġnational Ġlanguage .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: See you Aug. 12, or soon thereafter, we hope.\n",
      "\t Tokenized GPT2:See Ġyou ĠAug . Ġ12 , Ġor Ġsoon Ġthere after , Ġwe Ġhope .\n",
      "\t Tokenized LLAMA3:See Ġyou ĠAug . Ġ 12 , Ġor Ġsoon Ġthere after , Ġwe Ġhope .\n",
      "\t Unique Tokens GPT2: {'Ġ12'}\n",
      "\t Unique Tokens LLAMA3: {'Ġ', '12'}\n",
      "Text 2: The person was going to attend on August 12.\n",
      "\t Tokenized GPT2:The Ġperson Ġwas Ġgoing Ġto Ġattend Ġon ĠAugust Ġ12 .\n",
      "\t Tokenized LLAMA3:The Ġperson Ġwas Ġgoing Ġto Ġattend Ġon ĠAugust Ġ 12 .\n",
      "\t Unique Tokens GPT2: {'Ġ12'}\n",
      "\t Unique Tokens LLAMA3: {'Ġ', '12'}\n",
      "==contradiction==\n",
      "Text 1: The number of steps built down into the interior means that it is unsuitable for the infirm or those with heart problems.\n",
      "\t Tokenized GPT2:The Ġnumber Ġof Ġsteps Ġbuilt Ġdown Ġinto Ġthe Ġinterior Ġmeans Ġthat Ġit Ġis Ġun su itable Ġfor Ġthe Ġinf irm Ġor Ġthose Ġwith Ġheart Ġproblems .\n",
      "\t Tokenized LLAMA3:The Ġnumber Ġof Ġsteps Ġbuilt Ġdown Ġinto Ġthe Ġinterior Ġmeans Ġthat Ġit Ġis Ġun su itable Ġfor Ġthe Ġinf irm Ġor Ġthose Ġwith Ġheart Ġproblems .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The interior is well suited for those with cardiac issues.\n",
      "\t Tokenized GPT2:The Ġinterior Ġis Ġwell Ġsuited Ġfor Ġthose Ġwith Ġcardiac Ġissues .\n",
      "\t Tokenized LLAMA3:The Ġinterior Ġis Ġwell Ġsuited Ġfor Ġthose Ġwith Ġcardiac Ġissues .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: His off-the-cuff style seems amateurish next to Inglis' polished mini-essays.\n",
      "\t Tokenized GPT2:His Ġoff - the - c uff Ġstyle Ġseems Ġamateur ish Ġnext Ġto ĠIn gl is ' Ġpolished Ġmini - ess ays .\n",
      "\t Tokenized LLAMA3:His Ġoff -the -c uff Ġstyle Ġseems Ġamateur ish Ġnext Ġto ĠIn gl is ' Ġpolished Ġmini - ess ays .\n",
      "\t Unique Tokens GPT2: {'c', 'the'}\n",
      "\t Unique Tokens LLAMA3: {'-c', '-the'}\n",
      "Text 2: He didn't look like an amateur \n",
      "\t Tokenized GPT2:He Ġdidn 't Ġlook Ġlike Ġan Ġamateur Ġ\n",
      "\t Tokenized LLAMA3:He Ġdidn 't Ġlook Ġlike Ġan Ġamateur Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The emotional effect is undiminished, and the gory effects are usually horribly creative.\n",
      "\t Tokenized GPT2:The Ġemotional Ġeffect Ġis Ġund imin ished , Ġand Ġthe Ġg ory Ġeffects Ġare Ġusually Ġhorribly Ġcreative .\n",
      "\t Tokenized LLAMA3:The Ġemotional Ġeffect Ġis Ġund imin ished , Ġand Ġthe Ġg ory Ġeffects Ġare Ġusually Ġhorribly Ġcreative .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The emotional effect includes feelings of horror and dismay.\n",
      "\t Tokenized GPT2:The Ġemotional Ġeffect Ġincludes Ġfeelings Ġof Ġhorror Ġand Ġdis may .\n",
      "\t Tokenized LLAMA3:The Ġemotional Ġeffect Ġincludes Ġfeelings Ġof Ġhorror Ġand Ġdis may .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The islands' names refer to the different force winds hitting them, not their topography.\n",
      "\t Tokenized GPT2:The Ġislands ' Ġnames Ġrefer Ġto Ġthe Ġdifferent Ġforce Ġwinds Ġhitting Ġthem , Ġnot Ġtheir Ġtop ography .\n",
      "\t Tokenized LLAMA3:The Ġislands ' Ġnames Ġrefer Ġto Ġthe Ġdifferent Ġforce Ġwinds Ġhitting Ġthem , Ġnot Ġtheir Ġtop ography .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The name of the islands are based on their topography.\n",
      "\t Tokenized GPT2:The Ġname Ġof Ġthe Ġislands Ġare Ġbased Ġon Ġtheir Ġtop ography .\n",
      "\t Tokenized LLAMA3:The Ġname Ġof Ġthe Ġislands Ġare Ġbased Ġon Ġtheir Ġtop ography .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The students' reaction was swift and contentious, as if their feelings had been hurt.\n",
      "\t Tokenized GPT2:The Ġstudents ' Ġreaction Ġwas Ġswift Ġand Ġcontent ious , Ġas Ġif Ġtheir Ġfeelings Ġhad Ġbeen Ġhurt .\n",
      "\t Tokenized LLAMA3:The Ġstudents ' Ġreaction Ġwas Ġswift Ġand Ġcontent ious , Ġas Ġif Ġtheir Ġfeelings Ġhad Ġbeen Ġhurt .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The students reacted with horror.\n",
      "\t Tokenized GPT2:The Ġstudents Ġreacted Ġwith Ġhorror .\n",
      "\t Tokenized LLAMA3:The Ġstudents Ġreacted Ġwith Ġhorror .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: 'Would you like some tea?'\n",
      "\t Tokenized GPT2:' Would Ġyou Ġlike Ġsome Ġtea ?'\n",
      "\t Tokenized LLAMA3:' Would Ġyou Ġlike Ġsome Ġtea ?'\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: DO you want a cup of tea?\n",
      "\t Tokenized GPT2:DO Ġyou Ġwant Ġa Ġcup Ġof Ġtea ?\n",
      "\t Tokenized LLAMA3:DO Ġyou Ġwant Ġa Ġcup Ġof Ġtea ?\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: And you are wrong in condemning it. \n",
      "\t Tokenized GPT2:And Ġyou Ġare Ġwrong Ġin Ġcondem ning Ġit . Ġ\n",
      "\t Tokenized LLAMA3:And Ġyou Ġare Ġwrong Ġin Ġcondem ning Ġit . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: You shouldn't be speaking out against it.\n",
      "\t Tokenized GPT2:You Ġshouldn 't Ġbe Ġspeaking Ġout Ġagainst Ġit .\n",
      "\t Tokenized LLAMA3:You Ġshouldn 't Ġbe Ġspeaking Ġout Ġagainst Ġit .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Alonissos has been settled longer than any other Aegean island, estimated by archaeologists to date from 100,000 b.c. , and was valued by many leaders in classical Greek times.\n",
      "\t Tokenized GPT2:Al on iss os Ġhas Ġbeen Ġsettled Ġlonger Ġthan Ġany Ġother ĠA e ge an Ġisland , Ġestimated Ġby Ġarchae ologists Ġto Ġdate Ġfrom Ġ100 , 000 Ġb . c . Ġ, Ġand Ġwas Ġvalued Ġby Ġmany Ġleaders Ġin Ġclassical ĠGreek Ġtimes .\n",
      "\t Tokenized LLAMA3:Al on iss os Ġhas Ġbeen Ġsettled Ġlonger Ġthan Ġany Ġother ĠA e ge an Ġisland , Ġestimated Ġby Ġarchae ologists Ġto Ġdate Ġfrom Ġ 100 , 000 Ġb .c . Ġ, Ġand Ġwas Ġvalued Ġby Ġmany Ġleaders Ġin Ġclassical ĠGreek Ġtimes .\n",
      "\t Unique Tokens GPT2: {'Ġ100', 'c'}\n",
      "\t Unique Tokens LLAMA3: {'.c', '100', 'Ġ'}\n",
      "Text 2: In the classical Greek era, Alonissos was highly regarded.\n",
      "\t Tokenized GPT2:In Ġthe Ġclassical ĠGreek Ġera , ĠAl on iss os Ġwas Ġhighly Ġregarded .\n",
      "\t Tokenized LLAMA3:In Ġthe Ġclassical ĠGreek Ġera , ĠAl on iss os Ġwas Ġhighly Ġregarded .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: From the corner of his eye he saw Jamus look over the broken mare.\n",
      "\t Tokenized GPT2:From Ġthe Ġcorner Ġof Ġhis Ġeye Ġhe Ġsaw ĠJam us Ġlook Ġover Ġthe Ġbroken Ġm are .\n",
      "\t Tokenized LLAMA3:From Ġthe Ġcorner Ġof Ġhis Ġeye Ġhe Ġsaw ĠJam us Ġlook Ġover Ġthe Ġbroken Ġm are .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Jamus was blinded by the sandstorm.\n",
      "\t Tokenized GPT2:J am us Ġwas Ġblind ed Ġby Ġthe Ġsand storm .\n",
      "\t Tokenized LLAMA3:J am us Ġwas Ġblind ed Ġby Ġthe Ġsand storm .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Under Ferdinand and Isabella, Spain underwent a dramatic transformation.\n",
      "\t Tokenized GPT2:Under ĠFer din and Ġand ĠIs ab ella , ĠSpain Ġunderwent Ġa Ġdramatic Ġtransformation .\n",
      "\t Tokenized LLAMA3:Under ĠFer din and Ġand ĠIs ab ella , ĠSpain Ġunderwent Ġa Ġdramatic Ġtransformation .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Ferdinand started his transformation by emancipating the peasant class.\n",
      "\t Tokenized GPT2:F er din and Ġstarted Ġhis Ġtransformation Ġby Ġem anc ip ating Ġthe Ġpe asant Ġclass .\n",
      "\t Tokenized LLAMA3:F er din and Ġstarted Ġhis Ġtransformation Ġby Ġem anc ip ating Ġthe Ġpe asant Ġclass .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Through the opt-out approach, Texas attorneys contributed $1 million this year, doubling 2001 contributions.\n",
      "\t Tokenized GPT2:Through Ġthe Ġopt - out Ġapproach , ĠTexas Ġattorneys Ġcontributed Ġ$ 1 Ġmillion Ġthis Ġyear , Ġdoub ling Ġ2001 Ġcontributions .\n",
      "\t Tokenized LLAMA3:Through Ġthe Ġopt -out Ġapproach , ĠTexas Ġattorneys Ġcontributed Ġ$ 1 Ġmillion Ġthis Ġyear , Ġdoub ling Ġ 200 1 Ġcontributions .\n",
      "\t Unique Tokens GPT2: {'-', 'Ġ2001', 'out'}\n",
      "\t Unique Tokens LLAMA3: {'200', 'Ġ', '-out'}\n",
      "Text 2: The opt-out approach has never increased how much Texas attorneys can contribute in the last twenty years.\n",
      "\t Tokenized GPT2:The Ġopt - out Ġapproach Ġhas Ġnever Ġincreased Ġhow Ġmuch ĠTexas Ġattorneys Ġcan Ġcontribute Ġin Ġthe Ġlast Ġtwenty Ġyears .\n",
      "\t Tokenized LLAMA3:The Ġopt -out Ġapproach Ġhas Ġnever Ġincreased Ġhow Ġmuch ĠTexas Ġattorneys Ġcan Ġcontribute Ġin Ġthe Ġlast Ġtwenty Ġyears .\n",
      "\t Unique Tokens GPT2: {'-', 'out'}\n",
      "\t Unique Tokens LLAMA3: {'-out'}\n",
      "==entailment==\n",
      "Text 1: Though the two cities remained unlinked by rail, this was about to change quickly.\n",
      "\t Tokenized GPT2:Though Ġthe Ġtwo Ġcities Ġremained Ġun linked Ġby Ġrail , Ġthis Ġwas Ġabout Ġto Ġchange Ġquickly .\n",
      "\t Tokenized LLAMA3:Though Ġthe Ġtwo Ġcities Ġremained Ġun link ed Ġby Ġrail , Ġthis Ġwas Ġabout Ġto Ġchange Ġquickly .\n",
      "\t Unique Tokens GPT2: {'linked'}\n",
      "\t Unique Tokens LLAMA3: {'ed', 'link'}\n",
      "Text 2: The two cities did not have a railway between them.\n",
      "\t Tokenized GPT2:The Ġtwo Ġcities Ġdid Ġnot Ġhave Ġa Ġrailway Ġbetween Ġthem .\n",
      "\t Tokenized LLAMA3:The Ġtwo Ġcities Ġdid Ġnot Ġhave Ġa Ġrailway Ġbetween Ġthem .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The Fray's reputation as a home for hostile, rude, and mean-spirited exchanges suffered a severe beating at the hands of the Reading thread, which was so civilized that participants suggested taking insulin shots afterward.\n",
      "\t Tokenized GPT2:The ĠF ray 's Ġreputation Ġas Ġa Ġhome Ġfor Ġhostile , Ġrude , Ġand Ġmean - sp ir ited Ġexchanges Ġsuffered Ġa Ġsevere Ġbeating Ġat Ġthe Ġhands Ġof Ġthe ĠReading Ġthread , Ġwhich Ġwas Ġso Ġcivil ized Ġthat Ġparticipants Ġsuggested Ġtaking Ġinsulin Ġshots Ġafterward .\n",
      "\t Tokenized LLAMA3:The ĠF ray 's Ġreputation Ġas Ġa Ġhome Ġfor Ġhostile , Ġrude , Ġand Ġmean -s pir ited Ġexchanges Ġsuffered Ġa Ġsevere Ġbeating Ġat Ġthe Ġhands Ġof Ġthe ĠReading Ġthread , Ġwhich Ġwas Ġso Ġcivil ized Ġthat Ġparticipants Ġsuggested Ġtaking Ġinsulin Ġshots Ġafterward .\n",
      "\t Unique Tokens GPT2: {'-', 'sp', 'ir'}\n",
      "\t Unique Tokens LLAMA3: {'pir', '-s'}\n",
      "Text 2: The Fray is almost always a hostile, rude, and mean place.\n",
      "\t Tokenized GPT2:The ĠF ray Ġis Ġalmost Ġalways Ġa Ġhostile , Ġrude , Ġand Ġmean Ġplace .\n",
      "\t Tokenized LLAMA3:The ĠF ray Ġis Ġalmost Ġalways Ġa Ġhostile , Ġrude , Ġand Ġmean Ġplace .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: This site provides information links, tools, and resources developed for the benefit of the audit profession, including audit programs, best practices, and research services.\n",
      "\t Tokenized GPT2:This Ġsite Ġprovides Ġinformation Ġlinks , Ġtools , Ġand Ġresources Ġdeveloped Ġfor Ġthe Ġbenefit Ġof Ġthe Ġaudit Ġprofession , Ġincluding Ġaudit Ġprograms , Ġbest Ġpractices , Ġand Ġresearch Ġservices .\n",
      "\t Tokenized LLAMA3:This Ġsite Ġprovides Ġinformation Ġlinks , Ġtools , Ġand Ġresources Ġdeveloped Ġfor Ġthe Ġbenefit Ġof Ġthe Ġaudit Ġprofession , Ġincluding Ġaudit Ġprograms , Ġbest Ġpractices , Ġand Ġresearch Ġservices .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: This site is a special portal for people who wish to make anonymous complaints about auditors.\n",
      "\t Tokenized GPT2:This Ġsite Ġis Ġa Ġspecial Ġportal Ġfor Ġpeople Ġwho Ġwish Ġto Ġmake Ġanonymous Ġcomplaints Ġabout Ġaud itors .\n",
      "\t Tokenized LLAMA3:This Ġsite Ġis Ġa Ġspecial Ġportal Ġfor Ġpeople Ġwho Ġwish Ġto Ġmake Ġanonymous Ġcomplaints Ġabout Ġaud itors .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: (j) Promotional items a member receives as a consequence of using travel or transportation services procured by the United States or accepted pursuant to 31\n",
      "\t Tokenized GPT2:( j ) ĠProm otional Ġitems Ġa Ġmember Ġreceives Ġas Ġa Ġconsequence Ġof Ġusing Ġtravel Ġor Ġtransportation Ġservices Ġproc ured Ġby Ġthe ĠUnited ĠStates Ġor Ġaccepted Ġpursu ant Ġto Ġ31\n",
      "\t Tokenized LLAMA3:(j ) ĠProm ot ional Ġitems Ġa Ġmember Ġreceives Ġas Ġa Ġconsequence Ġof Ġusing Ġtravel Ġor Ġtransportation Ġservices Ġproc ured Ġby Ġthe ĠUnited ĠStates Ġor Ġaccepted Ġpursu ant Ġto Ġ 31\n",
      "\t Unique Tokens GPT2: {'(', 'otional', 'Ġ31', 'j'}\n",
      "\t Unique Tokens LLAMA3: {'(j', 'Ġ', 'ot', 'ional', '31'}\n",
      "Text 2: A non-member can receive promotional items if they are from Spain and do not travel.\n",
      "\t Tokenized GPT2:A Ġnon - member Ġcan Ġreceive Ġpromot ional Ġitems Ġif Ġthey Ġare Ġfrom ĠSpain Ġand Ġdo Ġnot Ġtravel .\n",
      "\t Tokenized LLAMA3:A Ġnon -m ember Ġcan Ġreceive Ġpromot ional Ġitems Ġif Ġthey Ġare Ġfrom ĠSpain Ġand Ġdo Ġnot Ġtravel .\n",
      "\t Unique Tokens GPT2: {'-', 'member'}\n",
      "\t Unique Tokens LLAMA3: {'ember', '-m'}\n",
      "==contradiction==\n",
      "Text 1: For more sweeping panoramas, you can hike for less than an hour to either summit Petit-Bourg (716 m/2,349 ft) or Pigeon (770 m/2,526 ft).\n",
      "\t Tokenized GPT2:For Ġmore Ġsweeping Ġpan or amas , Ġyou Ġcan Ġhike Ġfor Ġless Ġthan Ġan Ġhour Ġto Ġeither Ġsummit ĠPet it - B our g Ġ( 7 16 Ġm / 2 , 349 Ġft ) Ġor ĠP ige on Ġ( 7 70 Ġm / 2 , 5 26 Ġft ).\n",
      "\t Tokenized LLAMA3:For Ġmore Ġsweeping Ġpan or amas , Ġyou Ġcan Ġhike Ġfor Ġless Ġthan Ġan Ġhour Ġto Ġeither Ġsummit ĠPet it -B our g Ġ( 7 16 Ġm / 2 , 349 Ġft ) Ġor ĠP ige on Ġ( 770 Ġm / 2 , 526 Ġft ).\n",
      "\t Unique Tokens GPT2: {'26', 'B', '70', '-', '5'}\n",
      "\t Unique Tokens LLAMA3: {'-B', '770', '526'}\n",
      "Text 2: For more sweeping panoramas, you can go swimming in the canyon.\n",
      "\t Tokenized GPT2:For Ġmore Ġsweeping Ġpan or amas , Ġyou Ġcan Ġgo Ġswimming Ġin Ġthe Ġcan y on .\n",
      "\t Tokenized LLAMA3:For Ġmore Ġsweeping Ġpan or amas , Ġyou Ġcan Ġgo Ġswimming Ġin Ġthe Ġcan y on .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Thus, the imbalance in the volume of mail exchanged magnifies the effect of the relatively higher rates in these countries.\n",
      "\t Tokenized GPT2:Thus , Ġthe Ġim balance Ġin Ġthe Ġvolume Ġof Ġmail Ġexchanged Ġmagn ifies Ġthe Ġeffect Ġof Ġthe Ġrelatively Ġhigher Ġrates Ġin Ġthese Ġcountries .\n",
      "\t Tokenized LLAMA3:Thus , Ġthe Ġim balance Ġin Ġthe Ġvolume Ġof Ġmail Ġexchanged Ġmagn ifies Ġthe Ġeffect Ġof Ġthe Ġrelatively Ġhigher Ġrates Ġin Ġthese Ġcountries .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: There is more mail coming in than going out.\n",
      "\t Tokenized GPT2:There Ġis Ġmore Ġmail Ġcoming Ġin Ġthan Ġgoing Ġout .\n",
      "\t Tokenized LLAMA3:There Ġis Ġmore Ġmail Ġcoming Ġin Ġthan Ġgoing Ġout .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: We shouldn't have been here as soon as this even, if it hadn't been for the fact that there was a smart doctor on the spot, who gave us the tip through the Coroner. \n",
      "\t Tokenized GPT2:We Ġshouldn 't Ġhave Ġbeen Ġhere Ġas Ġsoon Ġas Ġthis Ġeven , Ġif Ġit Ġhadn 't Ġbeen Ġfor Ġthe Ġfact Ġthat Ġthere Ġwas Ġa Ġsmart Ġdoctor Ġon Ġthe Ġspot , Ġwho Ġgave Ġus Ġthe Ġtip Ġthrough Ġthe ĠCor oner . Ġ\n",
      "\t Tokenized LLAMA3:We Ġshouldn 't Ġhave Ġbeen Ġhere Ġas Ġsoon Ġas Ġthis Ġeven , Ġif Ġit Ġhadn 't Ġbeen Ġfor Ġthe Ġfact Ġthat Ġthere Ġwas Ġa Ġsmart Ġdoctor Ġon Ġthe Ġspot , Ġwho Ġgave Ġus Ġthe Ġtip Ġthrough Ġthe ĠCor oner . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The doctor and the Coroner decided not to give us the tip.\n",
      "\t Tokenized GPT2:The Ġdoctor Ġand Ġthe ĠCor oner Ġdecided Ġnot Ġto Ġgive Ġus Ġthe Ġtip .\n",
      "\t Tokenized LLAMA3:The Ġdoctor Ġand Ġthe ĠCor oner Ġdecided Ġnot Ġto Ġgive Ġus Ġthe Ġtip .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Instead of indulging in the usual teary nostalgia about baseball (that means you, Ken Burns), Will considered it as a craft, explaining exactly why a manager calls a hit-and-run now and not on the next pitch, how a pitcher sets up his fastball, why a shortstop moves in a step for one kind of double \n",
      "\t Tokenized GPT2:Instead Ġof Ġindul ging Ġin Ġthe Ġusual Ġte ary Ġnostalg ia Ġabout Ġbaseball Ġ( that Ġmeans Ġyou , ĠKen ĠBurn s ), ĠWill Ġconsidered Ġit Ġas Ġa Ġcraft , Ġexplaining Ġexactly Ġwhy Ġa Ġmanager Ġcalls Ġa Ġhit - and - run Ġnow Ġand Ġnot Ġon Ġthe Ġnext Ġpitch , Ġhow Ġa Ġpitcher Ġsets Ġup Ġhis Ġfast ball , Ġwhy Ġa Ġshort stop Ġmoves Ġin Ġa Ġstep Ġfor Ġone Ġkind Ġof Ġdouble Ġ\n",
      "\t Tokenized LLAMA3:Instead Ġof Ġindul ging Ġin Ġthe Ġusual Ġte ary Ġnostalg ia Ġabout Ġbaseball Ġ( that Ġmeans Ġyou , ĠKen ĠBurn s ), ĠWill Ġconsidered Ġit Ġas Ġa Ġcraft , Ġexplaining Ġexactly Ġwhy Ġa Ġmanager Ġcalls Ġa Ġhit -and -run Ġnow Ġand Ġnot Ġon Ġthe Ġnext Ġpitch , Ġhow Ġa Ġpitcher Ġsets Ġup Ġhis Ġfast ball , Ġwhy Ġa Ġshort stop Ġmoves Ġin Ġa Ġstep Ġfor Ġone Ġkind Ġof Ġdouble Ġ\n",
      "\t Unique Tokens GPT2: {'-', 'and', 'run'}\n",
      "\t Unique Tokens LLAMA3: {'-run', '-and'}\n",
      "Text 2: Will is such an expert in explaining the details of baseball. \n",
      "\t Tokenized GPT2:Will Ġis Ġsuch Ġan Ġexpert Ġin Ġexplaining Ġthe Ġdetails Ġof Ġbaseball . Ġ\n",
      "\t Tokenized LLAMA3:Will Ġis Ġsuch Ġan Ġexpert Ġin Ġexplaining Ġthe Ġdetails Ġof Ġbaseball . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Second, Clinton hasn't used the bully pulpit to speak out against drug use nearly as often as his two predecessors did.\n",
      "\t Tokenized GPT2:Second , ĠClinton Ġhasn 't Ġused Ġthe Ġbully Ġpul pit Ġto Ġspeak Ġout Ġagainst Ġdrug Ġuse Ġnearly Ġas Ġoften Ġas Ġhis Ġtwo Ġpredecess ors Ġdid .\n",
      "\t Tokenized LLAMA3:Second , ĠClinton Ġhasn 't Ġused Ġthe Ġbully Ġpul pit Ġto Ġspeak Ġout Ġagainst Ġdrug Ġuse Ġnearly Ġas Ġoften Ġas Ġhis Ġtwo Ġpredecess ors Ġdid .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Hillary Clinton used the bully pulpit to speak out against drug use.\n",
      "\t Tokenized GPT2:H illary ĠClinton Ġused Ġthe Ġbully Ġpul pit Ġto Ġspeak Ġout Ġagainst Ġdrug Ġuse .\n",
      "\t Tokenized LLAMA3:H illary ĠClinton Ġused Ġthe Ġbully Ġpul pit Ġto Ġspeak Ġout Ġagainst Ġdrug Ġuse .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: A group of guys went out for a drink after work, and sitting at the bar was a real  a 6 foot blonde with a fabulous face and figure to match.\n",
      "\t Tokenized GPT2:A Ġgroup Ġof Ġguys Ġwent Ġout Ġfor Ġa Ġdrink Ġafter Ġwork , Ġand Ġsitting Ġat Ġthe Ġbar Ġwas Ġa Ġreal Ġ Ġa Ġ6 Ġfoot Ġblonde Ġwith Ġa Ġfabulous Ġface Ġand Ġfigure Ġto Ġmatch .\n",
      "\t Tokenized LLAMA3:A Ġgroup Ġof Ġguys Ġwent Ġout Ġfor Ġa Ġdrink Ġafter Ġwork , Ġand Ġsitting Ġat Ġthe Ġbar Ġwas Ġa Ġreal Ġ Ġa Ġ 6 Ġfoot Ġblonde Ġwith Ġa Ġfabulous Ġface Ġand Ġfigure Ġto Ġmatch .\n",
      "\t Unique Tokens GPT2: {'Ġ6'}\n",
      "\t Unique Tokens LLAMA3: {'6'}\n",
      "Text 2: A stunning six foot blonde woman sat at the bar with the men after work. \n",
      "\t Tokenized GPT2:A Ġstunning Ġsix Ġfoot Ġblonde Ġwoman Ġsat Ġat Ġthe Ġbar Ġwith Ġthe Ġmen Ġafter Ġwork . Ġ\n",
      "\t Tokenized LLAMA3:A Ġstunning Ġsix Ġfoot Ġblonde Ġwoman Ġsat Ġat Ġthe Ġbar Ġwith Ġthe Ġmen Ġafter Ġwork . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The data would be presented as required supplementary stewardship information accompanying the consolidated financial statements of the Federal Government but not in individual reports of its component units.\n",
      "\t Tokenized GPT2:The Ġdata Ġwould Ġbe Ġpresented Ġas Ġrequired Ġsupplement ary Ġstew ard ship Ġinformation Ġaccompanying Ġthe Ġconsolid ated Ġfinancial Ġstatements Ġof Ġthe ĠFederal ĠGovernment Ġbut Ġnot Ġin Ġindividual Ġreports Ġof Ġits Ġcomponent Ġunits .\n",
      "\t Tokenized LLAMA3:The Ġdata Ġwould Ġbe Ġpresented Ġas Ġrequired Ġsupplement ary Ġstew ard ship Ġinformation Ġaccompanying Ġthe Ġconsolid ated Ġfinancial Ġstatements Ġof Ġthe ĠFederal ĠGovernment Ġbut Ġnot Ġin Ġindividual Ġreports Ġof Ġits Ġcomponent Ġunits .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The data would be included in individual reports concerning the constituent units of the federal government.\n",
      "\t Tokenized GPT2:The Ġdata Ġwould Ġbe Ġincluded Ġin Ġindividual Ġreports Ġconcerning Ġthe Ġconstitu ent Ġunits Ġof Ġthe Ġfederal Ġgovernment .\n",
      "\t Tokenized LLAMA3:The Ġdata Ġwould Ġbe Ġincluded Ġin Ġindividual Ġreports Ġconcerning Ġthe Ġconstitu ent Ġunits Ġof Ġthe Ġfederal Ġgovernment .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: There are many homes built into the hillsides; some have been converted into art galleries and shops selling collectibles.\n",
      "\t Tokenized GPT2:There Ġare Ġmany Ġhomes Ġbuilt Ġinto Ġthe Ġhills ides ; Ġsome Ġhave Ġbeen Ġconverted Ġinto Ġart Ġgall eries Ġand Ġshops Ġselling Ġcollect ibles .\n",
      "\t Tokenized LLAMA3:There Ġare Ġmany Ġhomes Ġbuilt Ġinto Ġthe Ġhills ides ; Ġsome Ġhave Ġbeen Ġconverted Ġinto Ġart Ġgall eries Ġand Ġshops Ġselling Ġcollect ibles .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The remaining homes that have not been converted are still home to many locals.\n",
      "\t Tokenized GPT2:The Ġremaining Ġhomes Ġthat Ġhave Ġnot Ġbeen Ġconverted Ġare Ġstill Ġhome Ġto Ġmany Ġlocals .\n",
      "\t Tokenized LLAMA3:The Ġremaining Ġhomes Ġthat Ġhave Ġnot Ġbeen Ġconverted Ġare Ġstill Ġhome Ġto Ġmany Ġlocals .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: i think the rate of processing is just about uh reached the rate of housing anyway so keep the keep the normal as it is can't upset the system very much\n",
      "\t Tokenized GPT2:i Ġthink Ġthe Ġrate Ġof Ġprocessing Ġis Ġjust Ġabout Ġuh Ġreached Ġthe Ġrate Ġof Ġhousing Ġanyway Ġso Ġkeep Ġthe Ġkeep Ġthe Ġnormal Ġas Ġit Ġis Ġcan 't Ġupset Ġthe Ġsystem Ġvery Ġmuch\n",
      "\t Tokenized LLAMA3:i Ġthink Ġthe Ġrate Ġof Ġprocessing Ġis Ġjust Ġabout Ġuh Ġreached Ġthe Ġrate Ġof Ġhousing Ġanyway Ġso Ġkeep Ġthe Ġkeep Ġthe Ġnormal Ġas Ġit Ġis Ġcan 't Ġupset Ġthe Ġsystem Ġvery Ġmuch\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The rate of processing is way higher than the rate of housing to upset the system.\n",
      "\t Tokenized GPT2:The Ġrate Ġof Ġprocessing Ġis Ġway Ġhigher Ġthan Ġthe Ġrate Ġof Ġhousing Ġto Ġupset Ġthe Ġsystem .\n",
      "\t Tokenized LLAMA3:The Ġrate Ġof Ġprocessing Ġis Ġway Ġhigher Ġthan Ġthe Ġrate Ġof Ġhousing Ġto Ġupset Ġthe Ġsystem .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Another alternative is that our heroes were pursuing the noble goal of academics everywhere--tenure.\n",
      "\t Tokenized GPT2:Another Ġalternative Ġis Ġthat Ġour Ġheroes Ġwere Ġpursuing Ġthe Ġnoble Ġgoal Ġof Ġacad emics Ġeverywhere -- ten ure .\n",
      "\t Tokenized LLAMA3:Another Ġalternative Ġis Ġthat Ġour Ġheroes Ġwere Ġpursuing Ġthe Ġnoble Ġgoal Ġof Ġacad emics Ġeverywhere -- ten ure .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Academics are a good goal to strive for. \n",
      "\t Tokenized GPT2:Ac ad emics Ġare Ġa Ġgood Ġgoal Ġto Ġstrive Ġfor . Ġ\n",
      "\t Tokenized LLAMA3:Ac ad emics Ġare Ġa Ġgood Ġgoal Ġto Ġstrive Ġfor . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: appropriate agency representatives, help resolve\n",
      "\t Tokenized GPT2:appropriate Ġagency Ġrepresentatives , Ġhelp Ġresolve\n",
      "\t Tokenized LLAMA3:appropriate Ġagency Ġrepresentatives , Ġhelp Ġresolve\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: the right agency employees, help fix\n",
      "\t Tokenized GPT2:the Ġright Ġagency Ġemployees , Ġhelp Ġfix\n",
      "\t Tokenized LLAMA3:the Ġright Ġagency Ġemployees , Ġhelp Ġfix\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: You wonder whether he could win a general election coming out of the right lane of the Democratic Party.\n",
      "\t Tokenized GPT2:You Ġwonder Ġwhether Ġhe Ġcould Ġwin Ġa Ġgeneral Ġelection Ġcoming Ġout Ġof Ġthe Ġright Ġlane Ġof Ġthe ĠDemocratic ĠParty .\n",
      "\t Tokenized LLAMA3:You Ġwonder Ġwhether Ġhe Ġcould Ġwin Ġa Ġgeneral Ġelection Ġcoming Ġout Ġof Ġthe Ġright Ġlane Ġof Ġthe ĠDemocratic ĠParty .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He might run in a general election for governor while he is a conservative Democrat.\n",
      "\t Tokenized GPT2:He Ġmight Ġrun Ġin Ġa Ġgeneral Ġelection Ġfor Ġgovernor Ġwhile Ġhe Ġis Ġa Ġconservative ĠDemocrat .\n",
      "\t Tokenized LLAMA3:He Ġmight Ġrun Ġin Ġa Ġgeneral Ġelection Ġfor Ġgovernor Ġwhile Ġhe Ġis Ġa Ġconservative ĠDemocrat .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: apparently apparently the appraisers likes it because our taxes sure is high  isn't it it really is\n",
      "\t Tokenized GPT2:app arently Ġapparently Ġthe Ġapp ra isers Ġlikes Ġit Ġbecause Ġour Ġtaxes Ġsure Ġis Ġhigh Ġ Ġisn 't Ġit Ġit Ġreally Ġis\n",
      "\t Tokenized LLAMA3:app arently Ġapparently Ġthe Ġapp ra isers Ġlikes Ġit Ġbecause Ġour Ġtaxes Ġsure Ġis Ġhigh Ġ Ġisn 't Ġit Ġit Ġreally Ġis\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The appraiser did not like it one bit.\n",
      "\t Tokenized GPT2:The Ġapp ra iser Ġdid Ġnot Ġlike Ġit Ġone Ġbit .\n",
      "\t Tokenized LLAMA3:The Ġapp ra iser Ġdid Ġnot Ġlike Ġit Ġone Ġbit .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Perhaps North Africans and eastern Europeans peopled the Ligurian coast, while the Adriatic and south may have been settled by people from the Balkans and Asia Minor.\n",
      "\t Tokenized GPT2:Perhaps ĠNorth ĠAfr icans Ġand Ġeastern ĠEurope ans Ġpe op led Ġthe ĠL ig ur ian Ġcoast , Ġwhile Ġthe ĠAd ri atic Ġand Ġsouth Ġmay Ġhave Ġbeen Ġsettled Ġby Ġpeople Ġfrom Ġthe ĠB alk ans Ġand ĠAsia ĠMin or .\n",
      "\t Tokenized LLAMA3:Perhaps ĠNorth ĠAfr icans Ġand Ġeastern ĠEurope ans Ġpe op led Ġthe ĠL ig ur ian Ġcoast , Ġwhile Ġthe ĠAd ri atic Ġand Ġsouth Ġmay Ġhave Ġbeen Ġsettled Ġby Ġpeople Ġfrom Ġthe ĠB alk ans Ġand ĠAsia ĠMin or .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The people had no complaints after settling their new lands.\n",
      "\t Tokenized GPT2:The Ġpeople Ġhad Ġno Ġcomplaints Ġafter Ġsettling Ġtheir Ġnew Ġlands .\n",
      "\t Tokenized LLAMA3:The Ġpeople Ġhad Ġno Ġcomplaints Ġafter Ġsettling Ġtheir Ġnew Ġlands .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Waterloo.\n",
      "\t Tokenized GPT2:Water l oo .\n",
      "\t Tokenized LLAMA3:Water l oo .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The defeat of Napoleon.\n",
      "\t Tokenized GPT2:The Ġdefeat Ġof ĠNapoleon .\n",
      "\t Tokenized LLAMA3:The Ġdefeat Ġof ĠNap oleon .\n",
      "\t Unique Tokens GPT2: {'ĠNapoleon'}\n",
      "\t Unique Tokens LLAMA3: {'oleon', 'ĠNap'}\n",
      "==entailment==\n",
      "Text 1: i was trying to think about some of my favorite people that i liked in music and they're none of them are recent right\n",
      "\t Tokenized GPT2:i Ġwas Ġtrying Ġto Ġthink Ġabout Ġsome Ġof Ġmy Ġfavorite Ġpeople Ġthat Ġi Ġliked Ġin Ġmusic Ġand Ġthey 're Ġnone Ġof Ġthem Ġare Ġrecent Ġright\n",
      "\t Tokenized LLAMA3:i Ġwas Ġtrying Ġto Ġthink Ġabout Ġsome Ġof Ġmy Ġfavorite Ġpeople Ġthat Ġi Ġliked Ġin Ġmusic Ġand Ġthey 're Ġnone Ġof Ġthem Ġare Ġrecent Ġright\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I enjoy older music. \n",
      "\t Tokenized GPT2:I Ġenjoy Ġolder Ġmusic . Ġ\n",
      "\t Tokenized LLAMA3:I Ġenjoy Ġolder Ġmusic . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The streets are crammed with vendors selling shrine offerings of sweets, curds, and coconut, as well as garlands and holy images.\n",
      "\t Tokenized GPT2:The Ġstreets Ġare Ġcr ammed Ġwith Ġvendors Ġselling Ġshr ine Ġofferings Ġof Ġswe ets , Ġcur ds , Ġand Ġcoconut , Ġas Ġwell Ġas Ġgar lands Ġand Ġholy Ġimages .\n",
      "\t Tokenized LLAMA3:The Ġstreets Ġare Ġcr ammed Ġwith Ġvendors Ġselling Ġshr ine Ġofferings Ġof Ġswe ets , Ġcur ds , Ġand Ġcoconut , Ġas Ġwell Ġas Ġgar lands Ġand Ġholy Ġimages .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Vendors have lined the streets with torches and fires.\n",
      "\t Tokenized GPT2:V end ors Ġhave Ġlined Ġthe Ġstreets Ġwith Ġtor ches Ġand Ġfires .\n",
      "\t Tokenized LLAMA3:V end ors Ġhave Ġlined Ġthe Ġstreets Ġwith Ġtor ches Ġand Ġfires .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: He was waiting for the Scotland Yard men. \n",
      "\t Tokenized GPT2:He Ġwas Ġwaiting Ġfor Ġthe ĠScotland ĠY ard Ġmen . Ġ\n",
      "\t Tokenized LLAMA3:He Ġwas Ġwaiting Ġfor Ġthe ĠScotland ĠY ard Ġmen . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The Scotland Yard men were coming.\n",
      "\t Tokenized GPT2:The ĠScotland ĠY ard Ġmen Ġwere Ġcoming .\n",
      "\t Tokenized LLAMA3:The ĠScotland ĠY ard Ġmen Ġwere Ġcoming .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: yeah okay yeah those games are fun to watch you you you watch those games\n",
      "\t Tokenized GPT2:yeah Ġokay Ġyeah Ġthose Ġgames Ġare Ġfun Ġto Ġwatch Ġyou Ġyou Ġyou Ġwatch Ġthose Ġgames\n",
      "\t Tokenized LLAMA3:yeah Ġokay Ġyeah Ġthose Ġgames Ġare Ġfun Ġto Ġwatch Ġyou Ġyou Ġyou Ġwatch Ġthose Ġgames\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Those games are a lot of fun.\n",
      "\t Tokenized GPT2:Those Ġgames Ġare Ġa Ġlot Ġof Ġfun .\n",
      "\t Tokenized LLAMA3:Those Ġgames Ġare Ġa Ġlot Ġof Ġfun .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Where lies the real Japan?\n",
      "\t Tokenized GPT2:Where Ġlies Ġthe Ġreal ĠJapan ?\n",
      "\t Tokenized LLAMA3:Where Ġlies Ġthe Ġreal ĠJapan ?\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The real Japan can be found.\n",
      "\t Tokenized GPT2:The Ġreal ĠJapan Ġcan Ġbe Ġfound .\n",
      "\t Tokenized LLAMA3:The Ġreal ĠJapan Ġcan Ġbe Ġfound .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The notable thing for me about the Left Behind series--beside the fact that few in the secular media have noticed that millions of Americans are busy reading books warning about the imminence of one-world government, mass death, and the return of the Messiah, is that all the Jewish characters are Ch\n",
      "\t Tokenized GPT2:The Ġnotable Ġthing Ġfor Ġme Ġabout Ġthe ĠLeft ĠBe hind Ġseries -- bes ide Ġthe Ġfact Ġthat Ġfew Ġin Ġthe Ġsecular Ġmedia Ġhave Ġnoticed Ġthat Ġmillions Ġof ĠAmericans Ġare Ġbusy Ġreading Ġbooks Ġwarning Ġabout Ġthe Ġim min ence Ġof Ġone - world Ġgovernment , Ġmass Ġdeath , Ġand Ġthe Ġreturn Ġof Ġthe ĠMess iah , Ġis Ġthat Ġall Ġthe ĠJewish Ġcharacters Ġare ĠCh\n",
      "\t Tokenized LLAMA3:The Ġnotable Ġthing Ġfor Ġme Ġabout Ġthe ĠLeft ĠBe hind Ġseries -- bes ide Ġthe Ġfact Ġthat Ġfew Ġin Ġthe Ġsecular Ġmedia Ġhave Ġnoticed Ġthat Ġmillions Ġof ĠAmericans Ġare Ġbusy Ġreading Ġbooks Ġwarning Ġabout Ġthe Ġim min ence Ġof Ġone -world Ġgovernment , Ġmass Ġdeath , Ġand Ġthe Ġreturn Ġof Ġthe ĠMess iah , Ġis Ġthat Ġall Ġthe ĠJewish Ġcharacters Ġare ĠCh\n",
      "\t Unique Tokens GPT2: {'world', '-'}\n",
      "\t Unique Tokens LLAMA3: {'-world'}\n",
      "Text 2: The Left Behind series is about people converting to Christianity.\n",
      "\t Tokenized GPT2:The ĠLeft ĠBe hind Ġseries Ġis Ġabout Ġpeople Ġconverting Ġto ĠChristianity .\n",
      "\t Tokenized LLAMA3:The ĠLeft ĠBe hind Ġseries Ġis Ġabout Ġpeople Ġconverting Ġto ĠChristianity .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The event is the definition of a crowd pleaser, replete with appearances by the Rockettes, the Mormon Tabernacle Choir, and Santa Claus (the act isn't entirely without bite; there's also a very funny moment involving a heart attack).\n",
      "\t Tokenized GPT2:The Ġevent Ġis Ġthe Ġdefinition Ġof Ġa Ġcrowd Ġple aser , Ġre plete Ġwith Ġappearances Ġby Ġthe ĠR ocket tes , Ġthe ĠMorm on ĠTab ern acle ĠCh oir , Ġand ĠSanta ĠCl aus Ġ( the Ġact Ġisn 't Ġentirely Ġwithout Ġbite ; Ġthere 's Ġalso Ġa Ġvery Ġfunny Ġmoment Ġinvolving Ġa Ġheart Ġattack ).\n",
      "\t Tokenized LLAMA3:The Ġevent Ġis Ġthe Ġdefinition Ġof Ġa Ġcrowd Ġple aser , Ġre plete Ġwith Ġappearances Ġby Ġthe ĠR ocket tes , Ġthe ĠMorm on ĠTab ern acle ĠCh oir , Ġand ĠSanta ĠCl aus Ġ( the Ġact Ġisn 't Ġentirely Ġwithout Ġbite ; Ġthere 's Ġalso Ġa Ġvery Ġfunny Ġmoment Ġinvolving Ġa Ġheart Ġattack ).\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Expectations are low for the event on the back of Santa Claus's absence due to health concerns.\n",
      "\t Tokenized GPT2:Ex pect ations Ġare Ġlow Ġfor Ġthe Ġevent Ġon Ġthe Ġback Ġof ĠSanta ĠCl aus 's Ġabsence Ġdue Ġto Ġhealth Ġconcerns .\n",
      "\t Tokenized LLAMA3:Ex pect ations Ġare Ġlow Ġfor Ġthe Ġevent Ġon Ġthe Ġback Ġof ĠSanta ĠCl aus 's Ġabsence Ġdue Ġto Ġhealth Ġconcerns .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Then Shuman claims that Linux provides no graphical user interface.\n",
      "\t Tokenized GPT2:Then ĠSh uman Ġclaims Ġthat ĠLinux Ġprovides Ġno Ġgraphical Ġuser Ġinterface .\n",
      "\t Tokenized LLAMA3:Then ĠSh uman Ġclaims Ġthat ĠLinux Ġprovides Ġno Ġgraphical Ġuser Ġinterface .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: They made accusations about the platform.\n",
      "\t Tokenized GPT2:They Ġmade Ġaccusations Ġabout Ġthe Ġplatform .\n",
      "\t Tokenized LLAMA3:They Ġmade Ġaccusations Ġabout Ġthe Ġplatform .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Founded in 1979, AFFIRM's members include information resource management professionals within the federal, academic, and industry sectors.\n",
      "\t Tokenized GPT2:Found ed Ġin Ġ1979 , ĠA FF IR M 's Ġmembers Ġinclude Ġinformation Ġresource Ġmanagement Ġprofessionals Ġwithin Ġthe Ġfederal , Ġacademic , Ġand Ġindustry Ġsectors .\n",
      "\t Tokenized LLAMA3:F ounded Ġin Ġ 197 9 , ĠA FF IR M 's Ġmembers Ġinclude Ġinformation Ġresource Ġmanagement Ġprofessionals Ġwithin Ġthe Ġfederal , Ġacademic , Ġand Ġindustry Ġsectors .\n",
      "\t Unique Tokens GPT2: {'Ġ1979', 'ed', 'Found'}\n",
      "\t Unique Tokens LLAMA3: {'Ġ', '197', 'ounded', '9', 'F'}\n",
      "Text 2: AFFIRM was founded in the early 2000s.\n",
      "\t Tokenized GPT2:A FF IR M Ġwas Ġfounded Ġin Ġthe Ġearly Ġ2000 s .\n",
      "\t Tokenized LLAMA3:A FF IR M Ġwas Ġfounded Ġin Ġthe Ġearly Ġ 200 0 s .\n",
      "\t Unique Tokens GPT2: {'Ġ2000'}\n",
      "\t Unique Tokens LLAMA3: {'0', 'Ġ', '200'}\n",
      "==contradiction==\n",
      "Text 1: You will learn later that the person who usually poured out Mrs. Inglethorp's medicine was always extremely careful not to shake the bottle, but to leave the sediment at the bottom of it undisturbed. \n",
      "\t Tokenized GPT2:You Ġwill Ġlearn Ġlater Ġthat Ġthe Ġperson Ġwho Ġusually Ġpoured Ġout ĠMrs . ĠIn gle th orp 's Ġmedicine Ġwas Ġalways Ġextremely Ġcareful Ġnot Ġto Ġshake Ġthe Ġbottle , Ġbut Ġto Ġleave Ġthe Ġsed iment Ġat Ġthe Ġbottom Ġof Ġit Ġund ist ur bed . Ġ\n",
      "\t Tokenized LLAMA3:You Ġwill Ġlearn Ġlater Ġthat Ġthe Ġperson Ġwho Ġusually Ġpoured Ġout ĠMrs . ĠIn gle th orp 's Ġmedicine Ġwas Ġalways Ġextremely Ġcareful Ġnot Ġto Ġshake Ġthe Ġbottle , Ġbut Ġto Ġleave Ġthe Ġsed iment Ġat Ġthe Ġbottom Ġof Ġit Ġund ist ur bed . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The person pouring Mrs. Inglethorp's medicine was always very careful to shake the bottle. \n",
      "\t Tokenized GPT2:The Ġperson Ġpouring ĠMrs . ĠIn gle th orp 's Ġmedicine Ġwas Ġalways Ġvery Ġcareful Ġto Ġshake Ġthe Ġbottle . Ġ\n",
      "\t Tokenized LLAMA3:The Ġperson Ġpouring ĠMrs . ĠIn gle th orp 's Ġmedicine Ġwas Ġalways Ġvery Ġcareful Ġto Ġshake Ġthe Ġbottle . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Major journeys from one part of the country to another, say, from Milan to Rome or down to Naples, is most enjoyed by train buffs and travelers with plenty of time, patience, and curiosity.\n",
      "\t Tokenized GPT2:Major Ġjour neys Ġfrom Ġone Ġpart Ġof Ġthe Ġcountry Ġto Ġanother , Ġsay , Ġfrom ĠMil an Ġto ĠRome Ġor Ġdown Ġto ĠNap les , Ġis Ġmost Ġenjoyed Ġby Ġtrain Ġbuff s Ġand Ġtravelers Ġwith Ġplenty Ġof Ġtime , Ġpatience , Ġand Ġcuriosity .\n",
      "\t Tokenized LLAMA3:Major Ġjour neys Ġfrom Ġone Ġpart Ġof Ġthe Ġcountry Ġto Ġanother , Ġsay , Ġfrom ĠMil an Ġto ĠRome Ġor Ġdown Ġto ĠNap les , Ġis Ġmost Ġenjoyed Ġby Ġtrain Ġbuff s Ġand Ġtravel ers Ġwith Ġplenty Ġof Ġtime , Ġpatience , Ġand Ġcuriosity .\n",
      "\t Unique Tokens GPT2: {'Ġtravelers'}\n",
      "\t Unique Tokens LLAMA3: {'Ġtravel', 'ers'}\n",
      "Text 2: Train travel in Italy is generally quick, making it a practical option for impatient travelers.\n",
      "\t Tokenized GPT2:Tr ain Ġtravel Ġin ĠItaly Ġis Ġgenerally Ġquick , Ġmaking Ġit Ġa Ġpractical Ġoption Ġfor Ġimpatient Ġtravelers .\n",
      "\t Tokenized LLAMA3:Tr ain Ġtravel Ġin ĠItaly Ġis Ġgenerally Ġquick , Ġmaking Ġit Ġa Ġpractical Ġoption Ġfor Ġimpatient Ġtravel ers .\n",
      "\t Unique Tokens GPT2: {'Ġtravelers'}\n",
      "\t Unique Tokens LLAMA3: {'ers'}\n",
      "==entailment==\n",
      "Text 1: Lucy screamed, I've got to know.\n",
      "\t Tokenized GPT2:Lu cy Ġscreamed , ĠI 've Ġgot Ġto Ġknow .\n",
      "\t Tokenized LLAMA3:Lu cy Ġscreamed , ĠI 've Ġgot Ġto Ġknow .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Lucy wanted to know.\n",
      "\t Tokenized GPT2:Lu cy Ġwanted Ġto Ġknow .\n",
      "\t Tokenized LLAMA3:Lu cy Ġwanted Ġto Ġknow .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Are you sure we should take him down there?' Greuze asked Natalia.\n",
      "\t Tokenized GPT2:Are Ġyou Ġsure Ġwe Ġshould Ġtake Ġhim Ġdown Ġthere ?' ĠGre u ze Ġasked ĠNatal ia .\n",
      "\t Tokenized LLAMA3:Are Ġyou Ġsure Ġwe Ġshould Ġtake Ġhim Ġdown Ġthere ?' ĠGre u ze Ġasked ĠNatal ia .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Natalia was certain it was alright to bring him down there, and told Greuze that no matter what, he must end up down there. \n",
      "\t Tokenized GPT2:N atal ia Ġwas Ġcertain Ġit Ġwas Ġalright Ġto Ġbring Ġhim Ġdown Ġthere , Ġand Ġtold ĠGre u ze Ġthat Ġno Ġmatter Ġwhat , Ġhe Ġmust Ġend Ġup Ġdown Ġthere . Ġ\n",
      "\t Tokenized LLAMA3:N atal ia Ġwas Ġcertain Ġit Ġwas Ġalright Ġto Ġbring Ġhim Ġdown Ġthere , Ġand Ġtold ĠGre u ze Ġthat Ġno Ġmatter Ġwhat , Ġhe Ġmust Ġend Ġup Ġdown Ġthere . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: yeah it's true it is in in fact i have a friend of mine that moved to North Carolina she's um an emergency room nurse she does the operating room\n",
      "\t Tokenized GPT2:yeah Ġit 's Ġtrue Ġit Ġis Ġin Ġin Ġfact Ġi Ġhave Ġa Ġfriend Ġof Ġmine Ġthat Ġmoved Ġto ĠNorth ĠCarolina Ġshe 's Ġum Ġan Ġemergency Ġroom Ġnurse Ġshe Ġdoes Ġthe Ġoperating Ġroom\n",
      "\t Tokenized LLAMA3:yeah Ġit 's Ġtrue Ġit Ġis Ġin Ġin Ġfact Ġi Ġhave Ġa Ġfriend Ġof Ġmine Ġthat Ġmoved Ġto ĠNorth ĠCarolina Ġshe 's Ġum Ġan Ġemergency Ġroom Ġnurse Ġshe Ġdoes Ġthe Ġoperating Ġroom\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: My friend moved to NC to take a job as a nurse in a trauma unit at a hospital emergency room.\n",
      "\t Tokenized GPT2:My Ġfriend Ġmoved Ġto ĠNC Ġto Ġtake Ġa Ġjob Ġas Ġa Ġnurse Ġin Ġa Ġtrauma Ġunit Ġat Ġa Ġhospital Ġemergency Ġroom .\n",
      "\t Tokenized LLAMA3:My Ġfriend Ġmoved Ġto ĠNC Ġto Ġtake Ġa Ġjob Ġas Ġa Ġnurse Ġin Ġa Ġtrauma Ġunit Ġat Ġa Ġhospital Ġemergency Ġroom .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: or just get out and walk uh or even jog a little although i don't do that regularly but Washington's a great place to do that\n",
      "\t Tokenized GPT2:or Ġjust Ġget Ġout Ġand Ġwalk Ġuh Ġor Ġeven Ġjog Ġa Ġlittle Ġalthough Ġi Ġdon 't Ġdo Ġthat Ġregularly Ġbut ĠWashington 's Ġa Ġgreat Ġplace Ġto Ġdo Ġthat\n",
      "\t Tokenized LLAMA3:or Ġjust Ġget Ġout Ġand Ġwalk Ġuh Ġor Ġeven Ġjog Ġa Ġlittle Ġalthough Ġi Ġdon 't Ġdo Ġthat Ġregularly Ġbut ĠWashington 's Ġa Ġgreat Ġplace Ġto Ġdo Ġthat\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: \"I regularly go for a walk or a jog at Washington's.\"\n",
      "\t Tokenized GPT2:\" I Ġregularly Ġgo Ġfor Ġa Ġwalk Ġor Ġa Ġjog Ġat ĠWashington 's .\"\n",
      "\t Tokenized LLAMA3:\"I Ġregularly Ġgo Ġfor Ġa Ġwalk Ġor Ġa Ġjog Ġat ĠWashington 's .\"\n",
      "\t Unique Tokens GPT2: {'I', '\"'}\n",
      "\t Unique Tokens LLAMA3: {'\"I'}\n",
      "==entailment==\n",
      "Text 1: well that's pretty typical though uh i don't uh i don't guess it's going to be any much different uh than than it has been in the past so i expect uh July and August we'll see our  or uh share of hundred degree days\n",
      "\t Tokenized GPT2:well Ġthat 's Ġpretty Ġtypical Ġthough Ġuh Ġi Ġdon 't Ġuh Ġi Ġdon 't Ġguess Ġit 's Ġgoing Ġto Ġbe Ġany Ġmuch Ġdifferent Ġuh Ġthan Ġthan Ġit Ġhas Ġbeen Ġin Ġthe Ġpast Ġso Ġi Ġexpect Ġuh ĠJuly Ġand ĠAugust Ġwe 'll Ġsee Ġour Ġ Ġor Ġuh Ġshare Ġof Ġhundred Ġdegree Ġdays\n",
      "\t Tokenized LLAMA3:well Ġthat 's Ġpretty Ġtypical Ġthough Ġuh Ġi Ġdon 't Ġuh Ġi Ġdon 't Ġguess Ġit 's Ġgoing Ġto Ġbe Ġany Ġmuch Ġdifferent Ġuh Ġthan Ġthan Ġit Ġhas Ġbeen Ġin Ġthe Ġpast Ġso Ġi Ġexpect Ġuh ĠJuly Ġand ĠAugust Ġwe 'll Ġsee Ġour Ġ Ġor Ġuh Ġshare Ġof Ġhundred Ġdegree Ġdays\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It's pretty normal to have a few hundred degree days out here.\n",
      "\t Tokenized GPT2:It 's Ġpretty Ġnormal Ġto Ġhave Ġa Ġfew Ġhundred Ġdegree Ġdays Ġout Ġhere .\n",
      "\t Tokenized LLAMA3:It 's Ġpretty Ġnormal Ġto Ġhave Ġa Ġfew Ġhundred Ġdegree Ġdays Ġout Ġhere .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: That couldn't happen in a sane world, either.\n",
      "\t Tokenized GPT2:That Ġcouldn 't Ġhappen Ġin Ġa Ġsane Ġworld , Ġeither .\n",
      "\t Tokenized LLAMA3:That Ġcouldn 't Ġhappen Ġin Ġa Ġsane Ġworld , Ġeither .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: That could not happen in a world that wasn't insane, either.\n",
      "\t Tokenized GPT2:That Ġcould Ġnot Ġhappen Ġin Ġa Ġworld Ġthat Ġwasn 't Ġinsane , Ġeither .\n",
      "\t Tokenized LLAMA3:That Ġcould Ġnot Ġhappen Ġin Ġa Ġworld Ġthat Ġwasn 't Ġinsane , Ġeither .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: And far, far away- lying still on the tracks- was the back of the train.\n",
      "\t Tokenized GPT2:And Ġfar , Ġfar Ġaway - Ġlying Ġstill Ġon Ġthe Ġtracks - Ġwas Ġthe Ġback Ġof Ġthe Ġtrain .\n",
      "\t Tokenized LLAMA3:And Ġfar , Ġfar Ġaway - Ġlying Ġstill Ġon Ġthe Ġtracks - Ġwas Ġthe Ġback Ġof Ġthe Ġtrain .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The train was speeding along the track. \n",
      "\t Tokenized GPT2:The Ġtrain Ġwas Ġspeeding Ġalong Ġthe Ġtrack . Ġ\n",
      "\t Tokenized LLAMA3:The Ġtrain Ġwas Ġspeeding Ġalong Ġthe Ġtrack . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Explanation building is the inverse  starting with the observations, the evaluator develops a picture of what is happening and why.\n",
      "\t Tokenized GPT2:Ex planation Ġbuilding Ġis Ġthe Ġinverse Ġ Ġstarting Ġwith Ġthe Ġobservations , Ġthe Ġevalu ator Ġdevelops Ġa Ġpicture Ġof Ġwhat Ġis Ġhappening Ġand Ġwhy .\n",
      "\t Tokenized LLAMA3:Ex plan ation Ġbuilding Ġis Ġthe Ġinverse Ġ Ġstarting Ġwith Ġthe Ġobservations , Ġthe Ġevalu ator Ġdevelops Ġa Ġpicture Ġof Ġwhat Ġis Ġhappening Ġand Ġwhy .\n",
      "\t Unique Tokens GPT2: {'planation'}\n",
      "\t Unique Tokens LLAMA3: {'ation', 'plan'}\n",
      "Text 2: Explanation building is going to be the standard approach in the future.\n",
      "\t Tokenized GPT2:Ex planation Ġbuilding Ġis Ġgoing Ġto Ġbe Ġthe Ġstandard Ġapproach Ġin Ġthe Ġfuture .\n",
      "\t Tokenized LLAMA3:Ex plan ation Ġbuilding Ġis Ġgoing Ġto Ġbe Ġthe Ġstandard Ġapproach Ġin Ġthe Ġfuture .\n",
      "\t Unique Tokens GPT2: {'planation'}\n",
      "\t Unique Tokens LLAMA3: {'ation', 'plan'}\n",
      "==neutral==\n",
      "Text 1: Why bother to sacrifice your lives for dirt farmers and slavers?\n",
      "\t Tokenized GPT2:Why Ġbother Ġto Ġsacrifice Ġyour Ġlives Ġfor Ġdirt Ġfarmers Ġand Ġsl a vers ?\n",
      "\t Tokenized LLAMA3:Why Ġbother Ġto Ġsacrifice Ġyour Ġlives Ġfor Ġdirt Ġfarmers Ġand Ġsla vers ?\n",
      "\t Unique Tokens GPT2: {'a', 'Ġsl'}\n",
      "\t Unique Tokens LLAMA3: {'Ġsla'}\n",
      "Text 2: People sacrifice their lives for farmers and slaves.\n",
      "\t Tokenized GPT2:People Ġsacrifice Ġtheir Ġlives Ġfor Ġfarmers Ġand Ġslaves .\n",
      "\t Tokenized LLAMA3:People Ġsacrifice Ġtheir Ġlives Ġfor Ġfarmers Ġand Ġslaves .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: 11 These departures permit them take advantage of the lower cost of living as well as to be reunited with their spouses and children.\n",
      "\t Tokenized GPT2:11 ĠThese Ġdepart ures Ġpermit Ġthem Ġtake Ġadvantage Ġof Ġthe Ġlower Ġcost Ġof Ġliving Ġas Ġwell Ġas Ġto Ġbe Ġreun ited Ġwith Ġtheir Ġsp ouses Ġand Ġchildren .\n",
      "\t Tokenized LLAMA3:11 ĠThese Ġdepart ures Ġpermit Ġthem Ġtake Ġadvantage Ġof Ġthe Ġlower Ġcost Ġof Ġliving Ġas Ġwell Ġas Ġto Ġbe Ġreun ited Ġwith Ġtheir Ġsp ouses Ġand Ġchildren .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The departures help them take advantage of the high cost of living in other areas.\n",
      "\t Tokenized GPT2:The Ġdepart ures Ġhelp Ġthem Ġtake Ġadvantage Ġof Ġthe Ġhigh Ġcost Ġof Ġliving Ġin Ġother Ġareas .\n",
      "\t Tokenized LLAMA3:The Ġdepart ures Ġhelp Ġthem Ġtake Ġadvantage Ġof Ġthe Ġhigh Ġcost Ġof Ġliving Ġin Ġother Ġareas .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: 36 AC usage nationally for mercury control from power plants should be roughly proportional to the total MWe of coal-fired facilities that are equipped with the technology (this assumes an average capacity factor of 85 percent and other assumptions of Tables 4-4 and 4-5).\n",
      "\t Tokenized GPT2:36 ĠAC Ġusage Ġnation ally Ġfor Ġmer cury Ġcontrol Ġfrom Ġpower Ġplants Ġshould Ġbe Ġroughly Ġproportional Ġto Ġthe Ġtotal ĠM We Ġof Ġcoal - f ired Ġfacilities Ġthat Ġare Ġequipped Ġwith Ġthe Ġtechnology Ġ( this Ġassumes Ġan Ġaverage Ġcapacity Ġfactor Ġof Ġ85 Ġpercent Ġand Ġother Ġassumptions Ġof ĠT ables Ġ4 - 4 Ġand Ġ4 - 5 ).\n",
      "\t Tokenized LLAMA3:36 ĠAC Ġusage Ġnation ally Ġfor Ġmer cury Ġcontrol Ġfrom Ġpower Ġplants Ġshould Ġbe Ġroughly Ġproportional Ġto Ġthe Ġtotal ĠM We Ġof Ġcoal -f ired Ġfacilities Ġthat Ġare Ġequipped Ġwith Ġthe Ġtechnology Ġ( this Ġassumes Ġan Ġaverage Ġcapacity Ġfactor Ġof Ġ 85 Ġpercent Ġand Ġother Ġassumptions Ġof ĠT ables Ġ 4 - 4 Ġand Ġ 4 - 5 ).\n",
      "\t Unique Tokens GPT2: {'f', 'Ġ85', 'Ġ4'}\n",
      "\t Unique Tokens LLAMA3: {'-f', 'Ġ', '85'}\n",
      "Text 2: Power plants' mercury control AC usage is higher than total MWe from coal facilities.\n",
      "\t Tokenized GPT2:Power Ġplants ' Ġmer cury Ġcontrol ĠAC Ġusage Ġis Ġhigher Ġthan Ġtotal ĠM We Ġfrom Ġcoal Ġfacilities .\n",
      "\t Tokenized LLAMA3:Power Ġplants ' Ġmer cury Ġcontrol ĠAC Ġusage Ġis Ġhigher Ġthan Ġtotal ĠM We Ġfrom Ġcoal Ġfacilities .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Long ago--or away, or whatever--there was a world called Thar?? and another called Erath.\n",
      "\t Tokenized GPT2:Long Ġago -- or Ġaway , Ġor Ġwhatever -- there Ġwas Ġa Ġworld Ġcalled ĠTh ar ?? Ġand Ġanother Ġcalled ĠEr ath .\n",
      "\t Tokenized LLAMA3:Long Ġago -- or Ġaway , Ġor Ġwhatever -- there Ġwas Ġa Ġworld Ġcalled ĠTh ar ?? Ġand Ġanother Ġcalled ĠEr ath .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Erath is the only world that has ever existed.\n",
      "\t Tokenized GPT2:Er ath Ġis Ġthe Ġonly Ġworld Ġthat Ġhas Ġever Ġexisted .\n",
      "\t Tokenized LLAMA3:Er ath Ġis Ġthe Ġonly Ġworld Ġthat Ġhas Ġever Ġexisted .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Don't expect to be swinging much after midnight, even in towns.\n",
      "\t Tokenized GPT2:Don 't Ġexpect Ġto Ġbe Ġswinging Ġmuch Ġafter Ġmidnight , Ġeven Ġin Ġtowns .\n",
      "\t Tokenized LLAMA3:Don 't Ġexpect Ġto Ġbe Ġswinging Ġmuch Ġafter Ġmidnight , Ġeven Ġin Ġtowns .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Things stay open all night because it's a place to party.\n",
      "\t Tokenized GPT2:Things Ġstay Ġopen Ġall Ġnight Ġbecause Ġit 's Ġa Ġplace Ġto Ġparty .\n",
      "\t Tokenized LLAMA3:Things Ġstay Ġopen Ġall Ġnight Ġbecause Ġit 's Ġa Ġplace Ġto Ġparty .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: For ideological free-marketeers (like myself), theories like Smith and Wright's can be intellectually jarring.\n",
      "\t Tokenized GPT2:For Ġide ological Ġfree - market e ers Ġ( like Ġmyself ), Ġtheories Ġlike ĠSmith Ġand ĠWright 's Ġcan Ġbe Ġintellect ually Ġj arring .\n",
      "\t Tokenized LLAMA3:For Ġide ological Ġfree -mark ete ers Ġ( like Ġmyself ), Ġtheories Ġlike ĠSmith Ġand ĠWright 's Ġcan Ġbe Ġintellect ually Ġj arring .\n",
      "\t Unique Tokens GPT2: {'market', 'e', '-'}\n",
      "\t Unique Tokens LLAMA3: {'ete', '-mark'}\n",
      "Text 2: I can appreciate their position even if it does contradict my opinions. \n",
      "\t Tokenized GPT2:I Ġcan Ġappreciate Ġtheir Ġposition Ġeven Ġif Ġit Ġdoes Ġcontradict Ġmy Ġopinions . Ġ\n",
      "\t Tokenized LLAMA3:I Ġcan Ġappreciate Ġtheir Ġposition Ġeven Ġif Ġit Ġdoes Ġcontradict Ġmy Ġopinions . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: We are also advocating enhanced reporting in connection with key federal performance and projection information.\n",
      "\t Tokenized GPT2:We Ġare Ġalso Ġadvoc ating Ġenhanced Ġreporting Ġin Ġconnection Ġwith Ġkey Ġfederal Ġperformance Ġand Ġprojection Ġinformation .\n",
      "\t Tokenized LLAMA3:We Ġare Ġalso Ġadvoc ating Ġenhanced Ġreporting Ġin Ġconnection Ġwith Ġkey Ġfederal Ġperformance Ġand Ġprojection Ġinformation .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: We think reporting should be enhanced as it has a vital connection with projection statistics.\n",
      "\t Tokenized GPT2:We Ġthink Ġreporting Ġshould Ġbe Ġenhanced Ġas Ġit Ġhas Ġa Ġvital Ġconnection Ġwith Ġprojection Ġstatistics .\n",
      "\t Tokenized LLAMA3:We Ġthink Ġreporting Ġshould Ġbe Ġenhanced Ġas Ġit Ġhas Ġa Ġvital Ġconnection Ġwith Ġprojection Ġstatistics .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: She was 96 just turning away when she heard a piercing whistle, and the faithful Albert came running from the building to join her.\n",
      "\t Tokenized GPT2:She Ġwas Ġ96 Ġjust Ġturning Ġaway Ġwhen Ġshe Ġheard Ġa Ġpiercing Ġwhistle , Ġand Ġthe Ġfaithful ĠAlbert Ġcame Ġrunning Ġfrom Ġthe Ġbuilding Ġto Ġjoin Ġher .\n",
      "\t Tokenized LLAMA3:She Ġwas Ġ 96 Ġjust Ġturning Ġaway Ġwhen Ġshe Ġheard Ġa Ġpiercing Ġwhistle , Ġand Ġthe Ġfaithful ĠAlbert Ġcame Ġrunning Ġfrom Ġthe Ġbuilding Ġto Ġjoin Ġher .\n",
      "\t Unique Tokens GPT2: {'Ġ96'}\n",
      "\t Unique Tokens LLAMA3: {'96', 'Ġ'}\n",
      "Text 2: She was an old lady who was out for a walk when she heard a noise. \n",
      "\t Tokenized GPT2:She Ġwas Ġan Ġold Ġlady Ġwho Ġwas Ġout Ġfor Ġa Ġwalk Ġwhen Ġshe Ġheard Ġa Ġnoise . Ġ\n",
      "\t Tokenized LLAMA3:She Ġwas Ġan Ġold Ġlady Ġwho Ġwas Ġout Ġfor Ġa Ġwalk Ġwhen Ġshe Ġheard Ġa Ġnoise . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: oh yeah all all mine are uh purebreds so i keep them in\n",
      "\t Tokenized GPT2:oh Ġyeah Ġall Ġall Ġmine Ġare Ġuh Ġpure b red s Ġso Ġi Ġkeep Ġthem Ġin\n",
      "\t Tokenized LLAMA3:oh Ġyeah Ġall Ġall Ġmine Ġare Ġuh Ġpure b red s Ġso Ġi Ġkeep Ġthem Ġin\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: mine are all mixed breeds\n",
      "\t Tokenized GPT2:mine Ġare Ġall Ġmixed Ġbre eds\n",
      "\t Tokenized LLAMA3:mine Ġare Ġall Ġmixed Ġbre eds\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: aChange in personal saving depends on how much of the $4,000 IRA contribution represents new saving.\n",
      "\t Tokenized GPT2:a Change Ġin Ġpersonal Ġsaving Ġdepends Ġon Ġhow Ġmuch Ġof Ġthe Ġ$ 4 , 000 ĠI RA Ġcontribution Ġrepresents Ġnew Ġsaving .\n",
      "\t Tokenized LLAMA3:a Change Ġin Ġpersonal Ġsaving Ġdepends Ġon Ġhow Ġmuch Ġof Ġthe Ġ$ 4 , 000 ĠI RA Ġcontribution Ġrepresents Ġnew Ġsaving .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Personal savings need to be set by the IRA\n",
      "\t Tokenized GPT2:Person al Ġsavings Ġneed Ġto Ġbe Ġset Ġby Ġthe ĠI RA\n",
      "\t Tokenized LLAMA3:Person al Ġsavings Ġneed Ġto Ġbe Ġset Ġby Ġthe ĠI RA\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: The National Football League semifinals are set.\n",
      "\t Tokenized GPT2:The ĠNational ĠFootball ĠLeague Ġsem if inals Ġare Ġset .\n",
      "\t Tokenized LLAMA3:The ĠNational ĠFootball ĠLeague Ġsem if inals Ġare Ġset .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The dates for the semifinals have been determined.\n",
      "\t Tokenized GPT2:The Ġdates Ġfor Ġthe Ġsem if inals Ġhave Ġbeen Ġdetermined .\n",
      "\t Tokenized LLAMA3:The Ġdates Ġfor Ġthe Ġsem if inals Ġhave Ġbeen Ġdetermined .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Crosethe Rue de Rivoli to the Palais-Royal, built for Car?­di?­nal Richelieu as his Paris residence in 1639, and originally named Palais-Cardinal.\n",
      "\t Tokenized GPT2:C ro set he ĠR ue Ġde ĠR iv oli Ġto Ġthe ĠPal ais - R oyal , Ġbuilt Ġfor ĠCar ? ÂŃ di ? ÂŃ nal ĠRic hel ieu Ġas Ġhis ĠParis Ġresidence Ġin Ġ16 39 , Ġand Ġoriginally Ġnamed ĠPal ais - Card inal .\n",
      "\t Tokenized LLAMA3:C ro set he ĠR ue Ġde ĠR iv oli Ġto Ġthe ĠPal ais -R oyal , Ġbuilt Ġfor ĠCar ? ÂŃ di ? ÂŃ nal ĠRic hel ieu Ġas Ġhis ĠParis Ġresidence Ġin Ġ 163 9 , Ġand Ġoriginally Ġnamed ĠPal ais -C ard inal .\n",
      "\t Unique Tokens GPT2: {'R', 'Card', '39', 'Ġ16', '-'}\n",
      "\t Unique Tokens LLAMA3: {'ard', 'Ġ', '-R', '163', '9', '-C'}\n",
      "Text 2: The Crosethe Rue De Rivoli was built for Cardinal Richelieu to live in.\n",
      "\t Tokenized GPT2:The ĠCro set he ĠR ue ĠDe ĠR iv oli Ġwas Ġbuilt Ġfor ĠCard inal ĠRic hel ieu Ġto Ġlive Ġin .\n",
      "\t Tokenized LLAMA3:The ĠCro set he ĠR ue ĠDe ĠR iv oli Ġwas Ġbuilt Ġfor ĠCard inal ĠRic hel ieu Ġto Ġlive Ġin .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: and uh you know it's like they they consider that but it would be the same way here you know it's like if if you had to do it you know you have a big sign i'm sorry i don't get paid you know\n",
      "\t Tokenized GPT2:and Ġuh Ġyou Ġknow Ġit 's Ġlike Ġthey Ġthey Ġconsider Ġthat Ġbut Ġit Ġwould Ġbe Ġthe Ġsame Ġway Ġhere Ġyou Ġknow Ġit 's Ġlike Ġif Ġif Ġyou Ġhad Ġto Ġdo Ġit Ġyou Ġknow Ġyou Ġhave Ġa Ġbig Ġsign Ġi 'm Ġsorry Ġi Ġdon 't Ġget Ġpaid Ġyou Ġknow\n",
      "\t Tokenized LLAMA3:and Ġuh Ġyou Ġknow Ġit 's Ġlike Ġthey Ġthey Ġconsider Ġthat Ġbut Ġit Ġwould Ġbe Ġthe Ġsame Ġway Ġhere Ġyou Ġknow Ġit 's Ġlike Ġif Ġif Ġyou Ġhad Ġto Ġdo Ġit Ġyou Ġknow Ġyou Ġhave Ġa Ġbig Ġsign Ġi 'm Ġsorry Ġi Ġdon 't Ġget Ġpaid Ġyou Ġknow\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I am paid well, even if it's here and the same.\n",
      "\t Tokenized GPT2:I Ġam Ġpaid Ġwell , Ġeven Ġif Ġit 's Ġhere Ġand Ġthe Ġsame .\n",
      "\t Tokenized LLAMA3:I Ġam Ġpaid Ġwell , Ġeven Ġif Ġit 's Ġhere Ġand Ġthe Ġsame .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Part of the reason for the difference in pieces per possible delivery may be due to the fact that five percent of possible residential deliveries are businesses, and it is thought, but not known, that a lesser percentage of possible deliveries on rural routes are businesses.\n",
      "\t Tokenized GPT2:Part Ġof Ġthe Ġreason Ġfor Ġthe Ġdifference Ġin Ġpieces Ġper Ġpossible Ġdelivery Ġmay Ġbe Ġdue Ġto Ġthe Ġfact Ġthat Ġfive Ġpercent Ġof Ġpossible Ġresidential Ġdeliver ies Ġare Ġbusinesses , Ġand Ġit Ġis Ġthought , Ġbut Ġnot Ġknown , Ġthat Ġa Ġlesser Ġpercentage Ġof Ġpossible Ġdeliver ies Ġon Ġrural Ġroutes Ġare Ġbusinesses .\n",
      "\t Tokenized LLAMA3:Part Ġof Ġthe Ġreason Ġfor Ġthe Ġdifference Ġin Ġpieces Ġper Ġpossible Ġdelivery Ġmay Ġbe Ġdue Ġto Ġthe Ġfact Ġthat Ġfive Ġpercent Ġof Ġpossible Ġresidential Ġdeliver ies Ġare Ġbusinesses , Ġand Ġit Ġis Ġthought , Ġbut Ġnot Ġknown , Ġthat Ġa Ġlesser Ġpercentage Ġof Ġpossible Ġdeliver ies Ġon Ġrural Ġroutes Ġare Ġbusinesses .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: We all know that the reason for a lesser percentage of possible deliveries on rural routes being businesses, is because of the fact that people prefer living in cities rather than rural areas.\n",
      "\t Tokenized GPT2:We Ġall Ġknow Ġthat Ġthe Ġreason Ġfor Ġa Ġlesser Ġpercentage Ġof Ġpossible Ġdeliver ies Ġon Ġrural Ġroutes Ġbeing Ġbusinesses , Ġis Ġbecause Ġof Ġthe Ġfact Ġthat Ġpeople Ġprefer Ġliving Ġin Ġcities Ġrather Ġthan Ġrural Ġareas .\n",
      "\t Tokenized LLAMA3:We Ġall Ġknow Ġthat Ġthe Ġreason Ġfor Ġa Ġlesser Ġpercentage Ġof Ġpossible Ġdeliver ies Ġon Ġrural Ġroutes Ġbeing Ġbusinesses , Ġis Ġbecause Ġof Ġthe Ġfact Ġthat Ġpeople Ġprefer Ġliving Ġin Ġcities Ġrather Ġthan Ġrural Ġareas .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: At the least, he was hired in an attempt to influence administration China policy.\n",
      "\t Tokenized GPT2:At Ġthe Ġleast , Ġhe Ġwas Ġhired Ġin Ġan Ġattempt Ġto Ġinfluence Ġadministration ĠChina Ġpolicy .\n",
      "\t Tokenized LLAMA3:At Ġthe Ġleast , Ġhe Ġwas Ġhired Ġin Ġan Ġattempt Ġto Ġinfluence Ġadministration ĠChina Ġpolicy .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He was hired to influence the British economy.\n",
      "\t Tokenized GPT2:He Ġwas Ġhired Ġto Ġinfluence Ġthe ĠBritish Ġeconomy .\n",
      "\t Tokenized LLAMA3:He Ġwas Ġhired Ġto Ġinfluence Ġthe ĠBritish Ġeconomy .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: At Gatehouse, in Kent.\n",
      "\t Tokenized GPT2:At ĠGate house , Ġin ĠKent .\n",
      "\t Tokenized LLAMA3:At ĠGate house , Ġin ĠKent .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It's in a tent out by the Hundred Acre Woods.\n",
      "\t Tokenized GPT2:It 's Ġin Ġa Ġtent Ġout Ġby Ġthe ĠH undred ĠA cre ĠWoods .\n",
      "\t Tokenized LLAMA3:It 's Ġin Ġa Ġtent Ġout Ġby Ġthe ĠH undred ĠA cre ĠWoods .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: you want to punch the button and go\n",
      "\t Tokenized GPT2:you Ġwant Ġto Ġpunch Ġthe Ġbutton Ġand Ġgo\n",
      "\t Tokenized LLAMA3:you Ġwant Ġto Ġpunch Ġthe Ġbutton Ġand Ġgo\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: You should start going before punching the button.\n",
      "\t Tokenized GPT2:You Ġshould Ġstart Ġgoing Ġbefore Ġpunching Ġthe Ġbutton .\n",
      "\t Tokenized LLAMA3:You Ġshould Ġstart Ġgoing Ġbefore Ġpunch ing Ġthe Ġbutton .\n",
      "\t Unique Tokens GPT2: {'Ġpunching'}\n",
      "\t Unique Tokens LLAMA3: {'Ġpunch', 'ing'}\n",
      "==neutral==\n",
      "Text 1: The island's burgeoning economic significance propelled population growth, and by the middle of the 15th century Madeira was home to 800 families.\n",
      "\t Tokenized GPT2:The Ġisland 's Ġbur geon ing Ġeconomic Ġsignificance Ġpro pelled Ġpopulation Ġgrowth , Ġand Ġby Ġthe Ġmiddle Ġof Ġthe Ġ15 th Ġcentury ĠMade ira Ġwas Ġhome Ġto Ġ800 Ġfamilies .\n",
      "\t Tokenized LLAMA3:The Ġisland 's Ġbur geon ing Ġeconomic Ġsignificance Ġpro pelled Ġpopulation Ġgrowth , Ġand Ġby Ġthe Ġmiddle Ġof Ġthe Ġ 15 th Ġcentury ĠMade ira Ġwas Ġhome Ġto Ġ 800 Ġfamilies .\n",
      "\t Unique Tokens GPT2: {'Ġ800', 'Ġ15'}\n",
      "\t Unique Tokens LLAMA3: {'800', '15', 'Ġ'}\n",
      "Text 2: The population of Madeira was devastated by illness in 1475.\n",
      "\t Tokenized GPT2:The Ġpopulation Ġof ĠMade ira Ġwas Ġdevastated Ġby Ġillness Ġin Ġ14 75 .\n",
      "\t Tokenized LLAMA3:The Ġpopulation Ġof ĠMade ira Ġwas Ġdevastated Ġby Ġillness Ġin Ġ 147 5 .\n",
      "\t Unique Tokens GPT2: {'Ġ14', '75'}\n",
      "\t Unique Tokens LLAMA3: {'5', '147', 'Ġ'}\n",
      "==entailment==\n",
      "Text 1: A fine Crusader arch leads down a dimly-lit broad stairway to the dark subterranean Church of the Assumption, a Greek Orthodox church.\n",
      "\t Tokenized GPT2:A Ġfine ĠCr us ader Ġarch Ġleads Ġdown Ġa Ġdim ly - lit Ġbroad Ġstair way Ġto Ġthe Ġdark Ġsub ter r anean ĠChurch Ġof Ġthe ĠAss um ption , Ġa ĠGreek ĠOrth odox Ġchurch .\n",
      "\t Tokenized LLAMA3:A Ġfine ĠCr us ader Ġarch Ġleads Ġdown Ġa Ġdim ly -l it Ġbroad Ġstair way Ġto Ġthe Ġdark Ġsub ter r anean ĠChurch Ġof Ġthe ĠAss um ption , Ġa ĠGreek ĠOrth odox Ġchurch .\n",
      "\t Unique Tokens GPT2: {'-', 'lit'}\n",
      "\t Unique Tokens LLAMA3: {'it', '-l'}\n",
      "Text 2: The Church of the Assumption is located underground through a Crusader arch.\n",
      "\t Tokenized GPT2:The ĠChurch Ġof Ġthe ĠAss um ption Ġis Ġlocated Ġunderground Ġthrough Ġa ĠCr us ader Ġarch .\n",
      "\t Tokenized LLAMA3:The ĠChurch Ġof Ġthe ĠAss um ption Ġis Ġlocated Ġunderground Ġthrough Ġa ĠCr us ader Ġarch .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: um-hum they keep you entertained they sure do we have a uh my wife's uh mother is uh oh about  seventy seven i guess she really gets a thrill when we go over to see her and bring the dog i think she's more happy to see the dog than she is us\n",
      "\t Tokenized GPT2:um - hum Ġthey Ġkeep Ġyou Ġentertained Ġthey Ġsure Ġdo Ġwe Ġhave Ġa Ġuh Ġmy Ġwife 's Ġuh Ġmother Ġis Ġuh Ġoh Ġabout Ġ Ġsevent y Ġseven Ġi Ġguess Ġshe Ġreally Ġgets Ġa Ġthrill Ġwhen Ġwe Ġgo Ġover Ġto Ġsee Ġher Ġand Ġbring Ġthe Ġdog Ġi Ġthink Ġshe 's Ġmore Ġhappy Ġto Ġsee Ġthe Ġdog Ġthan Ġshe Ġis Ġus\n",
      "\t Tokenized LLAMA3:um -h um Ġthey Ġkeep Ġyou Ġentertained Ġthey Ġsure Ġdo Ġwe Ġhave Ġa Ġuh Ġmy Ġwife 's Ġuh Ġmother Ġis Ġuh Ġoh Ġabout Ġ Ġsevent y Ġseven Ġi Ġguess Ġshe Ġreally Ġgets Ġa Ġthrill Ġwhen Ġwe Ġgo Ġover Ġto Ġsee Ġher Ġand Ġbring Ġthe Ġdog Ġi Ġthink Ġshe 's Ġmore Ġhappy Ġto Ġsee Ġthe Ġdog Ġthan Ġshe Ġis Ġus\n",
      "\t Unique Tokens GPT2: {'-', 'hum'}\n",
      "\t Unique Tokens LLAMA3: {'-h'}\n",
      "Text 2: The dog cheers up my wife's mother. \n",
      "\t Tokenized GPT2:The Ġdog Ġcheers Ġup Ġmy Ġwife 's Ġmother . Ġ\n",
      "\t Tokenized LLAMA3:The Ġdog Ġcheers Ġup Ġmy Ġwife 's Ġmother . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Pat Buchanan followed immediately behind, handing out smallpox-infected blankets and bottles of whiskey.\n",
      "\t Tokenized GPT2:Pat ĠB uch anan Ġfollowed Ġimmediately Ġbehind , Ġhanding Ġout Ġsmall p ox - infect ed Ġblankets Ġand Ġbottles Ġof Ġwhiskey .\n",
      "\t Tokenized LLAMA3:Pat ĠB uch anan Ġfollowed Ġimmediately Ġbehind , Ġhanding Ġout Ġsmall p ox -in fected Ġblankets Ġand Ġbottles Ġof Ġwhiskey .\n",
      "\t Unique Tokens GPT2: {'-', 'ed', 'infect'}\n",
      "\t Unique Tokens LLAMA3: {'fected', '-in'}\n",
      "Text 2: Pat Buchanan led the group.\n",
      "\t Tokenized GPT2:Pat ĠB uch anan Ġled Ġthe Ġgroup .\n",
      "\t Tokenized LLAMA3:Pat ĠB uch anan Ġled Ġthe Ġgroup .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: For their part, family-planning organizations and the Clinton administration seem equally adamant.\n",
      "\t Tokenized GPT2:For Ġtheir Ġpart , Ġfamily - plan ning Ġorganizations Ġand Ġthe ĠClinton Ġadministration Ġseem Ġequally Ġadam ant .\n",
      "\t Tokenized LLAMA3:For Ġtheir Ġpart , Ġfamily - plan ning Ġorganizations Ġand Ġthe ĠClinton Ġadministration Ġseem Ġequally Ġadam ant .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: family-planning organizations agree with the Clinton administration about certain things.\n",
      "\t Tokenized GPT2:family - plan ning Ġorganizations Ġagree Ġwith Ġthe ĠClinton Ġadministration Ġabout Ġcertain Ġthings .\n",
      "\t Tokenized LLAMA3:family - plan ning Ġorganizations Ġagree Ġwith Ġthe ĠClinton Ġadministration Ġabout Ġcertain Ġthings .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: The Weekly Standard argues that America should back Lee with words now and, if necessary, military force later, but the Washington Post reports that the U.S. envoys will pressure him to back down.\n",
      "\t Tokenized GPT2:The ĠWeekly ĠStandard Ġargues Ġthat ĠAmerica Ġshould Ġback ĠLee Ġwith Ġwords Ġnow Ġand , Ġif Ġnecessary , Ġmilitary Ġforce Ġlater , Ġbut Ġthe ĠWashington ĠPost Ġreports Ġthat Ġthe ĠU . S . Ġenv oys Ġwill Ġpressure Ġhim Ġto Ġback Ġdown .\n",
      "\t Tokenized LLAMA3:The ĠWeekly ĠStandard Ġargues Ġthat ĠAmerica Ġshould Ġback ĠLee Ġwith Ġwords Ġnow Ġand , Ġif Ġnecessary , Ġmilitary Ġforce Ġlater , Ġbut Ġthe ĠWashington ĠPost Ġreports Ġthat Ġthe ĠU .S . Ġenv oys Ġwill Ġpressure Ġhim Ġto Ġback Ġdown .\n",
      "\t Unique Tokens GPT2: {'S'}\n",
      "\t Unique Tokens LLAMA3: {'.S'}\n",
      "Text 2: The Weekly Standard and Washington Post have opposing views on how the U.S. will approach Lee.\n",
      "\t Tokenized GPT2:The ĠWeekly ĠStandard Ġand ĠWashington ĠPost Ġhave Ġopposing Ġviews Ġon Ġhow Ġthe ĠU . S . Ġwill Ġapproach ĠLee .\n",
      "\t Tokenized LLAMA3:The ĠWeekly ĠStandard Ġand ĠWashington ĠPost Ġhave Ġopposing Ġviews Ġon Ġhow Ġthe ĠU .S . Ġwill Ġapproach ĠLee .\n",
      "\t Unique Tokens GPT2: {'S'}\n",
      "\t Unique Tokens LLAMA3: {'.S'}\n",
      "==neutral==\n",
      "Text 1: So, as he and Tipper walked out, my friend and I were right behind them, and I took the opportunity to say hello and reintroduce myself--as a journalist, I might add--and we chatted about the movie for a few minutes.\n",
      "\t Tokenized GPT2:So , Ġas Ġhe Ġand ĠT ipper Ġwalked Ġout , Ġmy Ġfriend Ġand ĠI Ġwere Ġright Ġbehind Ġthem , Ġand ĠI Ġtook Ġthe Ġopportunity Ġto Ġsay Ġhello Ġand Ġre introdu ce Ġmyself -- as Ġa Ġjournalist , ĠI Ġmight Ġadd -- and Ġwe Ġchat ted Ġabout Ġthe Ġmovie Ġfor Ġa Ġfew Ġminutes .\n",
      "\t Tokenized LLAMA3:So , Ġas Ġhe Ġand ĠT ipper Ġwalked Ġout , Ġmy Ġfriend Ġand ĠI Ġwere Ġright Ġbehind Ġthem , Ġand ĠI Ġtook Ġthe Ġopportunity Ġto Ġsay Ġhello Ġand Ġre int rodu ce Ġmyself -- as Ġa Ġjournalist , ĠI Ġmight Ġadd -- and Ġwe Ġchat ted Ġabout Ġthe Ġmovie Ġfor Ġa Ġfew Ġminutes .\n",
      "\t Unique Tokens GPT2: {'introdu'}\n",
      "\t Unique Tokens LLAMA3: {'int', 'rodu'}\n",
      "Text 2: I saw Al and Tipper together at the wedding.\n",
      "\t Tokenized GPT2:I Ġsaw ĠAl Ġand ĠT ipper Ġtogether Ġat Ġthe Ġwedding .\n",
      "\t Tokenized LLAMA3:I Ġsaw ĠAl Ġand ĠT ipper Ġtogether Ġat Ġthe Ġwedding .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Also, why Princess Di was like President  The public cared more about her empathy than about her actions.\n",
      "\t Tokenized GPT2:Also , Ġwhy ĠPrincess ĠDi Ġwas Ġlike ĠPresident Ġ ĠThe Ġpublic Ġcared Ġmore Ġabout Ġher Ġempathy Ġthan Ġabout Ġher Ġactions .\n",
      "\t Tokenized LLAMA3:Also , Ġwhy ĠPrincess ĠDi Ġwas Ġlike ĠPresident Ġ ĠThe Ġpublic Ġcared Ġmore Ġabout Ġher Ġempathy Ġthan Ġabout Ġher Ġactions .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: They scared more about empathy than actions.\n",
      "\t Tokenized GPT2:They Ġscared Ġmore Ġabout Ġempathy Ġthan Ġactions .\n",
      "\t Tokenized LLAMA3:They Ġscared Ġmore Ġabout Ġempathy Ġthan Ġactions .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: 'Wait here,' I was ordered.\n",
      "\t Tokenized GPT2:' Wait Ġhere ,' ĠI Ġwas Ġordered .\n",
      "\t Tokenized LLAMA3:' Wait Ġhere ,' ĠI Ġwas Ġordered .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He told me to come with him.\n",
      "\t Tokenized GPT2:He Ġtold Ġme Ġto Ġcome Ġwith Ġhim .\n",
      "\t Tokenized LLAMA3:He Ġtold Ġme Ġto Ġcome Ġwith Ġhim .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: In the short term, U.S. consumers will benefit from cheap imports (as will U.S. multinationals that use parts made in East Asian factories).\n",
      "\t Tokenized GPT2:In Ġthe Ġshort Ġterm , ĠU . S . Ġconsumers Ġwill Ġbenefit Ġfrom Ġcheap Ġimports Ġ( as Ġwill ĠU . S . Ġmult ination als Ġthat Ġuse Ġparts Ġmade Ġin ĠEast ĠAsian Ġfactories ).\n",
      "\t Tokenized LLAMA3:In Ġthe Ġshort Ġterm , ĠU .S . Ġconsumers Ġwill Ġbenefit Ġfrom Ġcheap Ġimports Ġ( as Ġwill ĠU .S . Ġmult ination als Ġthat Ġuse Ġparts Ġmade Ġin ĠEast ĠAsian Ġfactories ).\n",
      "\t Unique Tokens GPT2: {'S'}\n",
      "\t Unique Tokens LLAMA3: {'.S'}\n",
      "Text 2: U.S. consumers will put money in the pockets of East Asia over time.\n",
      "\t Tokenized GPT2:U . S . Ġconsumers Ġwill Ġput Ġmoney Ġin Ġthe Ġpockets Ġof ĠEast ĠAsia Ġover Ġtime .\n",
      "\t Tokenized LLAMA3:U .S . Ġconsumers Ġwill Ġput Ġmoney Ġin Ġthe Ġpockets Ġof ĠEast ĠAsia Ġover Ġtime .\n",
      "\t Unique Tokens GPT2: {'S'}\n",
      "\t Unique Tokens LLAMA3: {'.S'}\n",
      "==entailment==\n",
      "Text 1: Ca'daan felt his skin get hot and unable to come up with any suitable response, moved on.\n",
      "\t Tokenized GPT2:Ca 'd aan Ġfelt Ġhis Ġskin Ġget Ġhot Ġand Ġunable Ġto Ġcome Ġup Ġwith Ġany Ġsuitable Ġresponse , Ġmoved Ġon .\n",
      "\t Tokenized LLAMA3:Ca 'd aan Ġfelt Ġhis Ġskin Ġget Ġhot Ġand Ġunable Ġto Ġcome Ġup Ġwith Ġany Ġsuitable Ġresponse , Ġmoved Ġon .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Ca'daan felt the heat on his skin.\n",
      "\t Tokenized GPT2:Ca 'd aan Ġfelt Ġthe Ġheat Ġon Ġhis Ġskin .\n",
      "\t Tokenized LLAMA3:Ca 'd aan Ġfelt Ġthe Ġheat Ġon Ġhis Ġskin .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: See the idea?\" 35 \"Then you think\" Tuppence paused to grasp the supposition fully \"that it WAS as Jane Finn that they wanted me to go to Paris?\" Mr. Carter smiled more wearily than ever.\n",
      "\t Tokenized GPT2:See Ġthe Ġidea ?\" Ġ35 Ġ\" Then Ġyou Ġthink \" ĠT upp ence Ġpaused Ġto Ġgrasp Ġthe Ġsupp osition Ġfully Ġ\" that Ġit ĠWAS Ġas ĠJane ĠFinn Ġthat Ġthey Ġwanted Ġme Ġto Ġgo Ġto ĠParis ?\" ĠMr . ĠCarter Ġsmiled Ġmore Ġwe arily Ġthan Ġever .\n",
      "\t Tokenized LLAMA3:See Ġthe Ġidea ?\" Ġ 35 Ġ\" Then Ġyou Ġthink \" ĠT upp ence Ġpaused Ġto Ġgrasp Ġthe Ġsupp osition Ġfully Ġ\" that Ġit ĠWAS Ġas ĠJane ĠFinn Ġthat Ġthey Ġwanted Ġme Ġto Ġgo Ġto ĠParis ?\" ĠMr . ĠCarter Ġsmiled Ġmore Ġwe arily Ġthan Ġever .\n",
      "\t Unique Tokens GPT2: {'Ġ35'}\n",
      "\t Unique Tokens LLAMA3: {'35', 'Ġ'}\n",
      "Text 2: Mr. Carter had no energy left to continue the conversation.\n",
      "\t Tokenized GPT2:Mr . ĠCarter Ġhad Ġno Ġenergy Ġleft Ġto Ġcontinue Ġthe Ġconversation .\n",
      "\t Tokenized LLAMA3:Mr . ĠCarter Ġhad Ġno Ġenergy Ġleft Ġto Ġcontinue Ġthe Ġconversation .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Try a selection at the Whisky Heritage Centre (they have over 100 for you to sample), where you can then buy a bottle or two of your personal favorite in the shop or in stores around the city.\n",
      "\t Tokenized GPT2:Try Ġa Ġselection Ġat Ġthe ĠW his ky ĠHer itage ĠCentre Ġ( they Ġhave Ġover Ġ100 Ġfor Ġyou Ġto Ġsample ), Ġwhere Ġyou Ġcan Ġthen Ġbuy Ġa Ġbottle Ġor Ġtwo Ġof Ġyour Ġpersonal Ġfavorite Ġin Ġthe Ġshop Ġor Ġin Ġstores Ġaround Ġthe Ġcity .\n",
      "\t Tokenized LLAMA3:Try Ġa Ġselection Ġat Ġthe ĠW his ky ĠHer itage ĠCentre Ġ( they Ġhave Ġover Ġ 100 Ġfor Ġyou Ġto Ġsample ), Ġwhere Ġyou Ġcan Ġthen Ġbuy Ġa Ġbottle Ġor Ġtwo Ġof Ġyour Ġpersonal Ġfavorite Ġin Ġthe Ġshop Ġor Ġin Ġstores Ġaround Ġthe Ġcity .\n",
      "\t Unique Tokens GPT2: {'Ġ100'}\n",
      "\t Unique Tokens LLAMA3: {'100', 'Ġ'}\n",
      "Text 2: Whisky Heritage Centre was established in the 1800s and has been a destination ever since.\n",
      "\t Tokenized GPT2:W his ky ĠHer itage ĠCentre Ġwas Ġestablished Ġin Ġthe Ġ1800 s Ġand Ġhas Ġbeen Ġa Ġdestination Ġever Ġsince .\n",
      "\t Tokenized LLAMA3:W his ky ĠHer itage ĠCentre Ġwas Ġestablished Ġin Ġthe Ġ 180 0 s Ġand Ġhas Ġbeen Ġa Ġdestination Ġever Ġsince .\n",
      "\t Unique Tokens GPT2: {'Ġ1800'}\n",
      "\t Unique Tokens LLAMA3: {'180', '0', 'Ġ'}\n",
      "==entailment==\n",
      "Text 1: As shown in Exhibits A-1 and A-2 in Appendix A, in the first phase of technology implementation, an engineering review and assessment of the combustion unit is conducted to determine the preferred compliance alternative.\n",
      "\t Tokenized GPT2:As Ġshown Ġin ĠEx hib its ĠA - 1 Ġand ĠA - 2 Ġin ĠAppend ix ĠA , Ġin Ġthe Ġfirst Ġphase Ġof Ġtechnology Ġimplementation , Ġan Ġengineering Ġreview Ġand Ġassessment Ġof Ġthe Ġcomb ustion Ġunit Ġis Ġconducted Ġto Ġdetermine Ġthe Ġpreferred Ġcompliance Ġalternative .\n",
      "\t Tokenized LLAMA3:As Ġshown Ġin ĠEx hib its ĠA - 1 Ġand ĠA - 2 Ġin ĠAppend ix ĠA , Ġin Ġthe Ġfirst Ġphase Ġof Ġtechnology Ġimplementation , Ġan Ġengineering Ġreview Ġand Ġassessment Ġof Ġthe Ġcomb ust ion Ġunit Ġis Ġconducted Ġto Ġdetermine Ġthe Ġpreferred Ġcompliance Ġalternative .\n",
      "\t Unique Tokens GPT2: {'ustion'}\n",
      "\t Unique Tokens LLAMA3: {'ion', 'ust'}\n",
      "Text 2: The exhibits within the appendix shoe the initial phase of the technological implementations.\n",
      "\t Tokenized GPT2:The Ġexhibits Ġwithin Ġthe Ġappend ix Ġshoe Ġthe Ġinitial Ġphase Ġof Ġthe Ġtechnological Ġimplementations .\n",
      "\t Tokenized LLAMA3:The Ġexhibits Ġwithin Ġthe Ġappend ix Ġshoe Ġthe Ġinitial Ġphase Ġof Ġthe Ġtechnological Ġimplementations .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Republican consultants agree that conservative candidates in the South, Southwest, Midwest, and Rocky Mountains will beg for Reed's talents and connections.\n",
      "\t Tokenized GPT2:Republic an Ġconsult ants Ġagree Ġthat Ġconservative Ġcandidates Ġin Ġthe ĠSouth , ĠSouth west , ĠMid west , Ġand ĠRock y ĠMount ains Ġwill Ġbeg Ġfor ĠReed 's Ġtalents Ġand Ġconnections .\n",
      "\t Tokenized LLAMA3:Republic an Ġconsult ants Ġagree Ġthat Ġconservative Ġcandidates Ġin Ġthe ĠSouth , ĠSouth west , ĠMid west , Ġand ĠRock y ĠMount ains Ġwill Ġbeg Ġfor ĠReed 's Ġtalents Ġand Ġconnections .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Reed's talents will be begged for by people across the country, according to Republicans.\n",
      "\t Tokenized GPT2:Re ed 's Ġtalents Ġwill Ġbe Ġbegged Ġfor Ġby Ġpeople Ġacross Ġthe Ġcountry , Ġaccording Ġto ĠRepublicans .\n",
      "\t Tokenized LLAMA3:Re ed 's Ġtalents Ġwill Ġbe Ġbegged Ġfor Ġby Ġpeople Ġacross Ġthe Ġcountry , Ġaccording Ġto ĠRepublicans .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The Throne Room is one of a series of apartments built during the reign of Charles II, though it was originally designed as a guard room that screened entrants to the private chambers beyond.\n",
      "\t Tokenized GPT2:The ĠThr one ĠRoom Ġis Ġone Ġof Ġa Ġseries Ġof Ġapartments Ġbuilt Ġduring Ġthe Ġreign Ġof ĠCharles ĠII , Ġthough Ġit Ġwas Ġoriginally Ġdesigned Ġas Ġa Ġguard Ġroom Ġthat Ġscreen ed Ġentr ants Ġto Ġthe Ġprivate Ġchambers Ġbeyond .\n",
      "\t Tokenized LLAMA3:The ĠThr one ĠRoom Ġis Ġone Ġof Ġa Ġseries Ġof Ġapartments Ġbuilt Ġduring Ġthe Ġreign Ġof ĠCharles ĠII , Ġthough Ġit Ġwas Ġoriginally Ġdesigned Ġas Ġa Ġguard Ġroom Ġthat Ġscreen ed Ġentr ants Ġto Ġthe Ġprivate Ġchambers Ġbeyond .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The Throne Room is an apartment built during the reign of George III.\n",
      "\t Tokenized GPT2:The ĠThr one ĠRoom Ġis Ġan Ġapartment Ġbuilt Ġduring Ġthe Ġreign Ġof ĠGeorge ĠIII .\n",
      "\t Tokenized LLAMA3:The ĠThr one ĠRoom Ġis Ġan Ġapartment Ġbuilt Ġduring Ġthe Ġreign Ġof ĠGeorge ĠIII .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The emotional effect is undiminished, and the gory effects are usually horribly creative.\n",
      "\t Tokenized GPT2:The Ġemotional Ġeffect Ġis Ġund imin ished , Ġand Ġthe Ġg ory Ġeffects Ġare Ġusually Ġhorribly Ġcreative .\n",
      "\t Tokenized LLAMA3:The Ġemotional Ġeffect Ġis Ġund imin ished , Ġand Ġthe Ġg ory Ġeffects Ġare Ġusually Ġhorribly Ġcreative .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The emotional impact is greatly lessened and the way that gore is used is unoriginal.\n",
      "\t Tokenized GPT2:The Ġemotional Ġimpact Ġis Ġgreatly Ġless ened Ġand Ġthe Ġway Ġthat Ġg ore Ġis Ġused Ġis Ġun original .\n",
      "\t Tokenized LLAMA3:The Ġemotional Ġimpact Ġis Ġgreatly Ġless ened Ġand Ġthe Ġway Ġthat Ġg ore Ġis Ġused Ġis Ġun original .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: In the same issue, a document entitled Analysis Regarding The Food And Drug Administration's Jurisdiction Over Nicotine-Containing Cigarettes And Smokeless Tobacco Products was published and comments were requested.\n",
      "\t Tokenized GPT2:In Ġthe Ġsame Ġissue , Ġa Ġdocument Ġentitled ĠAnalysis ĠReg arding ĠThe ĠFood ĠAnd ĠDrug ĠAdministration 's ĠJ ur isd iction ĠOver ĠNic otine - Cont aining ĠC ig aret tes ĠAnd ĠSm oke less ĠTob acco ĠProducts Ġwas Ġpublished Ġand Ġcomments Ġwere Ġrequested .\n",
      "\t Tokenized LLAMA3:In Ġthe Ġsame Ġissue , Ġa Ġdocument Ġentitled ĠAnalysis ĠReg arding ĠThe ĠFood ĠAnd ĠDrug ĠAdministration 's ĠJ ur isd iction ĠOver ĠNic otine - Cont aining ĠC ig aret tes ĠAnd ĠSm oke less ĠTob acco ĠProducts Ġwas Ġpublished Ġand Ġcomments Ġwere Ġrequested .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: A document was published about the FDA's jurisdiction over cigarettes.\n",
      "\t Tokenized GPT2:A Ġdocument Ġwas Ġpublished Ġabout Ġthe ĠFDA 's Ġjurisdiction Ġover Ġcigarettes .\n",
      "\t Tokenized LLAMA3:A Ġdocument Ġwas Ġpublished Ġabout Ġthe ĠFDA 's Ġjurisdiction Ġover Ġcigarettes .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The White House denies this.\n",
      "\t Tokenized GPT2:The ĠWhite ĠHouse Ġden ies Ġthis .\n",
      "\t Tokenized LLAMA3:The ĠWhite ĠHouse Ġden ies Ġthis .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The White House, off the record, knows it to be true.\n",
      "\t Tokenized GPT2:The ĠWhite ĠHouse , Ġoff Ġthe Ġrecord , Ġknows Ġit Ġto Ġbe Ġtrue .\n",
      "\t Tokenized LLAMA3:The ĠWhite ĠHouse , Ġoff Ġthe Ġrecord , Ġknows Ġit Ġto Ġbe Ġtrue .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: I am not aware of any studies comparing the number of words an average person could expect to hear spoken in a typical day 500 years ago vs. the number that can be heard now, but the increase surely is vast.\n",
      "\t Tokenized GPT2:I Ġam Ġnot Ġaware Ġof Ġany Ġstudies Ġcomparing Ġthe Ġnumber Ġof Ġwords Ġan Ġaverage Ġperson Ġcould Ġexpect Ġto Ġhear Ġspoken Ġin Ġa Ġtypical Ġday Ġ500 Ġyears Ġago Ġvs . Ġthe Ġnumber Ġthat Ġcan Ġbe Ġheard Ġnow , Ġbut Ġthe Ġincrease Ġsurely Ġis Ġvast .\n",
      "\t Tokenized LLAMA3:I Ġam Ġnot Ġaware Ġof Ġany Ġstudies Ġcomparing Ġthe Ġnumber Ġof Ġwords Ġan Ġaverage Ġperson Ġcould Ġexpect Ġto Ġhear Ġspoken Ġin Ġa Ġtypical Ġday Ġ 500 Ġyears Ġago Ġvs . Ġthe Ġnumber Ġthat Ġcan Ġbe Ġheard Ġnow , Ġbut Ġthe Ġincrease Ġsurely Ġis Ġvast .\n",
      "\t Unique Tokens GPT2: {'Ġ500'}\n",
      "\t Unique Tokens LLAMA3: {'500', 'Ġ'}\n",
      "Text 2: According to the research I've seen, the average person hundreds of years ago heard many more words over the course of the day compared to a modern human being.\n",
      "\t Tokenized GPT2:According Ġto Ġthe Ġresearch ĠI 've Ġseen , Ġthe Ġaverage Ġperson Ġhundreds Ġof Ġyears Ġago Ġheard Ġmany Ġmore Ġwords Ġover Ġthe Ġcourse Ġof Ġthe Ġday Ġcompared Ġto Ġa Ġmodern Ġhuman Ġbeing .\n",
      "\t Tokenized LLAMA3:According Ġto Ġthe Ġresearch ĠI 've Ġseen , Ġthe Ġaverage Ġperson Ġhundreds Ġof Ġyears Ġago Ġheard Ġmany Ġmore Ġwords Ġover Ġthe Ġcourse Ġof Ġthe Ġday Ġcompared Ġto Ġa Ġmodern Ġhuman Ġbeing .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: do you really romance\n",
      "\t Tokenized GPT2:do Ġyou Ġreally Ġromance\n",
      "\t Tokenized LLAMA3:do Ġyou Ġreally Ġromance\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Do you really love him?\n",
      "\t Tokenized GPT2:Do Ġyou Ġreally Ġlove Ġhim ?\n",
      "\t Tokenized LLAMA3:Do Ġyou Ġreally Ġlove Ġhim ?\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: I leap!\" And, in very truth, run and leap he did, gambolling wildly down the stretch of lawn outside the long window. \n",
      "\t Tokenized GPT2:I Ġleap !\" ĠAnd , Ġin Ġvery Ġtruth , Ġrun Ġand Ġleap Ġhe Ġdid , Ġg amb oll ing Ġwildly Ġdown Ġthe Ġstretch Ġof Ġlawn Ġoutside Ġthe Ġlong Ġwindow . Ġ\n",
      "\t Tokenized LLAMA3:I Ġleap !\" ĠAnd , Ġin Ġvery Ġtruth , Ġrun Ġand Ġleap Ġhe Ġdid , Ġg amb oll ing Ġwildly Ġdown Ġthe Ġstretch Ġof Ġlawn Ġoutside Ġthe Ġlong Ġwindow . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The man yelled that he would sit down, and sit down he did.\n",
      "\t Tokenized GPT2:The Ġman Ġyelled Ġthat Ġhe Ġwould Ġsit Ġdown , Ġand Ġsit Ġdown Ġhe Ġdid .\n",
      "\t Tokenized LLAMA3:The Ġman Ġyelled Ġthat Ġhe Ġwould Ġsit Ġdown , Ġand Ġsit Ġdown Ġhe Ġdid .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: football and baseball and\n",
      "\t Tokenized GPT2:foot ball Ġand Ġbaseball Ġand\n",
      "\t Tokenized LLAMA3:foot ball Ġand Ġbaseball Ġand\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Both football and baseball.\n",
      "\t Tokenized GPT2:Both Ġfootball Ġand Ġbaseball .\n",
      "\t Tokenized LLAMA3:Both Ġfootball Ġand Ġbaseball .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Catch up on the Indian avant-garde and the bohemian people of Caletta at the Academy of Fine Arts on the southeast corner of the Maidan.\n",
      "\t Tokenized GPT2:C atch Ġup Ġon Ġthe ĠIndian Ġav ant - gard e Ġand Ġthe Ġbo hem ian Ġpeople Ġof ĠCal etta Ġat Ġthe ĠAcademy Ġof ĠFine ĠArts Ġon Ġthe Ġs outheast Ġcorner Ġof Ġthe ĠM aid an .\n",
      "\t Tokenized LLAMA3:C atch Ġup Ġon Ġthe ĠIndian Ġav ant -g ard e Ġand Ġthe Ġbo hem ian Ġpeople Ġof ĠCal etta Ġat Ġthe ĠAcademy Ġof ĠFine ĠArts Ġon Ġthe Ġs outheast Ġcorner Ġof Ġthe ĠM aid an .\n",
      "\t Unique Tokens GPT2: {'-', 'gard'}\n",
      "\t Unique Tokens LLAMA3: {'ard', '-g'}\n",
      "Text 2: In South-Eastern Maidan you will find the Academy of Fine Arts.\n",
      "\t Tokenized GPT2:In ĠSouth - E astern ĠM aid an Ġyou Ġwill Ġfind Ġthe ĠAcademy Ġof ĠFine ĠArts .\n",
      "\t Tokenized LLAMA3:In ĠSouth -E astern ĠM aid an Ġyou Ġwill Ġfind Ġthe ĠAcademy Ġof ĠFine ĠArts .\n",
      "\t Unique Tokens GPT2: {'-', 'E'}\n",
      "\t Unique Tokens LLAMA3: {'-E'}\n",
      "==entailment==\n",
      "Text 1: if it had rained any more in the last two weeks instead of planting Saint Augustine grass in the front yard i think i would have plowed everything under and had a rice field\n",
      "\t Tokenized GPT2:if Ġit Ġhad Ġr ained Ġany Ġmore Ġin Ġthe Ġlast Ġtwo Ġweeks Ġinstead Ġof Ġplanting ĠSaint ĠAugust ine Ġgrass Ġin Ġthe Ġfront Ġyard Ġi Ġthink Ġi Ġwould Ġhave Ġpl owed Ġeverything Ġunder Ġand Ġhad Ġa Ġrice Ġfield\n",
      "\t Tokenized LLAMA3:if Ġit Ġhad Ġr ained Ġany Ġmore Ġin Ġthe Ġlast Ġtwo Ġweeks Ġinstead Ġof Ġplanting ĠSaint ĠAugust ine Ġgrass Ġin Ġthe Ġfront Ġyard Ġi Ġthink Ġi Ġwould Ġhave Ġpl owed Ġeverything Ġunder Ġand Ġhad Ġa Ġrice Ġfield\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It has rained enough to flood everything here and make rice pattys.\n",
      "\t Tokenized GPT2:It Ġhas Ġr ained Ġenough Ġto Ġflood Ġeverything Ġhere Ġand Ġmake Ġrice Ġpat ty s .\n",
      "\t Tokenized LLAMA3:It Ġhas Ġr ained Ġenough Ġto Ġflood Ġeverything Ġhere Ġand Ġmake Ġrice Ġpat ty s .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Kicked out of the house when she was only 16 (she was called Suzie in those days), Roy went to Delhi and then to architecture school, supporting herself by selling empty milk bottles (some say beer bottles).\n",
      "\t Tokenized GPT2:K icked Ġout Ġof Ġthe Ġhouse Ġwhen Ġshe Ġwas Ġonly Ġ16 Ġ( she Ġwas Ġcalled ĠSuz ie Ġin Ġthose Ġdays ), ĠRoy Ġwent Ġto ĠDelhi Ġand Ġthen Ġto Ġarchitecture Ġschool , Ġsupporting Ġherself Ġby Ġselling Ġempty Ġmilk Ġbottles Ġ( some Ġsay Ġbeer Ġbottles ).\n",
      "\t Tokenized LLAMA3:K icked Ġout Ġof Ġthe Ġhouse Ġwhen Ġshe Ġwas Ġonly Ġ 16 Ġ( she Ġwas Ġcalled ĠSuz ie Ġin Ġthose Ġdays ), ĠRoy Ġwent Ġto ĠDelhi Ġand Ġthen Ġto Ġarchitecture Ġschool , Ġsupporting Ġherself Ġby Ġselling Ġempty Ġmilk Ġbottles Ġ( some Ġsay Ġbeer Ġbottles ).\n",
      "\t Unique Tokens GPT2: {'Ġ16'}\n",
      "\t Unique Tokens LLAMA3: {'16', 'Ġ'}\n",
      "Text 2: Roy had to sell bottles to make money.\n",
      "\t Tokenized GPT2:R oy Ġhad Ġto Ġsell Ġbottles Ġto Ġmake Ġmoney .\n",
      "\t Tokenized LLAMA3:R oy Ġhad Ġto Ġsell Ġbottles Ġto Ġmake Ġmoney .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Czarek was welcomed enthusiastically, even though the poultry brotherhood was paying a lot of sudden attention to the newcomers - a strong group of young and talented managers from an egzemo-exotic chicken farm in Fodder Band nearby Podunkowice.\n",
      "\t Tokenized GPT2:C z are k Ġwas Ġwelcomed Ġenthusi astically , Ġeven Ġthough Ġthe Ġpou lt ry Ġbrother hood Ġwas Ġpaying Ġa Ġlot Ġof Ġsudden Ġattention Ġto Ġthe Ġnewcom ers Ġ- Ġa Ġstrong Ġgroup Ġof Ġyoung Ġand Ġtalented Ġmanagers Ġfrom Ġan Ġe gz emo - ex otic Ġchicken Ġfarm Ġin ĠF od der ĠBand Ġnearby ĠPod unk ow ice .\n",
      "\t Tokenized LLAMA3:C z are k Ġwas Ġwelcomed Ġenthusi astically , Ġeven Ġthough Ġthe Ġpou lt ry Ġbrother hood Ġwas Ġpaying Ġa Ġlot Ġof Ġsudden Ġattention Ġto Ġthe Ġnewcom ers Ġ- Ġa Ġstrong Ġgroup Ġof Ġyoung Ġand Ġtalented Ġmanagers Ġfrom Ġan Ġeg z emo -ex otic Ġchicken Ġfarm Ġin ĠF od der ĠBand Ġnearby ĠPod unk ow ice .\n",
      "\t Unique Tokens GPT2: {'ex', '-', 'gz', 'Ġe'}\n",
      "\t Unique Tokens LLAMA3: {'Ġeg', '-ex'}\n",
      "Text 2: Czarek was welcomed into the group by the farmers.\n",
      "\t Tokenized GPT2:C z are k Ġwas Ġwelcomed Ġinto Ġthe Ġgroup Ġby Ġthe Ġfarmers .\n",
      "\t Tokenized LLAMA3:C z are k Ġwas Ġwelcomed Ġinto Ġthe Ġgroup Ġby Ġthe Ġfarmers .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The media focused on Liggett's admissions of the obvious--that cigarettes are addictive and cause cancer and heart disease--and its agreement to pay the states a quarter of its (relatively small) pretax profits for the next 25 years.\n",
      "\t Tokenized GPT2:The Ġmedia Ġfocused Ġon ĠL ig get t 's Ġad missions Ġof Ġthe Ġobvious -- that Ġcigarettes Ġare Ġaddictive Ġand Ġcause Ġcancer Ġand Ġheart Ġdisease -- and Ġits Ġagreement Ġto Ġpay Ġthe Ġstates Ġa Ġquarter Ġof Ġits Ġ( rel atively Ġsmall ) Ġpret ax Ġprofits Ġfor Ġthe Ġnext Ġ25 Ġyears .\n",
      "\t Tokenized LLAMA3:The Ġmedia Ġfocused Ġon ĠL ig get t 's Ġad missions Ġof Ġthe Ġobvious -- that Ġcigarettes Ġare Ġaddict ive Ġand Ġcause Ġcancer Ġand Ġheart Ġdisease -- and Ġits Ġagreement Ġto Ġpay Ġthe Ġstates Ġa Ġquarter Ġof Ġits Ġ( rel atively Ġsmall ) Ġpret ax Ġprofits Ġfor Ġthe Ġnext Ġ 25 Ġyears .\n",
      "\t Unique Tokens GPT2: {'Ġ25', 'Ġaddictive'}\n",
      "\t Unique Tokens LLAMA3: {'ive', '25', 'Ġ', 'Ġaddict'}\n",
      "Text 2: The media reported on Lingett's insistence that cigarettes don't cause cancer.\n",
      "\t Tokenized GPT2:The Ġmedia Ġreported Ġon ĠL ing ett 's Ġins istence Ġthat Ġcigarettes Ġdon 't Ġcause Ġcancer .\n",
      "\t Tokenized LLAMA3:The Ġmedia Ġreported Ġon ĠL ing ett 's Ġins istence Ġthat Ġcigarettes Ġdon 't Ġcause Ġcancer .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: More works can be seen in the museum attached to the cathedral (admission is around 100 pe?­setas).\n",
      "\t Tokenized GPT2:More Ġworks Ġcan Ġbe Ġseen Ġin Ġthe Ġmuseum Ġattached Ġto Ġthe Ġcat hedral Ġ( ad mission Ġis Ġaround Ġ100 Ġpe ? ÂŃ set as ).\n",
      "\t Tokenized LLAMA3:More Ġworks Ġcan Ġbe Ġseen Ġin Ġthe Ġmuseum Ġattached Ġto Ġthe Ġcat hedral Ġ( ad mission Ġis Ġaround Ġ 100 Ġpe ? ÂŃ set as ).\n",
      "\t Unique Tokens GPT2: {'Ġ100'}\n",
      "\t Unique Tokens LLAMA3: {'100', 'Ġ'}\n",
      "Text 2: The museum attached to the cathedral has art in it.\n",
      "\t Tokenized GPT2:The Ġmuseum Ġattached Ġto Ġthe Ġcat hedral Ġhas Ġart Ġin Ġit .\n",
      "\t Tokenized LLAMA3:The Ġmuseum Ġattached Ġto Ġthe Ġcat hedral Ġhas Ġart Ġin Ġit .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: In Roman times a temple to Jupiter stood here, followed in the fourth century by the first Christian church, Saint-Etienne.\n",
      "\t Tokenized GPT2:In ĠRoman Ġtimes Ġa Ġtemple Ġto ĠJ upiter Ġstood Ġhere , Ġfollowed Ġin Ġthe Ġfourth Ġcentury Ġby Ġthe Ġfirst ĠChristian Ġchurch , ĠSaint - E t ien ne .\n",
      "\t Tokenized LLAMA3:In ĠRoman Ġtimes Ġa Ġtemple Ġto ĠJ upiter Ġstood Ġhere , Ġfollowed Ġin Ġthe Ġfourth Ġcentury Ġby Ġthe Ġfirst ĠChristian Ġchurch , ĠSaint -E t ien ne .\n",
      "\t Unique Tokens GPT2: {'-', 'E'}\n",
      "\t Unique Tokens LLAMA3: {'-E'}\n",
      "Text 2: Saint-Etienne burned to the ground during the Roman times.\n",
      "\t Tokenized GPT2:S aint - E t ien ne Ġburned Ġto Ġthe Ġground Ġduring Ġthe ĠRoman Ġtimes .\n",
      "\t Tokenized LLAMA3:S aint -E t ien ne Ġburned Ġto Ġthe Ġground Ġduring Ġthe ĠRoman Ġtimes .\n",
      "\t Unique Tokens GPT2: {'-', 'E'}\n",
      "\t Unique Tokens LLAMA3: {'-E'}\n",
      "==neutral==\n",
      "Text 1: Diets for men in their prime\n",
      "\t Tokenized GPT2:Di ets Ġfor Ġmen Ġin Ġtheir Ġprime\n",
      "\t Tokenized LLAMA3:Di ets Ġfor Ġmen Ġin Ġtheir Ġprime\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: A diet for men made up completely of meat. \n",
      "\t Tokenized GPT2:A Ġdiet Ġfor Ġmen Ġmade Ġup Ġcompletely Ġof Ġmeat . Ġ\n",
      "\t Tokenized LLAMA3:A Ġdiet Ġfor Ġmen Ġmade Ġup Ġcompletely Ġof Ġmeat . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The Passaic office is refusing to join in that reconfiguration, which goes into effect Jan.\n",
      "\t Tokenized GPT2:The ĠPass a ic Ġoffice Ġis Ġrefusing Ġto Ġjoin Ġin Ġthat Ġre configuration , Ġwhich Ġgoes Ġinto Ġeffect ĠJan .\n",
      "\t Tokenized LLAMA3:The ĠPass a ic Ġoffice Ġis Ġrefusing Ġto Ġjoin Ġin Ġthat Ġre configuration , Ġwhich Ġgoes Ġinto Ġeffect ĠJan .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It will be reconfigured in March. \n",
      "\t Tokenized GPT2:It Ġwill Ġbe Ġre config ured Ġin ĠMarch . Ġ\n",
      "\t Tokenized LLAMA3:It Ġwill Ġbe Ġre config ured Ġin ĠMarch . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: After being diagnosed with cancer, Carrey's Kaufman decides to do a show at Carnegie Hall.\n",
      "\t Tokenized GPT2:After Ġbeing Ġdiagnosed Ġwith Ġcancer , ĠCar rey 's ĠK au f man Ġdecides Ġto Ġdo Ġa Ġshow Ġat ĠCar neg ie ĠHall .\n",
      "\t Tokenized LLAMA3:After Ġbeing Ġdiagnosed Ġwith Ġcancer , ĠCar rey 's ĠK au f man Ġdecides Ġto Ġdo Ġa Ġshow Ġat ĠCar neg ie ĠHall .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Carrey's Kaufman eventually recovers from the cancer he was diagnosed with.\n",
      "\t Tokenized GPT2:Car rey 's ĠK au f man Ġeventually Ġrec overs Ġfrom Ġthe Ġcancer Ġhe Ġwas Ġdiagnosed Ġwith .\n",
      "\t Tokenized LLAMA3:Car rey 's ĠK au f man Ġeventually Ġrec overs Ġfrom Ġthe Ġcancer Ġhe Ġwas Ġdiagnosed Ġwith .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: In May 1967, Gallup found that the number of people who said they intensely disliked RFK--who was also probably more intensely liked than any other practicing politician--was twice as high as the number who intensely disliked Johnson, the architect of the increasingly unpopular war in Vietnam.\n",
      "\t Tokenized GPT2:In ĠMay Ġ19 67 , ĠGall up Ġfound Ġthat Ġthe Ġnumber Ġof Ġpeople Ġwho Ġsaid Ġthey Ġintense ly Ġdisli ked ĠRF K -- who Ġwas Ġalso Ġprobably Ġmore Ġintense ly Ġliked Ġthan Ġany Ġother Ġpracticing Ġpolitician -- was Ġtwice Ġas Ġhigh Ġas Ġthe Ġnumber Ġwho Ġintense ly Ġdisli ked ĠJohnson , Ġthe Ġarchitect Ġof Ġthe Ġincreasingly Ġun popular Ġwar Ġin ĠVietnam .\n",
      "\t Tokenized LLAMA3:In ĠMay Ġ 196 7 , ĠGall up Ġfound Ġthat Ġthe Ġnumber Ġof Ġpeople Ġwho Ġsaid Ġthey Ġintense ly Ġdisli ked ĠRF K -- who Ġwas Ġalso Ġprobably Ġmore Ġintense ly Ġliked Ġthan Ġany Ġother Ġpracticing Ġpolitician -- was Ġtwice Ġas Ġhigh Ġas Ġthe Ġnumber Ġwho Ġintense ly Ġdisli ked ĠJohnson , Ġthe Ġarchitect Ġof Ġthe Ġincreasingly Ġun popular Ġwar Ġin ĠVietnam .\n",
      "\t Unique Tokens GPT2: {'67', 'Ġ19'}\n",
      "\t Unique Tokens LLAMA3: {'196', '7', 'Ġ'}\n",
      "Text 2: In 1967 more people preferred Johnson than RFK.\n",
      "\t Tokenized GPT2:In Ġ19 67 Ġmore Ġpeople Ġpreferred ĠJohnson Ġthan ĠRF K .\n",
      "\t Tokenized LLAMA3:In Ġ 196 7 Ġmore Ġpeople Ġpreferred ĠJohnson Ġthan ĠRF K .\n",
      "\t Unique Tokens GPT2: {'67', 'Ġ19'}\n",
      "\t Unique Tokens LLAMA3: {'196', '7', 'Ġ'}\n",
      "==contradiction==\n",
      "Text 1: 17 An alternative to unaddressed mail would be to auction off the right to be a third bundle on specific days in specific post offices.\n",
      "\t Tokenized GPT2:17 ĠAn Ġalternative Ġto Ġun address ed Ġmail Ġwould Ġbe Ġto Ġauction Ġoff Ġthe Ġright Ġto Ġbe Ġa Ġthird Ġbundle Ġon Ġspecific Ġdays Ġin Ġspecific Ġpost Ġoffices .\n",
      "\t Tokenized LLAMA3:17 ĠAn Ġalternative Ġto Ġun add ressed Ġmail Ġwould Ġbe Ġto Ġauction Ġoff Ġthe Ġright Ġto Ġbe Ġa Ġthird Ġbundle Ġon Ġspecific Ġdays Ġin Ġspecific Ġpost Ġoffices .\n",
      "\t Unique Tokens GPT2: {'ed', 'address'}\n",
      "\t Unique Tokens LLAMA3: {'add', 'ressed'}\n",
      "Text 2: You could auction off the right to a fourth bundle instead of doing unaddressed mail.\n",
      "\t Tokenized GPT2:You Ġcould Ġauction Ġoff Ġthe Ġright Ġto Ġa Ġfourth Ġbundle Ġinstead Ġof Ġdoing Ġun address ed Ġmail .\n",
      "\t Tokenized LLAMA3:You Ġcould Ġauction Ġoff Ġthe Ġright Ġto Ġa Ġfourth Ġbundle Ġinstead Ġof Ġdoing Ġun add ressed Ġmail .\n",
      "\t Unique Tokens GPT2: {'ed', 'address'}\n",
      "\t Unique Tokens LLAMA3: {'add', 'ressed'}\n",
      "==contradiction==\n",
      "Text 1: We always knew it was an outside chance.\n",
      "\t Tokenized GPT2:We Ġalways Ġknew Ġit Ġwas Ġan Ġoutside Ġchance .\n",
      "\t Tokenized LLAMA3:We Ġalways Ġknew Ġit Ġwas Ġan Ġoutside Ġchance .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: We felt it was definitely going to happen.\n",
      "\t Tokenized GPT2:We Ġfelt Ġit Ġwas Ġdefinitely Ġgoing Ġto Ġhappen .\n",
      "\t Tokenized LLAMA3:We Ġfelt Ġit Ġwas Ġdefinitely Ġgoing Ġto Ġhappen .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: She had thrown away her cloak and tied her hair back into a topknot to keep it out of the way.\n",
      "\t Tokenized GPT2:She Ġhad Ġthrown Ġaway Ġher Ġcloak Ġand Ġtied Ġher Ġhair Ġback Ġinto Ġa Ġtop k not Ġto Ġkeep Ġit Ġout Ġof Ġthe Ġway .\n",
      "\t Tokenized LLAMA3:She Ġhad Ġthrown Ġaway Ġher Ġcloak Ġand Ġtied Ġher Ġhair Ġback Ġinto Ġa Ġtop k not Ġto Ġkeep Ġit Ġout Ġof Ġthe Ġway .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: She put her hair up.\n",
      "\t Tokenized GPT2:She Ġput Ġher Ġhair Ġup .\n",
      "\t Tokenized LLAMA3:She Ġput Ġher Ġhair Ġup .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1:  Other villages are much less developed, and therein lies the essence of many delights.\n",
      "\t Tokenized GPT2:ĠOther Ġvillages Ġare Ġmuch Ġless Ġdeveloped , Ġand Ġthere in Ġlies Ġthe Ġessence Ġof Ġmany Ġdelight s .\n",
      "\t Tokenized LLAMA3:ĠOther Ġvillages Ġare Ġmuch Ġless Ġdeveloped , Ġand Ġthere in Ġlies Ġthe Ġessence Ġof Ġmany Ġdelight s .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The other villages are greatly developed.\n",
      "\t Tokenized GPT2:The Ġother Ġvillages Ġare Ġgreatly Ġdeveloped .\n",
      "\t Tokenized LLAMA3:The Ġother Ġvillages Ġare Ġgreatly Ġdeveloped .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Given the limits on the WTO's jurisdiction, it was probably unreasonable of Kodak to expect a real victory.\n",
      "\t Tokenized GPT2:Given Ġthe Ġlimits Ġon Ġthe ĠW TO 's Ġjurisdiction , Ġit Ġwas Ġprobably Ġunreasonable Ġof ĠK od ak Ġto Ġexpect Ġa Ġreal Ġvictory .\n",
      "\t Tokenized LLAMA3:Given Ġthe Ġlimits Ġon Ġthe ĠW TO 's Ġjurisdiction , Ġit Ġwas Ġprobably Ġunreasonable Ġof ĠK od ak Ġto Ġexpect Ġa Ġreal Ġvictory .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Kodak was naive and is still just a baby of a company.\n",
      "\t Tokenized GPT2:K od ak Ġwas Ġnaive Ġand Ġis Ġstill Ġjust Ġa Ġbaby Ġof Ġa Ġcompany .\n",
      "\t Tokenized LLAMA3:K od ak Ġwas Ġnaive Ġand Ġis Ġstill Ġjust Ġa Ġbaby Ġof Ġa Ġcompany .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: He said the Web site will help bridge the digital divide that keeps the poor from using the Internet as a resource.\n",
      "\t Tokenized GPT2:He Ġsaid Ġthe ĠWeb Ġsite Ġwill Ġhelp Ġbridge Ġthe Ġdigital Ġdivide Ġthat Ġkeeps Ġthe Ġpoor Ġfrom Ġusing Ġthe ĠInternet Ġas Ġa Ġresource .\n",
      "\t Tokenized LLAMA3:He Ġsaid Ġthe ĠWeb Ġsite Ġwill Ġhelp Ġbridge Ġthe Ġdigital Ġdivide Ġthat Ġkeeps Ġthe Ġpoor Ġfrom Ġusing Ġthe ĠInternet Ġas Ġa Ġresource .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He was telling us that the website is designed to make it harder for the poor to get online. \n",
      "\t Tokenized GPT2:He Ġwas Ġtelling Ġus Ġthat Ġthe Ġwebsite Ġis Ġdesigned Ġto Ġmake Ġit Ġharder Ġfor Ġthe Ġpoor Ġto Ġget Ġonline . Ġ\n",
      "\t Tokenized LLAMA3:He Ġwas Ġtelling Ġus Ġthat Ġthe Ġwebsite Ġis Ġdesigned Ġto Ġmake Ġit Ġharder Ġfor Ġthe Ġpoor Ġto Ġget Ġonline . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: uh high humidity\n",
      "\t Tokenized GPT2:uh Ġhigh Ġhumidity\n",
      "\t Tokenized LLAMA3:uh Ġhigh Ġhumidity\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Air with increased water content.\n",
      "\t Tokenized GPT2:Air Ġwith Ġincreased Ġwater Ġcontent .\n",
      "\t Tokenized LLAMA3:Air Ġwith Ġincreased Ġwater Ġcontent .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: She has believed that the sleeping draught she administered was perfectly harmless, but there is no doubt that for one terrible moment she must have feared that Mrs. Inglethorp's death lay at her door. \n",
      "\t Tokenized GPT2:She Ġhas Ġbelieved Ġthat Ġthe Ġsleeping Ġdr aught Ġshe Ġadministered Ġwas Ġperfectly Ġharmless , Ġbut Ġthere Ġis Ġno Ġdoubt Ġthat Ġfor Ġone Ġterrible Ġmoment Ġshe Ġmust Ġhave Ġfeared Ġthat ĠMrs . ĠIn gle th orp 's Ġdeath Ġlay Ġat Ġher Ġdoor . Ġ\n",
      "\t Tokenized LLAMA3:She Ġhas Ġbelieved Ġthat Ġthe Ġsleeping Ġdr aught Ġshe Ġadministered Ġwas Ġperfectly Ġharmless , Ġbut Ġthere Ġis Ġno Ġdoubt Ġthat Ġfor Ġone Ġterrible Ġmoment Ġshe Ġmust Ġhave Ġfeared Ġthat ĠMrs . ĠIn gle th orp 's Ġdeath Ġlay Ġat Ġher Ġdoor . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: She had no doubt that Mrs. Inglethorp's death was not a concern.\n",
      "\t Tokenized GPT2:She Ġhad Ġno Ġdoubt Ġthat ĠMrs . ĠIn gle th orp 's Ġdeath Ġwas Ġnot Ġa Ġconcern .\n",
      "\t Tokenized LLAMA3:She Ġhad Ġno Ġdoubt Ġthat ĠMrs . ĠIn gle th orp 's Ġdeath Ġwas Ġnot Ġa Ġconcern .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: you know like CODA comes out of your out of your pay and the credit union comes out of your pay so we don't have to do anything there and the rest of it as far as my salary goes i just have it automatically deposited in into our bank\n",
      "\t Tokenized GPT2:you Ġknow Ġlike ĠC OD A Ġcomes Ġout Ġof Ġyour Ġout Ġof Ġyour Ġpay Ġand Ġthe Ġcredit Ġunion Ġcomes Ġout Ġof Ġyour Ġpay Ġso Ġwe Ġdon 't Ġhave Ġto Ġdo Ġanything Ġthere Ġand Ġthe Ġrest Ġof Ġit Ġas Ġfar Ġas Ġmy Ġsalary Ġgoes Ġi Ġjust Ġhave Ġit Ġautomatically Ġdeposited Ġin Ġinto Ġour Ġbank\n",
      "\t Tokenized LLAMA3:you Ġknow Ġlike ĠC OD A Ġcomes Ġout Ġof Ġyour Ġout Ġof Ġyour Ġpay Ġand Ġthe Ġcredit Ġunion Ġcomes Ġout Ġof Ġyour Ġpay Ġso Ġwe Ġdon 't Ġhave Ġto Ġdo Ġanything Ġthere Ġand Ġthe Ġrest Ġof Ġit Ġas Ġfar Ġas Ġmy Ġsalary Ġgoes Ġi Ġjust Ġhave Ġit Ġautomatically Ġdeposited Ġin Ġinto Ġour Ġbank\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I set things up so that my salary automatically deposits into our bank.\n",
      "\t Tokenized GPT2:I Ġset Ġthings Ġup Ġso Ġthat Ġmy Ġsalary Ġautomatically Ġdeposits Ġinto Ġour Ġbank .\n",
      "\t Tokenized LLAMA3:I Ġset Ġthings Ġup Ġso Ġthat Ġmy Ġsalary Ġautomatically Ġdeposits Ġinto Ġour Ġbank .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: But Fish is not an upbeat pragmatist.\n",
      "\t Tokenized GPT2:But ĠFish Ġis Ġnot Ġan Ġup beat Ġprag mat ist .\n",
      "\t Tokenized LLAMA3:But ĠFish Ġis Ġnot Ġan Ġup beat Ġpr ag mat ist .\n",
      "\t Unique Tokens GPT2: {'Ġprag'}\n",
      "\t Unique Tokens LLAMA3: {'ag', 'Ġpr'}\n",
      "Text 2: Fish is the nickname of a human. \n",
      "\t Tokenized GPT2:F ish Ġis Ġthe Ġnickname Ġof Ġa Ġhuman . Ġ\n",
      "\t Tokenized LLAMA3:F ish Ġis Ġthe Ġnickname Ġof Ġa Ġhuman . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: This marvelous Victorian-Gothic building is famous for the fanciful stone carvings around the base of its pillars (one pillar, reputedly depicting the club members, shows monkeys playing billiards).\n",
      "\t Tokenized GPT2:This Ġmarvel ous ĠVictorian - G oth ic Ġbuilding Ġis Ġfamous Ġfor Ġthe Ġfan c iful Ġstone Ġcar v ings Ġaround Ġthe Ġbase Ġof Ġits Ġpill ars Ġ( one Ġpill ar , Ġreput edly Ġdep icting Ġthe Ġclub Ġmembers , Ġshows Ġmon keys Ġplaying Ġbill i ards ).\n",
      "\t Tokenized LLAMA3:This Ġmarvel ous ĠVictor ian -G oth ic Ġbuilding Ġis Ġfamous Ġfor Ġthe Ġfan c iful Ġstone Ġcar v ings Ġaround Ġthe Ġbase Ġof Ġits Ġpill ars Ġ( one Ġpill ar , Ġreput ed ly Ġdep icting Ġthe Ġclub Ġmembers , Ġshows Ġmon keys Ġplaying Ġbill i ards ).\n",
      "\t Unique Tokens GPT2: {'ĠVictorian', 'G', 'edly', '-'}\n",
      "\t Unique Tokens LLAMA3: {'ĠVictor', 'ly', 'ed', '-G', 'ian'}\n",
      "Text 2: Club members of the marvelous and famous Victorian-Gothic building are likened to monkeys for being rich douchebags.\n",
      "\t Tokenized GPT2:Cl ub Ġmembers Ġof Ġthe Ġmarvel ous Ġand Ġfamous ĠVictorian - G oth ic Ġbuilding Ġare Ġli ken ed Ġto Ġmon keys Ġfor Ġbeing Ġrich Ġdouche b ags .\n",
      "\t Tokenized LLAMA3:Cl ub Ġmembers Ġof Ġthe Ġmarvel ous Ġand Ġfamous ĠVictor ian -G oth ic Ġbuilding Ġare Ġli ken ed Ġto Ġmon keys Ġfor Ġbeing Ġrich Ġdouche b ags .\n",
      "\t Unique Tokens GPT2: {'ĠVictorian', 'G', '-'}\n",
      "\t Unique Tokens LLAMA3: {'ĠVictor', 'ian', '-G'}\n",
      "==contradiction==\n",
      "Text 1: and the professors who go there and you're not going to see the professors you know you're going to see some TA you know uh\n",
      "\t Tokenized GPT2:and Ġthe Ġprofessors Ġwho Ġgo Ġthere Ġand Ġyou 're Ġnot Ġgoing Ġto Ġsee Ġthe Ġprofessors Ġyou Ġknow Ġyou 're Ġgoing Ġto Ġsee Ġsome ĠTA Ġyou Ġknow Ġuh\n",
      "\t Tokenized LLAMA3:and Ġthe Ġprofessors Ġwho Ġgo Ġthere Ġand Ġyou 're Ġnot Ġgoing Ġto Ġsee Ġthe Ġprofessors Ġyou Ġknow Ġyou 're Ġgoing Ġto Ġsee Ġsome ĠTA Ġyou Ġknow Ġuh\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: You don't really see the TAs.\n",
      "\t Tokenized GPT2:You Ġdon 't Ġreally Ġsee Ġthe ĠT As .\n",
      "\t Tokenized LLAMA3:You Ġdon 't Ġreally Ġsee Ġthe ĠT As .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: see too much crime on TV and they think it's way to go i don't know what do you think\n",
      "\t Tokenized GPT2:see Ġtoo Ġmuch Ġcrime Ġon ĠTV Ġand Ġthey Ġthink Ġit 's Ġway Ġto Ġgo Ġi Ġdon 't Ġknow Ġwhat Ġdo Ġyou Ġthink\n",
      "\t Tokenized LLAMA3:see Ġtoo Ġmuch Ġcrime Ġon ĠTV Ġand Ġthey Ġthink Ġit 's Ġway Ġto Ġgo Ġi Ġdon 't Ġknow Ġwhat Ġdo Ġyou Ġthink\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: TV has a lot of crime shown on it.\n",
      "\t Tokenized GPT2:TV Ġhas Ġa Ġlot Ġof Ġcrime Ġshown Ġon Ġit .\n",
      "\t Tokenized LLAMA3:TV Ġhas Ġa Ġlot Ġof Ġcrime Ġshown Ġon Ġit .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Once or twice, but they seem more show than battle, said Adrin.\n",
      "\t Tokenized GPT2:Once Ġor Ġtwice , Ġbut Ġthey Ġseem Ġmore Ġshow Ġthan Ġbattle , Ġsaid ĠAd rin .\n",
      "\t Tokenized LLAMA3:Once Ġor Ġtwice , Ġbut Ġthey Ġseem Ġmore Ġshow Ġthan Ġbattle , Ġsaid ĠAd rin .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Adrin said they were amazing warriors.\n",
      "\t Tokenized GPT2:Ad rin Ġsaid Ġthey Ġwere Ġamazing Ġwarriors .\n",
      "\t Tokenized LLAMA3:Ad rin Ġsaid Ġthey Ġwere Ġamazing Ġwarriors .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: yeah yeah i i went i went off to school wanting to either be a high school algebra teacher or high school French teacher because my two favorite people in the in high school were my algebra teacher and French teacher and uh and i was going to do that until the end of our sophomore year when we wante\n",
      "\t Tokenized GPT2:yeah Ġyeah Ġi Ġi Ġwent Ġi Ġwent Ġoff Ġto Ġschool Ġwanting Ġto Ġeither Ġbe Ġa Ġhigh Ġschool Ġalgebra Ġteacher Ġor Ġhigh Ġschool ĠFrench Ġteacher Ġbecause Ġmy Ġtwo Ġfavorite Ġpeople Ġin Ġthe Ġin Ġhigh Ġschool Ġwere Ġmy Ġalgebra Ġteacher Ġand ĠFrench Ġteacher Ġand Ġuh Ġand Ġi Ġwas Ġgoing Ġto Ġdo Ġthat Ġuntil Ġthe Ġend Ġof Ġour Ġsophomore Ġyear Ġwhen Ġwe Ġwant e\n",
      "\t Tokenized LLAMA3:yeah Ġyeah Ġi Ġi Ġwent Ġi Ġwent Ġoff Ġto Ġschool Ġwanting Ġto Ġeither Ġbe Ġa Ġhigh Ġschool Ġalgebra Ġteacher Ġor Ġhigh Ġschool ĠFrench Ġteacher Ġbecause Ġmy Ġtwo Ġfavorite Ġpeople Ġin Ġthe Ġin Ġhigh Ġschool Ġwere Ġmy Ġalgebra Ġteacher Ġand ĠFrench Ġteacher Ġand Ġuh Ġand Ġi Ġwas Ġgoing Ġto Ġdo Ġthat Ġuntil Ġthe Ġend Ġof Ġour Ġsophomore Ġyear Ġwhen Ġwe Ġwant e\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I was going to major in algebra or french but I ended falling in love with chemistry. \n",
      "\t Tokenized GPT2:I Ġwas Ġgoing Ġto Ġmajor Ġin Ġalgebra Ġor Ġfrench Ġbut ĠI Ġended Ġfalling Ġin Ġlove Ġwith Ġchemistry . Ġ\n",
      "\t Tokenized LLAMA3:I Ġwas Ġgoing Ġto Ġmajor Ġin Ġalgebra Ġor Ġfrench Ġbut ĠI Ġended Ġfalling Ġin Ġlove Ġwith Ġchemistry . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: But in fact Haveman and Wolfe's statistical analysis is designed to rule out this and similar alternative theories, leaving us to conclude that the moves themselves are harmful.\n",
      "\t Tokenized GPT2:But Ġin Ġfact ĠHave man Ġand ĠWol fe 's Ġstatistical Ġanalysis Ġis Ġdesigned Ġto Ġrule Ġout Ġthis Ġand Ġsimilar Ġalternative Ġtheories , Ġleaving Ġus Ġto Ġconclude Ġthat Ġthe Ġmoves Ġthemselves Ġare Ġharmful .\n",
      "\t Tokenized LLAMA3:But Ġin Ġfact ĠHave man Ġand ĠWol fe 's Ġstatistical Ġanalysis Ġis Ġdesigned Ġto Ġrule Ġout Ġthis Ġand Ġsimilar Ġalternative Ġtheories , Ġleaving Ġus Ġto Ġconclude Ġthat Ġthe Ġmoves Ġthemselves Ġare Ġharmful .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: We think these moves are benevolent.\n",
      "\t Tokenized GPT2:We Ġthink Ġthese Ġmoves Ġare Ġbene vol ent .\n",
      "\t Tokenized LLAMA3:We Ġthink Ġthese Ġmoves Ġare Ġbene vol ent .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: It is worth a visit, if only to see the theater itself.\n",
      "\t Tokenized GPT2:It Ġis Ġworth Ġa Ġvisit , Ġif Ġonly Ġto Ġsee Ġthe Ġtheater Ġitself .\n",
      "\t Tokenized LLAMA3:It Ġis Ġworth Ġa Ġvisit , Ġif Ġonly Ġto Ġsee Ġthe Ġtheater Ġitself .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The place is definitely worth visiting, especially for its theater.\n",
      "\t Tokenized GPT2:The Ġplace Ġis Ġdefinitely Ġworth Ġvisiting , Ġespecially Ġfor Ġits Ġtheater .\n",
      "\t Tokenized LLAMA3:The Ġplace Ġis Ġdefinitely Ġworth Ġvisiting , Ġespecially Ġfor Ġits Ġtheater .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The Standard , published a few days before Deng's death, covers similar territory.\n",
      "\t Tokenized GPT2:The ĠStandard Ġ, Ġpublished Ġa Ġfew Ġdays Ġbefore ĠD eng 's Ġdeath , Ġcovers Ġsimilar Ġterritory .\n",
      "\t Tokenized LLAMA3:The ĠStandard Ġ, Ġpublished Ġa Ġfew Ġdays Ġbefore ĠD eng 's Ġdeath , Ġcovers Ġsimilar Ġterritory .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The Washington Post covers similar territory.\n",
      "\t Tokenized GPT2:The ĠWashington ĠPost Ġcovers Ġsimilar Ġterritory .\n",
      "\t Tokenized LLAMA3:The ĠWashington ĠPost Ġcovers Ġsimilar Ġterritory .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The stuff was strong, but somewhat brittle.\n",
      "\t Tokenized GPT2:The Ġstuff Ġwas Ġstrong , Ġbut Ġsomewhat Ġbr ittle .\n",
      "\t Tokenized LLAMA3:The Ġstuff Ġwas Ġstrong , Ġbut Ġsomewhat Ġbr ittle .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The plaster was strong, yet brittle.\n",
      "\t Tokenized GPT2:The Ġpl aster Ġwas Ġstrong , Ġyet Ġbr ittle .\n",
      "\t Tokenized LLAMA3:The Ġpl aster Ġwas Ġstrong , Ġyet Ġbr ittle .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Don't mean the police, but the people that are right in it. \n",
      "\t Tokenized GPT2:Don 't Ġmean Ġthe Ġpolice , Ġbut Ġthe Ġpeople Ġthat Ġare Ġright Ġin Ġit . Ġ\n",
      "\t Tokenized LLAMA3:Don 't Ġmean Ġthe Ġpolice , Ġbut Ġthe Ġpeople Ġthat Ġare Ġright Ġin Ġit . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The people were right. \n",
      "\t Tokenized GPT2:The Ġpeople Ġwere Ġright . Ġ\n",
      "\t Tokenized LLAMA3:The Ġpeople Ġwere Ġright . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Last year, Arafat cracked down on Hamas after a string of bombings in Tel Aviv and Jerusalem, arresting more than 1,200 suspected terrorists, destroying Hamas safe houses, and confiscating its weapons caches.\n",
      "\t Tokenized GPT2:Last Ġyear , ĠA ra fat Ġcracked Ġdown Ġon ĠHam as Ġafter Ġa Ġstring Ġof Ġbomb ings Ġin ĠTel ĠAv iv Ġand ĠJerusalem , Ġarrest ing Ġmore Ġthan Ġ1 , 200 Ġsuspected Ġterrorists , Ġdestroying ĠHam as Ġsafe Ġhouses , Ġand Ġconf isc ating Ġits Ġweapons Ġc aches .\n",
      "\t Tokenized LLAMA3:Last Ġyear , ĠA raf at Ġcracked Ġdown Ġon ĠHam as Ġafter Ġa Ġstring Ġof Ġbomb ings Ġin ĠTel ĠAv iv Ġand ĠJerusalem , Ġarrest ing Ġmore Ġthan Ġ 1 , 200 Ġsuspected Ġterrorists , Ġdestroying ĠHam as Ġsafe Ġhouses , Ġand Ġconf isc ating Ġits Ġweapons Ġc aches .\n",
      "\t Unique Tokens GPT2: {'ra', 'Ġ1', 'fat'}\n",
      "\t Unique Tokens LLAMA3: {'1', 'at', 'raf', 'Ġ'}\n",
      "Text 2: Arafat stood back and watched Hamas attack Tel Aviv and Jerusalem.\n",
      "\t Tokenized GPT2:A ra fat Ġstood Ġback Ġand Ġwatched ĠHam as Ġattack ĠTel ĠAv iv Ġand ĠJerusalem .\n",
      "\t Tokenized LLAMA3:A raf at Ġstood Ġback Ġand Ġwatched ĠHam as Ġattack ĠTel ĠAv iv Ġand ĠJerusalem .\n",
      "\t Unique Tokens GPT2: {'ra', 'fat'}\n",
      "\t Unique Tokens LLAMA3: {'at', 'raf'}\n",
      "==contradiction==\n",
      "Text 1: Allow time in Thirasia to explore Santorini's smaller sibling islands.\n",
      "\t Tokenized GPT2:Allow Ġtime Ġin ĠTh ir asia Ġto Ġexplore ĠSant or ini 's Ġsmaller Ġsibling Ġislands .\n",
      "\t Tokenized LLAMA3:Allow Ġtime Ġin ĠTh ir asia Ġto Ġexplore ĠSant or ini 's Ġsmaller Ġsibling Ġislands .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Allow time in Thirasia to visit the famous Guy Fieri restaurant.\n",
      "\t Tokenized GPT2:Allow Ġtime Ġin ĠTh ir asia Ġto Ġvisit Ġthe Ġfamous ĠGuy ĠF ier i Ġrestaurant .\n",
      "\t Tokenized LLAMA3:Allow Ġtime Ġin ĠTh ir asia Ġto Ġvisit Ġthe Ġfamous ĠGuy ĠF ier i Ġrestaurant .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: we wouldn't be expected to cast a ballet on the subject\n",
      "\t Tokenized GPT2:we Ġwouldn 't Ġbe Ġexpected Ġto Ġcast Ġa Ġbal let Ġon Ġthe Ġsubject\n",
      "\t Tokenized LLAMA3:we Ġwouldn 't Ġbe Ġexpected Ġto Ġcast Ġa Ġbal let Ġon Ġthe Ġsubject\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: There is no expectation for us to vote on the matter.\n",
      "\t Tokenized GPT2:There Ġis Ġno Ġexpectation Ġfor Ġus Ġto Ġvote Ġon Ġthe Ġmatter .\n",
      "\t Tokenized LLAMA3:There Ġis Ġno Ġexpectation Ġfor Ġus Ġto Ġvote Ġon Ġthe Ġmatter .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: oh that's not really important the the other stuff is just you know window dressing because we we've never ordered anything fact the the van that we've got we bought uh from an estate it was an estate trade uh it was almost brand new the the gentlemen who owned it had died\n",
      "\t Tokenized GPT2:oh Ġthat 's Ġnot Ġreally Ġimportant Ġthe Ġthe Ġother Ġstuff Ġis Ġjust Ġyou Ġknow Ġwindow Ġdressing Ġbecause Ġwe Ġwe 've Ġnever Ġordered Ġanything Ġfact Ġthe Ġthe Ġvan Ġthat Ġwe 've Ġgot Ġwe Ġbought Ġuh Ġfrom Ġan Ġestate Ġit Ġwas Ġan Ġestate Ġtrade Ġuh Ġit Ġwas Ġalmost Ġbrand Ġnew Ġthe Ġthe Ġgentlemen Ġwho Ġowned Ġit Ġhad Ġdied\n",
      "\t Tokenized LLAMA3:oh Ġthat 's Ġnot Ġreally Ġimportant Ġthe Ġthe Ġother Ġstuff Ġis Ġjust Ġyou Ġknow Ġwindow Ġdressing Ġbecause Ġwe Ġwe 've Ġnever Ġordered Ġanything Ġfact Ġthe Ġthe Ġvan Ġthat Ġwe 've Ġgot Ġwe Ġbought Ġuh Ġfrom Ġan Ġestate Ġit Ġwas Ġan Ġestate Ġtrade Ġuh Ġit Ġwas Ġalmost Ġbrand Ġnew Ġthe Ġthe Ġgentlemen Ġwho Ġowned Ġit Ġhad Ġdied\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: We ordered our van and bought it from a used car dealer.\n",
      "\t Tokenized GPT2:We Ġordered Ġour Ġvan Ġand Ġbought Ġit Ġfrom Ġa Ġused Ġcar Ġdealer .\n",
      "\t Tokenized LLAMA3:We Ġordered Ġour Ġvan Ġand Ġbought Ġit Ġfrom Ġa Ġused Ġcar Ġdealer .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Do you trust me, Uncle?Gauve hesitated.\n",
      "\t Tokenized GPT2:Do Ġyou Ġtrust Ġme , ĠUncle ? G au ve Ġhesitated .\n",
      "\t Tokenized LLAMA3:Do Ġyou Ġtrust Ġme , ĠUncle ? G au ve Ġhesitated .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Gauve's uncle has trust issues.\n",
      "\t Tokenized GPT2:G au ve 's Ġuncle Ġhas Ġtrust Ġissues .\n",
      "\t Tokenized LLAMA3:G au ve 's Ġuncle Ġhas Ġtrust Ġissues .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Critics praise Goodman's finely honed descriptive abilities and instinctive grasp of familial dynamics, the ways in which dreams and emotional habits are handed down ...\n",
      "\t Tokenized GPT2:Cr itics Ġpraise ĠGood man 's Ġfine ly Ġhon ed Ġdescriptive Ġabilities Ġand Ġinstinct ive Ġgrasp Ġof Ġfam il ial Ġdynamics , Ġthe Ġways Ġin Ġwhich Ġdreams Ġand Ġemotional Ġhabits Ġare Ġhanded Ġdown Ġ...\n",
      "\t Tokenized LLAMA3:Cr itics Ġpraise ĠGood man 's Ġfine ly Ġhon ed Ġdescriptive Ġabilities Ġand Ġinstinct ive Ġgrasp Ġof Ġfam il ial Ġdynamics , Ġthe Ġways Ġin Ġwhich Ġdreams Ġand Ġemotional Ġhabits Ġare Ġhanded Ġdown Ġ...\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Emotional habits and dreams are always derived from candy and popcorn.\n",
      "\t Tokenized GPT2:Em otional Ġhabits Ġand Ġdreams Ġare Ġalways Ġderived Ġfrom Ġcandy Ġand Ġpopcorn .\n",
      "\t Tokenized LLAMA3:Em ot ional Ġhabits Ġand Ġdreams Ġare Ġalways Ġderived Ġfrom Ġcandy Ġand Ġpop corn .\n",
      "\t Unique Tokens GPT2: {'otional', 'Ġpopcorn'}\n",
      "\t Unique Tokens LLAMA3: {'Ġpop', 'ional', 'corn', 'ot'}\n",
      "==neutral==\n",
      "Text 1: The most comfortable courses are in the cooler hill stations, notably Cameron Highlands and Fraser's Hill.\n",
      "\t Tokenized GPT2:The Ġmost Ġcomfortable Ġcourses Ġare Ġin Ġthe Ġcooler Ġhill Ġstations , Ġnotably ĠCameron ĠHigh lands Ġand ĠFr aser 's ĠHill .\n",
      "\t Tokenized LLAMA3:The Ġmost Ġcomfortable Ġcourses Ġare Ġin Ġthe Ġcooler Ġhill Ġstations , Ġnotably ĠCameron ĠHigh lands Ġand ĠFr aser 's ĠHill .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It's best to golf in the cooler hill stations.\n",
      "\t Tokenized GPT2:It 's Ġbest Ġto Ġgolf Ġin Ġthe Ġcooler Ġhill Ġstations .\n",
      "\t Tokenized LLAMA3:It 's Ġbest Ġto Ġgolf Ġin Ġthe Ġcooler Ġhill Ġstations .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Sometimes it flattens entire neighbourhoods to make life easier for them.\n",
      "\t Tokenized GPT2:Sometimes Ġit Ġfl att ens Ġentire Ġneighbourhood s Ġto Ġmake Ġlife Ġeasier Ġfor Ġthem .\n",
      "\t Tokenized LLAMA3:Sometimes Ġit Ġfl att ens Ġentire Ġneighbourhood s Ġto Ġmake Ġlife Ġeasier Ġfor Ġthem .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Entire neighborhoods have been flattened just to make life easier for them.\n",
      "\t Tokenized GPT2:Ent ire Ġneighborhoods Ġhave Ġbeen Ġflatten ed Ġjust Ġto Ġmake Ġlife Ġeasier Ġfor Ġthem .\n",
      "\t Tokenized LLAMA3:Ent ire Ġneighborhoods Ġhave Ġbeen Ġflatten ed Ġjust Ġto Ġmake Ġlife Ġeasier Ġfor Ġthem .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Each working group met several times to develop recommendations for changes to the legal services delivery system.\n",
      "\t Tokenized GPT2:Each Ġworking Ġgroup Ġmet Ġseveral Ġtimes Ġto Ġdevelop Ġrecommendations Ġfor Ġchanges Ġto Ġthe Ġlegal Ġservices Ġdelivery Ġsystem .\n",
      "\t Tokenized LLAMA3:Each Ġworking Ġgroup Ġmet Ġseveral Ġtimes Ġto Ġdevelop Ġrecommendations Ġfor Ġchanges Ġto Ġthe Ġlegal Ġservices Ġdelivery Ġsystem .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The groups disagreed on the appropriate action to take, but they finally found a solution. \n",
      "\t Tokenized GPT2:The Ġgroups Ġdisag reed Ġon Ġthe Ġappropriate Ġaction Ġto Ġtake , Ġbut Ġthey Ġfinally Ġfound Ġa Ġsolution . Ġ\n",
      "\t Tokenized LLAMA3:The Ġgroups Ġdisag reed Ġon Ġthe Ġappropriate Ġaction Ġto Ġtake , Ġbut Ġthey Ġfinally Ġfound Ġa Ġsolution . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: We are assured of success?\"\n",
      "\t Tokenized GPT2:We Ġare Ġassured Ġof Ġsuccess ?\"\n",
      "\t Tokenized LLAMA3:We Ġare Ġassured Ġof Ġsuccess ?\"\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: \"Are we definitely going to be successful and stay alive?\" \n",
      "\t Tokenized GPT2:\" Are Ġwe Ġdefinitely Ġgoing Ġto Ġbe Ġsuccessful Ġand Ġstay Ġalive ?\" Ġ\n",
      "\t Tokenized LLAMA3:\"Are Ġwe Ġdefinitely Ġgoing Ġto Ġbe Ġsuccessful Ġand Ġstay Ġalive ?\" Ġ\n",
      "\t Unique Tokens GPT2: {'\"', 'Are'}\n",
      "\t Unique Tokens LLAMA3: {'\"Are'}\n",
      "==neutral==\n",
      "Text 1: At the western end of Cowgate (where it meets Holyrood Road), you will see one of the few remaining sections of Edinburgh's old city wall (Flodden Wall), built following the Lang Siege of the 1570s.\n",
      "\t Tokenized GPT2:At Ġthe Ġwestern Ġend Ġof ĠCow gate Ġ( where Ġit Ġmeets ĠHoly ro od ĠRoad ), Ġyou Ġwill Ġsee Ġone Ġof Ġthe Ġfew Ġremaining Ġsections Ġof ĠEdinburgh 's Ġold Ġcity Ġwall Ġ( Fl od den ĠWall ), Ġbuilt Ġfollowing Ġthe ĠLang ĠSie ge Ġof Ġthe Ġ15 70 s .\n",
      "\t Tokenized LLAMA3:At Ġthe Ġwestern Ġend Ġof ĠCow gate Ġ( where Ġit Ġmeets ĠHoly ro od ĠRoad ), Ġyou Ġwill Ġsee Ġone Ġof Ġthe Ġfew Ġremaining Ġsections Ġof ĠEdinburgh 's Ġold Ġcity Ġwall Ġ( Fl od den ĠWall ), Ġbuilt Ġfollowing Ġthe ĠLang ĠSie ge Ġof Ġthe Ġ 157 0 s .\n",
      "\t Unique Tokens GPT2: {'Ġ15', '70'}\n",
      "\t Unique Tokens LLAMA3: {'157', '0', 'Ġ'}\n",
      "Text 2: Flodden Wall was built by the townspeople to protect against further invasion.\n",
      "\t Tokenized GPT2:Fl od den ĠWall Ġwas Ġbuilt Ġby Ġthe Ġtown spe ople Ġto Ġprotect Ġagainst Ġfurther Ġinvasion .\n",
      "\t Tokenized LLAMA3:Fl od den ĠWall Ġwas Ġbuilt Ġby Ġthe Ġtown spe ople Ġto Ġprotect Ġagainst Ġfurther Ġinvasion .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: they'll they'll say yeah why didn't you buy why didn't you try something more mainline\n",
      "\t Tokenized GPT2:they 'll Ġthey 'll Ġsay Ġyeah Ġwhy Ġdidn 't Ġyou Ġbuy Ġwhy Ġdidn 't Ġyou Ġtry Ġsomething Ġmore Ġmain line\n",
      "\t Tokenized LLAMA3:they 'll Ġthey 'll Ġsay Ġyeah Ġwhy Ġdidn 't Ġyou Ġbuy Ġwhy Ġdidn 't Ġyou Ġtry Ġsomething Ġmore Ġmain line\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: They will scoff at you for doing something so mainline.\n",
      "\t Tokenized GPT2:They Ġwill Ġscoff Ġat Ġyou Ġfor Ġdoing Ġsomething Ġso Ġmain line .\n",
      "\t Tokenized LLAMA3:They Ġwill Ġscoff Ġat Ġyou Ġfor Ġdoing Ġsomething Ġso Ġmain line .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Ah, yes, actually, two weeks ago we had a very similar situation, the captain alertly added and quickly changed the subject, 'What's important now is that you get ready for about 2 minutes in the state of weightlessness, and not some Slovakian satellite from two weeks ago.\n",
      "\t Tokenized GPT2:Ah , Ġyes , Ġactually , Ġtwo Ġweeks Ġago Ġwe Ġhad Ġa Ġvery Ġsimilar Ġsituation , Ġthe Ġcaptain Ġalert ly Ġadded Ġand Ġquickly Ġchanged Ġthe Ġsubject , Ġ' What 's Ġimportant Ġnow Ġis Ġthat Ġyou Ġget Ġready Ġfor Ġabout Ġ2 Ġminutes Ġin Ġthe Ġstate Ġof Ġweight lessness , Ġand Ġnot Ġsome ĠSl ov ak ian Ġsatellite Ġfrom Ġtwo Ġweeks Ġago .\n",
      "\t Tokenized LLAMA3:Ah , Ġyes , Ġactually , Ġtwo Ġweeks Ġago Ġwe Ġhad Ġa Ġvery Ġsimilar Ġsituation , Ġthe Ġcaptain Ġalert ly Ġadded Ġand Ġquickly Ġchanged Ġthe Ġsubject , Ġ' What 's Ġimportant Ġnow Ġis Ġthat Ġyou Ġget Ġready Ġfor Ġabout Ġ 2 Ġminutes Ġin Ġthe Ġstate Ġof Ġweight lessness , Ġand Ġnot Ġsome ĠSl ov ak ian Ġsatellite Ġfrom Ġtwo Ġweeks Ġago .\n",
      "\t Unique Tokens GPT2: {'Ġ2'}\n",
      "\t Unique Tokens LLAMA3: {'2', 'Ġ'}\n",
      "Text 2: The captain of the ship didn't want to bring up the destruction of the Slovakian satellite that happened two weeks prior. \n",
      "\t Tokenized GPT2:The Ġcaptain Ġof Ġthe Ġship Ġdidn 't Ġwant Ġto Ġbring Ġup Ġthe Ġdestruction Ġof Ġthe ĠSl ov ak ian Ġsatellite Ġthat Ġhappened Ġtwo Ġweeks Ġprior . Ġ\n",
      "\t Tokenized LLAMA3:The Ġcaptain Ġof Ġthe Ġship Ġdidn 't Ġwant Ġto Ġbring Ġup Ġthe Ġdestruction Ġof Ġthe ĠSl ov ak ian Ġsatellite Ġthat Ġhappened Ġtwo Ġweeks Ġprior . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: He touched it and felt his skin swelling and growing hot.\n",
      "\t Tokenized GPT2:He Ġtouched Ġit Ġand Ġfelt Ġhis Ġskin Ġswelling Ġand Ġgrowing Ġhot .\n",
      "\t Tokenized LLAMA3:He Ġtouched Ġit Ġand Ġfelt Ġhis Ġskin Ġswelling Ġand Ġgrowing Ġhot .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He touched a hot rod.\n",
      "\t Tokenized GPT2:He Ġtouched Ġa Ġhot Ġrod .\n",
      "\t Tokenized LLAMA3:He Ġtouched Ġa Ġhot Ġrod .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: According to a 1995 Financial Executives Research Foundation report,5 transaction processing and other routine accounting activities, such as accounts payable, payroll, and external reporting, consume about 69 percent of costs within finance.\n",
      "\t Tokenized GPT2:According Ġto Ġa Ġ1995 ĠFinancial ĠExecut ives ĠResearch ĠFoundation Ġreport , 5 Ġtransaction Ġprocessing Ġand Ġother Ġroutine Ġaccounting Ġactivities , Ġsuch Ġas Ġaccounts Ġpay able , Ġpayroll , Ġand Ġexternal Ġreporting , Ġconsume Ġabout Ġ69 Ġpercent Ġof Ġcosts Ġwithin Ġfinance .\n",
      "\t Tokenized LLAMA3:According Ġto Ġa Ġ 199 5 ĠFinancial ĠExecut ives ĠResearch ĠFoundation Ġreport , 5 Ġtransaction Ġprocessing Ġand Ġother Ġroutine Ġaccounting Ġactivities , Ġsuch Ġas Ġaccounts Ġpay able , Ġpay roll , Ġand Ġexternal Ġreporting , Ġconsume Ġabout Ġ 69 Ġpercent Ġof Ġcosts Ġwithin Ġfinance .\n",
      "\t Unique Tokens GPT2: {'Ġ1995', 'Ġ69', 'Ġpayroll'}\n",
      "\t Unique Tokens LLAMA3: {'roll', '199', 'Ġ', '69'}\n",
      "Text 2: Almost 70% of costs within finance are for routine accounting activities. \n",
      "\t Tokenized GPT2:Almost Ġ70 % Ġof Ġcosts Ġwithin Ġfinance Ġare Ġfor Ġroutine Ġaccounting Ġactivities . Ġ\n",
      "\t Tokenized LLAMA3:Almost Ġ 70 % Ġof Ġcosts Ġwithin Ġfinance Ġare Ġfor Ġroutine Ġaccounting Ġactivities . Ġ\n",
      "\t Unique Tokens GPT2: {'Ġ70'}\n",
      "\t Unique Tokens LLAMA3: {'70'}\n",
      "==entailment==\n",
      "Text 1: (The employee was later rehired, and Bob denies the charge.)\n",
      "\t Tokenized GPT2:( The Ġemployee Ġwas Ġlater Ġre h ired , Ġand ĠBob Ġden ies Ġthe Ġcharge .)\n",
      "\t Tokenized LLAMA3:( The Ġemployee Ġwas Ġlater Ġre h ired , Ġand ĠBob Ġden ies Ġthe Ġcharge .)\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The employee got their job back.\n",
      "\t Tokenized GPT2:The Ġemployee Ġgot Ġtheir Ġjob Ġback .\n",
      "\t Tokenized LLAMA3:The Ġemployee Ġgot Ġtheir Ġjob Ġback .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Despite their 17th-century origins, these gardens avoid the rigid geometry of the Tuileries and Ver?­sailles.\n",
      "\t Tokenized GPT2:Despite Ġtheir Ġ17 th - century Ġorigins , Ġthese Ġgardens Ġavoid Ġthe Ġrigid Ġgeometry Ġof Ġthe ĠTu iler ies Ġand ĠVer ? ÂŃ sa illes .\n",
      "\t Tokenized LLAMA3:Despite Ġtheir Ġ 17 th -century Ġorigins , Ġthese Ġgardens Ġavoid Ġthe Ġrigid Ġgeometry Ġof Ġthe ĠTu iler ies Ġand ĠVer ? ÂŃ sa illes .\n",
      "\t Unique Tokens GPT2: {'-', 'century', 'Ġ17'}\n",
      "\t Unique Tokens LLAMA3: {'-century', 'Ġ', '17'}\n",
      "Text 2: These gardens were around well before the 17th-century.\n",
      "\t Tokenized GPT2:These Ġgardens Ġwere Ġaround Ġwell Ġbefore Ġthe Ġ17 th - century .\n",
      "\t Tokenized LLAMA3:These Ġgardens Ġwere Ġaround Ġwell Ġbefore Ġthe Ġ 17 th -century .\n",
      "\t Unique Tokens GPT2: {'-', 'century', 'Ġ17'}\n",
      "\t Unique Tokens LLAMA3: {'-century', 'Ġ', '17'}\n",
      "==neutral==\n",
      "Text 1: (The Ramseys buried their daughter in Atlanta, then vacationed in Sea Island, Ga.) This absence, some speculate, gave the Ramseys time to work out a story to explain their innocence.\n",
      "\t Tokenized GPT2:( The ĠRam se ys Ġburied Ġtheir Ġdaughter Ġin ĠAtlanta , Ġthen Ġvacation ed Ġin ĠSea ĠIsland , ĠGa .) ĠThis Ġabsence , Ġsome Ġspec ulate , Ġgave Ġthe ĠRam se ys Ġtime Ġto Ġwork Ġout Ġa Ġstory Ġto Ġexplain Ġtheir Ġinnocence .\n",
      "\t Tokenized LLAMA3:( The ĠRam se ys Ġburied Ġtheir Ġdaughter Ġin ĠAtlanta , Ġthen Ġvacation ed Ġin ĠSea ĠIsland , ĠGa .) ĠThis Ġabsence , Ġsome Ġspec ulate , Ġgave Ġthe ĠRam se ys Ġtime Ġto Ġwork Ġout Ġa Ġstory Ġto Ġexplain Ġtheir Ġinnocence .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The Ramseys went on vacation to relieve themselves of killing their daughter.\n",
      "\t Tokenized GPT2:The ĠRam se ys Ġwent Ġon Ġvacation Ġto Ġrelieve Ġthemselves Ġof Ġkilling Ġtheir Ġdaughter .\n",
      "\t Tokenized LLAMA3:The ĠRam se ys Ġwent Ġon Ġvacation Ġto Ġrel ieve Ġthemselves Ġof Ġkilling Ġtheir Ġdaughter .\n",
      "\t Unique Tokens GPT2: {'Ġrelieve'}\n",
      "\t Unique Tokens LLAMA3: {'ieve', 'Ġrel'}\n",
      "==entailment==\n",
      "Text 1: The students' reaction was swift and contentious, as if their feelings had been hurt.\n",
      "\t Tokenized GPT2:The Ġstudents ' Ġreaction Ġwas Ġswift Ġand Ġcontent ious , Ġas Ġif Ġtheir Ġfeelings Ġhad Ġbeen Ġhurt .\n",
      "\t Tokenized LLAMA3:The Ġstudents ' Ġreaction Ġwas Ġswift Ġand Ġcontent ious , Ġas Ġif Ġtheir Ġfeelings Ġhad Ġbeen Ġhurt .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The students responded strongly.\n",
      "\t Tokenized GPT2:The Ġstudents Ġresponded Ġstrongly .\n",
      "\t Tokenized LLAMA3:The Ġstudents Ġresponded Ġstrongly .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Changes in technology and its application to electronic commerce and expanding Internet applications will change the specific control activities that may be employed and how they are implemented, but the basic requirements of control will not have changed.\n",
      "\t Tokenized GPT2:Changes Ġin Ġtechnology Ġand Ġits Ġapplication Ġto Ġelectronic Ġcommerce Ġand Ġexpanding ĠInternet Ġapplications Ġwill Ġchange Ġthe Ġspecific Ġcontrol Ġactivities Ġthat Ġmay Ġbe Ġemployed Ġand Ġhow Ġthey Ġare Ġimplemented , Ġbut Ġthe Ġbasic Ġrequirements Ġof Ġcontrol Ġwill Ġnot Ġhave Ġchanged .\n",
      "\t Tokenized LLAMA3:Changes Ġin Ġtechnology Ġand Ġits Ġapplication Ġto Ġelectronic Ġcommerce Ġand Ġexpanding ĠInternet Ġapplications Ġwill Ġchange Ġthe Ġspecific Ġcontrol Ġactivities Ġthat Ġmay Ġbe Ġemployed Ġand Ġhow Ġthey Ġare Ġimplemented , Ġbut Ġthe Ġbasic Ġrequirements Ġof Ġcontrol Ġwill Ġnot Ġhave Ġchanged .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Technology will make it so we have less control of activities. \n",
      "\t Tokenized GPT2:Techn ology Ġwill Ġmake Ġit Ġso Ġwe Ġhave Ġless Ġcontrol Ġof Ġactivities . Ġ\n",
      "\t Tokenized LLAMA3:Te chnology Ġwill Ġmake Ġit Ġso Ġwe Ġhave Ġless Ġcontrol Ġof Ġactivities . Ġ\n",
      "\t Unique Tokens GPT2: {'Techn', 'ology'}\n",
      "\t Unique Tokens LLAMA3: {'Te', 'chnology'}\n",
      "==contradiction==\n",
      "Text 1: The idea that Clinton's approval represents something new and immoral in the country is historically shortsighted.\n",
      "\t Tokenized GPT2:The Ġidea Ġthat ĠClinton 's Ġapproval Ġrepresents Ġsomething Ġnew Ġand Ġimm oral Ġin Ġthe Ġcountry Ġis Ġhistorically Ġshort sight ed .\n",
      "\t Tokenized LLAMA3:The Ġidea Ġthat ĠClinton 's Ġapproval Ġrepresents Ġsomething Ġnew Ġand Ġimm oral Ġin Ġthe Ġcountry Ġis Ġhistorically Ġshorts ighted .\n",
      "\t Unique Tokens GPT2: {'sight', 'ed', 'Ġshort'}\n",
      "\t Unique Tokens LLAMA3: {'Ġshorts', 'ighted'}\n",
      "Text 2: It's accurate to conclude that Clinton's approvals signify the start of a new form of immorality in the country.\n",
      "\t Tokenized GPT2:It 's Ġaccurate Ġto Ġconclude Ġthat ĠClinton 's Ġapproval s Ġsign ify Ġthe Ġstart Ġof Ġa Ġnew Ġform Ġof Ġimm or ality Ġin Ġthe Ġcountry .\n",
      "\t Tokenized LLAMA3:It 's Ġaccurate Ġto Ġconclude Ġthat ĠClinton 's Ġapproval s Ġsign ify Ġthe Ġstart Ġof Ġa Ġnew Ġform Ġof Ġimm or ality Ġin Ġthe Ġcountry .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Catch up on the Indian avant-garde and the bohemian people of Caletta at the Academy of Fine Arts on the southeast corner of the Maidan.\n",
      "\t Tokenized GPT2:C atch Ġup Ġon Ġthe ĠIndian Ġav ant - gard e Ġand Ġthe Ġbo hem ian Ġpeople Ġof ĠCal etta Ġat Ġthe ĠAcademy Ġof ĠFine ĠArts Ġon Ġthe Ġs outheast Ġcorner Ġof Ġthe ĠM aid an .\n",
      "\t Tokenized LLAMA3:C atch Ġup Ġon Ġthe ĠIndian Ġav ant -g ard e Ġand Ġthe Ġbo hem ian Ġpeople Ġof ĠCal etta Ġat Ġthe ĠAcademy Ġof ĠFine ĠArts Ġon Ġthe Ġs outheast Ġcorner Ġof Ġthe ĠM aid an .\n",
      "\t Unique Tokens GPT2: {'-', 'gard'}\n",
      "\t Unique Tokens LLAMA3: {'ard', '-g'}\n",
      "Text 2: The Academy of Fine Arts is located in Northern Maidan.\n",
      "\t Tokenized GPT2:The ĠAcademy Ġof ĠFine ĠArts Ġis Ġlocated Ġin ĠNorthern ĠM aid an .\n",
      "\t Tokenized LLAMA3:The ĠAcademy Ġof ĠFine ĠArts Ġis Ġlocated Ġin ĠNorthern ĠM aid an .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Wagonheim said the program not only will benefit the needy, but also will help improve the public image of lawyers.\n",
      "\t Tokenized GPT2:W agon heim Ġsaid Ġthe Ġprogram Ġnot Ġonly Ġwill Ġbenefit Ġthe Ġneed y , Ġbut Ġalso Ġwill Ġhelp Ġimprove Ġthe Ġpublic Ġimage Ġof Ġlawyers .\n",
      "\t Tokenized LLAMA3:W agon heim Ġsaid Ġthe Ġprogram Ġnot Ġonly Ġwill Ġbenefit Ġthe Ġneed y , Ġbut Ġalso Ġwill Ġhelp Ġimprove Ġthe Ġpublic Ġimage Ġof Ġlawyers .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The program isn't going to improve lawyers' public image.\n",
      "\t Tokenized GPT2:The Ġprogram Ġisn 't Ġgoing Ġto Ġimprove Ġlawyers ' Ġpublic Ġimage .\n",
      "\t Tokenized LLAMA3:The Ġprogram Ġisn 't Ġgoing Ġto Ġimprove Ġlawyers ' Ġpublic Ġimage .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: you know even even into major things just to keep our car longer because i don't think we get the money that we put into them out of them in two years or three years and of course i was never in a position where i could trade my car off every two years\n",
      "\t Tokenized GPT2:you Ġknow Ġeven Ġeven Ġinto Ġmajor Ġthings Ġjust Ġto Ġkeep Ġour Ġcar Ġlonger Ġbecause Ġi Ġdon 't Ġthink Ġwe Ġget Ġthe Ġmoney Ġthat Ġwe Ġput Ġinto Ġthem Ġout Ġof Ġthem Ġin Ġtwo Ġyears Ġor Ġthree Ġyears Ġand Ġof Ġcourse Ġi Ġwas Ġnever Ġin Ġa Ġposition Ġwhere Ġi Ġcould Ġtrade Ġmy Ġcar Ġoff Ġevery Ġtwo Ġyears\n",
      "\t Tokenized LLAMA3:you Ġknow Ġeven Ġeven Ġinto Ġmajor Ġthings Ġjust Ġto Ġkeep Ġour Ġcar Ġlonger Ġbecause Ġi Ġdon 't Ġthink Ġwe Ġget Ġthe Ġmoney Ġthat Ġwe Ġput Ġinto Ġthem Ġout Ġof Ġthem Ġin Ġtwo Ġyears Ġor Ġthree Ġyears Ġand Ġof Ġcourse Ġi Ġwas Ġnever Ġin Ġa Ġposition Ġwhere Ġi Ġcould Ġtrade Ġmy Ġcar Ġoff Ġevery Ġtwo Ġyears\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I find it to be more cost effective to keep and maintain an older car.\n",
      "\t Tokenized GPT2:I Ġfind Ġit Ġto Ġbe Ġmore Ġcost Ġeffective Ġto Ġkeep Ġand Ġmaintain Ġan Ġolder Ġcar .\n",
      "\t Tokenized LLAMA3:I Ġfind Ġit Ġto Ġbe Ġmore Ġcost Ġeffective Ġto Ġkeep Ġand Ġmaintain Ġan Ġolder Ġcar .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: there and they uh they in fact they had this was in uh the late twenties and they in fact used some of the equipment that had been left over and uh he turned them down it it's interesting that that most people don't realize how small the canal is have you ever been there\n",
      "\t Tokenized GPT2:there Ġand Ġthey Ġuh Ġthey Ġin Ġfact Ġthey Ġhad Ġthis Ġwas Ġin Ġuh Ġthe Ġlate Ġtwent ies Ġand Ġthey Ġin Ġfact Ġused Ġsome Ġof Ġthe Ġequipment Ġthat Ġhad Ġbeen Ġleft Ġover Ġand Ġuh Ġhe Ġturned Ġthem Ġdown Ġit Ġit 's Ġinteresting Ġthat Ġthat Ġmost Ġpeople Ġdon 't Ġrealize Ġhow Ġsmall Ġthe Ġcanal Ġis Ġhave Ġyou Ġever Ġbeen Ġthere\n",
      "\t Tokenized LLAMA3:there Ġand Ġthey Ġuh Ġthey Ġin Ġfact Ġthey Ġhad Ġthis Ġwas Ġin Ġuh Ġthe Ġlate Ġtwent ies Ġand Ġthey Ġin Ġfact Ġused Ġsome Ġof Ġthe Ġequipment Ġthat Ġhad Ġbeen Ġleft Ġover Ġand Ġuh Ġhe Ġturned Ġthem Ġdown Ġit Ġit 's Ġinteresting Ġthat Ġthat Ġmost Ġpeople Ġdon 't Ġrealize Ġhow Ġsmall Ġthe Ġcanal Ġis Ġhave Ġyou Ġever Ġbeen Ġthere\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Most people think the canal is tiny \n",
      "\t Tokenized GPT2:Most Ġpeople Ġthink Ġthe Ġcanal Ġis Ġtiny Ġ\n",
      "\t Tokenized LLAMA3:Most Ġpeople Ġthink Ġthe Ġcanal Ġis Ġtiny Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Cirque du Soleil's The latest from the acclaimed international troupe, O dazzles in an aquatic environment that utilizes 1.5 million gallons (6.8 million liters) of water.\n",
      "\t Tokenized GPT2:C ir que Ġdu ĠSo le il 's ĠThe Ġlatest Ġfrom Ġthe Ġac claimed Ġinternational Ġtr ou pe , ĠO Ġd azz les Ġin Ġan Ġaqu atic Ġenvironment Ġthat Ġutil izes Ġ1 . 5 Ġmillion Ġgall ons Ġ( 6 . 8 Ġmillion Ġlit ers ) Ġof Ġwater .\n",
      "\t Tokenized LLAMA3:C ir que Ġdu ĠSo le il 's ĠThe Ġlatest Ġfrom Ġthe Ġac claimed Ġinternational Ġtr ou pe , ĠO Ġd azz les Ġin Ġan Ġaqu atic Ġenvironment Ġthat Ġutil izes Ġ 1 . 5 Ġmillion Ġgall ons Ġ( 6 . 8 Ġmillion Ġlit ers ) Ġof Ġwater .\n",
      "\t Unique Tokens GPT2: {'Ġ1'}\n",
      "\t Unique Tokens LLAMA3: {'1', 'Ġ'}\n",
      "Text 2: Cirque du Soleil is an international troupe that performs a lot in Vegas.\n",
      "\t Tokenized GPT2:C ir que Ġdu ĠSo le il Ġis Ġan Ġinternational Ġtr ou pe Ġthat Ġperforms Ġa Ġlot Ġin ĠVegas .\n",
      "\t Tokenized LLAMA3:C ir que Ġdu ĠSo le il Ġis Ġan Ġinternational Ġtr ou pe Ġthat Ġperforms Ġa Ġlot Ġin ĠVegas .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Ah, triple pig! \n",
      "\t Tokenized GPT2:Ah , Ġtriple Ġpig ! Ġ\n",
      "\t Tokenized LLAMA3:Ah , Ġtriple Ġpig ! Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The pig tripled.\n",
      "\t Tokenized GPT2:The Ġpig Ġtri pled .\n",
      "\t Tokenized LLAMA3:The Ġpig Ġtri pled .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Managing better requires that agencies have, and rely upon, sound financial and program information.\n",
      "\t Tokenized GPT2:Man aging Ġbetter Ġrequires Ġthat Ġagencies Ġhave , Ġand Ġrely Ġupon , Ġsound Ġfinancial Ġand Ġprogram Ġinformation .\n",
      "\t Tokenized LLAMA3:Man aging Ġbetter Ġrequires Ġthat Ġagencies Ġhave , Ġand Ġrely Ġupon , Ġsound Ġfinancial Ġand Ġprogram Ġinformation .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Agencies that rely on information based on unsound financial information will have management problems.\n",
      "\t Tokenized GPT2:Ag encies Ġthat Ġrely Ġon Ġinformation Ġbased Ġon Ġuns ound Ġfinancial Ġinformation Ġwill Ġhave Ġmanagement Ġproblems .\n",
      "\t Tokenized LLAMA3:Ag encies Ġthat Ġrely Ġon Ġinformation Ġbased Ġon Ġuns ound Ġfinancial Ġinformation Ġwill Ġhave Ġmanagement Ġproblems .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: yeah i went to i went to uh Rice and we had the marching owl band which is quite a it's not known for its musical abilities more so its um comedy abilities\n",
      "\t Tokenized GPT2:yeah Ġi Ġwent Ġto Ġi Ġwent Ġto Ġuh ĠRice Ġand Ġwe Ġhad Ġthe Ġmarch ing Ġow l Ġband Ġwhich Ġis Ġquite Ġa Ġit 's Ġnot Ġknown Ġfor Ġits Ġmusical Ġabilities Ġmore Ġso Ġits Ġum Ġcomedy Ġabilities\n",
      "\t Tokenized LLAMA3:yeah Ġi Ġwent Ġto Ġi Ġwent Ġto Ġuh ĠRice Ġand Ġwe Ġhad Ġthe Ġmarch ing Ġow l Ġband Ġwhich Ġis Ġquite Ġa Ġit 's Ġnot Ġknown Ġfor Ġits Ġmusical Ġabilities Ġmore Ġso Ġits Ġum Ġcomedy Ġabilities\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The owl band was well known for its music abilities.\n",
      "\t Tokenized GPT2:The Ġow l Ġband Ġwas Ġwell Ġknown Ġfor Ġits Ġmusic Ġabilities .\n",
      "\t Tokenized LLAMA3:The Ġow l Ġband Ġwas Ġwell Ġknown Ġfor Ġits Ġmusic Ġabilities .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Have her show it,\" said Thorn.\n",
      "\t Tokenized GPT2:Have Ġher Ġshow Ġit ,\" Ġsaid ĠTh orn .\n",
      "\t Tokenized LLAMA3:Have Ġher Ġshow Ġit ,\" Ġsaid ĠTh orn .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Thorn told her to hide it.\n",
      "\t Tokenized GPT2:Th orn Ġtold Ġher Ġto Ġhide Ġit .\n",
      "\t Tokenized LLAMA3:Th orn Ġtold Ġher Ġto Ġhide Ġit .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: They are levied through the power of the Government to compel payment, and the person or entity that pays these fees does not receive anything of value from the Government in exchange.\n",
      "\t Tokenized GPT2:They Ġare Ġle v ied Ġthrough Ġthe Ġpower Ġof Ġthe ĠGovernment Ġto Ġcom pel Ġpayment , Ġand Ġthe Ġperson Ġor Ġentity Ġthat Ġpays Ġthese Ġfees Ġdoes Ġnot Ġreceive Ġanything Ġof Ġvalue Ġfrom Ġthe ĠGovernment Ġin Ġexchange .\n",
      "\t Tokenized LLAMA3:They Ġare Ġle v ied Ġthrough Ġthe Ġpower Ġof Ġthe ĠGovernment Ġto Ġcom pel Ġpayment , Ġand Ġthe Ġperson Ġor Ġentity Ġthat Ġpays Ġthese Ġfees Ġdoes Ġnot Ġreceive Ġanything Ġof Ġvalue Ġfrom Ġthe ĠGovernment Ġin Ġexchange .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: They are levied through the power of the Government to compel payment.\n",
      "\t Tokenized GPT2:They Ġare Ġle v ied Ġthrough Ġthe Ġpower Ġof Ġthe ĠGovernment Ġto Ġcom pel Ġpayment .\n",
      "\t Tokenized LLAMA3:They Ġare Ġle v ied Ġthrough Ġthe Ġpower Ġof Ġthe ĠGovernment Ġto Ġcom pel Ġpayment .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: yeah yeah yeah well because that's the way they they might seem outwardly but boy there's a lots going on in there\n",
      "\t Tokenized GPT2:yeah Ġyeah Ġyeah Ġwell Ġbecause Ġthat 's Ġthe Ġway Ġthey Ġthey Ġmight Ġseem Ġoutward ly Ġbut Ġboy Ġthere 's Ġa Ġlots Ġgoing Ġon Ġin Ġthere\n",
      "\t Tokenized LLAMA3:yeah Ġyeah Ġyeah Ġwell Ġbecause Ġthat 's Ġthe Ġway Ġthey Ġthey Ġmight Ġseem Ġoutward ly Ġbut Ġboy Ġthere 's Ġa Ġlots Ġgoing Ġon Ġin Ġthere\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: There is a lot of bad in the inside.\n",
      "\t Tokenized GPT2:There Ġis Ġa Ġlot Ġof Ġbad Ġin Ġthe Ġinside .\n",
      "\t Tokenized LLAMA3:There Ġis Ġa Ġlot Ġof Ġbad Ġin Ġthe Ġinside .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: not only that but they don't pay the money either\n",
      "\t Tokenized GPT2:not Ġonly Ġthat Ġbut Ġthey Ġdon 't Ġpay Ġthe Ġmoney Ġeither\n",
      "\t Tokenized LLAMA3:not Ġonly Ġthat Ġbut Ġthey Ġdon 't Ġpay Ġthe Ġmoney Ġeither\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: They've paid out their life savings.\n",
      "\t Tokenized GPT2:They 've Ġpaid Ġout Ġtheir Ġlife Ġsavings .\n",
      "\t Tokenized LLAMA3:They 've Ġpaid Ġout Ġtheir Ġlife Ġsavings .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: I'm sure I won't get stuck to it,' Julia remarked about the suitcase she was carrying.\n",
      "\t Tokenized GPT2:I 'm Ġsure ĠI Ġwon 't Ġget Ġstuck Ġto Ġit ,' ĠJulia Ġremarked Ġabout Ġthe Ġsuit case Ġshe Ġwas Ġcarrying .\n",
      "\t Tokenized LLAMA3:I 'm Ġsure ĠI Ġwon 't Ġget Ġstuck Ġto Ġit ,' ĠJulia Ġremarked Ġabout Ġthe Ġsuit case Ġshe Ġwas Ġcarrying .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Julia said that she was sure she would get stuck to the suitcase. \n",
      "\t Tokenized GPT2:Jul ia Ġsaid Ġthat Ġshe Ġwas Ġsure Ġshe Ġwould Ġget Ġstuck Ġto Ġthe Ġsuit case . Ġ\n",
      "\t Tokenized LLAMA3:Jul ia Ġsaid Ġthat Ġshe Ġwas Ġsure Ġshe Ġwould Ġget Ġstuck Ġto Ġthe Ġsuit case . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Each room was outfitted with a leather sofa and three fold-out beds for students exhausted after a full day of hard work.\n",
      "\t Tokenized GPT2:Each Ġroom Ġwas Ġout f itted Ġwith Ġa Ġleather Ġsofa Ġand Ġthree Ġfold - out Ġbeds Ġfor Ġstudents Ġexhausted Ġafter Ġa Ġfull Ġday Ġof Ġhard Ġwork .\n",
      "\t Tokenized LLAMA3:Each Ġroom Ġwas Ġout f itted Ġwith Ġa Ġleather Ġsofa Ġand Ġthree Ġfold -out Ġbeds Ġfor Ġstudents Ġexhausted Ġafter Ġa Ġfull Ġday Ġof Ġhard Ġwork .\n",
      "\t Unique Tokens GPT2: {'-', 'out'}\n",
      "\t Unique Tokens LLAMA3: {'-out'}\n",
      "Text 2: Every student slept in these housings, no matter what.\n",
      "\t Tokenized GPT2:Every Ġstudent Ġslept Ġin Ġthese Ġhous ings , Ġno Ġmatter Ġwhat .\n",
      "\t Tokenized LLAMA3:Every Ġstudent Ġslept Ġin Ġthese Ġhous ings , Ġno Ġmatter Ġwhat .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: She wears either revealing clothes or professional clothes (or perhaps both).\n",
      "\t Tokenized GPT2:She Ġwears Ġeither Ġrevealing Ġclothes Ġor Ġprofessional Ġclothes Ġ( or Ġperhaps Ġboth ).\n",
      "\t Tokenized LLAMA3:She Ġwears Ġeither Ġrevealing Ġclothes Ġor Ġprofessional Ġclothes Ġ( or Ġperhaps Ġboth ).\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: She only wears short skirts.\n",
      "\t Tokenized GPT2:She Ġonly Ġwears Ġshort Ġsk irts .\n",
      "\t Tokenized LLAMA3:She Ġonly Ġwears Ġshort Ġsk irts .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Lalley also is enthused about other bar efforts on behalf of the poor, most notably the Legal Assistance Center will operate out of the new courthouse.\n",
      "\t Tokenized GPT2:L al ley Ġalso Ġis Ġent h used Ġabout Ġother Ġbar Ġefforts Ġon Ġbehalf Ġof Ġthe Ġpoor , Ġmost Ġnotably Ġthe ĠLe gal ĠAss istance ĠCenter Ġwill Ġoperate Ġout Ġof Ġthe Ġnew Ġcour th ouse .\n",
      "\t Tokenized LLAMA3:L al ley Ġalso Ġis Ġent h used Ġabout Ġother Ġbar Ġefforts Ġon Ġbehalf Ġof Ġthe Ġpoor , Ġmost Ġnotably Ġthe ĠLe gal ĠAss istance ĠCenter Ġwill Ġoperate Ġout Ġof Ġthe Ġnew Ġcour th ouse .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The Legal Assistance Center will keep offering its services from its current location.\n",
      "\t Tokenized GPT2:The ĠLe gal ĠAss istance ĠCenter Ġwill Ġkeep Ġoffering Ġits Ġservices Ġfrom Ġits Ġcurrent Ġlocation .\n",
      "\t Tokenized LLAMA3:The ĠLe gal ĠAss istance ĠCenter Ġwill Ġkeep Ġoffering Ġits Ġservices Ġfrom Ġits Ġcurrent Ġlocation .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The center had become a hodgepodge of unconnected programs--a day-care center, a library, a nonviolence training school.\n",
      "\t Tokenized GPT2:The Ġcenter Ġhad Ġbecome Ġa Ġh odge p odge Ġof Ġun connected Ġprograms -- a Ġday - care Ġcenter , Ġa Ġlibrary , Ġa Ġnon viol ence Ġtraining Ġschool .\n",
      "\t Tokenized LLAMA3:The Ġcenter Ġhad Ġbecome Ġa Ġh odge p odge Ġof Ġun connected Ġprograms -- a Ġday -care Ġcenter , Ġa Ġlibrary , Ġa Ġnon viol ence Ġtraining Ġschool .\n",
      "\t Unique Tokens GPT2: {'-', 'care'}\n",
      "\t Unique Tokens LLAMA3: {'-care'}\n",
      "Text 2: The center was lacking a library.\n",
      "\t Tokenized GPT2:The Ġcenter Ġwas Ġlacking Ġa Ġlibrary .\n",
      "\t Tokenized LLAMA3:The Ġcenter Ġwas Ġlacking Ġa Ġlibrary .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Randy's Anecdotal Wrap-Up\n",
      "\t Tokenized GPT2:R andy 's ĠA ne cd otal ĠW rap - Up\n",
      "\t Tokenized LLAMA3:R andy 's ĠA ne cd otal ĠW rap - Up\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Randy's Conclusions\n",
      "\t Tokenized GPT2:R andy 's ĠCon clusions\n",
      "\t Tokenized LLAMA3:R andy 's ĠCon clusions\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Moreover, Las Vegas has recently started to show signs of maturity in its cultural status as well.\n",
      "\t Tokenized GPT2:Moreover , ĠLas ĠVegas Ġhas Ġrecently Ġstarted Ġto Ġshow Ġsigns Ġof Ġmaturity Ġin Ġits Ġcultural Ġstatus Ġas Ġwell .\n",
      "\t Tokenized LLAMA3:More over , ĠLas ĠVegas Ġhas Ġrecently Ġstarted Ġto Ġshow Ġsigns Ġof Ġmaturity Ġin Ġits Ġcultural Ġstatus Ġas Ġwell .\n",
      "\t Unique Tokens GPT2: {'Moreover'}\n",
      "\t Unique Tokens LLAMA3: {'over', 'More'}\n",
      "Text 2: The culture of Las Vegas has a lot of room for improvement.\n",
      "\t Tokenized GPT2:The Ġculture Ġof ĠLas ĠVegas Ġhas Ġa Ġlot Ġof Ġroom Ġfor Ġimprovement .\n",
      "\t Tokenized LLAMA3:The Ġculture Ġof ĠLas ĠVegas Ġhas Ġa Ġlot Ġof Ġroom Ġfor Ġimprovement .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Placido Domingo's appearance on the package, compellingly photographed in costume as the ancient King of Crete, (Anthony Tommasini, the New York Times ) is the main selling point for this new recording of one of Mozart's more obscure operas--a fact that does not make critics happy.\n",
      "\t Tokenized GPT2:Pl ac ido ĠDom ingo 's Ġappearance Ġon Ġthe Ġpackage , Ġcompelling ly Ġphotograp hed Ġin Ġcostume Ġas Ġthe Ġancient ĠKing Ġof ĠCre te , Ġ( An thony ĠTom mas ini , Ġthe ĠNew ĠYork ĠTimes Ġ) Ġis Ġthe Ġmain Ġselling Ġpoint Ġfor Ġthis Ġnew Ġrecording Ġof Ġone Ġof ĠM oz art 's Ġmore Ġobscure Ġoper as -- a Ġfact Ġthat Ġdoes Ġnot Ġmake Ġcritics Ġhappy .\n",
      "\t Tokenized LLAMA3:Pl ac ido ĠDom ingo 's Ġappearance Ġon Ġthe Ġpackage , Ġcompelling ly Ġphotograp hed Ġin Ġcostume Ġas Ġthe Ġancient ĠKing Ġof ĠCre te , Ġ( An thony ĠTom mas ini , Ġthe ĠNew ĠYork ĠTimes Ġ) Ġis Ġthe Ġmain Ġselling Ġpoint Ġfor Ġthis Ġnew Ġrecording Ġof Ġone Ġof ĠM oz art 's Ġmore Ġobscure Ġoper as -- a Ġfact Ġthat Ġdoes Ġnot Ġmake Ġcritics Ġhappy .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Placido Domingo is the reason that people are purchasing the new Mozard opera recordings.\n",
      "\t Tokenized GPT2:Pl ac ido ĠDom ingo Ġis Ġthe Ġreason Ġthat Ġpeople Ġare Ġpurchasing Ġthe Ġnew ĠM oz ard Ġopera Ġrecordings .\n",
      "\t Tokenized LLAMA3:Pl ac ido ĠDom ingo Ġis Ġthe Ġreason Ġthat Ġpeople Ġare Ġpurchasing Ġthe Ġnew ĠM oz ard Ġopera Ġrecordings .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: 'These are human lives.\n",
      "\t Tokenized GPT2:' These Ġare Ġhuman Ġlives .\n",
      "\t Tokenized LLAMA3:'T he se Ġare Ġhuman Ġlives .\n",
      "\t Unique Tokens GPT2: {'These', \"'\"}\n",
      "\t Unique Tokens LLAMA3: {\"'T\", 'se', 'he'}\n",
      "Text 2: These are animal lives \n",
      "\t Tokenized GPT2:These Ġare Ġanimal Ġlives Ġ\n",
      "\t Tokenized LLAMA3:These Ġare Ġanimal Ġlives Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: so do you have do you have the long i guess not not if there's see i was raised in New York but i guess up there you all don't have too long of a growing season do you\n",
      "\t Tokenized GPT2:so Ġdo Ġyou Ġhave Ġdo Ġyou Ġhave Ġthe Ġlong Ġi Ġguess Ġnot Ġnot Ġif Ġthere 's Ġsee Ġi Ġwas Ġraised Ġin ĠNew ĠYork Ġbut Ġi Ġguess Ġup Ġthere Ġyou Ġall Ġdon 't Ġhave Ġtoo Ġlong Ġof Ġa Ġgrowing Ġseason Ġdo Ġyou\n",
      "\t Tokenized LLAMA3:so Ġdo Ġyou Ġhave Ġdo Ġyou Ġhave Ġthe Ġlong Ġi Ġguess Ġnot Ġnot Ġif Ġthere 's Ġsee Ġi Ġwas Ġraised Ġin ĠNew ĠYork Ġbut Ġi Ġguess Ġup Ġthere Ġyou Ġall Ġdon 't Ġhave Ġtoo Ġlong Ġof Ġa Ġgrowing Ġseason Ġdo Ġyou\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I suppose you have a shorter growing season where you are.\n",
      "\t Tokenized GPT2:I Ġsuppose Ġyou Ġhave Ġa Ġshorter Ġgrowing Ġseason Ġwhere Ġyou Ġare .\n",
      "\t Tokenized LLAMA3:I Ġsuppose Ġyou Ġhave Ġa Ġshorter Ġgrowing Ġseason Ġwhere Ġyou Ġare .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: In the 1980s, and as late as 1994, a major Republican theme was a sort of taunting, nyah-nyah populism.\n",
      "\t Tokenized GPT2:In Ġthe Ġ1980 s , Ġand Ġas Ġlate Ġas Ġ1994 , Ġa Ġmajor ĠRepublican Ġtheme Ġwas Ġa Ġsort Ġof Ġta unting , Ġn y ah - ny ah Ġpopul ism .\n",
      "\t Tokenized LLAMA3:In Ġthe Ġ 198 0 s , Ġand Ġas Ġlate Ġas Ġ 199 4 , Ġa Ġmajor ĠRepublican Ġtheme Ġwas Ġa Ġsort Ġof Ġt aun ting , Ġn y ah - ny ah Ġpopul ism .\n",
      "\t Unique Tokens GPT2: {'Ġ1980', 'Ġta', 'unting', 'Ġ1994'}\n",
      "\t Unique Tokens LLAMA3: {'ting', '198', 'aun', 'Ġt', '199', 'Ġ', '0', '4'}\n",
      "Text 2: The Republicans changed their theme after 1995.\n",
      "\t Tokenized GPT2:The ĠRepublicans Ġchanged Ġtheir Ġtheme Ġafter Ġ1995 .\n",
      "\t Tokenized LLAMA3:The ĠRepublicans Ġchanged Ġtheir Ġtheme Ġafter Ġ 199 5 .\n",
      "\t Unique Tokens GPT2: {'Ġ1995'}\n",
      "\t Unique Tokens LLAMA3: {'5', 'Ġ', '199'}\n",
      "==neutral==\n",
      "Text 1: Julius nodded gravely.\n",
      "\t Tokenized GPT2:Jul ius Ġnodded Ġgrave ly .\n",
      "\t Tokenized LLAMA3:Jul ius Ġnodded Ġgrave ly .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Julius loves to ask questions. \n",
      "\t Tokenized GPT2:Jul ius Ġloves Ġto Ġask Ġquestions . Ġ\n",
      "\t Tokenized LLAMA3:Jul ius Ġloves Ġto Ġask Ġquestions . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: The Drawing Room was partially destroyed by fire in 1941, and its furnishings are faithful reproductions; the huge (repaired) Ming punch bowl is striking.\n",
      "\t Tokenized GPT2:The ĠDraw ing ĠRoom Ġwas Ġpartially Ġdestroyed Ġby Ġfire Ġin Ġ19 41 , Ġand Ġits Ġfurn ishing s Ġare Ġfaithful Ġreprodu ctions ; Ġthe Ġhuge Ġ( rep aired ) ĠMing Ġpunch Ġbowl Ġis Ġstriking .\n",
      "\t Tokenized LLAMA3:The ĠDraw ing ĠRoom Ġwas Ġpartially Ġdestroyed Ġby Ġfire Ġin Ġ 194 1 , Ġand Ġits Ġfurn ishing s Ġare Ġfaithful Ġreprodu ctions ; Ġthe Ġhuge Ġ( rep aired ) ĠM ing Ġpunch Ġbowl Ġis Ġstriking .\n",
      "\t Unique Tokens GPT2: {'41', 'ĠMing', 'Ġ19'}\n",
      "\t Unique Tokens LLAMA3: {'ĠM', '194', '1', 'Ġ'}\n",
      "Text 2: Furnishings in the Drawing room are all reproductions.\n",
      "\t Tokenized GPT2:F urn ishing s Ġin Ġthe ĠDraw ing Ġroom Ġare Ġall Ġreprodu ctions .\n",
      "\t Tokenized LLAMA3:F urn ishing s Ġin Ġthe ĠDraw ing Ġroom Ġare Ġall Ġreprodu ctions .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Sainte-Anne itself has a long, broad beach used not only by fishermen in vividly painted boats, but also by families with small children.\n",
      "\t Tokenized GPT2:S ain te - An ne Ġitself Ġhas Ġa Ġlong , Ġbroad Ġbeach Ġused Ġnot Ġonly Ġby Ġfisher men Ġin Ġvivid ly Ġpainted Ġboats , Ġbut Ġalso Ġby Ġfamilies Ġwith Ġsmall Ġchildren .\n",
      "\t Tokenized LLAMA3:S ain te - An ne Ġitself Ġhas Ġa Ġlong , Ġbroad Ġbeach Ġused Ġnot Ġonly Ġby Ġfisher men Ġin Ġvivid ly Ġpainted Ġboats , Ġbut Ġalso Ġby Ġfamilies Ġwith Ġsmall Ġchildren .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Only fishermen and their boats can be seen lining the beaches of Sainte-Anne.\n",
      "\t Tokenized GPT2:Only Ġfisher men Ġand Ġtheir Ġboats Ġcan Ġbe Ġseen Ġlining Ġthe Ġbeaches Ġof ĠS ain te - An ne .\n",
      "\t Tokenized LLAMA3:Only Ġfisher men Ġand Ġtheir Ġboats Ġcan Ġbe Ġseen Ġlining Ġthe Ġbe aches Ġof ĠS ain te - An ne .\n",
      "\t Unique Tokens GPT2: {'Ġbeaches'}\n",
      "\t Unique Tokens LLAMA3: {'aches'}\n",
      "==contradiction==\n",
      "Text 1: probably yeah i would imagine the judge could throw it out\n",
      "\t Tokenized GPT2:probably Ġyeah Ġi Ġwould Ġimagine Ġthe Ġjudge Ġcould Ġthrow Ġit Ġout\n",
      "\t Tokenized LLAMA3:probably Ġyeah Ġi Ġwould Ġimagine Ġthe Ġjudge Ġcould Ġthrow Ġit Ġout\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I cannot believe the judge threw the book at them so fast.\n",
      "\t Tokenized GPT2:I Ġcannot Ġbelieve Ġthe Ġjudge Ġthrew Ġthe Ġbook Ġat Ġthem Ġso Ġfast .\n",
      "\t Tokenized LLAMA3:I Ġcannot Ġbelieve Ġthe Ġjudge Ġthrew Ġthe Ġbook Ġat Ġthem Ġso Ġfast .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Tom is the winner of a year's supply of Turtle Wax, and he will receive his prize just as soon as the Shopping Avenger figures out how much Turtle Wax actually constitutes a year's supply.\n",
      "\t Tokenized GPT2:Tom Ġis Ġthe Ġwinner Ġof Ġa Ġyear 's Ġsupply Ġof ĠT urt le ĠW ax , Ġand Ġhe Ġwill Ġreceive Ġhis Ġprize Ġjust Ġas Ġsoon Ġas Ġthe ĠSh opping ĠAven ger Ġfigures Ġout Ġhow Ġmuch ĠT urt le ĠW ax Ġactually Ġconstitutes Ġa Ġyear 's Ġsupply .\n",
      "\t Tokenized LLAMA3:Tom Ġis Ġthe Ġwinner Ġof Ġa Ġyear 's Ġsupply Ġof ĠT urt le ĠW ax , Ġand Ġhe Ġwill Ġreceive Ġhis Ġprize Ġjust Ġas Ġsoon Ġas Ġthe ĠSh opping ĠAven ger Ġfigures Ġout Ġhow Ġmuch ĠT urt le ĠW ax Ġactually Ġconstitutes Ġa Ġyear 's Ġsupply .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Tom is the winner of this years contest.\n",
      "\t Tokenized GPT2:Tom Ġis Ġthe Ġwinner Ġof Ġthis Ġyears Ġcontest .\n",
      "\t Tokenized LLAMA3:Tom Ġis Ġthe Ġwinner Ġof Ġthis Ġyears Ġcontest .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: The author began with a set of hunches or hypotheses about what can go wrong in agency management, and what would be evidence supporting-or contradicting-these hypotheses.\n",
      "\t Tokenized GPT2:The Ġauthor Ġbegan Ġwith Ġa Ġset Ġof Ġhun ches Ġor Ġhypot heses Ġabout Ġwhat Ġcan Ġgo Ġwrong Ġin Ġagency Ġmanagement , Ġand Ġwhat Ġwould Ġbe Ġevidence Ġsupporting - or Ġcontrad icting - these Ġhypot heses .\n",
      "\t Tokenized LLAMA3:The Ġauthor Ġbegan Ġwith Ġa Ġset Ġof Ġhun ches Ġor Ġhypot heses Ġabout Ġwhat Ġcan Ġgo Ġwrong Ġin Ġagency Ġmanagement , Ġand Ġwhat Ġwould Ġbe Ġevidence Ġsupporting -or Ġcontrad icting -the se Ġhypot heses .\n",
      "\t Unique Tokens GPT2: {'-', 'these', 'or'}\n",
      "\t Unique Tokens LLAMA3: {'se', '-the', '-or'}\n",
      "Text 2: The author had several theories about the ways in which agency management can go awry.\n",
      "\t Tokenized GPT2:The Ġauthor Ġhad Ġseveral Ġtheories Ġabout Ġthe Ġways Ġin Ġwhich Ġagency Ġmanagement Ġcan Ġgo Ġaw ry .\n",
      "\t Tokenized LLAMA3:The Ġauthor Ġhad Ġseveral Ġtheories Ġabout Ġthe Ġways Ġin Ġwhich Ġagency Ġmanagement Ġcan Ġgo Ġaw ry .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: After their savage battles, the warriors recuperated through meditation in the peace of a Zen monastery rock garden.\n",
      "\t Tokenized GPT2:After Ġtheir Ġsavage Ġbattles , Ġthe Ġwarriors Ġrec u per ated Ġthrough Ġmeditation Ġin Ġthe Ġpeace Ġof Ġa ĠZen Ġmon aster y Ġrock Ġgarden .\n",
      "\t Tokenized LLAMA3:After Ġtheir Ġsavage Ġbattles , Ġthe Ġwarriors Ġrec uper ated Ġthrough Ġmeditation Ġin Ġthe Ġpeace Ġof Ġa ĠZen Ġmon aster y Ġrock Ġgarden .\n",
      "\t Unique Tokens GPT2: {'per', 'u'}\n",
      "\t Unique Tokens LLAMA3: {'uper'}\n",
      "Text 2: The warriors recuperated through mediation learned from monks.\n",
      "\t Tokenized GPT2:The Ġwarriors Ġrec u per ated Ġthrough Ġmed iation Ġlearned Ġfrom Ġmon ks .\n",
      "\t Tokenized LLAMA3:The Ġwarriors Ġrec uper ated Ġthrough Ġmed iation Ġlearned Ġfrom Ġmon ks .\n",
      "\t Unique Tokens GPT2: {'per', 'u'}\n",
      "\t Unique Tokens LLAMA3: {'uper'}\n",
      "==neutral==\n",
      "Text 1:  Dinghies are available for hire from the marinas at Tel Aviv, Jaffa, Akko, Netanya, and Nahariya.\n",
      "\t Tokenized GPT2:ĠD ing h ies Ġare Ġavailable Ġfor Ġhire Ġfrom Ġthe Ġmar inas Ġat ĠTel ĠAv iv , ĠJ aff a , ĠAk ko , ĠNet anya , Ġand ĠN ah ari ya .\n",
      "\t Tokenized LLAMA3:ĠD ing h ies Ġare Ġavailable Ġfor Ġhire Ġfrom Ġthe Ġmar inas Ġat ĠTel ĠAv iv , ĠJ aff a , ĠAk ko , ĠNet anya , Ġand ĠN ah ari ya .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Excellent quality dinghies are rent-able for the marinas at Tel Aviv, Jaffa, Akko, Akko, Netanya, and Nahariya.\n",
      "\t Tokenized GPT2:Excellent Ġquality Ġd ing h ies Ġare Ġrent - able Ġfor Ġthe Ġmar inas Ġat ĠTel ĠAv iv , ĠJ aff a , ĠAk ko , ĠAk ko , ĠNet anya , Ġand ĠN ah ari ya .\n",
      "\t Tokenized LLAMA3:Excellent Ġquality Ġd ing h ies Ġare Ġrent - able Ġfor Ġthe Ġmar inas Ġat ĠTel ĠAv iv , ĠJ aff a , ĠAk ko , ĠAk ko , ĠNet anya , Ġand ĠN ah ari ya .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: no not it not no it's a it's not something\n",
      "\t Tokenized GPT2:no Ġnot Ġit Ġnot Ġno Ġit 's Ġa Ġit 's Ġnot Ġsomething\n",
      "\t Tokenized LLAMA3:no Ġnot Ġit Ġnot Ġno Ġit 's Ġa Ġit 's Ġnot Ġsomething\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It is something\n",
      "\t Tokenized GPT2:It Ġis Ġsomething\n",
      "\t Tokenized LLAMA3:It Ġis Ġsomething\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: i think yeah and it's a just a nice escape and you know it's something to laugh at and enjoy\n",
      "\t Tokenized GPT2:i Ġthink Ġyeah Ġand Ġit 's Ġa Ġjust Ġa Ġnice Ġescape Ġand Ġyou Ġknow Ġit 's Ġsomething Ġto Ġlaugh Ġat Ġand Ġenjoy\n",
      "\t Tokenized LLAMA3:i Ġthink Ġyeah Ġand Ġit 's Ġa Ġjust Ġa Ġnice Ġescape Ġand Ġyou Ġknow Ġit 's Ġsomething Ġto Ġlaugh Ġat Ġand Ġenjoy\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It's an escape that is short-lived.\n",
      "\t Tokenized GPT2:It 's Ġan Ġescape Ġthat Ġis Ġshort - lived .\n",
      "\t Tokenized LLAMA3:It 's Ġan Ġescape Ġthat Ġis Ġshort -l ived .\n",
      "\t Unique Tokens GPT2: {'-', 'lived'}\n",
      "\t Unique Tokens LLAMA3: {'ived', '-l'}\n",
      "==entailment==\n",
      "Text 1: some of the professors i think imitate Big Bird\n",
      "\t Tokenized GPT2:some Ġof Ġthe Ġprofessors Ġi Ġthink Ġim itate ĠBig ĠBird\n",
      "\t Tokenized LLAMA3:some Ġof Ġthe Ġprofessors Ġi Ġthink Ġim itate ĠBig ĠBird\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: There are some professors that re an awful lot like Big Bird.\n",
      "\t Tokenized GPT2:There Ġare Ġsome Ġprofessors Ġthat Ġre Ġan Ġawful Ġlot Ġlike ĠBig ĠBird .\n",
      "\t Tokenized LLAMA3:There Ġare Ġsome Ġprofessors Ġthat Ġre Ġan Ġawful Ġlot Ġlike ĠBig ĠBird .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: He jumped up, planting one hand on the charging horse, and came at the brute with the axe.\n",
      "\t Tokenized GPT2:He Ġjumped Ġup , Ġplanting Ġone Ġhand Ġon Ġthe Ġcharging Ġhorse , Ġand Ġcame Ġat Ġthe Ġbrute Ġwith Ġthe Ġaxe .\n",
      "\t Tokenized LLAMA3:He Ġjumped Ġup , Ġplanting Ġone Ġhand Ġon Ġthe Ġcharging Ġhorse , Ġand Ġcame Ġat Ġthe Ġbr ute Ġwith Ġthe Ġaxe .\n",
      "\t Unique Tokens GPT2: {'Ġbrute'}\n",
      "\t Unique Tokens LLAMA3: {'Ġbr', 'ute'}\n",
      "Text 2: He swung at the brute with his sword.\n",
      "\t Tokenized GPT2:He Ġswung Ġat Ġthe Ġbrute Ġwith Ġhis Ġsword .\n",
      "\t Tokenized LLAMA3:He Ġswung Ġat Ġthe Ġbr ute Ġwith Ġhis Ġsword .\n",
      "\t Unique Tokens GPT2: {'Ġbrute'}\n",
      "\t Unique Tokens LLAMA3: {'Ġbr', 'ute'}\n",
      "==neutral==\n",
      "Text 1: The Saver-Spender Theory of Fiscal Policy, Working Paper 7571.\n",
      "\t Tokenized GPT2:The ĠS aver - Spe nder ĠTheory Ġof ĠF iscal ĠPolicy , ĠWorking ĠPaper Ġ7 57 1 .\n",
      "\t Tokenized LLAMA3:The ĠS aver -S pe nder ĠTheory Ġof ĠF iscal ĠPolicy , ĠWorking ĠPaper Ġ 75 7 1 .\n",
      "\t Unique Tokens GPT2: {'-', 'Ġ7', 'Spe', '57'}\n",
      "\t Unique Tokens LLAMA3: {'pe', 'Ġ', '-S', '7', '75'}\n",
      "Text 2: The paper was peer-reviewed.\n",
      "\t Tokenized GPT2:The Ġpaper Ġwas Ġpeer - review ed .\n",
      "\t Tokenized LLAMA3:The Ġpaper Ġwas Ġpeer -re view ed .\n",
      "\t Unique Tokens GPT2: {'-', 'review'}\n",
      "\t Unique Tokens LLAMA3: {'view', '-re'}\n",
      "==contradiction==\n",
      "Text 1: The park was established in 1935 and was given Corbett's name after India became independent.\n",
      "\t Tokenized GPT2:The Ġpark Ġwas Ġestablished Ġin Ġ19 35 Ġand Ġwas Ġgiven ĠCor bet t 's Ġname Ġafter ĠIndia Ġbecame Ġindependent .\n",
      "\t Tokenized LLAMA3:The Ġpark Ġwas Ġestablished Ġin Ġ 193 5 Ġand Ġwas Ġgiven ĠCor bet t 's Ġname Ġafter ĠIndia Ġbecame Ġindependent .\n",
      "\t Unique Tokens GPT2: {'35', 'Ġ19'}\n",
      "\t Unique Tokens LLAMA3: {'5', '193', 'Ġ'}\n",
      "Text 2: The name of the park has always been Corbett.\n",
      "\t Tokenized GPT2:The Ġname Ġof Ġthe Ġpark Ġhas Ġalways Ġbeen ĠCor bet t .\n",
      "\t Tokenized LLAMA3:The Ġname Ġof Ġthe Ġpark Ġhas Ġalways Ġbeen ĠCor bet t .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: It is worth a visit, if only to see the theater itself.\n",
      "\t Tokenized GPT2:It Ġis Ġworth Ġa Ġvisit , Ġif Ġonly Ġto Ġsee Ġthe Ġtheater Ġitself .\n",
      "\t Tokenized LLAMA3:It Ġis Ġworth Ġa Ġvisit , Ġif Ġonly Ġto Ġsee Ġthe Ġtheater Ġitself .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The theater is on your left when you first walk inside.\n",
      "\t Tokenized GPT2:The Ġtheater Ġis Ġon Ġyour Ġleft Ġwhen Ġyou Ġfirst Ġwalk Ġinside .\n",
      "\t Tokenized LLAMA3:The Ġtheater Ġis Ġon Ġyour Ġleft Ġwhen Ġyou Ġfirst Ġwalk Ġinside .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: 2.5 Financial audits are performed under the American Institute of Certified Public Accountants' (AICPA) generally accepted auditing standards for field work and reporting, as well as the related AICPA Statements on Auditing Standards (SASs) which interpret the standards and provide guidance on cond\n",
      "\t Tokenized GPT2:2 . 5 ĠFinancial Ġaud its Ġare Ġperformed Ġunder Ġthe ĠAmerican ĠInstitute Ġof ĠCert ified ĠPublic ĠAccount ants ' Ġ( A IC PA ) Ġgenerally Ġaccepted Ġaud iting Ġstandards Ġfor Ġfield Ġwork Ġand Ġreporting , Ġas Ġwell Ġas Ġthe Ġrelated ĠA IC PA ĠState ments Ġon ĠAud iting ĠStand ards Ġ( S AS s ) Ġwhich Ġinterpret Ġthe Ġstandards Ġand Ġprovide Ġguidance Ġon Ġcond\n",
      "\t Tokenized LLAMA3:2 . 5 ĠFinancial Ġaud its Ġare Ġperformed Ġunder Ġthe ĠAmerican ĠInstitute Ġof ĠCert ified ĠPublic ĠAccount ants ' Ġ( A IC PA ) Ġgenerally Ġaccepted Ġaud iting Ġstandards Ġfor Ġfield Ġwork Ġand Ġreporting , Ġas Ġwell Ġas Ġthe Ġrelated ĠA IC PA ĠState ments Ġon ĠAud iting ĠStand ards Ġ( S AS s ) Ġwhich Ġinterpret Ġthe Ġstandards Ġand Ġprovide Ġguidance Ġon Ġcond\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The AICPA makes financial audits with generally accepted standards.\n",
      "\t Tokenized GPT2:The ĠA IC PA Ġmakes Ġfinancial Ġaud its Ġwith Ġgenerally Ġaccepted Ġstandards .\n",
      "\t Tokenized LLAMA3:The ĠA IC PA Ġmakes Ġfinancial Ġaud its Ġwith Ġgenerally Ġaccepted Ġstandards .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Scutari is traditionally associated with the name of Florence Nightingale.\n",
      "\t Tokenized GPT2:Sc ut ari Ġis Ġtraditionally Ġassociated Ġwith Ġthe Ġname Ġof ĠFlore nce ĠNight ing ale .\n",
      "\t Tokenized LLAMA3:Sc ut ari Ġis Ġtraditionally Ġassociated Ġwith Ġthe Ġname Ġof ĠFlore nce ĠNight ing ale .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Scutari was linked with Florence Nightingale posthumously.\n",
      "\t Tokenized GPT2:Sc ut ari Ġwas Ġlinked Ġwith ĠFlore nce ĠNight ing ale Ġpost hum ously .\n",
      "\t Tokenized LLAMA3:Sc ut ari Ġwas Ġlinked Ġwith ĠFlore nce ĠNight ing ale Ġpost hum ously .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: right right they left a woman and a child or the cat the sheep yeah\n",
      "\t Tokenized GPT2:right Ġright Ġthey Ġleft Ġa Ġwoman Ġand Ġa Ġchild Ġor Ġthe Ġcat Ġthe Ġsheep Ġyeah\n",
      "\t Tokenized LLAMA3:right Ġright Ġthey Ġleft Ġa Ġwoman Ġand Ġa Ġchild Ġor Ġthe Ġcat Ġthe Ġsheep Ġyeah\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: They let a woman and child remain, or it might have been the cat or sheep.\n",
      "\t Tokenized GPT2:They Ġlet Ġa Ġwoman Ġand Ġchild Ġremain , Ġor Ġit Ġmight Ġhave Ġbeen Ġthe Ġcat Ġor Ġsheep .\n",
      "\t Tokenized LLAMA3:They Ġlet Ġa Ġwoman Ġand Ġchild Ġremain , Ġor Ġit Ġmight Ġhave Ġbeen Ġthe Ġcat Ġor Ġsheep .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: In kampung workshops you can watch fantastic birds and butterflies being made of paper (and increasingly, nowadays, of plastic, too) drawn over strong, flexible bamboo frames.\n",
      "\t Tokenized GPT2:In Ġk amp ung Ġworkshops Ġyou Ġcan Ġwatch Ġfantastic Ġbirds Ġand Ġbutter flies Ġbeing Ġmade Ġof Ġpaper Ġ( and Ġincreasingly , Ġnowadays , Ġof Ġplastic , Ġtoo ) Ġdrawn Ġover Ġstrong , Ġflexible Ġb amb oo Ġframes .\n",
      "\t Tokenized LLAMA3:In Ġk amp ung Ġworkshops Ġyou Ġcan Ġwatch Ġfantastic Ġbirds Ġand Ġbutter flies Ġbeing Ġmade Ġof Ġpaper Ġ( and Ġincreasingly , Ġnowadays , Ġof Ġplastic , Ġtoo ) Ġdrawn Ġover Ġstrong , Ġflexible Ġb amb oo Ġframes .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Birds and butterflies can only been seen in the kampung workshops.\n",
      "\t Tokenized GPT2:B ird s Ġand Ġbutter flies Ġcan Ġonly Ġbeen Ġseen Ġin Ġthe Ġk amp ung Ġworkshops .\n",
      "\t Tokenized LLAMA3:B ird s Ġand Ġbutter flies Ġcan Ġonly Ġbeen Ġseen Ġin Ġthe Ġk amp ung Ġworkshops .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: I understand,\" continued the Coroner deliberately, \"that you were sitting reading on the bench just outside the long window of the boudoir. \n",
      "\t Tokenized GPT2:I Ġunderstand ,\" Ġcontinued Ġthe ĠCor oner Ġdeliberately , Ġ\" that Ġyou Ġwere Ġsitting Ġreading Ġon Ġthe Ġbench Ġjust Ġoutside Ġthe Ġlong Ġwindow Ġof Ġthe Ġb oud oir . Ġ\n",
      "\t Tokenized LLAMA3:I Ġunderstand ,\" Ġcontinued Ġthe ĠCor oner Ġdeliberately , Ġ\" that Ġyou Ġwere Ġsitting Ġreading Ġon Ġthe Ġbench Ġjust Ġoutside Ġthe Ġlong Ġwindow Ġof Ġthe Ġb oud oir . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: \"I understand that you were reading inside the boudoir.\", continued the Coroner.\n",
      "\t Tokenized GPT2:\" I Ġunderstand Ġthat Ġyou Ġwere Ġreading Ġinside Ġthe Ġb oud oir .\", Ġcontinued Ġthe ĠCor oner .\n",
      "\t Tokenized LLAMA3:\"I Ġunderstand Ġthat Ġyou Ġwere Ġreading Ġinside Ġthe Ġb oud oir .\", Ġcontinued Ġthe ĠCor oner .\n",
      "\t Unique Tokens GPT2: {'I', '\"'}\n",
      "\t Unique Tokens LLAMA3: {'\"I'}\n",
      "==entailment==\n",
      "Text 1: He pulled his cloak tighter and wished for a moment that he had not shaved his head.\n",
      "\t Tokenized GPT2:He Ġpulled Ġhis Ġcloak Ġtighter Ġand Ġwished Ġfor Ġa Ġmoment Ġthat Ġhe Ġhad Ġnot Ġsh aved Ġhis Ġhead .\n",
      "\t Tokenized LLAMA3:He Ġpulled Ġhis Ġcloak Ġtighter Ġand Ġwished Ġfor Ġa Ġmoment Ġthat Ġhe Ġhad Ġnot Ġsh aved Ġhis Ġhead .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The man  wrapped himself tightly in his cloak and was distressed about not having hair. \n",
      "\t Tokenized GPT2:The Ġman Ġ Ġwrapped Ġhimself Ġtightly Ġin Ġhis Ġcloak Ġand Ġwas Ġdist ressed Ġabout Ġnot Ġhaving Ġhair . Ġ\n",
      "\t Tokenized LLAMA3:The Ġman Ġ Ġwrapped Ġhimself Ġtightly Ġin Ġhis Ġcloak Ġand Ġwas Ġdist ressed Ġabout Ġnot Ġhaving Ġhair . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: There was no longer any  when you wanted some unbridled adult fun, Las Vegas was the place to be.\n",
      "\t Tokenized GPT2:There Ġwas Ġno Ġlonger Ġany Ġ Ġwhen Ġyou Ġwanted Ġsome Ġun brid led Ġadult Ġfun , ĠLas ĠVegas Ġwas Ġthe Ġplace Ġto Ġbe .\n",
      "\t Tokenized LLAMA3:There Ġwas Ġno Ġlonger Ġany Ġ Ġwhen Ġyou Ġwanted Ġsome Ġun brid led Ġadult Ġfun , ĠLas ĠVegas Ġwas Ġthe Ġplace Ġto Ġbe .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Las Vegas was the destination for pure fun for adults.\n",
      "\t Tokenized GPT2:L as ĠVegas Ġwas Ġthe Ġdestination Ġfor Ġpure Ġfun Ġfor Ġadults .\n",
      "\t Tokenized LLAMA3:L as ĠVegas Ġwas Ġthe Ġdestination Ġfor Ġpure Ġfun Ġfor Ġadults .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Jamaican music ska and, especially, reggae has since the 1970s been exported and enjoyed around the world.\n",
      "\t Tokenized GPT2:J ama ican Ġmusic Ġsk a Ġand , Ġespecially , Ġreg g ae Ġhas Ġsince Ġthe Ġ1970 s Ġbeen Ġexported Ġand Ġenjoyed Ġaround Ġthe Ġworld .\n",
      "\t Tokenized LLAMA3:J ama ican Ġmusic Ġsk a Ġand , Ġespecially , Ġreg g ae Ġhas Ġsince Ġthe Ġ 197 0 s Ġbeen Ġexported Ġand Ġenjoyed Ġaround Ġthe Ġworld .\n",
      "\t Unique Tokens GPT2: {'Ġ1970'}\n",
      "\t Unique Tokens LLAMA3: {'0', '197', 'Ġ'}\n",
      "Text 2: Reggae is one of the American music style.\n",
      "\t Tokenized GPT2:Re gg ae Ġis Ġone Ġof Ġthe ĠAmerican Ġmusic Ġstyle .\n",
      "\t Tokenized LLAMA3:Re gg ae Ġis Ġone Ġof Ġthe ĠAmerican Ġmusic Ġstyle .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The following are examples of how teams were used in the agency initiatives we reviewed.\n",
      "\t Tokenized GPT2:The Ġfollowing Ġare Ġexamples Ġof Ġhow Ġteams Ġwere Ġused Ġin Ġthe Ġagency Ġinitiatives Ġwe Ġreviewed .\n",
      "\t Tokenized LLAMA3:The Ġfollowing Ġare Ġexamples Ġof Ġhow Ġteams Ġwere Ġused Ġin Ġthe Ġagency Ġinitiatives Ġwe Ġreviewed .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: We reviewed how sales teams were used in the initiatives.\n",
      "\t Tokenized GPT2:We Ġreviewed Ġhow Ġsales Ġteams Ġwere Ġused Ġin Ġthe Ġinitiatives .\n",
      "\t Tokenized LLAMA3:We Ġreviewed Ġhow Ġsales Ġteams Ġwere Ġused Ġin Ġthe Ġinitiatives .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: oh really it wouldn't matter if we plant them when it was starting to get warmer\n",
      "\t Tokenized GPT2:oh Ġreally Ġit Ġwouldn 't Ġmatter Ġif Ġwe Ġplant Ġthem Ġwhen Ġit Ġwas Ġstarting Ġto Ġget Ġwarmer\n",
      "\t Tokenized LLAMA3:oh Ġreally Ġit Ġwouldn 't Ġmatter Ġif Ġwe Ġplant Ġthem Ġwhen Ġit Ġwas Ġstarting Ġto Ġget Ġwarmer\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The plants are strictly seasonal, only grown during the winter.\n",
      "\t Tokenized GPT2:The Ġplants Ġare Ġstrictly Ġseasonal , Ġonly Ġgrown Ġduring Ġthe Ġwinter .\n",
      "\t Tokenized LLAMA3:The Ġplants Ġare Ġstrictly Ġseasonal , Ġonly Ġgrown Ġduring Ġthe Ġwinter .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: When a GAGAS attestation engagement is the basis for an auditor's subsequent report under the AICPA standards, it would be advantageous to users of the subsequent report for the auditor's report to include the information on compliance with laws and regulations and internal control that is required \n",
      "\t Tokenized GPT2:When Ġa ĠG AG AS Ġatt est ation Ġengagement Ġis Ġthe Ġbasis Ġfor Ġan Ġaud itor 's Ġsubsequent Ġreport Ġunder Ġthe ĠA IC PA Ġstandards , Ġit Ġwould Ġbe Ġadvantage ous Ġto Ġusers Ġof Ġthe Ġsubsequent Ġreport Ġfor Ġthe Ġaud itor 's Ġreport Ġto Ġinclude Ġthe Ġinformation Ġon Ġcompliance Ġwith Ġlaws Ġand Ġregulations Ġand Ġinternal Ġcontrol Ġthat Ġis Ġrequired Ġ\n",
      "\t Tokenized LLAMA3:When Ġa ĠG AG AS Ġatt est ation Ġengagement Ġis Ġthe Ġbasis Ġfor Ġan Ġaud itor 's Ġsubsequent Ġreport Ġunder Ġthe ĠA IC PA Ġstandards , Ġit Ġwould Ġbe Ġadvantage ous Ġto Ġusers Ġof Ġthe Ġsubsequent Ġreport Ġfor Ġthe Ġaud itor 's Ġreport Ġto Ġinclude Ġthe Ġinformation Ġon Ġcompliance Ġwith Ġlaws Ġand Ġregulations Ġand Ġinternal Ġcontrol Ġthat Ġis Ġrequired Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The report is required by GAGAS but not AICPA.\n",
      "\t Tokenized GPT2:The Ġreport Ġis Ġrequired Ġby ĠG AG AS Ġbut Ġnot ĠA IC PA .\n",
      "\t Tokenized LLAMA3:The Ġreport Ġis Ġrequired Ġby ĠG AG AS Ġbut Ġnot ĠA IC PA .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1:  said San'doro.\n",
      "\t Tokenized GPT2:Ġsaid ĠSan 'd oro .\n",
      "\t Tokenized LLAMA3:Ġsaid ĠSan 'd oro .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: San'doro whispered. \n",
      "\t Tokenized GPT2:San 'd oro Ġwhispered . Ġ\n",
      "\t Tokenized LLAMA3:San 'd oro Ġwhispered . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Slate could have put someone with a reasonable grasp of elementary finance and a balanced viewpoint in charge of writing a tax piece.\n",
      "\t Tokenized GPT2:Sl ate Ġcould Ġhave Ġput Ġsomeone Ġwith Ġa Ġreasonable Ġgrasp Ġof Ġelementary Ġfinance Ġand Ġa Ġbalanced Ġviewpoint Ġin Ġcharge Ġof Ġwriting Ġa Ġtax Ġpiece .\n",
      "\t Tokenized LLAMA3:Sl ate Ġcould Ġhave Ġput Ġsomeone Ġwith Ġa Ġreasonable Ġgrasp Ġof Ġelementary Ġfinance Ġand Ġa Ġbalanced Ġviewpoint Ġin Ġcharge Ġof Ġwriting Ġa Ġtax Ġpiece .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Slate has incompetent people write their tax pieces.\n",
      "\t Tokenized GPT2:Sl ate Ġhas Ġincompetent Ġpeople Ġwrite Ġtheir Ġtax Ġpieces .\n",
      "\t Tokenized LLAMA3:Sl ate Ġhas Ġincompet ent Ġpeople Ġwrite Ġtheir Ġtax Ġpieces .\n",
      "\t Unique Tokens GPT2: {'Ġincompetent'}\n",
      "\t Unique Tokens LLAMA3: {'Ġincompet', 'ent'}\n",
      "==contradiction==\n",
      "Text 1: Act Accounting the Great Management Reform Act\n",
      "\t Tokenized GPT2:Act ĠAccount ing Ġthe ĠGreat ĠManagement ĠRe form ĠAct\n",
      "\t Tokenized LLAMA3:Act ĠAccount ing Ġthe ĠGreat ĠManagement ĠRe form ĠAct\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Accounting bad management \n",
      "\t Tokenized GPT2:Account ing Ġbad Ġmanagement Ġ\n",
      "\t Tokenized LLAMA3:Account ing Ġbad Ġmanagement Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: medical and surgical expense coverage.\n",
      "\t Tokenized GPT2:med ical Ġand Ġsurgical Ġexpense Ġcoverage .\n",
      "\t Tokenized LLAMA3:med ical Ġand Ġsurgical Ġexpense Ġcoverage .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Medical and surgical expenses were fully covered\n",
      "\t Tokenized GPT2:Medical Ġand Ġsurgical Ġexpenses Ġwere Ġfully Ġcovered\n",
      "\t Tokenized LLAMA3:Med ical Ġand Ġsurgical Ġexpenses Ġwere Ġfully Ġcovered\n",
      "\t Unique Tokens GPT2: {'Medical'}\n",
      "\t Unique Tokens LLAMA3: {'ical', 'Med'}\n",
      "==entailment==\n",
      "Text 1: Maybe in that sense, the behavior of the Pippens and Iversons of the world is defensible.\n",
      "\t Tokenized GPT2:Maybe Ġin Ġthat Ġsense , Ġthe Ġbehavior Ġof Ġthe ĠP ipp ens Ġand ĠI vers ons Ġof Ġthe Ġworld Ġis Ġdef ens ible .\n",
      "\t Tokenized LLAMA3:Maybe Ġin Ġthat Ġsense , Ġthe Ġbehavior Ġof Ġthe ĠP ipp ens Ġand ĠI vers ons Ġof Ġthe Ġworld Ġis Ġdef ens ible .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: In one sense their antics are justifiable.\n",
      "\t Tokenized GPT2:In Ġone Ġsense Ġtheir Ġant ics Ġare Ġjust ifiable .\n",
      "\t Tokenized LLAMA3:In Ġone Ġsense Ġtheir Ġant ics Ġare Ġjust ifiable .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: and uh really they're about it they've got a guy named Herb Williams that that i guess sort of was supposed to take the place of uh Tarpley but he uh he just doesn't have the offensive skills\n",
      "\t Tokenized GPT2:and Ġuh Ġreally Ġthey 're Ġabout Ġit Ġthey 've Ġgot Ġa Ġguy Ġnamed ĠHer b ĠWilliams Ġthat Ġthat Ġi Ġguess Ġsort Ġof Ġwas Ġsupposed Ġto Ġtake Ġthe Ġplace Ġof Ġuh ĠTar ple y Ġbut Ġhe Ġuh Ġhe Ġjust Ġdoesn 't Ġhave Ġthe Ġoffensive Ġskills\n",
      "\t Tokenized LLAMA3:and Ġuh Ġreally Ġthey 're Ġabout Ġit Ġthey 've Ġgot Ġa Ġguy Ġnamed ĠHer b ĠWilliams Ġthat Ġthat Ġi Ġguess Ġsort Ġof Ġwas Ġsupposed Ġto Ġtake Ġthe Ġplace Ġof Ġuh ĠTar ple y Ġbut Ġhe Ġuh Ġhe Ġjust Ġdoesn 't Ġhave Ġthe Ġoffensive Ġskills\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Herb Williams and Tarpley are on par in terms of skills.\n",
      "\t Tokenized GPT2:Her b ĠWilliams Ġand ĠTar ple y Ġare Ġon Ġpar Ġin Ġterms Ġof Ġskills .\n",
      "\t Tokenized LLAMA3:Her b ĠWilliams Ġand ĠTar ple y Ġare Ġon Ġpar Ġin Ġterms Ġof Ġskills .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: because we don't always read the newspaper sometimes it just sits around for a while and then we just chuck it\n",
      "\t Tokenized GPT2:because Ġwe Ġdon 't Ġalways Ġread Ġthe Ġnewspaper Ġsometimes Ġit Ġjust Ġsits Ġaround Ġfor Ġa Ġwhile Ġand Ġthen Ġwe Ġjust Ġchuck Ġit\n",
      "\t Tokenized LLAMA3:because Ġwe Ġdon 't Ġalways Ġread Ġthe Ġnewspaper Ġsometimes Ġit Ġjust Ġsits Ġaround Ġfor Ġa Ġwhile Ġand Ġthen Ġwe Ġjust Ġchuck Ġit\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: We save all of our old newspapers whether we read them or not.\n",
      "\t Tokenized GPT2:We Ġsave Ġall Ġof Ġour Ġold Ġnewspapers Ġwhether Ġwe Ġread Ġthem Ġor Ġnot .\n",
      "\t Tokenized LLAMA3:We Ġsave Ġall Ġof Ġour Ġold Ġnewspapers Ġwhether Ġwe Ġread Ġthem Ġor Ġnot .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1:  He found himself thinking in circles of worry and pulled himself back to his problem.\n",
      "\t Tokenized GPT2:ĠHe Ġfound Ġhimself Ġthinking Ġin Ġcircles Ġof Ġworry Ġand Ġpulled Ġhimself Ġback Ġto Ġhis Ġproblem .\n",
      "\t Tokenized LLAMA3:ĠHe Ġfound Ġhimself Ġthinking Ġin Ġcircles Ġof Ġworry Ġand Ġpulled Ġhimself Ġback Ġto Ġhis Ġproblem .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He got lost in loops of worry, but snapped himself back to his problem.\n",
      "\t Tokenized GPT2:He Ġgot Ġlost Ġin Ġloops Ġof Ġworry , Ġbut Ġsnapped Ġhimself Ġback Ġto Ġhis Ġproblem .\n",
      "\t Tokenized LLAMA3:He Ġgot Ġlost Ġin Ġloops Ġof Ġworry , Ġbut Ġsnapped Ġhimself Ġback Ġto Ġhis Ġproblem .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Meanwhile, critics on the left argue that because the United States failed to intervene in Rwanda, its intervention in Kosovo is morally suspect and probably racist.\n",
      "\t Tokenized GPT2:Meanwhile , Ġcritics Ġon Ġthe Ġleft Ġargue Ġthat Ġbecause Ġthe ĠUnited ĠStates Ġfailed Ġto Ġinterven e Ġin ĠR w anda , Ġits Ġintervention Ġin ĠKos ovo Ġis Ġmorally Ġsuspect Ġand Ġprobably Ġracist .\n",
      "\t Tokenized LLAMA3:Meanwhile , Ġcritics Ġon Ġthe Ġleft Ġargue Ġthat Ġbecause Ġthe ĠUnited ĠStates Ġfailed Ġto Ġinterven e Ġin ĠR w anda , Ġits Ġintervention Ġin ĠK os ovo Ġis Ġmorally Ġsuspect Ġand Ġprobably Ġracist .\n",
      "\t Unique Tokens GPT2: {'ĠKos'}\n",
      "\t Unique Tokens LLAMA3: {'ĠK', 'os'}\n",
      "Text 2: The US did not intervene in the Rwandan conflict.\n",
      "\t Tokenized GPT2:The ĠUS Ġdid Ġnot Ġinterven e Ġin Ġthe ĠR w and an Ġconflict .\n",
      "\t Tokenized LLAMA3:The ĠUS Ġdid Ġnot Ġinterven e Ġin Ġthe ĠR w and an Ġconflict .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The last 12 years of his life are a blank.\n",
      "\t Tokenized GPT2:The Ġlast Ġ12 Ġyears Ġof Ġhis Ġlife Ġare Ġa Ġblank .\n",
      "\t Tokenized LLAMA3:The Ġlast Ġ 12 Ġyears Ġof Ġhis Ġlife Ġare Ġa Ġblank .\n",
      "\t Unique Tokens GPT2: {'Ġ12'}\n",
      "\t Unique Tokens LLAMA3: {'Ġ', '12'}\n",
      "Text 2: He recalls every moment of the last 12 years in excruciating detail\n",
      "\t Tokenized GPT2:He Ġrecall s Ġevery Ġmoment Ġof Ġthe Ġlast Ġ12 Ġyears Ġin Ġexc ru ci ating Ġdetail\n",
      "\t Tokenized LLAMA3:He Ġrecall s Ġevery Ġmoment Ġof Ġthe Ġlast Ġ 12 Ġyears Ġin Ġexc ru ci ating Ġdetail\n",
      "\t Unique Tokens GPT2: {'Ġ12'}\n",
      "\t Unique Tokens LLAMA3: {'Ġ', '12'}\n",
      "==neutral==\n",
      "Text 1: Two, most other productive operations are easier to study and understand, since few firms have 40,000 locations and a large proportion of their workforce working outdoors.\n",
      "\t Tokenized GPT2:Two , Ġmost Ġother Ġproductive Ġoperations Ġare Ġeasier Ġto Ġstudy Ġand Ġunderstand , Ġsince Ġfew Ġfirms Ġhave Ġ40 , 000 Ġlocations Ġand Ġa Ġlarge Ġproportion Ġof Ġtheir Ġworkforce Ġworking Ġoutdoors .\n",
      "\t Tokenized LLAMA3:Two , Ġmost Ġother Ġproductive Ġoperations Ġare Ġeasier Ġto Ġstudy Ġand Ġunderstand , Ġsince Ġfew Ġfirms Ġhave Ġ 40 , 000 Ġlocations Ġand Ġa Ġlarge Ġproportion Ġof Ġtheir Ġworkforce Ġworking Ġoutdoors .\n",
      "\t Unique Tokens GPT2: {'Ġ40'}\n",
      "\t Unique Tokens LLAMA3: {'40', 'Ġ'}\n",
      "Text 2: The productivity of the operations is directly related to the workforce that's based outdoors.\n",
      "\t Tokenized GPT2:The Ġproductivity Ġof Ġthe Ġoperations Ġis Ġdirectly Ġrelated Ġto Ġthe Ġworkforce Ġthat 's Ġbased Ġoutdoors .\n",
      "\t Tokenized LLAMA3:The Ġproductivity Ġof Ġthe Ġoperations Ġis Ġdirectly Ġrelated Ġto Ġthe Ġworkforce Ġthat 's Ġbased Ġoutdoors .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: right and that was back in nineteen fifty nine\n",
      "\t Tokenized GPT2:right Ġand Ġthat Ġwas Ġback Ġin Ġninet een Ġfifty Ġnine\n",
      "\t Tokenized LLAMA3:right Ġand Ġthat Ġwas Ġback Ġin Ġninet een Ġfifty Ġnine\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It was in the month of August.\n",
      "\t Tokenized GPT2:It Ġwas Ġin Ġthe Ġmonth Ġof ĠAugust .\n",
      "\t Tokenized LLAMA3:It Ġwas Ġin Ġthe Ġmonth Ġof ĠAugust .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Good sir, Jon began.\n",
      "\t Tokenized GPT2:Good Ġsir , ĠJon Ġbegan .\n",
      "\t Tokenized LLAMA3:Good Ġsir , ĠJon Ġbegan .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Jon addressed the king.\n",
      "\t Tokenized GPT2:Jon Ġaddressed Ġthe Ġking .\n",
      "\t Tokenized LLAMA3:Jon Ġaddressed Ġthe Ġking .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: From here it's all through the charming hillside village of Saint-Claude, with its upper-income homes, and on toward the summit or as far as the gendarmes are allowing traffic to proceed that day.\n",
      "\t Tokenized GPT2:From Ġhere Ġit 's Ġall Ġthrough Ġthe Ġcharming Ġhill side Ġvillage Ġof ĠSaint - Cl a ude , Ġwith Ġits Ġupper - income Ġhomes , Ġand Ġon Ġtoward Ġthe Ġsummit Ġor Ġas Ġfar Ġas Ġthe Ġg end arm es Ġare Ġallowing Ġtraffic Ġto Ġproceed Ġthat Ġday .\n",
      "\t Tokenized LLAMA3:From Ġhere Ġit 's Ġall Ġthrough Ġthe Ġcharming Ġhill side Ġvillage Ġof ĠSaint - Cl a ude , Ġwith Ġits Ġupper -income Ġhomes , Ġand Ġon Ġtoward Ġthe Ġsummit Ġor Ġas Ġfar Ġas Ġthe Ġg end arm es Ġare Ġallowing Ġtraffic Ġto Ġproceed Ġthat Ġday .\n",
      "\t Unique Tokens GPT2: {'income'}\n",
      "\t Unique Tokens LLAMA3: {'-income'}\n",
      "Text 2: The upper-income homes are very decadent and majestic.\n",
      "\t Tokenized GPT2:The Ġupper - income Ġhomes Ġare Ġvery Ġdec ad ent Ġand Ġmaj estic .\n",
      "\t Tokenized LLAMA3:The Ġupper -income Ġhomes Ġare Ġvery Ġdec ad ent Ġand Ġmaj estic .\n",
      "\t Unique Tokens GPT2: {'-', 'income'}\n",
      "\t Unique Tokens LLAMA3: {'-income'}\n",
      "==contradiction==\n",
      "Text 1:  \"You're not going to marry him, do you hear?\" he said dictatorially.\n",
      "\t Tokenized GPT2:Ġ\" You 're Ġnot Ġgoing Ġto Ġmarry Ġhim , Ġdo Ġyou Ġhear ?\" Ġhe Ġsaid Ġdict ator ially .\n",
      "\t Tokenized LLAMA3:Ġ\" You 're Ġnot Ġgoing Ġto Ġmarry Ġhim , Ġdo Ġyou Ġhear ?\" Ġhe Ġsaid Ġdict ator ially .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: \"I approve of your marriage to this man.\"\n",
      "\t Tokenized GPT2:\" I Ġapprove Ġof Ġyour Ġmarriage Ġto Ġthis Ġman .\"\n",
      "\t Tokenized LLAMA3:\"I Ġapprove Ġof Ġyour Ġmarriage Ġto Ġthis Ġman .\"\n",
      "\t Unique Tokens GPT2: {'I', '\"'}\n",
      "\t Unique Tokens LLAMA3: {'\"I'}\n",
      "==contradiction==\n",
      "Text 1: well his knees were bothering him yeah\n",
      "\t Tokenized GPT2:well Ġhis Ġknees Ġwere Ġbothering Ġhim Ġyeah\n",
      "\t Tokenized LLAMA3:well Ġhis Ġknees Ġwere Ġbothering Ġhim Ġyeah\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He was in tip-top condition.\n",
      "\t Tokenized GPT2:He Ġwas Ġin Ġtip - top Ġcondition .\n",
      "\t Tokenized LLAMA3:He Ġwas Ġin Ġtip -top Ġcondition .\n",
      "\t Unique Tokens GPT2: {'-', 'top'}\n",
      "\t Unique Tokens LLAMA3: {'-top'}\n",
      "==contradiction==\n",
      "Text 1: Exhibitions are often held in the splendid entrance hall.\n",
      "\t Tokenized GPT2:Ex hib itions Ġare Ġoften Ġheld Ġin Ġthe Ġsplendid Ġentrance Ġhall .\n",
      "\t Tokenized LLAMA3:Ex hib itions Ġare Ġoften Ġheld Ġin Ġthe Ġsple ndid Ġentrance Ġhall .\n",
      "\t Unique Tokens GPT2: {'Ġsplendid'}\n",
      "\t Unique Tokens LLAMA3: {'Ġsple', 'ndid'}\n",
      "Text 2: The entrance hall is kept clear of any exhibitions.\n",
      "\t Tokenized GPT2:The Ġentrance Ġhall Ġis Ġkept Ġclear Ġof Ġany Ġexhib itions .\n",
      "\t Tokenized LLAMA3:The Ġentrance Ġhall Ġis Ġkept Ġclear Ġof Ġany Ġexhib itions .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Mallorca prospered.\n",
      "\t Tokenized GPT2:M all or ca Ġpro spe red .\n",
      "\t Tokenized LLAMA3:M all or ca Ġpros pered .\n",
      "\t Unique Tokens GPT2: {'red', 'Ġpro', 'spe'}\n",
      "\t Unique Tokens LLAMA3: {'pered', 'Ġpros'}\n",
      "Text 2: Mallorca did extremely well.\n",
      "\t Tokenized GPT2:M all or ca Ġdid Ġextremely Ġwell .\n",
      "\t Tokenized LLAMA3:M all or ca Ġdid Ġextremely Ġwell .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: You've got the keys still, haven't you, Poirot? I asked, as we reached the door of the locked room. \n",
      "\t Tokenized GPT2:You 've Ġgot Ġthe Ġkeys Ġstill , Ġhaven 't Ġyou , ĠP oir ot ? ĠI Ġasked , Ġas Ġwe Ġreached Ġthe Ġdoor Ġof Ġthe Ġlocked Ġroom . Ġ\n",
      "\t Tokenized LLAMA3:You 've Ġgot Ġthe Ġkeys Ġstill , Ġhaven 't Ġyou , ĠP oir ot ? ĠI Ġasked , Ġas Ġwe Ġreached Ġthe Ġdoor Ġof Ġthe Ġlocked Ġroom . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I had the keys in my pocket.\n",
      "\t Tokenized GPT2:I Ġhad Ġthe Ġkeys Ġin Ġmy Ġpocket .\n",
      "\t Tokenized LLAMA3:I Ġhad Ġthe Ġkeys Ġin Ġmy Ġpocket .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: he's a college graduate type guy he's been in all he's an entrepreneur and he gives very practical financial advice about cars very you know not not nothing college level basic stuff his name is Bruce Williams he's on national radio uh i don't know what it would be down there you might want to whate\n",
      "\t Tokenized GPT2:he 's Ġa Ġcollege Ġgraduate Ġtype Ġguy Ġhe 's Ġbeen Ġin Ġall Ġhe 's Ġan Ġentrepreneur Ġand Ġhe Ġgives Ġvery Ġpractical Ġfinancial Ġadvice Ġabout Ġcars Ġvery Ġyou Ġknow Ġnot Ġnot Ġnothing Ġcollege Ġlevel Ġbasic Ġstuff Ġhis Ġname Ġis ĠBruce ĠWilliams Ġhe 's Ġon Ġnational Ġradio Ġuh Ġi Ġdon 't Ġknow Ġwhat Ġit Ġwould Ġbe Ġdown Ġthere Ġyou Ġmight Ġwant Ġto Ġwh ate\n",
      "\t Tokenized LLAMA3:he 's Ġa Ġcollege Ġgraduate Ġtype Ġguy Ġhe 's Ġbeen Ġin Ġall Ġhe 's Ġan Ġentrepreneur Ġand Ġhe Ġgives Ġvery Ġpractical Ġfinancial Ġadvice Ġabout Ġcars Ġvery Ġyou Ġknow Ġnot Ġnot Ġnothing Ġcollege Ġlevel Ġbasic Ġstuff Ġhis Ġname Ġis ĠBruce ĠWilliams Ġhe 's Ġon Ġnational Ġradio Ġuh Ġi Ġdon 't Ġknow Ġwhat Ġit Ġwould Ġbe Ġdown Ġthere Ġyou Ġmight Ġwant Ġto Ġwh ate\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: the entrepreneur gives people financial advice about real estate, but he doesn't know anything about cars\n",
      "\t Tokenized GPT2:the Ġentrepreneur Ġgives Ġpeople Ġfinancial Ġadvice Ġabout Ġreal Ġestate , Ġbut Ġhe Ġdoesn 't Ġknow Ġanything Ġabout Ġcars\n",
      "\t Tokenized LLAMA3:the Ġentrepreneur Ġgives Ġpeople Ġfinancial Ġadvice Ġabout Ġreal Ġestate , Ġbut Ġhe Ġdoesn 't Ġknow Ġanything Ġabout Ġcars\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: so they don't deal much in cash anymore either\n",
      "\t Tokenized GPT2:so Ġthey Ġdon 't Ġdeal Ġmuch Ġin Ġcash Ġanymore Ġeither\n",
      "\t Tokenized LLAMA3:so Ġthey Ġdon 't Ġdeal Ġmuch Ġin Ġcash Ġanymore Ġeither\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: They still heavily use cash for their transactions.\n",
      "\t Tokenized GPT2:They Ġstill Ġheavily Ġuse Ġcash Ġfor Ġtheir Ġtransactions .\n",
      "\t Tokenized LLAMA3:They Ġstill Ġheavily Ġuse Ġcash Ġfor Ġtheir Ġtransactions .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: He sat up, trying to free himself.\n",
      "\t Tokenized GPT2:He Ġsat Ġup , Ġtrying Ġto Ġfree Ġhimself .\n",
      "\t Tokenized LLAMA3:He Ġsat Ġup , Ġtrying Ġto Ġfree Ġhimself .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He was trying to break free while sitting up.\n",
      "\t Tokenized GPT2:He Ġwas Ġtrying Ġto Ġbreak Ġfree Ġwhile Ġsitting Ġup .\n",
      "\t Tokenized LLAMA3:He Ġwas Ġtrying Ġto Ġbreak Ġfree Ġwhile Ġsitting Ġup .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: It is not a surprise, either, that Al Pacino chews the scenery in Devil's Advocate . And the idea that if the devil showed up on Earth he'd be running a New York corporate-law firm is also, to say the least, pre-chewed.\n",
      "\t Tokenized GPT2:It Ġis Ġnot Ġa Ġsurprise , Ġeither , Ġthat ĠAl ĠPac ino Ġche ws Ġthe Ġscenery Ġin ĠDevil 's ĠAdv ocate Ġ. ĠAnd Ġthe Ġidea Ġthat Ġif Ġthe Ġdevil Ġshowed Ġup Ġon ĠEarth Ġhe 'd Ġbe Ġrunning Ġa ĠNew ĠYork Ġcorporate - law Ġfirm Ġis Ġalso , Ġto Ġsay Ġthe Ġleast , Ġpre - che wed .\n",
      "\t Tokenized LLAMA3:It Ġis Ġnot Ġa Ġsurprise , Ġeither , Ġthat ĠAl ĠPac ino Ġche ws Ġthe Ġscenery Ġin ĠDevil 's ĠAdv ocate Ġ. ĠAnd Ġthe Ġidea Ġthat Ġif Ġthe Ġdevil Ġshowed Ġup Ġon ĠEarth Ġhe 'd Ġbe Ġrunning Ġa ĠNew ĠYork Ġcorporate -law Ġfirm Ġis Ġalso , Ġto Ġsay Ġthe Ġleast , Ġpre -c he wed .\n",
      "\t Unique Tokens GPT2: {'-', 'che', 'law'}\n",
      "\t Unique Tokens LLAMA3: {'-c', 'he', '-law'}\n",
      "Text 2: Nobody expects that the devil would take the form of a lawyer.\n",
      "\t Tokenized GPT2:Nobody Ġexpects Ġthat Ġthe Ġdevil Ġwould Ġtake Ġthe Ġform Ġof Ġa Ġlawyer .\n",
      "\t Tokenized LLAMA3:Nobody Ġexpects Ġthat Ġthe Ġdevil Ġwould Ġtake Ġthe Ġform Ġof Ġa Ġlawyer .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: His plan was to drive straight up to the house.\n",
      "\t Tokenized GPT2:His Ġplan Ġwas Ġto Ġdrive Ġstraight Ġup Ġto Ġthe Ġhouse .\n",
      "\t Tokenized LLAMA3:His Ġplan Ġwas Ġto Ġdrive Ġstraight Ġup Ġto Ġthe Ġhouse .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Driving directly to the house was no longer an option with the roadblock.\n",
      "\t Tokenized GPT2:D riving Ġdirectly Ġto Ġthe Ġhouse Ġwas Ġno Ġlonger Ġan Ġoption Ġwith Ġthe Ġroad block .\n",
      "\t Tokenized LLAMA3:D riving Ġdirectly Ġto Ġthe Ġhouse Ġwas Ġno Ġlonger Ġan Ġoption Ġwith Ġthe Ġroad block .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: In 1995 and again in 1998, the Legal Services Corporation recognized that legal services programs were going to have to change the method and manner in which they conducted their business if they were going to remain viable and responsive to the needs of low income persons.\n",
      "\t Tokenized GPT2:In Ġ1995 Ġand Ġagain Ġin Ġ1998 , Ġthe ĠLe gal ĠServices ĠCorporation Ġrecognized Ġthat Ġlegal Ġservices Ġprograms Ġwere Ġgoing Ġto Ġhave Ġto Ġchange Ġthe Ġmethod Ġand Ġmanner Ġin Ġwhich Ġthey Ġconducted Ġtheir Ġbusiness Ġif Ġthey Ġwere Ġgoing Ġto Ġremain Ġviable Ġand Ġresponsive Ġto Ġthe Ġneeds Ġof Ġlow Ġincome Ġpersons .\n",
      "\t Tokenized LLAMA3:In Ġ 199 5 Ġand Ġagain Ġin Ġ 199 8 , Ġthe ĠLe gal ĠServices ĠCorporation Ġrecognized Ġthat Ġlegal Ġservices Ġprograms Ġwere Ġgoing Ġto Ġhave Ġto Ġchange Ġthe Ġmethod Ġand Ġmanner Ġin Ġwhich Ġthey Ġconducted Ġtheir Ġbusiness Ġif Ġthey Ġwere Ġgoing Ġto Ġremain Ġviable Ġand Ġresponsive Ġto Ġthe Ġneeds Ġof Ġlow Ġincome Ġpersons .\n",
      "\t Unique Tokens GPT2: {'Ġ1995', 'Ġ1998'}\n",
      "\t Unique Tokens LLAMA3: {'199', '5', 'Ġ', '8'}\n",
      "Text 2: Low income persons have very difficult needs to meet.\n",
      "\t Tokenized GPT2:Low Ġincome Ġpersons Ġhave Ġvery Ġdifficult Ġneeds Ġto Ġmeet .\n",
      "\t Tokenized LLAMA3:Low Ġincome Ġpersons Ġhave Ġvery Ġdifficult Ġneeds Ġto Ġmeet .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: He's too cautious.\n",
      "\t Tokenized GPT2:He 's Ġtoo Ġcautious .\n",
      "\t Tokenized LLAMA3:He 's Ġtoo Ġcautious .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He's not brave enough.\n",
      "\t Tokenized GPT2:He 's Ġnot Ġbrave Ġenough .\n",
      "\t Tokenized LLAMA3:He 's Ġnot Ġbrave Ġenough .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The long-sought, the mysterious, the elusive Jane Finn! \n",
      "\t Tokenized GPT2:The Ġlong - s ought , Ġthe Ġmysterious , Ġthe Ġel usive ĠJane ĠFinn ! Ġ\n",
      "\t Tokenized LLAMA3:The Ġlong -s ought , Ġthe Ġmysterious , Ġthe Ġel usive ĠJane ĠFinn ! Ġ\n",
      "\t Unique Tokens GPT2: {'s', '-'}\n",
      "\t Unique Tokens LLAMA3: {'-s'}\n",
      "Text 2: Jane Finn is as beautiful as she is mysterious.\n",
      "\t Tokenized GPT2:Jane ĠFinn Ġis Ġas Ġbeautiful Ġas Ġshe Ġis Ġmysterious .\n",
      "\t Tokenized LLAMA3:J ane ĠFinn Ġis Ġas Ġbeautiful Ġas Ġshe Ġis Ġmysterious .\n",
      "\t Unique Tokens GPT2: {'Jane'}\n",
      "\t Unique Tokens LLAMA3: {'ane', 'J'}\n",
      "==contradiction==\n",
      "Text 1: Ocho Rios is Spanish for  eight rivers,  but this name is not descriptive of the area.\n",
      "\t Tokenized GPT2:O cho ĠR ios Ġis ĠSpanish Ġfor Ġ Ġeight Ġrivers , Ġ Ġbut Ġthis Ġname Ġis Ġnot Ġdescriptive Ġof Ġthe Ġarea .\n",
      "\t Tokenized LLAMA3:O cho ĠR ios Ġis ĠSpanish Ġfor Ġ Ġeight Ġrivers , Ġ Ġbut Ġthis Ġname Ġis Ġnot Ġdescriptive Ġof Ġthe Ġarea .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Ocho Rios means pink penguins in Spanish, and this name describes the area.\n",
      "\t Tokenized GPT2:O cho ĠR ios Ġmeans Ġpink Ġpeng u ins Ġin ĠSpanish , Ġand Ġthis Ġname Ġdescribes Ġthe Ġarea .\n",
      "\t Tokenized LLAMA3:O cho ĠR ios Ġmeans Ġpink Ġpeng u ins Ġin ĠSpanish , Ġand Ġthis Ġname Ġdescribes Ġthe Ġarea .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: There is.\n",
      "\t Tokenized GPT2:There Ġis .\n",
      "\t Tokenized LLAMA3:There Ġis .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: There sure is.\n",
      "\t Tokenized GPT2:There Ġsure Ġis .\n",
      "\t Tokenized LLAMA3:There Ġsure Ġis .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Once or twice, but they seem more show than battle, said Adrin.\n",
      "\t Tokenized GPT2:Once Ġor Ġtwice , Ġbut Ġthey Ġseem Ġmore Ġshow Ġthan Ġbattle , Ġsaid ĠAd rin .\n",
      "\t Tokenized LLAMA3:Once Ġor Ġtwice , Ġbut Ġthey Ġseem Ġmore Ġshow Ġthan Ġbattle , Ġsaid ĠAd rin .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Adrin said they weren't serious about battling.\n",
      "\t Tokenized GPT2:Ad rin Ġsaid Ġthey Ġweren 't Ġserious Ġabout Ġbattling .\n",
      "\t Tokenized LLAMA3:Ad rin Ġsaid Ġthey Ġweren 't Ġserious Ġabout Ġbattling .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1:  13-year-olds.\n",
      "\t Tokenized GPT2:Ġ13 - year - olds .\n",
      "\t Tokenized LLAMA3:Ġ 13 -year -old s .\n",
      "\t Unique Tokens GPT2: {'olds', 'year', 'Ġ13', '-'}\n",
      "\t Unique Tokens LLAMA3: {'-year', 'Ġ', 's', '-old', '13'}\n",
      "Text 2: Young teenagers \n",
      "\t Tokenized GPT2:Young Ġteenagers Ġ\n",
      "\t Tokenized LLAMA3:Young Ġteenagers Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Component modularization and prefabrication off-site can reduce the amount of time cranes are needed on a site, as well as provide opportunities to reduce project schedules and construction costs and to concentrate jobs locally at the prefabrication facility.\n",
      "\t Tokenized GPT2:Component Ġmodular ization Ġand Ġpref ab ric ation Ġoff - site Ġcan Ġreduce Ġthe Ġamount Ġof Ġtime Ġcr anes Ġare Ġneeded Ġon Ġa Ġsite , Ġas Ġwell Ġas Ġprovide Ġopportunities Ġto Ġreduce Ġproject Ġschedules Ġand Ġconstruction Ġcosts Ġand Ġto Ġconcentrate Ġjobs Ġlocally Ġat Ġthe Ġpref ab ric ation Ġfacility .\n",
      "\t Tokenized LLAMA3:Component Ġmodular ization Ġand Ġpref ab ric ation Ġoff -site Ġcan Ġreduce Ġthe Ġamount Ġof Ġtime Ġcr anes Ġare Ġneeded Ġon Ġa Ġsite , Ġas Ġwell Ġas Ġprovide Ġopportunities Ġto Ġreduce Ġproject Ġschedules Ġand Ġconstruction Ġcosts Ġand Ġto Ġconcentrate Ġjobs Ġlocally Ġat Ġthe Ġpref ab ric ation Ġfacility .\n",
      "\t Unique Tokens GPT2: {'-', 'site'}\n",
      "\t Unique Tokens LLAMA3: {'-site'}\n",
      "Text 2: When work is done off-site, such as prefabrication, it increases the time cranes are need on a site and raises construction costs.\n",
      "\t Tokenized GPT2:When Ġwork Ġis Ġdone Ġoff - site , Ġsuch Ġas Ġpref ab ric ation , Ġit Ġincreases Ġthe Ġtime Ġcr anes Ġare Ġneed Ġon Ġa Ġsite Ġand Ġraises Ġconstruction Ġcosts .\n",
      "\t Tokenized LLAMA3:When Ġwork Ġis Ġdone Ġoff -site , Ġsuch Ġas Ġpref ab ric ation , Ġit Ġincreases Ġthe Ġtime Ġcr anes Ġare Ġneed Ġon Ġa Ġsite Ġand Ġraises Ġconstruction Ġcosts .\n",
      "\t Unique Tokens GPT2: {'-', 'site'}\n",
      "\t Unique Tokens LLAMA3: {'-site'}\n",
      "==entailment==\n",
      "Text 1: Students of human misery can savor its underlying sadness and futility.\n",
      "\t Tokenized GPT2:Stud ents Ġof Ġhuman Ġmisery Ġcan Ġs avor Ġits Ġunderlying Ġsadness Ġand Ġfut ility .\n",
      "\t Tokenized LLAMA3:Stud ents Ġof Ġhuman Ġmisery Ġcan Ġs avor Ġits Ġunderlying Ġsadness Ġand Ġfut ility .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Students of human misery will be delighted to see how sad it truly is.\n",
      "\t Tokenized GPT2:Stud ents Ġof Ġhuman Ġmisery Ġwill Ġbe Ġdelighted Ġto Ġsee Ġhow Ġsad Ġit Ġtruly Ġis .\n",
      "\t Tokenized LLAMA3:Stud ents Ġof Ġhuman Ġmisery Ġwill Ġbe Ġdelighted Ġto Ġsee Ġhow Ġsad Ġit Ġtruly Ġis .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: How long, Thaler and Siegel ask, will it take most investors to get wise to the fact that the equity premium is just too damned high?\n",
      "\t Tokenized GPT2:How Ġlong , ĠTh aler Ġand ĠSie gel Ġask , Ġwill Ġit Ġtake Ġmost Ġinvestors Ġto Ġget Ġwise Ġto Ġthe Ġfact Ġthat Ġthe Ġequity Ġpremium Ġis Ġjust Ġtoo Ġdamned Ġhigh ?\n",
      "\t Tokenized LLAMA3:How Ġlong , ĠTh aler Ġand ĠSie gel Ġask , Ġwill Ġit Ġtake Ġmost Ġinvestors Ġto Ġget Ġwise Ġto Ġthe Ġfact Ġthat Ġthe Ġequity Ġpremium Ġis Ġjust Ġtoo Ġdamned Ġhigh ?\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Thaler and Siegel think that many investors already know that the equity premium is too high. \n",
      "\t Tokenized GPT2:Th aler Ġand ĠSie gel Ġthink Ġthat Ġmany Ġinvestors Ġalready Ġknow Ġthat Ġthe Ġequity Ġpremium Ġis Ġtoo Ġhigh . Ġ\n",
      "\t Tokenized LLAMA3:Th aler Ġand ĠSie gel Ġthink Ġthat Ġmany Ġinvestors Ġalready Ġknow Ġthat Ġthe Ġequity Ġpremium Ġis Ġtoo Ġhigh . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: I was pulled into the bar.\n",
      "\t Tokenized GPT2:I Ġwas Ġpulled Ġinto Ġthe Ġbar .\n",
      "\t Tokenized LLAMA3:I Ġwas Ġpulled Ġinto Ġthe Ġbar .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I managed to escape their grasp and ran just outside the bar.\n",
      "\t Tokenized GPT2:I Ġmanaged Ġto Ġescape Ġtheir Ġgrasp Ġand Ġran Ġjust Ġoutside Ġthe Ġbar .\n",
      "\t Tokenized LLAMA3:I Ġmanaged Ġto Ġescape Ġtheir Ġgrasp Ġand Ġran Ġjust Ġoutside Ġthe Ġbar .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: STANDARD COSTING - A costing method that attaches costs to cost objects based on reasonable estimates or cost studies and by means of budgeted rates rather than according to actual costs incurred.\n",
      "\t Tokenized GPT2:ST AND ARD ĠC OST ING Ġ- ĠA Ġcost ing Ġmethod Ġthat Ġatt aches Ġcosts Ġto Ġcost Ġobjects Ġbased Ġon Ġreasonable Ġestimates Ġor Ġcost Ġstudies Ġand Ġby Ġmeans Ġof Ġbudget ed Ġrates Ġrather Ġthan Ġaccording Ġto Ġactual Ġcosts Ġincur red .\n",
      "\t Tokenized LLAMA3:ST AND ARD ĠC OST ING Ġ- ĠA Ġcost ing Ġmethod Ġthat Ġatt aches Ġcosts Ġto Ġcost Ġobjects Ġbased Ġon Ġreasonable Ġestimates Ġor Ġcost Ġstudies Ġand Ġby Ġmeans Ġof Ġbudget ed Ġrates Ġrather Ġthan Ġaccording Ġto Ġactual Ġcosts Ġinc urred .\n",
      "\t Unique Tokens GPT2: {'red', 'Ġincur'}\n",
      "\t Unique Tokens LLAMA3: {'Ġinc', 'urred'}\n",
      "Text 2: This costing model is based on research and analytical activity.\n",
      "\t Tokenized GPT2:This Ġcost ing Ġmodel Ġis Ġbased Ġon Ġresearch Ġand Ġanalytical Ġactivity .\n",
      "\t Tokenized LLAMA3:This Ġcost ing Ġmodel Ġis Ġbased Ġon Ġresearch Ġand Ġanalytical Ġactivity .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: 'Can I get a drink?'\n",
      "\t Tokenized GPT2:' Can ĠI Ġget Ġa Ġdrink ?'\n",
      "\t Tokenized LLAMA3:' Can ĠI Ġget Ġa Ġdrink ?'\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Can you make me an espresso?\n",
      "\t Tokenized GPT2:Can Ġyou Ġmake Ġme Ġan Ġes pres so ?\n",
      "\t Tokenized LLAMA3:Can Ġyou Ġmake Ġme Ġan Ġes pres so ?\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The mansions have been downgraded to consulates since the capital was transferred to Ankara in 1923, and modern shops and restaurants have sprung up.\n",
      "\t Tokenized GPT2:The Ġmans ions Ġhave Ġbeen Ġdown grad ed Ġto Ġcons ulates Ġsince Ġthe Ġcapital Ġwas Ġtransferred Ġto ĠAn k ara Ġin Ġ19 23 , Ġand Ġmodern Ġshops Ġand Ġrestaurants Ġhave Ġspr ung Ġup .\n",
      "\t Tokenized LLAMA3:The Ġmans ions Ġhave Ġbeen Ġdown grad ed Ġto Ġcons ulates Ġsince Ġthe Ġcapital Ġwas Ġtransferred Ġto ĠAn k ara Ġin Ġ 192 3 , Ġand Ġmodern Ġshops Ġand Ġrestaurants Ġhave Ġspr ung Ġup .\n",
      "\t Unique Tokens GPT2: {'Ġ19', '23'}\n",
      "\t Unique Tokens LLAMA3: {'3', '192', 'Ġ'}\n",
      "Text 2: The capital had been located in the city of Ankara once before.\n",
      "\t Tokenized GPT2:The Ġcapital Ġhad Ġbeen Ġlocated Ġin Ġthe Ġcity Ġof ĠAn k ara Ġonce Ġbefore .\n",
      "\t Tokenized LLAMA3:The Ġcapital Ġhad Ġbeen Ġlocated Ġin Ġthe Ġcity Ġof ĠAn k ara Ġonce Ġbefore .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: 1 Now that each unit is fully staffed, the LSC Office of Program Performance and its state planning team contain over 260 years of experience in LSC-funded programs.\n",
      "\t Tokenized GPT2:1 ĠNow Ġthat Ġeach Ġunit Ġis Ġfully Ġstaff ed , Ġthe ĠL SC ĠOffice Ġof ĠProgram ĠPerformance Ġand Ġits Ġstate Ġplanning Ġteam Ġcontain Ġover Ġ260 Ġyears Ġof Ġexperience Ġin ĠL SC - fund ed Ġprograms .\n",
      "\t Tokenized LLAMA3:1 ĠNow Ġthat Ġeach Ġunit Ġis Ġfully Ġstaff ed , Ġthe ĠL SC ĠOffice Ġof ĠProgram ĠPerformance Ġand Ġits Ġstate Ġplanning Ġteam Ġcontain Ġover Ġ 260 Ġyears Ġof Ġexperience Ġin ĠL SC -f und ed Ġprograms .\n",
      "\t Unique Tokens GPT2: {'-', 'Ġ260', 'fund'}\n",
      "\t Unique Tokens LLAMA3: {'-f', '260', 'und', 'Ġ'}\n",
      "Text 2: The LSC has over 1260 years of experience with their staff.\n",
      "\t Tokenized GPT2:The ĠL SC Ġhas Ġover Ġ12 60 Ġyears Ġof Ġexperience Ġwith Ġtheir Ġstaff .\n",
      "\t Tokenized LLAMA3:The ĠL SC Ġhas Ġover Ġ 126 0 Ġyears Ġof Ġexperience Ġwith Ġtheir Ġstaff .\n",
      "\t Unique Tokens GPT2: {'Ġ12', '60'}\n",
      "\t Unique Tokens LLAMA3: {'126', '0', 'Ġ'}\n",
      "==entailment==\n",
      "Text 1: Who? asked Tommy.\n",
      "\t Tokenized GPT2:Who ? Ġasked ĠTommy .\n",
      "\t Tokenized LLAMA3:Who ? Ġasked ĠTommy .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Tommy inquired about the identity of the person.\n",
      "\t Tokenized GPT2:Tom my Ġinqu ired Ġabout Ġthe Ġidentity Ġof Ġthe Ġperson .\n",
      "\t Tokenized LLAMA3:Tom my Ġinqu ired Ġabout Ġthe Ġidentity Ġof Ġthe Ġperson .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: profit rather\n",
      "\t Tokenized GPT2:profit Ġrather\n",
      "\t Tokenized LLAMA3:pro fit Ġrather\n",
      "\t Unique Tokens GPT2: {'profit'}\n",
      "\t Unique Tokens LLAMA3: {'pro', 'fit'}\n",
      "Text 2: Our profit has not been good.\n",
      "\t Tokenized GPT2:Our Ġprofit Ġhas Ġnot Ġbeen Ġgood .\n",
      "\t Tokenized LLAMA3:Our Ġprofit Ġhas Ġnot Ġbeen Ġgood .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: He was of two minds, one reveled in the peace of this village.\n",
      "\t Tokenized GPT2:He Ġwas Ġof Ġtwo Ġminds , Ġone Ġrevel ed Ġin Ġthe Ġpeace Ġof Ġthis Ġvillage .\n",
      "\t Tokenized LLAMA3:He Ġwas Ġof Ġtwo Ġminds , Ġone Ġrevel ed Ġin Ġthe Ġpeace Ġof Ġthis Ġvillage .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The village was full of violence.\n",
      "\t Tokenized GPT2:The Ġvillage Ġwas Ġfull Ġof Ġviolence .\n",
      "\t Tokenized LLAMA3:The Ġvillage Ġwas Ġfull Ġof Ġviolence .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Too bad it chose to use McIntyre instead.\n",
      "\t Tokenized GPT2:Too Ġbad Ġit Ġchose Ġto Ġuse ĠMc Int y re Ġinstead .\n",
      "\t Tokenized LLAMA3:Too Ġbad Ġit Ġchose Ġto Ġuse ĠMc Int y re Ġinstead .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: McIntyre was picked to be used.\n",
      "\t Tokenized GPT2:Mc Int y re Ġwas Ġpicked Ġto Ġbe Ġused .\n",
      "\t Tokenized LLAMA3:Mc Int y re Ġwas Ġpicked Ġto Ġbe Ġused .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The arts also flourished in India during these early times.\n",
      "\t Tokenized GPT2:The Ġarts Ġalso Ġflour ished Ġin ĠIndia Ġduring Ġthese Ġearly Ġtimes .\n",
      "\t Tokenized LLAMA3:The Ġarts Ġalso Ġflour ished Ġin ĠIndia Ġduring Ġthese Ġearly Ġtimes .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The arts languished during those early days.\n",
      "\t Tokenized GPT2:The Ġarts Ġl angu ished Ġduring Ġthose Ġearly Ġdays .\n",
      "\t Tokenized LLAMA3:The Ġarts Ġl angu ished Ġduring Ġthose Ġearly Ġdays .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The Wall Street Journal Business Bulletin has a fact that dramatizes how profoundly well-off this country is--Americans throw out approximately 12 percent of the stuff they buy at the supermarket.\n",
      "\t Tokenized GPT2:The ĠWall ĠStreet ĠJournal ĠBusiness ĠBul let in Ġhas Ġa Ġfact Ġthat Ġdram at izes Ġhow Ġprofound ly Ġwell - off Ġthis Ġcountry Ġis -- Americans Ġthrow Ġout Ġapproximately Ġ12 Ġpercent Ġof Ġthe Ġstuff Ġthey Ġbuy Ġat Ġthe Ġsupermarket .\n",
      "\t Tokenized LLAMA3:The ĠWall ĠStreet ĠJournal ĠBusiness ĠBul let in Ġhas Ġa Ġfact Ġthat Ġdram at izes Ġhow Ġprofound ly Ġwell -off Ġthis Ġcountry Ġis -- Americ ans Ġthrow Ġout Ġapproximately Ġ 12 Ġpercent Ġof Ġthe Ġstuff Ġthey Ġbuy Ġat Ġthe Ġsuper market .\n",
      "\t Unique Tokens GPT2: {'Ġ12', 'Americans', '-', 'off', 'Ġsupermarket'}\n",
      "\t Unique Tokens LLAMA3: {'12', 'Ġ', 'ans', 'Ġsuper', 'Americ', 'market', '-off'}\n",
      "Text 2: Americans just throw away 12 percent of what they buy at foreign supermarkets.\n",
      "\t Tokenized GPT2:Americans Ġjust Ġthrow Ġaway Ġ12 Ġpercent Ġof Ġwhat Ġthey Ġbuy Ġat Ġforeign Ġsuper mark ets .\n",
      "\t Tokenized LLAMA3:Americ ans Ġjust Ġthrow Ġaway Ġ 12 Ġpercent Ġof Ġwhat Ġthey Ġbuy Ġat Ġforeign Ġsuper mark ets .\n",
      "\t Unique Tokens GPT2: {'Americans', 'Ġ12'}\n",
      "\t Unique Tokens LLAMA3: {'Ġ', 'ans', 'Americ', '12'}\n",
      "==entailment==\n",
      "Text 1: yeah yeah you probably get this probably pretty sticky after you get done then you've got to drain the water out of the watermelon because you know when you scrape it it makes the water\n",
      "\t Tokenized GPT2:yeah Ġyeah Ġyou Ġprobably Ġget Ġthis Ġprobably Ġpretty Ġsticky Ġafter Ġyou Ġget Ġdone Ġthen Ġyou 've Ġgot Ġto Ġdrain Ġthe Ġwater Ġout Ġof Ġthe Ġwater mel on Ġbecause Ġyou Ġknow Ġwhen Ġyou Ġsc rape Ġit Ġit Ġmakes Ġthe Ġwater\n",
      "\t Tokenized LLAMA3:yeah Ġyeah Ġyou Ġprobably Ġget Ġthis Ġprobably Ġpretty Ġsticky Ġafter Ġyou Ġget Ġdone Ġthen Ġyou 've Ġgot Ġto Ġdrain Ġthe Ġwater Ġout Ġof Ġthe Ġwater mel on Ġbecause Ġyou Ġknow Ġwhen Ġyou Ġsc rape Ġit Ġit Ġmakes Ġthe Ġwater\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: You need to drain the water our of the watermelon.\n",
      "\t Tokenized GPT2:You Ġneed Ġto Ġdrain Ġthe Ġwater Ġour Ġof Ġthe Ġwater mel on .\n",
      "\t Tokenized LLAMA3:You Ġneed Ġto Ġdrain Ġthe Ġwater Ġour Ġof Ġthe Ġwater mel on .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: But the world is not run for the edification of tourists.\n",
      "\t Tokenized GPT2:But Ġthe Ġworld Ġis Ġnot Ġrun Ġfor Ġthe Ġed ification Ġof Ġtourists .\n",
      "\t Tokenized LLAMA3:But Ġthe Ġworld Ġis Ġnot Ġrun Ġfor Ġthe Ġed ification Ġof Ġtourists .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The world does not try and morally subject to tourists.\n",
      "\t Tokenized GPT2:The Ġworld Ġdoes Ġnot Ġtry Ġand Ġmorally Ġsubject Ġto Ġtourists .\n",
      "\t Tokenized LLAMA3:The Ġworld Ġdoes Ġnot Ġtry Ġand Ġmorally Ġsubject Ġto Ġtourists .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: There were maybe three hundred people present.\n",
      "\t Tokenized GPT2:There Ġwere Ġmaybe Ġthree Ġhundred Ġpeople Ġpresent .\n",
      "\t Tokenized LLAMA3:There Ġwere Ġmaybe Ġthree Ġhundred Ġpeople Ġpresent .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: 300 people were there.\n",
      "\t Tokenized GPT2:300 Ġpeople Ġwere Ġthere .\n",
      "\t Tokenized LLAMA3:300 Ġpeople Ġwere Ġthere .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: yeah they uh they the voters voted one way and it and then uh some federal judge said no that was unconstitutional and they have had two or three votes and the city council is divided over what the district should be because they divide it one way and the minorities say we're losing representation  \n",
      "\t Tokenized GPT2:yeah Ġthey Ġuh Ġthey Ġthe Ġvoters Ġvoted Ġone Ġway Ġand Ġit Ġand Ġthen Ġuh Ġsome Ġfederal Ġjudge Ġsaid Ġno Ġthat Ġwas Ġun const itutional Ġand Ġthey Ġhave Ġhad Ġtwo Ġor Ġthree Ġvotes Ġand Ġthe Ġcity Ġcouncil Ġis Ġdivided Ġover Ġwhat Ġthe Ġdistrict Ġshould Ġbe Ġbecause Ġthey Ġdivide Ġit Ġone Ġway Ġand Ġthe Ġminorities Ġsay Ġwe 're Ġlosing Ġrepresentation ĠĠ\n",
      "\t Tokenized LLAMA3:yeah Ġthey Ġuh Ġthey Ġthe Ġvoters Ġvoted Ġone Ġway Ġand Ġit Ġand Ġthen Ġuh Ġsome Ġfederal Ġjudge Ġsaid Ġno Ġthat Ġwas Ġun const itutional Ġand Ġthey Ġhave Ġhad Ġtwo Ġor Ġthree Ġvotes Ġand Ġthe Ġcity Ġcouncil Ġis Ġdivided Ġover Ġwhat Ġthe Ġdistrict Ġshould Ġbe Ġbecause Ġthey Ġdivide Ġit Ġone Ġway Ġand Ġthe Ġminorities Ġsay Ġwe 're Ġlosing Ġrepresentation ĠĠ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: the federal judge overruled the vote of the people on the ground that it was unconstitutional\n",
      "\t Tokenized GPT2:the Ġfederal Ġjudge Ġoverr uled Ġthe Ġvote Ġof Ġthe Ġpeople Ġon Ġthe Ġground Ġthat Ġit Ġwas Ġun const itutional\n",
      "\t Tokenized LLAMA3:the Ġfederal Ġjudge Ġoverr uled Ġthe Ġvote Ġof Ġthe Ġpeople Ġon Ġthe Ġground Ġthat Ġit Ġwas Ġun const itutional\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: They keep romance and marriage apart  \" Tommy flushed.\n",
      "\t Tokenized GPT2:They Ġkeep Ġromance Ġand Ġmarriage Ġapart Ġ Ġ\" ĠTommy Ġflushed .\n",
      "\t Tokenized LLAMA3:They Ġkeep Ġromance Ġand Ġmarriage Ġapart Ġ Ġ\" ĠTommy Ġflushed .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Tommy said they don't mix romance and marriage and they have a good relationship.\n",
      "\t Tokenized GPT2:Tom my Ġsaid Ġthey Ġdon 't Ġmix Ġromance Ġand Ġmarriage Ġand Ġthey Ġhave Ġa Ġgood Ġrelationship .\n",
      "\t Tokenized LLAMA3:Tom my Ġsaid Ġthey Ġdon 't Ġmix Ġromance Ġand Ġmarriage Ġand Ġthey Ġhave Ġa Ġgood Ġrelationship .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Companies that were foreign had to accept Indian financial participation and management.\n",
      "\t Tokenized GPT2:Comp anies Ġthat Ġwere Ġforeign Ġhad Ġto Ġaccept ĠIndian Ġfinancial Ġparticipation Ġand Ġmanagement .\n",
      "\t Tokenized LLAMA3:Comp anies Ġthat Ġwere Ġforeign Ġhad Ġto Ġaccept ĠIndian Ġfinancial Ġparticipation Ġand Ġmanagement .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Foreign companies had to take Indian money in order to operate their businesses.\n",
      "\t Tokenized GPT2:Foreign Ġcompanies Ġhad Ġto Ġtake ĠIndian Ġmoney Ġin Ġorder Ġto Ġoperate Ġtheir Ġbusinesses .\n",
      "\t Tokenized LLAMA3:Fore ign Ġcompanies Ġhad Ġto Ġtake ĠIndian Ġmoney Ġin Ġorder Ġto Ġoperate Ġtheir Ġbusinesses .\n",
      "\t Unique Tokens GPT2: {'Foreign'}\n",
      "\t Unique Tokens LLAMA3: {'ign', 'Fore'}\n",
      "==neutral==\n",
      "Text 1: Castlerigg near Keswick is the best example.\n",
      "\t Tokenized GPT2:Cast ler ig g Ġnear ĠK es wick Ġis Ġthe Ġbest Ġexample .\n",
      "\t Tokenized LLAMA3:Cast ler ig g Ġnear ĠK es wick Ġis Ġthe Ġbest Ġexample .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: A good example would be Castlerigg near Keswick, in Scotland.\n",
      "\t Tokenized GPT2:A Ġgood Ġexample Ġwould Ġbe ĠCast ler ig g Ġnear ĠK es wick , Ġin ĠScotland .\n",
      "\t Tokenized LLAMA3:A Ġgood Ġexample Ġwould Ġbe ĠCast ler ig g Ġnear ĠK es wick , Ġin ĠScotland .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: 4) Not enough is known about how nontransportation costs vary with distance.\n",
      "\t Tokenized GPT2:4 ) ĠNot Ġenough Ġis Ġknown Ġabout Ġhow Ġnon transport ation Ġcosts Ġvary Ġwith Ġdistance .\n",
      "\t Tokenized LLAMA3:4 ) ĠNot Ġenough Ġis Ġknown Ġabout Ġhow Ġnon trans port ation Ġcosts Ġvary Ġwith Ġdistance .\n",
      "\t Unique Tokens GPT2: {'transport'}\n",
      "\t Unique Tokens LLAMA3: {'port', 'trans'}\n",
      "Text 2: There's more than enough known concerning the ways in which costs associated with nontransportation change according to the distance.\n",
      "\t Tokenized GPT2:There 's Ġmore Ġthan Ġenough Ġknown Ġconcerning Ġthe Ġways Ġin Ġwhich Ġcosts Ġassociated Ġwith Ġnon transport ation Ġchange Ġaccording Ġto Ġthe Ġdistance .\n",
      "\t Tokenized LLAMA3:There 's Ġmore Ġthan Ġenough Ġknown Ġconcerning Ġthe Ġways Ġin Ġwhich Ġcosts Ġassociated Ġwith Ġnon trans port ation Ġchange Ġaccording Ġto Ġthe Ġdistance .\n",
      "\t Unique Tokens GPT2: {'transport'}\n",
      "\t Unique Tokens LLAMA3: {'port', 'trans'}\n",
      "==neutral==\n",
      "Text 1: She admits to Dorcas, 'I don't know what to do; scandal between husband and wife is a dreadful thing.' At 4 o'clock she has been angry, but completely mistress of herself. \n",
      "\t Tokenized GPT2:She Ġadmits Ġto ĠDor cas , Ġ' I Ġdon 't Ġknow Ġwhat Ġto Ġdo ; Ġscandal Ġbetween Ġhusband Ġand Ġwife Ġis Ġa Ġdread ful Ġthing .' ĠAt Ġ4 Ġo ' clock Ġshe Ġhas Ġbeen Ġangry , Ġbut Ġcompletely Ġmist ress Ġof Ġherself . Ġ\n",
      "\t Tokenized LLAMA3:She Ġadmits Ġto ĠDor cas , Ġ' I Ġdon 't Ġknow Ġwhat Ġto Ġdo ; Ġscandal Ġbetween Ġhusband Ġand Ġwife Ġis Ġa Ġdread ful Ġthing .' ĠAt Ġ 4 Ġo 'clock Ġshe Ġhas Ġbeen Ġangry , Ġbut Ġcompletely Ġmist ress Ġof Ġherself . Ġ\n",
      "\t Unique Tokens GPT2: {'Ġ4', \"'\", 'clock'}\n",
      "\t Unique Tokens LLAMA3: {\"'clock\", '4'}\n",
      "Text 2: Dorcas agreed with her comments about scandals between husbands and wives.\n",
      "\t Tokenized GPT2:D or cas Ġagreed Ġwith Ġher Ġcomments Ġabout Ġsc and als Ġbetween Ġhus bands Ġand Ġwives .\n",
      "\t Tokenized LLAMA3:D or cas Ġagreed Ġwith Ġher Ġcomments Ġabout Ġsc and als Ġbetween Ġhus bands Ġand Ġwives .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: yeah i mean this this Escort even when the head gasket went i mean it would start first time every time\n",
      "\t Tokenized GPT2:yeah Ġi Ġmean Ġthis Ġthis ĠEsc ort Ġeven Ġwhen Ġthe Ġhead Ġg asket Ġwent Ġi Ġmean Ġit Ġwould Ġstart Ġfirst Ġtime Ġevery Ġtime\n",
      "\t Tokenized LLAMA3:yeah Ġi Ġmean Ġthis Ġthis ĠEsc ort Ġeven Ġwhen Ġthe Ġhead Ġg asket Ġwent Ġi Ġmean Ġit Ġwould Ġstart Ġfirst Ġtime Ġevery Ġtime\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Once the head gasket went out, the Escort stopped working.\n",
      "\t Tokenized GPT2:Once Ġthe Ġhead Ġg asket Ġwent Ġout , Ġthe ĠEsc ort Ġstopped Ġworking .\n",
      "\t Tokenized LLAMA3:Once Ġthe Ġhead Ġg asket Ġwent Ġout , Ġthe ĠEsc ort Ġstopped Ġworking .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Daniel nodded, fetching me a glass of beer.\n",
      "\t Tokenized GPT2:Daniel Ġnodded , Ġfetch ing Ġme Ġa Ġglass Ġof Ġbeer .\n",
      "\t Tokenized LLAMA3:Daniel Ġnodded , Ġfetch ing Ġme Ġa Ġglass Ġof Ġbeer .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Daniel got me a vodka and tonic. \n",
      "\t Tokenized GPT2:Daniel Ġgot Ġme Ġa Ġv odka Ġand Ġton ic . Ġ\n",
      "\t Tokenized LLAMA3:Daniel Ġgot Ġme Ġa Ġv od ka Ġand Ġton ic . Ġ\n",
      "\t Unique Tokens GPT2: {'odka'}\n",
      "\t Unique Tokens LLAMA3: {'ka', 'od'}\n",
      "==contradiction==\n",
      "Text 1: He reported masterfully on the '72 campaign and the Hell's Angels.\n",
      "\t Tokenized GPT2:He Ġreported Ġmaster fully Ġon Ġthe Ġ' 72 Ġcampaign Ġand Ġthe ĠHell 's ĠAngels .\n",
      "\t Tokenized LLAMA3:He Ġreported Ġmaster fully Ġon Ġthe Ġ' 72 Ġcampaign Ġand Ġthe ĠHell 's ĠAngels .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He did an extraordinarily bad job reporting on the Hell's Angels.\n",
      "\t Tokenized GPT2:He Ġdid Ġan Ġextra ordin arily Ġbad Ġjob Ġreporting Ġon Ġthe ĠHell 's ĠAngels .\n",
      "\t Tokenized LLAMA3:He Ġdid Ġan Ġextra ordin arily Ġbad Ġjob Ġreporting Ġon Ġthe ĠHell 's ĠAngels .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: There followed the Balkan Wars, in which Turkey lost western Thrace and Macedonia, then World War I, into which Turkey entered on Germany's side.\n",
      "\t Tokenized GPT2:There Ġfollowed Ġthe ĠB alk an ĠWars , Ġin Ġwhich ĠTurkey Ġlost Ġwestern ĠThr ace Ġand ĠM aced onia , Ġthen ĠWorld ĠWar ĠI , Ġinto Ġwhich ĠTurkey Ġentered Ġon ĠGermany 's Ġside .\n",
      "\t Tokenized LLAMA3:There Ġfollowed Ġthe ĠB alk an ĠWars , Ġin Ġwhich ĠTurkey Ġlost Ġwestern ĠThr ace Ġand ĠM aced onia , Ġthen ĠWorld ĠWar ĠI , Ġinto Ġwhich ĠTurkey Ġentered Ġon ĠGermany 's Ġside .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Turkey entered World War I in order to regain territory lost during the Balkan Wars.\n",
      "\t Tokenized GPT2:Tur key Ġentered ĠWorld ĠWar ĠI Ġin Ġorder Ġto Ġregain Ġterritory Ġlost Ġduring Ġthe ĠB alk an ĠWars .\n",
      "\t Tokenized LLAMA3:Tur key Ġentered ĠWorld ĠWar ĠI Ġin Ġorder Ġto Ġregain Ġterritory Ġlost Ġduring Ġthe ĠB alk an ĠWars .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: She had thrown away her cloak and tied her hair back into a topknot to keep it out of the way.\n",
      "\t Tokenized GPT2:She Ġhad Ġthrown Ġaway Ġher Ġcloak Ġand Ġtied Ġher Ġhair Ġback Ġinto Ġa Ġtop k not Ġto Ġkeep Ġit Ġout Ġof Ġthe Ġway .\n",
      "\t Tokenized LLAMA3:She Ġhad Ġthrown Ġaway Ġher Ġcloak Ġand Ġtied Ġher Ġhair Ġback Ġinto Ġa Ġtop k not Ġto Ġkeep Ġit Ġout Ġof Ġthe Ġway .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: She shaved her head.\n",
      "\t Tokenized GPT2:She Ġsh aved Ġher Ġhead .\n",
      "\t Tokenized LLAMA3:She Ġsh aved Ġher Ġhead .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Daniel sat buried by the lights, occasionally pressing things.\n",
      "\t Tokenized GPT2:Daniel Ġsat Ġburied Ġby Ġthe Ġlights , Ġoccasionally Ġpressing Ġthings .\n",
      "\t Tokenized LLAMA3:Daniel Ġsat Ġburied Ġby Ġthe Ġlights , Ġoccasionally Ġpressing Ġthings .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Daniel was covered by lights and he was not standing. \n",
      "\t Tokenized GPT2:Daniel Ġwas Ġcovered Ġby Ġlights Ġand Ġhe Ġwas Ġnot Ġstanding . Ġ\n",
      "\t Tokenized LLAMA3:Daniel Ġwas Ġcovered Ġby Ġlights Ġand Ġhe Ġwas Ġnot Ġstanding . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: You wonder what youre going to be when you grow up, lawyer Smith said. \n",
      "\t Tokenized GPT2:You Ġwonder Ġwhat Ġyoure Ġgoing Ġto Ġbe Ġwhen Ġyou Ġgrow Ġup , Ġlawyer ĠSmith Ġsaid . Ġ\n",
      "\t Tokenized LLAMA3:You Ġwonder Ġwhat Ġyoure Ġgoing Ġto Ġbe Ġwhen Ġyou Ġgrow Ġup , Ġlawyer ĠSmith Ġsaid . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The lawyer, Smith, pointed out that you wanted to know what you would be when you grew up.\n",
      "\t Tokenized GPT2:The Ġlawyer , ĠSmith , Ġpointed Ġout Ġthat Ġyou Ġwanted Ġto Ġknow Ġwhat Ġyou Ġwould Ġbe Ġwhen Ġyou Ġgrew Ġup .\n",
      "\t Tokenized LLAMA3:The Ġlawyer , ĠSmith , Ġpointed Ġout Ġthat Ġyou Ġwanted Ġto Ġknow Ġwhat Ġyou Ġwould Ġbe Ġwhen Ġyou Ġgrew Ġup .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Yes, you've done very well, young man.\n",
      "\t Tokenized GPT2:Yes , Ġyou 've Ġdone Ġvery Ġwell , Ġyoung Ġman .\n",
      "\t Tokenized LLAMA3:Yes , Ġyou 've Ġdone Ġvery Ġwell , Ġyoung Ġman .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Yes, you have a done a great job, young man.\n",
      "\t Tokenized GPT2:Yes , Ġyou Ġhave Ġa Ġdone Ġa Ġgreat Ġjob , Ġyoung Ġman .\n",
      "\t Tokenized LLAMA3:Yes , Ġyou Ġhave Ġa Ġdone Ġa Ġgreat Ġjob , Ġyoung Ġman .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Yet, despite the stock market boom of the 1990s, many households have accumulated little, if any, wealth (see figure 1.3), and half of American households did not own stocks as of 1998.\n",
      "\t Tokenized GPT2:Yet , Ġdespite Ġthe Ġstock Ġmarket Ġboom Ġof Ġthe Ġ1990 s , Ġmany Ġhouseholds Ġhave Ġaccumulated Ġlittle , Ġif Ġany , Ġwealth Ġ( see Ġfigure Ġ1 . 3 ), Ġand Ġhalf Ġof ĠAmerican Ġhouseholds Ġdid Ġnot Ġown Ġstocks Ġas Ġof Ġ1998 .\n",
      "\t Tokenized LLAMA3:Yet , Ġdespite Ġthe Ġstock Ġmarket Ġboom Ġof Ġthe Ġ 199 0 s , Ġmany Ġhouseholds Ġhave Ġaccumulated Ġlittle , Ġif Ġany , Ġwealth Ġ( see Ġfigure Ġ 1 . 3 ), Ġand Ġhalf Ġof ĠAmerican Ġhouseholds Ġdid Ġnot Ġown Ġstocks Ġas Ġof Ġ 199 8 .\n",
      "\t Unique Tokens GPT2: {'Ġ1990', 'Ġ1', 'Ġ1998'}\n",
      "\t Unique Tokens LLAMA3: {'199', 'Ġ', '0', '8', '1'}\n",
      "Text 2: The stock market boom of the 1990s led to explosive wealth accumulation in most households.\n",
      "\t Tokenized GPT2:The Ġstock Ġmarket Ġboom Ġof Ġthe Ġ1990 s Ġled Ġto Ġexplosive Ġwealth Ġaccumulation Ġin Ġmost Ġhouseholds .\n",
      "\t Tokenized LLAMA3:The Ġstock Ġmarket Ġboom Ġof Ġthe Ġ 199 0 s Ġled Ġto Ġexplosive Ġwealth Ġaccumulation Ġin Ġmost Ġhouseholds .\n",
      "\t Unique Tokens GPT2: {'Ġ1990'}\n",
      "\t Unique Tokens LLAMA3: {'199', '0', 'Ġ'}\n",
      "==contradiction==\n",
      "Text 1: and uh well if you if you got got him a power mower it'd probably take him a lot less time to do it but i enjoy doing it i feel good doing it uh i i feel a lot better doing it with a power mower with that with a  with a pull tractor on it so i don't have to push so hard\n",
      "\t Tokenized GPT2:and Ġuh Ġwell Ġif Ġyou Ġif Ġyou Ġgot Ġgot Ġhim Ġa Ġpower Ġm ower Ġit 'd Ġprobably Ġtake Ġhim Ġa Ġlot Ġless Ġtime Ġto Ġdo Ġit Ġbut Ġi Ġenjoy Ġdoing Ġit Ġi Ġfeel Ġgood Ġdoing Ġit Ġuh Ġi Ġi Ġfeel Ġa Ġlot Ġbetter Ġdoing Ġit Ġwith Ġa Ġpower Ġm ower Ġwith Ġthat Ġwith Ġa Ġ Ġwith Ġa Ġpull Ġtr actor Ġon Ġit Ġso Ġi Ġdon 't Ġhave Ġto Ġpush Ġso Ġhard\n",
      "\t Tokenized LLAMA3:and Ġuh Ġwell Ġif Ġyou Ġif Ġyou Ġgot Ġgot Ġhim Ġa Ġpower Ġm ower Ġit 'd Ġprobably Ġtake Ġhim Ġa Ġlot Ġless Ġtime Ġto Ġdo Ġit Ġbut Ġi Ġenjoy Ġdoing Ġit Ġi Ġfeel Ġgood Ġdoing Ġit Ġuh Ġi Ġi Ġfeel Ġa Ġlot Ġbetter Ġdoing Ġit Ġwith Ġa Ġpower Ġm ower Ġwith Ġthat Ġwith Ġa Ġ Ġwith Ġa Ġpull Ġtr actor Ġon Ġit Ġso Ġi Ġdon 't Ġhave Ġto Ġpush Ġso Ġhard\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Keeping the old push mower is the best idea, since it gets the job done as fast as a power mower.\n",
      "\t Tokenized GPT2:Ke eping Ġthe Ġold Ġpush Ġm ower Ġis Ġthe Ġbest Ġidea , Ġsince Ġit Ġgets Ġthe Ġjob Ġdone Ġas Ġfast Ġas Ġa Ġpower Ġm ower .\n",
      "\t Tokenized LLAMA3:Ke eping Ġthe Ġold Ġpush Ġm ower Ġis Ġthe Ġbest Ġidea , Ġsince Ġit Ġgets Ġthe Ġjob Ġdone Ġas Ġfast Ġas Ġa Ġpower Ġm ower .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: We know they will have to come from the south but that gives them a space as wide as the town in which to launch their attack.\n",
      "\t Tokenized GPT2:We Ġknow Ġthey Ġwill Ġhave Ġto Ġcome Ġfrom Ġthe Ġsouth Ġbut Ġthat Ġgives Ġthem Ġa Ġspace Ġas Ġwide Ġas Ġthe Ġtown Ġin Ġwhich Ġto Ġlaunch Ġtheir Ġattack .\n",
      "\t Tokenized LLAMA3:We Ġknow Ġthey Ġwill Ġhave Ġto Ġcome Ġfrom Ġthe Ġsouth Ġbut Ġthat Ġgives Ġthem Ġa Ġspace Ġas Ġwide Ġas Ġthe Ġtown Ġin Ġwhich Ġto Ġlaunch Ġtheir Ġattack .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The people will come from the south with lots of weapons.\n",
      "\t Tokenized GPT2:The Ġpeople Ġwill Ġcome Ġfrom Ġthe Ġsouth Ġwith Ġlots Ġof Ġweapons .\n",
      "\t Tokenized LLAMA3:The Ġpeople Ġwill Ġcome Ġfrom Ġthe Ġsouth Ġwith Ġlots Ġof Ġweapons .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: In 1995 and again in 1998, the Legal Services Corporation recognized that legal services programs were going to have to change the method and manner in which they conducted their business if they were going to remain viable and responsive to the needs of low income persons.\n",
      "\t Tokenized GPT2:In Ġ1995 Ġand Ġagain Ġin Ġ1998 , Ġthe ĠLe gal ĠServices ĠCorporation Ġrecognized Ġthat Ġlegal Ġservices Ġprograms Ġwere Ġgoing Ġto Ġhave Ġto Ġchange Ġthe Ġmethod Ġand Ġmanner Ġin Ġwhich Ġthey Ġconducted Ġtheir Ġbusiness Ġif Ġthey Ġwere Ġgoing Ġto Ġremain Ġviable Ġand Ġresponsive Ġto Ġthe Ġneeds Ġof Ġlow Ġincome Ġpersons .\n",
      "\t Tokenized LLAMA3:In Ġ 199 5 Ġand Ġagain Ġin Ġ 199 8 , Ġthe ĠLe gal ĠServices ĠCorporation Ġrecognized Ġthat Ġlegal Ġservices Ġprograms Ġwere Ġgoing Ġto Ġhave Ġto Ġchange Ġthe Ġmethod Ġand Ġmanner Ġin Ġwhich Ġthey Ġconducted Ġtheir Ġbusiness Ġif Ġthey Ġwere Ġgoing Ġto Ġremain Ġviable Ġand Ġresponsive Ġto Ġthe Ġneeds Ġof Ġlow Ġincome Ġpersons .\n",
      "\t Unique Tokens GPT2: {'Ġ1995', 'Ġ1998'}\n",
      "\t Unique Tokens LLAMA3: {'199', '5', 'Ġ', '8'}\n",
      "Text 2: The Legal Services Corporation realized things had to change more than once in the past.\n",
      "\t Tokenized GPT2:The ĠLe gal ĠServices ĠCorporation Ġrealized Ġthings Ġhad Ġto Ġchange Ġmore Ġthan Ġonce Ġin Ġthe Ġpast .\n",
      "\t Tokenized LLAMA3:The ĠLe gal ĠServices ĠCorporation Ġrealized Ġthings Ġhad Ġto Ġchange Ġmore Ġthan Ġonce Ġin Ġthe Ġpast .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: By seeding packs with a few high-value cards, the manufacturer is encouraging kids to buy Pokemon cards like lottery tickets.\n",
      "\t Tokenized GPT2:By Ġse eding Ġpacks Ġwith Ġa Ġfew Ġhigh - value Ġcards , Ġthe Ġmanufacturer Ġis Ġencouraging Ġkids Ġto Ġbuy ĠPokemon Ġcards Ġlike Ġlottery Ġtickets .\n",
      "\t Tokenized LLAMA3:By Ġse eding Ġpacks Ġwith Ġa Ġfew Ġhigh -value Ġcards , Ġthe Ġmanufacturer Ġis Ġencouraging Ġkids Ġto Ġbuy ĠPokemon Ġcards Ġlike Ġlottery Ġtickets .\n",
      "\t Unique Tokens GPT2: {'-', 'value'}\n",
      "\t Unique Tokens LLAMA3: {'-value'}\n",
      "Text 2: Each Pokemon card pack is filled with every rare card a kid could want.\n",
      "\t Tokenized GPT2:Each ĠPokemon Ġcard Ġpack Ġis Ġfilled Ġwith Ġevery Ġrare Ġcard Ġa Ġkid Ġcould Ġwant .\n",
      "\t Tokenized LLAMA3:Each ĠPokemon Ġcard Ġpack Ġis Ġfilled Ġwith Ġevery Ġrare Ġcard Ġa Ġkid Ġcould Ġwant .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Citing conservative critics of Brown vs.\n",
      "\t Tokenized GPT2:C iting Ġconservative Ġcritics Ġof ĠBrown Ġvs .\n",
      "\t Tokenized LLAMA3:C iting Ġconservative Ġcritics Ġof ĠBrown Ġvs .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Conservative critics wrote about the Brown case.  \n",
      "\t Tokenized GPT2:Cons erv ative Ġcritics Ġwrote Ġabout Ġthe ĠBrown Ġcase . ĠĠ\n",
      "\t Tokenized LLAMA3:Cons erv ative Ġcritics Ġwrote Ġabout Ġthe ĠBrown Ġcase . ĠĠ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: You are sure that you did not in any way disclose your identity?\" Tommy shook his head.\n",
      "\t Tokenized GPT2:You Ġare Ġsure Ġthat Ġyou Ġdid Ġnot Ġin Ġany Ġway Ġdisclose Ġyour Ġidentity ?\" ĠTommy Ġshook Ġhis Ġhead .\n",
      "\t Tokenized LLAMA3:You Ġare Ġsure Ġthat Ġyou Ġdid Ġnot Ġin Ġany Ġway Ġdisclose Ġyour Ġidentity ?\" ĠTommy Ġshook Ġhis Ġhead .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: You are sure that you did not in any way disclose that your last name is Smith? \n",
      "\t Tokenized GPT2:You Ġare Ġsure Ġthat Ġyou Ġdid Ġnot Ġin Ġany Ġway Ġdisclose Ġthat Ġyour Ġlast Ġname Ġis ĠSmith ? Ġ\n",
      "\t Tokenized LLAMA3:You Ġare Ġsure Ġthat Ġyou Ġdid Ġnot Ġin Ġany Ġway Ġdisclose Ġthat Ġyour Ġlast Ġname Ġis ĠSmith ? Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: In the first instance, IRS would have no record of time before the person could get through to an agent and of discouraged callers.\n",
      "\t Tokenized GPT2:In Ġthe Ġfirst Ġinstance , ĠIRS Ġwould Ġhave Ġno Ġrecord Ġof Ġtime Ġbefore Ġthe Ġperson Ġcould Ġget Ġthrough Ġto Ġan Ġagent Ġand Ġof Ġdiscouraged Ġcall ers .\n",
      "\t Tokenized LLAMA3:In Ġthe Ġfirst Ġinstance , ĠIRS Ġwould Ġhave Ġno Ġrecord Ġof Ġtime Ġbefore Ġthe Ġperson Ġcould Ġget Ġthrough Ġto Ġan Ġagent Ġand Ġof Ġdiscouraged Ġcall ers .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: There is no recording of the time for callers.\n",
      "\t Tokenized GPT2:There Ġis Ġno Ġrecording Ġof Ġthe Ġtime Ġfor Ġcall ers .\n",
      "\t Tokenized LLAMA3:There Ġis Ġno Ġrecording Ġof Ġthe Ġtime Ġfor Ġcall ers .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: A profile crowns Chris Rock The Funniest Man in America.\n",
      "\t Tokenized GPT2:A Ġprofile Ġcrown s ĠChris ĠRock ĠThe ĠFun n iest ĠMan Ġin ĠAmerica .\n",
      "\t Tokenized LLAMA3:A Ġprofile Ġcrown s ĠChris ĠRock ĠThe ĠFun n iest ĠMan Ġin ĠAmerica .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Chris Rock has been crowned The Funniest Man in America.\n",
      "\t Tokenized GPT2:Chris ĠRock Ġhas Ġbeen Ġcr owned ĠThe ĠFun n iest ĠMan Ġin ĠAmerica .\n",
      "\t Tokenized LLAMA3:Chris ĠRock Ġhas Ġbeen Ġcr owned ĠThe ĠFun n iest ĠMan Ġin ĠAmerica .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: She wears either revealing clothes or professional clothes (or perhaps both).\n",
      "\t Tokenized GPT2:She Ġwears Ġeither Ġrevealing Ġclothes Ġor Ġprofessional Ġclothes Ġ( or Ġperhaps Ġboth ).\n",
      "\t Tokenized LLAMA3:She Ġwears Ġeither Ġrevealing Ġclothes Ġor Ġprofessional Ġclothes Ġ( or Ġperhaps Ġboth ).\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Her clothes are either provocative or conservative.\n",
      "\t Tokenized GPT2:Her Ġclothes Ġare Ġeither Ġprov oc ative Ġor Ġconservative .\n",
      "\t Tokenized LLAMA3:Her Ġclothes Ġare Ġeither Ġprov oc ative Ġor Ġconservative .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: More than half of 800,000 native islanders are children, and the mother is traditionally responsible for bringing them up, handling the money, and making key domestic decisions.\n",
      "\t Tokenized GPT2:More Ġthan Ġhalf Ġof Ġ800 , 000 Ġnative Ġisland ers Ġare Ġchildren , Ġand Ġthe Ġmother Ġis Ġtraditionally Ġresponsible Ġfor Ġbringing Ġthem Ġup , Ġhandling Ġthe Ġmoney , Ġand Ġmaking Ġkey Ġdomestic Ġdecisions .\n",
      "\t Tokenized LLAMA3:More Ġthan Ġhalf Ġof Ġ 800 , 000 Ġnative Ġisland ers Ġare Ġchildren , Ġand Ġthe Ġmother Ġis Ġtraditionally Ġresponsible Ġfor Ġbringing Ġthem Ġup , Ġhandling Ġthe Ġmoney , Ġand Ġmaking Ġkey Ġdomestic Ġdecisions .\n",
      "\t Unique Tokens GPT2: {'Ġ800'}\n",
      "\t Unique Tokens LLAMA3: {'800', 'Ġ'}\n",
      "Text 2: The mother is responsible for the raising of the native islander children.\n",
      "\t Tokenized GPT2:The Ġmother Ġis Ġresponsible Ġfor Ġthe Ġraising Ġof Ġthe Ġnative Ġisland er Ġchildren .\n",
      "\t Tokenized LLAMA3:The Ġmother Ġis Ġresponsible Ġfor Ġthe Ġraising Ġof Ġthe Ġnative Ġisland er Ġchildren .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: right well the warmth that developed between them and again it i think was a picture of relationships\n",
      "\t Tokenized GPT2:right Ġwell Ġthe Ġwarmth Ġthat Ġdeveloped Ġbetween Ġthem Ġand Ġagain Ġit Ġi Ġthink Ġwas Ġa Ġpicture Ġof Ġrelationships\n",
      "\t Tokenized LLAMA3:right Ġwell Ġthe Ġwarmth Ġthat Ġdeveloped Ġbetween Ġthem Ġand Ġagain Ġit Ġi Ġthink Ġwas Ġa Ġpicture Ġof Ġrelationships\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: There was a mutual feeling in their relationship.\n",
      "\t Tokenized GPT2:There Ġwas Ġa Ġmutual Ġfeeling Ġin Ġtheir Ġrelationship .\n",
      "\t Tokenized LLAMA3:There Ġwas Ġa Ġmutual Ġfeeling Ġin Ġtheir Ġrelationship .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: He reverted to his former point of view.\n",
      "\t Tokenized GPT2:He Ġreverted Ġto Ġhis Ġformer Ġpoint Ġof Ġview .\n",
      "\t Tokenized LLAMA3:He Ġreverted Ġto Ġhis Ġformer Ġpoint Ġof Ġview .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He went back to his previous thoughts.\n",
      "\t Tokenized GPT2:He Ġwent Ġback Ġto Ġhis Ġprevious Ġthoughts .\n",
      "\t Tokenized LLAMA3:He Ġwent Ġback Ġto Ġhis Ġprevious Ġthoughts .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: For the next two centuries Aelia Capitolina enjoyed an innocuous history.\n",
      "\t Tokenized GPT2:For Ġthe Ġnext Ġtwo Ġcenturies ĠA elia ĠCapitol ina Ġenjoyed Ġan Ġinnoc uous Ġhistory .\n",
      "\t Tokenized LLAMA3:For Ġthe Ġnext Ġtwo Ġcenturies ĠA elia ĠCapitol ina Ġenjoyed Ġan Ġinnoc uous Ġhistory .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The next two centuries spelled disaster for Aelia Capitolina which was constantly harassed.\n",
      "\t Tokenized GPT2:The Ġnext Ġtwo Ġcenturies Ġspell ed Ġdisaster Ġfor ĠA elia ĠCapitol ina Ġwhich Ġwas Ġconstantly Ġharass ed .\n",
      "\t Tokenized LLAMA3:The Ġnext Ġtwo Ġcenturies Ġspell ed Ġdisaster Ġfor ĠA elia ĠCapitol ina Ġwhich Ġwas Ġconstantly Ġharass ed .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: In the meantime we must send for a doctor, but before we do so, is there anything in this room that might be of value to us?\" Hastily, the three searched.\n",
      "\t Tokenized GPT2:In Ġthe Ġmeantime Ġwe Ġmust Ġsend Ġfor Ġa Ġdoctor , Ġbut Ġbefore Ġwe Ġdo Ġso , Ġis Ġthere Ġanything Ġin Ġthis Ġroom Ġthat Ġmight Ġbe Ġof Ġvalue Ġto Ġus ?\" ĠH ast ily , Ġthe Ġthree Ġsearched .\n",
      "\t Tokenized LLAMA3:In Ġthe Ġmeantime Ġwe Ġmust Ġsend Ġfor Ġa Ġdoctor , Ġbut Ġbefore Ġwe Ġdo Ġso , Ġis Ġthere Ġanything Ġin Ġthis Ġroom Ġthat Ġmight Ġbe Ġof Ġvalue Ġto Ġus ?\" ĠH ast ily , Ġthe Ġthree Ġsearched .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The three searched for valuable items before sending for a doctor.\n",
      "\t Tokenized GPT2:The Ġthree Ġsearched Ġfor Ġvaluable Ġitems Ġbefore Ġsending Ġfor Ġa Ġdoctor .\n",
      "\t Tokenized LLAMA3:The Ġthree Ġsearched Ġfor Ġvaluable Ġitems Ġbefore Ġsending Ġfor Ġa Ġdoctor .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: saving that did not finance domestic investment would increase net foreign investment and improve the current account balance.\n",
      "\t Tokenized GPT2:s aving Ġthat Ġdid Ġnot Ġfinance Ġdomestic Ġinvestment Ġwould Ġincrease Ġnet Ġforeign Ġinvestment Ġand Ġimprove Ġthe Ġcurrent Ġaccount Ġbalance .\n",
      "\t Tokenized LLAMA3:s aving Ġthat Ġdid Ġnot Ġfinance Ġdomestic Ġinvestment Ġwould Ġincrease Ġnet Ġforeign Ġinvestment Ġand Ġimprove Ġthe Ġcurrent Ġaccount Ġbalance .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Saving could increase net foreign investment  substantially and quickly. \n",
      "\t Tokenized GPT2:S aving Ġcould Ġincrease Ġnet Ġforeign Ġinvestment Ġ Ġsubstantially Ġand Ġquickly . Ġ\n",
      "\t Tokenized LLAMA3:S aving Ġcould Ġincrease Ġnet Ġforeign Ġinvestment Ġ Ġsubstantially Ġand Ġquickly . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Tourist Information offices can be very helpful.\n",
      "\t Tokenized GPT2:T our ist ĠInformation Ġoffices Ġcan Ġbe Ġvery Ġhelpful .\n",
      "\t Tokenized LLAMA3:T our ist ĠInformation Ġoffices Ġcan Ġbe Ġvery Ġhelpful .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: One can often get help at Tourist Information offices.\n",
      "\t Tokenized GPT2:One Ġcan Ġoften Ġget Ġhelp Ġat ĠTour ist ĠInformation Ġoffices .\n",
      "\t Tokenized LLAMA3:One Ġcan Ġoften Ġget Ġhelp Ġat ĠTour ist ĠInformation Ġoffices .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: so i how do you feel that it should be applied\n",
      "\t Tokenized GPT2:so Ġi Ġhow Ġdo Ġyou Ġfeel Ġthat Ġit Ġshould Ġbe Ġapplied\n",
      "\t Tokenized LLAMA3:so Ġi Ġhow Ġdo Ġyou Ġfeel Ġthat Ġit Ġshould Ġbe Ġapplied\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: With application how do you think it should be done?\n",
      "\t Tokenized GPT2:With Ġapplication Ġhow Ġdo Ġyou Ġthink Ġit Ġshould Ġbe Ġdone ?\n",
      "\t Tokenized LLAMA3:With Ġapplication Ġhow Ġdo Ġyou Ġthink Ġit Ġshould Ġbe Ġdone ?\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Robust  came in third among words and phrases submitted (220 citations in the CR ), and unlike the previous two, it seems to be a genuinely new cliche; at any rate, Chatterbox hadn't previously been aware of its overuse.\n",
      "\t Tokenized GPT2:Rob ust Ġ Ġcame Ġin Ġthird Ġamong Ġwords Ġand Ġphrases Ġsubmitted Ġ( 220 Ġcit ations Ġin Ġthe ĠCR Ġ), Ġand Ġunlike Ġthe Ġprevious Ġtwo , Ġit Ġseems Ġto Ġbe Ġa Ġgenuinely Ġnew Ġcliche ; Ġat Ġany Ġrate , ĠChat ter box Ġhadn 't Ġpreviously Ġbeen Ġaware Ġof Ġits Ġover use .\n",
      "\t Tokenized LLAMA3:Rob ust Ġ Ġcame Ġin Ġthird Ġamong Ġwords Ġand Ġphrases Ġsubmitted Ġ( 220 Ġcit ations Ġin Ġthe ĠCR Ġ), Ġand Ġunlike Ġthe Ġprevious Ġtwo , Ġit Ġseems Ġto Ġbe Ġa Ġgenuinely Ġnew Ġcl iche ; Ġat Ġany Ġrate , ĠChat ter box Ġhadn 't Ġpreviously Ġbeen Ġaware Ġof Ġits Ġover use .\n",
      "\t Unique Tokens GPT2: {'Ġcliche'}\n",
      "\t Unique Tokens LLAMA3: {'iche', 'Ġcl'}\n",
      "Text 2: Robust came in last place among the submitted words and phrases.\n",
      "\t Tokenized GPT2:Rob ust Ġcame Ġin Ġlast Ġplace Ġamong Ġthe Ġsubmitted Ġwords Ġand Ġphrases .\n",
      "\t Tokenized LLAMA3:Rob ust Ġcame Ġin Ġlast Ġplace Ġamong Ġthe Ġsubmitted Ġwords Ġand Ġphrases .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: This time around, Lloyd believes he's the Messiah.\n",
      "\t Tokenized GPT2:This Ġtime Ġaround , ĠLl oyd Ġbelieves Ġhe 's Ġthe ĠMess iah .\n",
      "\t Tokenized LLAMA3:This Ġtime Ġaround , ĠLl oyd Ġbelieves Ġhe 's Ġthe ĠMess iah .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: This time, Lloyd believes he is a space ship travelling through space. \n",
      "\t Tokenized GPT2:This Ġtime , ĠLl oyd Ġbelieves Ġhe Ġis Ġa Ġspace Ġship Ġtravelling Ġthrough Ġspace . Ġ\n",
      "\t Tokenized LLAMA3:This Ġtime , ĠLl oyd Ġbelieves Ġhe Ġis Ġa Ġspace Ġship Ġtravelling Ġthrough Ġspace . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: it's the very same type of paint and everything\n",
      "\t Tokenized GPT2:it 's Ġthe Ġvery Ġsame Ġtype Ġof Ġpaint Ġand Ġeverything\n",
      "\t Tokenized LLAMA3:it 's Ġthe Ġvery Ġsame Ġtype Ġof Ġpaint Ġand Ġeverything\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It's the same paint formula, it's great!\n",
      "\t Tokenized GPT2:It 's Ġthe Ġsame Ġpaint Ġformula , Ġit 's Ġgreat !\n",
      "\t Tokenized LLAMA3:It 's Ġthe Ġsame Ġpaint Ġformula , Ġit 's Ġgreat !\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: They were so sure of themselves that they took it for granted he had made a mistake.\"\n",
      "\t Tokenized GPT2:They Ġwere Ġso Ġsure Ġof Ġthemselves Ġthat Ġthey Ġtook Ġit Ġfor Ġgranted Ġhe Ġhad Ġmade Ġa Ġmistake .\"\n",
      "\t Tokenized LLAMA3:They Ġwere Ġso Ġsure Ġof Ġthemselves Ġthat Ġthey Ġtook Ġit Ġfor Ġgranted Ġhe Ġhad Ġmade Ġa Ġmistake .\"\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: They were unconfident so they were hesitant to think he might have made a mistake.\n",
      "\t Tokenized GPT2:They Ġwere Ġun conf ident Ġso Ġthey Ġwere Ġhesitant Ġto Ġthink Ġhe Ġmight Ġhave Ġmade Ġa Ġmistake .\n",
      "\t Tokenized LLAMA3:They Ġwere Ġun conf ident Ġso Ġthey Ġwere Ġhesitant Ġto Ġthink Ġhe Ġmight Ġhave Ġmade Ġa Ġmistake .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: It's a great novelty, but very expensive.\n",
      "\t Tokenized GPT2:It 's Ġa Ġgreat Ġnovel ty , Ġbut Ġvery Ġexpensive .\n",
      "\t Tokenized LLAMA3:It 's Ġa Ġgreat Ġnovel ty , Ġbut Ġvery Ġexpensive .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The novelty comes at a large price.\n",
      "\t Tokenized GPT2:The Ġnovel ty Ġcomes Ġat Ġa Ġlarge Ġprice .\n",
      "\t Tokenized LLAMA3:The Ġnovel ty Ġcomes Ġat Ġa Ġlarge Ġprice .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Should we invite these young wealthies back to our comparatively humble, small home?\n",
      "\t Tokenized GPT2:Should Ġwe Ġinvite Ġthese Ġyoung Ġwealth ies Ġback Ġto Ġour Ġcompar atively Ġhumble , Ġsmall Ġhome ?\n",
      "\t Tokenized LLAMA3:Should Ġwe Ġinvite Ġthese Ġyoung Ġwealth ies Ġback Ġto Ġour Ġcompar atively Ġhumble , Ġsmall Ġhome ?\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Our home is full of love and pets. \n",
      "\t Tokenized GPT2:Our Ġhome Ġis Ġfull Ġof Ġlove Ġand Ġpets . Ġ\n",
      "\t Tokenized LLAMA3:Our Ġhome Ġis Ġfull Ġof Ġlove Ġand Ġpets . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Randy's Anecdotal Wrap-Up\n",
      "\t Tokenized GPT2:R andy 's ĠA ne cd otal ĠW rap - Up\n",
      "\t Tokenized LLAMA3:R andy 's ĠA ne cd otal ĠW rap - Up\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Randy's Introduction\n",
      "\t Tokenized GPT2:R andy 's ĠIntroduction\n",
      "\t Tokenized LLAMA3:R andy 's ĠIntroduction\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: It is also sometimes called simply Beaubourg, after the 13th-century neighborhood that surrounds it.\n",
      "\t Tokenized GPT2:It Ġis Ġalso Ġsometimes Ġcalled Ġsimply ĠBe au bour g , Ġafter Ġthe Ġ13 th - century Ġneighborhood Ġthat Ġsur r ounds Ġit .\n",
      "\t Tokenized LLAMA3:It Ġis Ġalso Ġsometimes Ġcalled Ġsimply ĠBe au bour g , Ġafter Ġthe Ġ 13 th -century Ġneighborhood Ġthat Ġsur r ounds Ġit .\n",
      "\t Unique Tokens GPT2: {'-', 'century', 'Ġ13'}\n",
      "\t Unique Tokens LLAMA3: {'13', '-century', 'Ġ'}\n",
      "Text 2: It is usually referred to as Little Paris because of the many French immigrants.\n",
      "\t Tokenized GPT2:It Ġis Ġusually Ġreferred Ġto Ġas ĠLittle ĠParis Ġbecause Ġof Ġthe Ġmany ĠFrench Ġimmigrants .\n",
      "\t Tokenized LLAMA3:It Ġis Ġusually Ġreferred Ġto Ġas ĠLittle ĠParis Ġbecause Ġof Ġthe Ġmany ĠFrench Ġimmigrants .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: There is a roller coaster up there as well, but experienced riders consider it too slow and uneventful despite the altitude.\n",
      "\t Tokenized GPT2:There Ġis Ġa Ġroller Ġco aster Ġup Ġthere Ġas Ġwell , Ġbut Ġexperienced Ġriders Ġconsider Ġit Ġtoo Ġslow Ġand Ġun event ful Ġdespite Ġthe Ġaltitude .\n",
      "\t Tokenized LLAMA3:There Ġis Ġa Ġroller Ġco aster Ġup Ġthere Ġas Ġwell , Ġbut Ġexperienced Ġriders Ġconsider Ġit Ġtoo Ġslow Ġand Ġun event ful Ġdespite Ġthe Ġaltitude .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Experienced riders think that the roller coaster is too fast and scary.\n",
      "\t Tokenized GPT2:Exper ienced Ġriders Ġthink Ġthat Ġthe Ġroller Ġco aster Ġis Ġtoo Ġfast Ġand Ġscary .\n",
      "\t Tokenized LLAMA3:Exper ienced Ġriders Ġthink Ġthat Ġthe Ġroller Ġco aster Ġis Ġtoo Ġfast Ġand Ġscary .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Ricky Martin was filming his triumphant return to the gay porn industry.\n",
      "\t Tokenized GPT2:R icky ĠMartin Ġwas Ġfilming Ġhis Ġtriumph ant Ġreturn Ġto Ġthe Ġgay Ġporn Ġindustry .\n",
      "\t Tokenized LLAMA3:R icky ĠMartin Ġwas Ġfilming Ġhis Ġtriumph ant Ġreturn Ġto Ġthe Ġgay Ġporn Ġindustry .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Ricky Martin is heterosexual.\n",
      "\t Tokenized GPT2:R icky ĠMartin Ġis Ġheter osex ual .\n",
      "\t Tokenized LLAMA3:R icky ĠMartin Ġis Ġheter osex ual .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: away from the children\n",
      "\t Tokenized GPT2:away Ġfrom Ġthe Ġchildren\n",
      "\t Tokenized LLAMA3:away Ġfrom Ġthe Ġchildren\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: No adults allowed near the children\n",
      "\t Tokenized GPT2:No Ġadults Ġallowed Ġnear Ġthe Ġchildren\n",
      "\t Tokenized LLAMA3:No Ġadults Ġallowed Ġnear Ġthe Ġchildren\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: It spoke of thousands of years, even before the times of the old empire.\n",
      "\t Tokenized GPT2:It Ġspoke Ġof Ġthousands Ġof Ġyears , Ġeven Ġbefore Ġthe Ġtimes Ġof Ġthe Ġold Ġempire .\n",
      "\t Tokenized LLAMA3:It Ġspoke Ġof Ġthousands Ġof Ġyears , Ġeven Ġbefore Ġthe Ġtimes Ġof Ġthe Ġold Ġempire .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The old Empire died out.\n",
      "\t Tokenized GPT2:The Ġold ĠEmpire Ġdied Ġout .\n",
      "\t Tokenized LLAMA3:The Ġold ĠEmpire Ġdied Ġout .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: I had an additional reason for that belief in the fact that all the cups found contained sugar, which Mademoiselle Cynthia never took in her coffee. \n",
      "\t Tokenized GPT2:I Ġhad Ġan Ġadditional Ġreason Ġfor Ġthat Ġbelief Ġin Ġthe Ġfact Ġthat Ġall Ġthe Ġcups Ġfound Ġcontained Ġsugar , Ġwhich ĠMad em ois elle ĠC ynth ia Ġnever Ġtook Ġin Ġher Ġcoffee . Ġ\n",
      "\t Tokenized LLAMA3:I Ġhad Ġan Ġadditional Ġreason Ġfor Ġthat Ġbelief Ġin Ġthe Ġfact Ġthat Ġall Ġthe Ġcups Ġfound Ġcontained Ġsugar , Ġwhich ĠMad emo is elle ĠC ynth ia Ġnever Ġtook Ġin Ġher Ġcoffee . Ġ\n",
      "\t Unique Tokens GPT2: {'ois', 'em'}\n",
      "\t Unique Tokens LLAMA3: {'is', 'emo'}\n",
      "Text 2: There was evidence that there had been sugar in all of the cups.\n",
      "\t Tokenized GPT2:There Ġwas Ġevidence Ġthat Ġthere Ġhad Ġbeen Ġsugar Ġin Ġall Ġof Ġthe Ġcups .\n",
      "\t Tokenized LLAMA3:There Ġwas Ġevidence Ġthat Ġthere Ġhad Ġbeen Ġsugar Ġin Ġall Ġof Ġthe Ġcups .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: The m??tro (subway) is the fastest way to move around the city, but the buses, both in the capital and the other big towns, are best for taking in the sights.\n",
      "\t Tokenized GPT2:The Ġm ?? t ro Ġ( sub way ) Ġis Ġthe Ġfastest Ġway Ġto Ġmove Ġaround Ġthe Ġcity , Ġbut Ġthe Ġbuses , Ġboth Ġin Ġthe Ġcapital Ġand Ġthe Ġother Ġbig Ġtowns , Ġare Ġbest Ġfor Ġtaking Ġin Ġthe Ġsights .\n",
      "\t Tokenized LLAMA3:The Ġm ?? t ro Ġ( sub way ) Ġis Ġthe Ġfastest Ġway Ġto Ġmove Ġaround Ġthe Ġcity , Ġbut Ġthe Ġbuses , Ġboth Ġin Ġthe Ġcapital Ġand Ġthe Ġother Ġbig Ġtowns , Ġare Ġbest Ġfor Ġtaking Ġin Ġthe Ġsights .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: If you'd like to experience the city sights taking the bus is the best mode of transportation, though taking the subway is faster. \n",
      "\t Tokenized GPT2:If Ġyou 'd Ġlike Ġto Ġexperience Ġthe Ġcity Ġsights Ġtaking Ġthe Ġbus Ġis Ġthe Ġbest Ġmode Ġof Ġtransportation , Ġthough Ġtaking Ġthe Ġsubway Ġis Ġfaster . Ġ\n",
      "\t Tokenized LLAMA3:If Ġyou 'd Ġlike Ġto Ġexperience Ġthe Ġcity Ġsights Ġtaking Ġthe Ġbus Ġis Ġthe Ġbest Ġmode Ġof Ġtransportation , Ġthough Ġtaking Ġthe Ġsubway Ġis Ġfaster . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: You wonder whether he could win a general election coming out of the right lane of the Democratic Party.\n",
      "\t Tokenized GPT2:You Ġwonder Ġwhether Ġhe Ġcould Ġwin Ġa Ġgeneral Ġelection Ġcoming Ġout Ġof Ġthe Ġright Ġlane Ġof Ġthe ĠDemocratic ĠParty .\n",
      "\t Tokenized LLAMA3:You Ġwonder Ġwhether Ġhe Ġcould Ġwin Ġa Ġgeneral Ġelection Ġcoming Ġout Ġof Ġthe Ġright Ġlane Ġof Ġthe ĠDemocratic ĠParty .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He might run in a general election while he is a conservative Democrat.\n",
      "\t Tokenized GPT2:He Ġmight Ġrun Ġin Ġa Ġgeneral Ġelection Ġwhile Ġhe Ġis Ġa Ġconservative ĠDemocrat .\n",
      "\t Tokenized LLAMA3:He Ġmight Ġrun Ġin Ġa Ġgeneral Ġelection Ġwhile Ġhe Ġis Ġa Ġconservative ĠDemocrat .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Kom Ombo is an unusual temple in that it is dedicated to two gods.\n",
      "\t Tokenized GPT2:K om ĠO mb o Ġis Ġan Ġunusual Ġtemple Ġin Ġthat Ġit Ġis Ġdedicated Ġto Ġtwo Ġgods .\n",
      "\t Tokenized LLAMA3:K om ĠO mb o Ġis Ġan Ġunusual Ġtemple Ġin Ġthat Ġit Ġis Ġdedicated Ġto Ġtwo Ġgods .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Standard in every way, Kom Ombo is a temple devoted to several deities. \n",
      "\t Tokenized GPT2:Standard Ġin Ġevery Ġway , ĠK om ĠO mb o Ġis Ġa Ġtemple Ġdevoted Ġto Ġseveral Ġde ities . Ġ\n",
      "\t Tokenized LLAMA3:Standard Ġin Ġevery Ġway , ĠK om ĠO mb o Ġis Ġa Ġtemple Ġdevoted Ġto Ġseveral Ġde ities . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: I will practice The Look on old French ladies who are happy to have any old look at all, I say, and then, as I get the hang of it, move gradually into the big leagues.\n",
      "\t Tokenized GPT2:I Ġwill Ġpractice ĠThe ĠLook Ġon Ġold ĠFrench Ġladies Ġwho Ġare Ġhappy Ġto Ġhave Ġany Ġold Ġlook Ġat Ġall , ĠI Ġsay , Ġand Ġthen , Ġas ĠI Ġget Ġthe Ġhang Ġof Ġit , Ġmove Ġgradually Ġinto Ġthe Ġbig Ġleagues .\n",
      "\t Tokenized LLAMA3:I Ġwill Ġpractice ĠThe ĠLook Ġon Ġold ĠFrench Ġladies Ġwho Ġare Ġhappy Ġto Ġhave Ġany Ġold Ġlook Ġat Ġall , ĠI Ġsay , Ġand Ġthen , Ġas ĠI Ġget Ġthe Ġhang Ġof Ġit , Ġmove Ġgradually Ġinto Ġthe Ġbig Ġleagues .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I will practice the look on older French women.\n",
      "\t Tokenized GPT2:I Ġwill Ġpractice Ġthe Ġlook Ġon Ġolder ĠFrench Ġwomen .\n",
      "\t Tokenized LLAMA3:I Ġwill Ġpractice Ġthe Ġlook Ġon Ġolder ĠFrench Ġwomen .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: It hopes to bring on another 25 or 35 people when the new building opens next fall.\n",
      "\t Tokenized GPT2:It Ġhopes Ġto Ġbring Ġon Ġanother Ġ25 Ġor Ġ35 Ġpeople Ġwhen Ġthe Ġnew Ġbuilding Ġopens Ġnext Ġfall .\n",
      "\t Tokenized LLAMA3:It Ġhopes Ġto Ġbring Ġon Ġanother Ġ 25 Ġor Ġ 35 Ġpeople Ġwhen Ġthe Ġnew Ġbuilding Ġopens Ġnext Ġfall .\n",
      "\t Unique Tokens GPT2: {'Ġ35', 'Ġ25'}\n",
      "\t Unique Tokens LLAMA3: {'25', '35', 'Ġ'}\n",
      "Text 2: They already have a waiting list for the new building.\n",
      "\t Tokenized GPT2:They Ġalready Ġhave Ġa Ġwaiting Ġlist Ġfor Ġthe Ġnew Ġbuilding .\n",
      "\t Tokenized LLAMA3:They Ġalready Ġhave Ġa Ġwaiting Ġlist Ġfor Ġthe Ġnew Ġbuilding .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: well Jerry do you have a favorite team\n",
      "\t Tokenized GPT2:well ĠJerry Ġdo Ġyou Ġhave Ġa Ġfavorite Ġteam\n",
      "\t Tokenized LLAMA3:well ĠJerry Ġdo Ġyou Ġhave Ġa Ġfavorite Ġteam\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Jerry, do you follow any sports?\n",
      "\t Tokenized GPT2:J erry , Ġdo Ġyou Ġfollow Ġany Ġsports ?\n",
      "\t Tokenized LLAMA3:J erry , Ġdo Ġyou Ġfollow Ġany Ġsports ?\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Their rights have been the source of conflicts in the central government.\n",
      "\t Tokenized GPT2:Their Ġrights Ġhave Ġbeen Ġthe Ġsource Ġof Ġconflicts Ġin Ġthe Ġcentral Ġgovernment .\n",
      "\t Tokenized LLAMA3:Their Ġrights Ġhave Ġbeen Ġthe Ġsource Ġof Ġconflicts Ġin Ġthe Ġcentral Ġgovernment .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Their rights have been an area of turmoil within the central government.\n",
      "\t Tokenized GPT2:Their Ġrights Ġhave Ġbeen Ġan Ġarea Ġof Ġturmoil Ġwithin Ġthe Ġcentral Ġgovernment .\n",
      "\t Tokenized LLAMA3:Their Ġrights Ġhave Ġbeen Ġan Ġarea Ġof Ġtur m oil Ġwithin Ġthe Ġcentral Ġgovernment .\n",
      "\t Unique Tokens GPT2: {'Ġturmoil'}\n",
      "\t Unique Tokens LLAMA3: {'m', 'Ġtur', 'oil'}\n",
      "==contradiction==\n",
      "Text 1: Any point you failed to win by rigging the questions and categories can be cleaned up in the executive summary (the pollster's spin) and the press release and news conference (the client's spin on the pollster's spin).\n",
      "\t Tokenized GPT2:Any Ġpoint Ġyou Ġfailed Ġto Ġwin Ġby Ġrig ging Ġthe Ġquestions Ġand Ġcategories Ġcan Ġbe Ġcleaned Ġup Ġin Ġthe Ġexecutive Ġsummary Ġ( the Ġpoll ster 's Ġspin ) Ġand Ġthe Ġpress Ġrelease Ġand Ġnews Ġconference Ġ( the Ġclient 's Ġspin Ġon Ġthe Ġpoll ster 's Ġspin ).\n",
      "\t Tokenized LLAMA3:Any Ġpoint Ġyou Ġfailed Ġto Ġwin Ġby Ġrig ging Ġthe Ġquestions Ġand Ġcategories Ġcan Ġbe Ġcleaned Ġup Ġin Ġthe Ġexecutive Ġsummary Ġ( the Ġpoll ster 's Ġspin ) Ġand Ġthe Ġpress Ġrelease Ġand Ġnews Ġconference Ġ( the Ġclient 's Ġspin Ġon Ġthe Ġpoll ster 's Ġspin ).\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Any point you didn't get by fixing the questions cannot be added to the executive summary.\n",
      "\t Tokenized GPT2:Any Ġpoint Ġyou Ġdidn 't Ġget Ġby Ġfixing Ġthe Ġquestions Ġcannot Ġbe Ġadded Ġto Ġthe Ġexecutive Ġsummary .\n",
      "\t Tokenized LLAMA3:Any Ġpoint Ġyou Ġdidn 't Ġget Ġby Ġfixing Ġthe Ġquestions Ġcannot Ġbe Ġadded Ġto Ġthe Ġexecutive Ġsummary .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: They encourage us to indulge ourselves, and they exhort us to worry about our competence at work.\n",
      "\t Tokenized GPT2:They Ġencourage Ġus Ġto Ġindul ge Ġourselves , Ġand Ġthey Ġexh ort Ġus Ġto Ġworry Ġabout Ġour Ġcompet ence Ġat Ġwork .\n",
      "\t Tokenized LLAMA3:They Ġencourage Ġus Ġto Ġindul ge Ġourselves , Ġand Ġthey Ġexh ort Ġus Ġto Ġworry Ġabout Ġour Ġcompet ence Ġat Ġwork .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: They want us to to indulge ourselves with booze. \n",
      "\t Tokenized GPT2:They Ġwant Ġus Ġto Ġto Ġindul ge Ġourselves Ġwith Ġboo ze . Ġ\n",
      "\t Tokenized LLAMA3:They Ġwant Ġus Ġto Ġto Ġindul ge Ġourselves Ġwith Ġboo ze . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: I don't know what I would have done without Legal Services, said James. \n",
      "\t Tokenized GPT2:I Ġdon 't Ġknow Ġwhat ĠI Ġwould Ġhave Ġdone Ġwithout ĠLe gal ĠServices , Ġsaid ĠJames . Ġ\n",
      "\t Tokenized LLAMA3:I Ġdon 't Ġknow Ġwhat ĠI Ġwould Ġhave Ġdone Ġwithout ĠLe gal ĠServices , Ġsaid ĠJames . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: James said Legal Services was of no help.\n",
      "\t Tokenized GPT2:James Ġsaid ĠLe gal ĠServices Ġwas Ġof Ġno Ġhelp .\n",
      "\t Tokenized LLAMA3:James Ġsaid ĠLe gal ĠServices Ġwas Ġof Ġno Ġhelp .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: ' She gets a little obsessive about her sauce.\n",
      "\t Tokenized GPT2:' ĠShe Ġgets Ġa Ġlittle Ġobsess ive Ġabout Ġher Ġsauce .\n",
      "\t Tokenized LLAMA3:' ĠShe Ġgets Ġa Ġlittle Ġobsess ive Ġabout Ġher Ġsauce .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: She becomes overly focused about her sauce. \n",
      "\t Tokenized GPT2:She Ġbecomes Ġoverly Ġfocused Ġabout Ġher Ġsauce . Ġ\n",
      "\t Tokenized LLAMA3:She Ġbecomes Ġoverly Ġfocused Ġabout Ġher Ġsauce . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Up here, gazing out at strikingly lush mountains, you may find yourself higher than the clouds, which adds to the extraordinarily eerie atmosphere of the place.\n",
      "\t Tokenized GPT2:Up Ġhere , Ġg azing Ġout Ġat Ġstriking ly Ġl ush Ġmountains , Ġyou Ġmay Ġfind Ġyourself Ġhigher Ġthan Ġthe Ġclouds , Ġwhich Ġadds Ġto Ġthe Ġextra ordin arily Ġe erie Ġatmosphere Ġof Ġthe Ġplace .\n",
      "\t Tokenized LLAMA3:Up Ġhere , Ġg azing Ġout Ġat Ġstriking ly Ġl ush Ġmountains , Ġyou Ġmay Ġfind Ġyourself Ġhigher Ġthan Ġthe Ġclouds , Ġwhich Ġadds Ġto Ġthe Ġextra ordin arily Ġe erie Ġatmosphere Ġof Ġthe Ġplace .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Down here, you can see the gold mines from the old explorers, you are way lower than the sea level, so be careful.\n",
      "\t Tokenized GPT2:Down Ġhere , Ġyou Ġcan Ġsee Ġthe Ġgold Ġmines Ġfrom Ġthe Ġold Ġexplore rs , Ġyou Ġare Ġway Ġlower Ġthan Ġthe Ġsea Ġlevel , Ġso Ġbe Ġcareful .\n",
      "\t Tokenized LLAMA3:Down Ġhere , Ġyou Ġcan Ġsee Ġthe Ġgold Ġmines Ġfrom Ġthe Ġold Ġexplore rs , Ġyou Ġare Ġway Ġlower Ġthan Ġthe Ġsea Ġlevel , Ġso Ġbe Ġcareful .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The levadas were largely built by slave laborers from Africa, whose primary employment was on sugar plantations.\n",
      "\t Tokenized GPT2:The Ġle v adas Ġwere Ġlargely Ġbuilt Ġby Ġslave Ġlab ore rs Ġfrom ĠAfrica , Ġwhose Ġprimary Ġemployment Ġwas Ġon Ġsugar Ġplant ations .\n",
      "\t Tokenized LLAMA3:The Ġle v adas Ġwere Ġlargely Ġbuilt Ġby Ġslave Ġlab ore rs Ġfrom ĠAfrica , Ġwhose Ġprimary Ġemployment Ġwas Ġon Ġsugar Ġplant ations .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The levadas were built by the workers.\n",
      "\t Tokenized GPT2:The Ġle v adas Ġwere Ġbuilt Ġby Ġthe Ġworkers .\n",
      "\t Tokenized LLAMA3:The Ġle v adas Ġwere Ġbuilt Ġby Ġthe Ġworkers .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: As a result, their services may be more effective when conducted in the emergency department environment.\n",
      "\t Tokenized GPT2:As Ġa Ġresult , Ġtheir Ġservices Ġmay Ġbe Ġmore Ġeffective Ġwhen Ġconducted Ġin Ġthe Ġemergency Ġdepartment Ġenvironment .\n",
      "\t Tokenized LLAMA3:As Ġa Ġresult , Ġtheir Ġservices Ġmay Ġbe Ġmore Ġeffective Ġwhen Ġconducted Ġin Ġthe Ġemergency Ġdepartment Ġenvironment .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Their services might be more effective if they're done in the OR.\n",
      "\t Tokenized GPT2:Their Ġservices Ġmight Ġbe Ġmore Ġeffective Ġif Ġthey 're Ġdone Ġin Ġthe ĠOR .\n",
      "\t Tokenized LLAMA3:Their Ġservices Ġmight Ġbe Ġmore Ġeffective Ġif Ġthey 're Ġdone Ġin Ġthe ĠOR .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: yeah i can believe that\n",
      "\t Tokenized GPT2:yeah Ġi Ġcan Ġbelieve Ġthat\n",
      "\t Tokenized LLAMA3:yeah Ġi Ġcan Ġbelieve Ġthat\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I agree with what you said.\n",
      "\t Tokenized GPT2:I Ġagree Ġwith Ġwhat Ġyou Ġsaid .\n",
      "\t Tokenized LLAMA3:I Ġagree Ġwith Ġwhat Ġyou Ġsaid .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: If the collecting entity transfers the nonexchange revenue to the General Fund or another entity, the amount is accounted for as a custodial activity by the collecting entity.\n",
      "\t Tokenized GPT2:If Ġthe Ġcollecting Ġentity Ġtransfers Ġthe Ġnone x change Ġrevenue Ġto Ġthe ĠGeneral ĠFund Ġor Ġanother Ġentity , Ġthe Ġamount Ġis Ġaccounted Ġfor Ġas Ġa Ġcust od ial Ġactivity Ġby Ġthe Ġcollecting Ġentity .\n",
      "\t Tokenized LLAMA3:If Ġthe Ġcollecting Ġentity Ġtransfers Ġthe Ġnone x change Ġrevenue Ġto Ġthe ĠGeneral ĠFund Ġor Ġanother Ġentity , Ġthe Ġamount Ġis Ġaccounted Ġfor Ġas Ġa Ġcust od ial Ġactivity Ġby Ġthe Ġcollecting Ġentity .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Nonexchange revenue to the General Mills.\n",
      "\t Tokenized GPT2:None x change Ġrevenue Ġto Ġthe ĠGeneral ĠMills .\n",
      "\t Tokenized LLAMA3:None x change Ġrevenue Ġto Ġthe ĠGeneral ĠMills .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The data would be presented as required supplementary stewardship information accompanying the consolidated financial statements of the Federal Government but not in individual reports of its component units.\n",
      "\t Tokenized GPT2:The Ġdata Ġwould Ġbe Ġpresented Ġas Ġrequired Ġsupplement ary Ġstew ard ship Ġinformation Ġaccompanying Ġthe Ġconsolid ated Ġfinancial Ġstatements Ġof Ġthe ĠFederal ĠGovernment Ġbut Ġnot Ġin Ġindividual Ġreports Ġof Ġits Ġcomponent Ġunits .\n",
      "\t Tokenized LLAMA3:The Ġdata Ġwould Ġbe Ġpresented Ġas Ġrequired Ġsupplement ary Ġstew ard ship Ġinformation Ġaccompanying Ġthe Ġconsolid ated Ġfinancial Ġstatements Ġof Ġthe ĠFederal ĠGovernment Ġbut Ġnot Ġin Ġindividual Ġreports Ġof Ġits Ġcomponent Ġunits .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The data is only necessary when looking at the big picture within federal government.\n",
      "\t Tokenized GPT2:The Ġdata Ġis Ġonly Ġnecessary Ġwhen Ġlooking Ġat Ġthe Ġbig Ġpicture Ġwithin Ġfederal Ġgovernment .\n",
      "\t Tokenized LLAMA3:The Ġdata Ġis Ġonly Ġnecessary Ġwhen Ġlooking Ġat Ġthe Ġbig Ġpicture Ġwithin Ġfederal Ġgovernment .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Both professors soon realized that creating a new language was not an easy task.\n",
      "\t Tokenized GPT2:Both Ġprofessors Ġsoon Ġrealized Ġthat Ġcreating Ġa Ġnew Ġlanguage Ġwas Ġnot Ġan Ġeasy Ġtask .\n",
      "\t Tokenized LLAMA3:Both Ġprofessors Ġsoon Ġrealized Ġthat Ġcreating Ġa Ġnew Ġlanguage Ġwas Ġnot Ġan Ġeasy Ġtask .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Professors realized it was easy to make a new language.\n",
      "\t Tokenized GPT2:Pro fess ors Ġrealized Ġit Ġwas Ġeasy Ġto Ġmake Ġa Ġnew Ġlanguage .\n",
      "\t Tokenized LLAMA3:Pro fess ors Ġrealized Ġit Ġwas Ġeasy Ġto Ġmake Ġa Ġnew Ġlanguage .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The volumes are available again but won't be returned to the stacks until the damp library itself gets renovated.\n",
      "\t Tokenized GPT2:The Ġvolumes Ġare Ġavailable Ġagain Ġbut Ġwon 't Ġbe Ġreturned Ġto Ġthe Ġstacks Ġuntil Ġthe Ġdamp Ġlibrary Ġitself Ġgets Ġren ov ated .\n",
      "\t Tokenized LLAMA3:The Ġvolumes Ġare Ġavailable Ġagain Ġbut Ġwon 't Ġbe Ġreturned Ġto Ġthe Ġstacks Ġuntil Ġthe Ġdamp Ġlibrary Ġitself Ġgets Ġren ov ated .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The widely sought after volumes will be available to the public after renovation.\n",
      "\t Tokenized GPT2:The Ġwidely Ġsought Ġafter Ġvolumes Ġwill Ġbe Ġavailable Ġto Ġthe Ġpublic Ġafter Ġren ovation .\n",
      "\t Tokenized LLAMA3:The Ġwidely Ġsought Ġafter Ġvolumes Ġwill Ġbe Ġavailable Ġto Ġthe Ġpublic Ġafter Ġren ovation .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: To check this, the central bank has tripled interest rates and used hard currency reserves (now reduced to $10 billion in ready cash) to buy back rubles.\n",
      "\t Tokenized GPT2:To Ġcheck Ġthis , Ġthe Ġcentral Ġbank Ġhas Ġtri pled Ġinterest Ġrates Ġand Ġused Ġhard Ġcurrency Ġreserves Ġ( now Ġreduced Ġto Ġ$ 10 Ġbillion Ġin Ġready Ġcash ) Ġto Ġbuy Ġback Ġrub les .\n",
      "\t Tokenized LLAMA3:To Ġcheck Ġthis , Ġthe Ġcentral Ġbank Ġhas Ġtri pled Ġinterest Ġrates Ġand Ġused Ġhard Ġcurrency Ġreserves Ġ( now Ġreduced Ġto Ġ$ 10 Ġbillion Ġin Ġready Ġcash ) Ġto Ġbuy Ġback Ġrub les .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The central bank's interest rates have tripled in margin.\n",
      "\t Tokenized GPT2:The Ġcentral Ġbank 's Ġinterest Ġrates Ġhave Ġtri pled Ġin Ġmargin .\n",
      "\t Tokenized LLAMA3:The Ġcentral Ġbank 's Ġinterest Ġrates Ġhave Ġtri pled Ġin Ġmargin .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Suddenly she started, and her face blanched.\n",
      "\t Tokenized GPT2:Suddenly Ġshe Ġstarted , Ġand Ġher Ġface Ġbl an ched .\n",
      "\t Tokenized LLAMA3:Suddenly Ġshe Ġstarted , Ġand Ġher Ġface Ġbl an ched .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: She stood immobile, and had a stern expression on her face.\n",
      "\t Tokenized GPT2:She Ġstood Ġimm obile , Ġand Ġhad Ġa Ġstern Ġexpression Ġon Ġher Ġface .\n",
      "\t Tokenized LLAMA3:She Ġstood Ġimm obile , Ġand Ġhad Ġa Ġstern Ġexpression Ġon Ġher Ġface .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: They drive it around the country in a dilapidated ice-cream truck trying to keep it cool.\n",
      "\t Tokenized GPT2:They Ġdrive Ġit Ġaround Ġthe Ġcountry Ġin Ġa Ġdil ap id ated Ġice - cream Ġtruck Ġtrying Ġto Ġkeep Ġit Ġcool .\n",
      "\t Tokenized LLAMA3:They Ġdrive Ġit Ġaround Ġthe Ġcountry Ġin Ġa Ġdil ap id ated Ġice - cream Ġtruck Ġtrying Ġto Ġkeep Ġit Ġcool .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: They drove around a brand new ice cream truck to make sure they could keep it cold.\n",
      "\t Tokenized GPT2:They Ġdrove Ġaround Ġa Ġbrand Ġnew Ġice Ġcream Ġtruck Ġto Ġmake Ġsure Ġthey Ġcould Ġkeep Ġit Ġcold .\n",
      "\t Tokenized LLAMA3:They Ġdrove Ġaround Ġa Ġbrand Ġnew Ġice Ġcream Ġtruck Ġto Ġmake Ġsure Ġthey Ġcould Ġkeep Ġit Ġcold .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Rouen is the ancient center of Normandy's thriving textile industry, and the place of Joan of Arc's martyrdom ' a national symbol of resistance to tyranny.\n",
      "\t Tokenized GPT2:R ou en Ġis Ġthe Ġancient Ġcenter Ġof ĠNorm andy 's Ġth riving Ġtext ile Ġindustry , Ġand Ġthe Ġplace Ġof ĠJoan Ġof ĠArc 's Ġmart yr dom Ġ' Ġa Ġnational Ġsymbol Ġof Ġresistance Ġto Ġty ran ny .\n",
      "\t Tokenized LLAMA3:R ou en Ġis Ġthe Ġancient Ġcenter Ġof ĠNorm andy 's Ġth riving Ġtext ile Ġindustry , Ġand Ġthe Ġplace Ġof ĠJoan Ġof ĠArc 's Ġmart yr dom Ġ' Ġa Ġnational Ġsymbol Ġof Ġresistance Ġto Ġty ran ny .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Joan of Arc was the daughter of a textile worker.\n",
      "\t Tokenized GPT2:Jo an Ġof ĠArc Ġwas Ġthe Ġdaughter Ġof Ġa Ġtext ile Ġworker .\n",
      "\t Tokenized LLAMA3:Jo an Ġof ĠArc Ġwas Ġthe Ġdaughter Ġof Ġa Ġtext ile Ġworker .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Isn't a woman's body her most personal property?\n",
      "\t Tokenized GPT2:Isn 't Ġa Ġwoman 's Ġbody Ġher Ġmost Ġpersonal Ġproperty ?\n",
      "\t Tokenized LLAMA3:Isn 't Ġa Ġwoman 's Ġbody Ġher Ġmost Ġpersonal Ġproperty ?\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Isn't a woman's body sacred property?\n",
      "\t Tokenized GPT2:Isn 't Ġa Ġwoman 's Ġbody Ġsacred Ġproperty ?\n",
      "\t Tokenized LLAMA3:Isn 't Ġa Ġwoman 's Ġbody Ġsacred Ġproperty ?\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The purpose of the Self-Inspection process was to provide programs a means to verify, by reviewing a sample of cases, that their 1999 CSR data satisfies LSC's standards for accuracy.\n",
      "\t Tokenized GPT2:The Ġpurpose Ġof Ġthe ĠSelf - In spe ction Ġprocess Ġwas Ġto Ġprovide Ġprograms Ġa Ġmeans Ġto Ġverify , Ġby Ġreviewing Ġa Ġsample Ġof Ġcases , Ġthat Ġtheir Ġ1999 ĠCS R Ġdata Ġsatisfies ĠL SC 's Ġstandards Ġfor Ġaccuracy .\n",
      "\t Tokenized LLAMA3:The Ġpurpose Ġof Ġthe ĠSelf -In spe ction Ġprocess Ġwas Ġto Ġprovide Ġprograms Ġa Ġmeans Ġto Ġverify , Ġby Ġreviewing Ġa Ġsample Ġof Ġcases , Ġthat Ġtheir Ġ 199 9 ĠCS R Ġdata Ġsatisfies ĠL SC 's Ġstandards Ġfor Ġaccuracy .\n",
      "\t Unique Tokens GPT2: {'-', 'Ġ1999', 'In'}\n",
      "\t Unique Tokens LLAMA3: {'199', '-In', '9', 'Ġ'}\n",
      "Text 2: The Self-Inspection process has no other purpose than to hurt legitimacy of cases.\n",
      "\t Tokenized GPT2:The ĠSelf - In spe ction Ġprocess Ġhas Ġno Ġother Ġpurpose Ġthan Ġto Ġhurt Ġlegitim acy Ġof Ġcases .\n",
      "\t Tokenized LLAMA3:The ĠSelf -In spe ction Ġprocess Ġhas Ġno Ġother Ġpurpose Ġthan Ġto Ġhurt Ġlegitim acy Ġof Ġcases .\n",
      "\t Unique Tokens GPT2: {'-', 'In'}\n",
      "\t Unique Tokens LLAMA3: {'-In'}\n",
      "==neutral==\n",
      "Text 1: Standard print film is available in many shops in the major towns, but serious shutterbugs will want to seek out one of the following photography stores for a full range of specialist film and Abbey Photographic, 25, Stramongate, Kendal LA9 4BH; Tel. (01539) 720-085, or The Photo Shop, North Road, A\n",
      "\t Tokenized GPT2:Standard Ġprint Ġfilm Ġis Ġavailable Ġin Ġmany Ġshops Ġin Ġthe Ġmajor Ġtowns , Ġbut Ġserious Ġshut ter bugs Ġwill Ġwant Ġto Ġseek Ġout Ġone Ġof Ġthe Ġfollowing Ġphotography Ġstores Ġfor Ġa Ġfull Ġrange Ġof Ġspecialist Ġfilm Ġand ĠAb bey ĠPhot ographic , Ġ25 , ĠStr among ate , ĠKend al ĠLA 9 Ġ4 B H ; ĠTel . Ġ( 015 39 ) Ġ7 20 - 0 85 , Ġor ĠThe ĠPhoto ĠShop , ĠNorth ĠRoad , ĠA\n",
      "\t Tokenized LLAMA3:Standard Ġprint Ġfilm Ġis Ġavailable Ġin Ġmany Ġshops Ġin Ġthe Ġmajor Ġtowns , Ġbut Ġserious Ġshut ter bugs Ġwill Ġwant Ġto Ġseek Ġout Ġone Ġof Ġthe Ġfollowing Ġphotography Ġstores Ġfor Ġa Ġfull Ġrange Ġof Ġspecialist Ġfilm Ġand ĠAb bey ĠPhot ographic , Ġ 25 , ĠStr among ate , ĠKend al ĠLA 9 Ġ 4 B H ; ĠTel . Ġ( 015 39 ) Ġ 720 - 08 5 , Ġor ĠThe ĠPhoto ĠShop , ĠNorth ĠRoad , ĠA\n",
      "\t Unique Tokens GPT2: {'Ġ3', 'Ġ9', 'Ġ7', '20', '85', '0', 'Ġ4', '43', 'ĠAm', 'bles', 'Ġ25'}\n",
      "\t Unique Tokens LLAMA3: {'720', '394', 'Ġ', 'les', '08', '25', '343', 'ĠAmb', '5'}\n",
      "Text 2: The Photo Shop has a greater variety of film than Abbey Photographic.\n",
      "\t Tokenized GPT2:The ĠPhoto ĠShop Ġhas Ġa Ġgreater Ġvariety Ġof Ġfilm Ġthan ĠAb bey ĠPhot ographic .\n",
      "\t Tokenized LLAMA3:The ĠPhoto ĠShop Ġhas Ġa Ġgreater Ġvariety Ġof Ġfilm Ġthan ĠAb bey ĠPhot ographic .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: but you know they kids seem like when they get ten or twelve years old they fall out of that and and they don't follow it at all you know there're very few scouts go on and become Eagle Scouts and and i don't know what the high rank is for the gals but\n",
      "\t Tokenized GPT2:but Ġyou Ġknow Ġthey Ġkids Ġseem Ġlike Ġwhen Ġthey Ġget Ġten Ġor Ġtwelve Ġyears Ġold Ġthey Ġfall Ġout Ġof Ġthat Ġand Ġand Ġthey Ġdon 't Ġfollow Ġit Ġat Ġall Ġyou Ġknow Ġthere 're Ġvery Ġfew Ġsc outs Ġgo Ġon Ġand Ġbecome ĠEagle ĠSc outs Ġand Ġand Ġi Ġdon 't Ġknow Ġwhat Ġthe Ġhigh Ġrank Ġis Ġfor Ġthe Ġg als Ġbut\n",
      "\t Tokenized LLAMA3:but Ġyou Ġknow Ġthey Ġkids Ġseem Ġlike Ġwhen Ġthey Ġget Ġten Ġor Ġtwelve Ġyears Ġold Ġthey Ġfall Ġout Ġof Ġthat Ġand Ġand Ġthey Ġdon 't Ġfollow Ġit Ġat Ġall Ġyou Ġknow Ġthere 're Ġvery Ġfew Ġsc outs Ġgo Ġon Ġand Ġbecome ĠEagle ĠSc outs Ġand Ġand Ġi Ġdon 't Ġknow Ġwhat Ġthe Ġhigh Ġrank Ġis Ġfor Ġthe Ġg als Ġbut\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Many kids leave the Scouts when they are pre-teens.\n",
      "\t Tokenized GPT2:Many Ġkids Ġleave Ġthe ĠSc outs Ġwhen Ġthey Ġare Ġpre - te ens .\n",
      "\t Tokenized LLAMA3:Many Ġkids Ġleave Ġthe ĠSc outs Ġwhen Ġthey Ġare Ġpre -te ens .\n",
      "\t Unique Tokens GPT2: {'-', 'te'}\n",
      "\t Unique Tokens LLAMA3: {'-te'}\n",
      "==entailment==\n",
      "Text 1: Leather Wares\n",
      "\t Tokenized GPT2:Le ather ĠW ares\n",
      "\t Tokenized LLAMA3:Le ather ĠW ares\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The wares are made of leather.\n",
      "\t Tokenized GPT2:The Ġwa res Ġare Ġmade Ġof Ġleather .\n",
      "\t Tokenized LLAMA3:The Ġwa res Ġare Ġmade Ġof Ġleather .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: year, they gave morethan a half million dollars to Western Michigan Legal Services.\n",
      "\t Tokenized GPT2:year , Ġthey Ġgave Ġmore than Ġa Ġhalf Ġmillion Ġdollars Ġto ĠWestern ĠMichigan ĠLe gal ĠServices .\n",
      "\t Tokenized LLAMA3:year , Ġthey Ġgave Ġmore than Ġa Ġhalf Ġmillion Ġdollars Ġto ĠWestern ĠMichigan ĠLe gal ĠServices .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: They make annual donations to legal services.\n",
      "\t Tokenized GPT2:They Ġmake Ġannual Ġdonations Ġto Ġlegal Ġservices .\n",
      "\t Tokenized LLAMA3:They Ġmake Ġannual Ġdonations Ġto Ġlegal Ġservices .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: yeah and then about every five years you have to dig them up and throw them away and start over again they don't last forever\n",
      "\t Tokenized GPT2:yeah Ġand Ġthen Ġabout Ġevery Ġfive Ġyears Ġyou Ġhave Ġto Ġdig Ġthem Ġup Ġand Ġthrow Ġthem Ġaway Ġand Ġstart Ġover Ġagain Ġthey Ġdon 't Ġlast Ġforever\n",
      "\t Tokenized LLAMA3:yeah Ġand Ġthen Ġabout Ġevery Ġfive Ġyears Ġyou Ġhave Ġto Ġdig Ġthem Ġup Ġand Ġthrow Ġthem Ġaway Ġand Ġstart Ġover Ġagain Ġthey Ġdon 't Ġlast Ġforever\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: they last forever.\n",
      "\t Tokenized GPT2:they Ġlast Ġforever .\n",
      "\t Tokenized LLAMA3:they Ġlast Ġforever .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: life track\n",
      "\t Tokenized GPT2:life Ġtrack\n",
      "\t Tokenized LLAMA3:life Ġtrack\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Jobs and work.\n",
      "\t Tokenized GPT2:J obs Ġand Ġwork .\n",
      "\t Tokenized LLAMA3:J obs Ġand Ġwork .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: You're the Desert Ghost.\n",
      "\t Tokenized GPT2:You 're Ġthe ĠDes ert ĠGhost .\n",
      "\t Tokenized LLAMA3:You 're Ġthe ĠDes ert ĠGhost .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: You are actually the Desert Ghost.\n",
      "\t Tokenized GPT2:You Ġare Ġactually Ġthe ĠDes ert ĠGhost .\n",
      "\t Tokenized LLAMA3:You Ġare Ġactually Ġthe ĠDes ert ĠGhost .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The day my deadline came, I got a business card.\n",
      "\t Tokenized GPT2:The Ġday Ġmy Ġdeadline Ġcame , ĠI Ġgot Ġa Ġbusiness Ġcard .\n",
      "\t Tokenized LLAMA3:The Ġday Ġmy Ġdeadline Ġcame , ĠI Ġgot Ġa Ġbusiness Ġcard .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: On the day of the deadline, I received a gold trophy. \n",
      "\t Tokenized GPT2:On Ġthe Ġday Ġof Ġthe Ġdeadline , ĠI Ġreceived Ġa Ġgold Ġtrophy . Ġ\n",
      "\t Tokenized LLAMA3:On Ġthe Ġday Ġof Ġthe Ġdeadline , ĠI Ġreceived Ġa Ġgold Ġtrophy . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Though prehistoric remains from the Paleolithic, Neolithic, and Bronze Ages have been unearthed in the Manzanares Valley, prior to Madrid's sudden elevation to capital city in 1561 its history was rather undistinguished.\n",
      "\t Tokenized GPT2:Though Ġpre histor ic Ġremains Ġfrom Ġthe ĠP ale olith ic , ĠNe olith ic , Ġand ĠBron ze ĠA ges Ġhave Ġbeen Ġune art hed Ġin Ġthe ĠMan z ana res ĠValley , Ġprior Ġto ĠMadrid 's Ġsudden Ġelevation Ġto Ġcapital Ġcity Ġin Ġ15 61 Ġits Ġhistory Ġwas Ġrather Ġund ist ingu ished .\n",
      "\t Tokenized LLAMA3:Though Ġpre h istor ic Ġremains Ġfrom Ġthe ĠP ale ol ith ic , ĠNe ol ith ic , Ġand ĠBron ze ĠA ges Ġhave Ġbeen Ġune art hed Ġin Ġthe ĠMan z ana res ĠValley , Ġprior Ġto ĠMadrid 's Ġsudden Ġelevation Ġto Ġcapital Ġcity Ġin Ġ 156 1 Ġits Ġhistory Ġwas Ġrather Ġund ist ingu ished .\n",
      "\t Unique Tokens GPT2: {'histor', 'Ġ15', '61', 'olith'}\n",
      "\t Unique Tokens LLAMA3: {'ol', '1', 'ith', 'h', 'istor', 'Ġ', '156'}\n",
      "Text 2: There were remains in the Manzanares Valley that included cavemen.\n",
      "\t Tokenized GPT2:There Ġwere Ġremains Ġin Ġthe ĠMan z ana res ĠValley Ġthat Ġincluded Ġcave men .\n",
      "\t Tokenized LLAMA3:There Ġwere Ġremains Ġin Ġthe ĠMan z ana res ĠValley Ġthat Ġincluded Ġcave men .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: yeah um gosh i think it was only like three and a half pounds and for me that's big that's why i'm saying i love to go fishing because i've never caught anything really really big um so because it's always been you know in the on a lake and uh i know they have bigger fish than that but you know thre\n",
      "\t Tokenized GPT2:yeah Ġum Ġgosh Ġi Ġthink Ġit Ġwas Ġonly Ġlike Ġthree Ġand Ġa Ġhalf Ġpounds Ġand Ġfor Ġme Ġthat 's Ġbig Ġthat 's Ġwhy Ġi 'm Ġsaying Ġi Ġlove Ġto Ġgo Ġfishing Ġbecause Ġi 've Ġnever Ġcaught Ġanything Ġreally Ġreally Ġbig Ġum Ġso Ġbecause Ġit 's Ġalways Ġbeen Ġyou Ġknow Ġin Ġthe Ġon Ġa Ġlake Ġand Ġuh Ġi Ġknow Ġthey Ġhave Ġbigger Ġfish Ġthan Ġthat Ġbut Ġyou Ġknow Ġth re\n",
      "\t Tokenized LLAMA3:yeah Ġum Ġgosh Ġi Ġthink Ġit Ġwas Ġonly Ġlike Ġthree Ġand Ġa Ġhalf Ġpounds Ġand Ġfor Ġme Ġthat 's Ġbig Ġthat 's Ġwhy Ġi 'm Ġsaying Ġi Ġlove Ġto Ġgo Ġfishing Ġbecause Ġi 've Ġnever Ġcaught Ġanything Ġreally Ġreally Ġbig Ġum Ġso Ġbecause Ġit 's Ġalways Ġbeen Ġyou Ġknow Ġin Ġthe Ġon Ġa Ġlake Ġand Ġuh Ġi Ġknow Ġthey Ġhave Ġbigger Ġfish Ġthan Ġthat Ġbut Ġyou Ġknow Ġth re\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It was just a few pounds but I ate it all.\n",
      "\t Tokenized GPT2:It Ġwas Ġjust Ġa Ġfew Ġpounds Ġbut ĠI Ġate Ġit Ġall .\n",
      "\t Tokenized LLAMA3:It Ġwas Ġjust Ġa Ġfew Ġpounds Ġbut ĠI Ġate Ġit Ġall .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: As for the divisive issue of whether the Mass is a sacrifice for the remission of sins, the statement affirms that Christ's death upon the cross ...\n",
      "\t Tokenized GPT2:As Ġfor Ġthe Ġdivis ive Ġissue Ġof Ġwhether Ġthe ĠMass Ġis Ġa Ġsacrifice Ġfor Ġthe Ġrem ission Ġof Ġsins , Ġthe Ġstatement Ġaff ir ms Ġthat ĠChrist 's Ġdeath Ġupon Ġthe Ġcross Ġ...\n",
      "\t Tokenized LLAMA3:As Ġfor Ġthe Ġdiv isive Ġissue Ġof Ġwhether Ġthe ĠMass Ġis Ġa Ġsacrifice Ġfor Ġthe Ġrem ission Ġof Ġsins , Ġthe Ġstatement Ġaff ir ms Ġthat ĠChrist 's Ġdeath Ġupon Ġthe Ġcross Ġ...\n",
      "\t Unique Tokens GPT2: {'ive', 'Ġdivis'}\n",
      "\t Unique Tokens LLAMA3: {'Ġdiv', 'isive'}\n",
      "Text 2: The statement has ended the controversy over whether the Mass is a sacrifice for the remission of sins.\n",
      "\t Tokenized GPT2:The Ġstatement Ġhas Ġended Ġthe Ġcontroversy Ġover Ġwhether Ġthe ĠMass Ġis Ġa Ġsacrifice Ġfor Ġthe Ġrem ission Ġof Ġsins .\n",
      "\t Tokenized LLAMA3:The Ġstatement Ġhas Ġended Ġthe Ġcontroversy Ġover Ġwhether Ġthe ĠMass Ġis Ġa Ġsacrifice Ġfor Ġthe Ġrem ission Ġof Ġsins .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: If that investor were willing to pay extra for the security of limited downside, she could buy put options with a strike price of $98, which would lock in her profit on the shares at $18, less whatever the options cost.\n",
      "\t Tokenized GPT2:If Ġthat Ġinvestor Ġwere Ġwilling Ġto Ġpay Ġextra Ġfor Ġthe Ġsecurity Ġof Ġlimited Ġdownside , Ġshe Ġcould Ġbuy Ġput Ġoptions Ġwith Ġa Ġstrike Ġprice Ġof Ġ$ 98 , Ġwhich Ġwould Ġlock Ġin Ġher Ġprofit Ġon Ġthe Ġshares Ġat Ġ$ 18 , Ġless Ġwhatever Ġthe Ġoptions Ġcost .\n",
      "\t Tokenized LLAMA3:If Ġthat Ġinvestor Ġwere Ġwilling Ġto Ġpay Ġextra Ġfor Ġthe Ġsecurity Ġof Ġlimited Ġdownside , Ġshe Ġcould Ġbuy Ġput Ġoptions Ġwith Ġa Ġstrike Ġprice Ġof Ġ$ 98 , Ġwhich Ġwould Ġlock Ġin Ġher Ġprofit Ġon Ġthe Ġshares Ġat Ġ$ 18 , Ġless Ġwhatever Ġthe Ġoptions Ġcost .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The strike price of Lowe's stock could be $98.\n",
      "\t Tokenized GPT2:The Ġstrike Ġprice Ġof ĠLow e 's Ġstock Ġcould Ġbe Ġ$ 98 .\n",
      "\t Tokenized LLAMA3:The Ġstrike Ġprice Ġof ĠLow e 's Ġstock Ġcould Ġbe Ġ$ 98 .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: yeah TI people yeah and so i just figured no it's just this area you know\n",
      "\t Tokenized GPT2:yeah ĠTI Ġpeople Ġyeah Ġand Ġso Ġi Ġjust Ġfigured Ġno Ġit 's Ġjust Ġthis Ġarea Ġyou Ġknow\n",
      "\t Tokenized LLAMA3:yeah ĠTI Ġpeople Ġyeah Ġand Ġso Ġi Ġjust Ġfigured Ġno Ġit 's Ġjust Ġthis Ġarea Ġyou Ġknow\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: No, I figured is was all areas.\n",
      "\t Tokenized GPT2:No , ĠI Ġfigured Ġis Ġwas Ġall Ġareas .\n",
      "\t Tokenized LLAMA3:No , ĠI Ġfigured Ġis Ġwas Ġall Ġareas .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The man who had once come up with a has-been corner skit, in which, as Zmuda recalls, forgotten performers would be sent out to flounder in front of an audience ...\n",
      "\t Tokenized GPT2:The Ġman Ġwho Ġhad Ġonce Ġcome Ġup Ġwith Ġa Ġhas - been Ġcorner Ġsk it , Ġin Ġwhich , Ġas ĠZ m uda Ġrecall s , Ġforgotten Ġperform ers Ġwould Ġbe Ġsent Ġout Ġto Ġfl ound er Ġin Ġfront Ġof Ġan Ġaudience Ġ...\n",
      "\t Tokenized LLAMA3:The Ġman Ġwho Ġhad Ġonce Ġcome Ġup Ġwith Ġa Ġhas -be en Ġcorner Ġsk it , Ġin Ġwhich , Ġas ĠZ m uda Ġrecall s , Ġforgotten Ġperform ers Ġwould Ġbe Ġsent Ġout Ġto Ġfl ound er Ġin Ġfront Ġof Ġan Ġaudience Ġ...\n",
      "\t Unique Tokens GPT2: {'-', 'been'}\n",
      "\t Unique Tokens LLAMA3: {'-be', 'en'}\n",
      "Text 2: The man designed a skit where popular performers went out and enjoyed total success in front of a crowd.\n",
      "\t Tokenized GPT2:The Ġman Ġdesigned Ġa Ġsk it Ġwhere Ġpopular Ġperform ers Ġwent Ġout Ġand Ġenjoyed Ġtotal Ġsuccess Ġin Ġfront Ġof Ġa Ġcrowd .\n",
      "\t Tokenized LLAMA3:The Ġman Ġdesigned Ġa Ġsk it Ġwhere Ġpopular Ġperform ers Ġwent Ġout Ġand Ġenjoyed Ġtotal Ġsuccess Ġin Ġfront Ġof Ġa Ġcrowd .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Poor Dave, she said.\n",
      "\t Tokenized GPT2:Poor ĠDave , Ġshe Ġsaid .\n",
      "\t Tokenized LLAMA3:P oor ĠDave , Ġshe Ġsaid .\n",
      "\t Unique Tokens GPT2: {'Poor'}\n",
      "\t Unique Tokens LLAMA3: {'P', 'oor'}\n",
      "Text 2: She felt bad for Dave.\n",
      "\t Tokenized GPT2:She Ġfelt Ġbad Ġfor ĠDave .\n",
      "\t Tokenized LLAMA3:She Ġfelt Ġbad Ġfor ĠDave .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: with little back packs of their own and you know things like that\n",
      "\t Tokenized GPT2:with Ġlittle Ġback Ġpacks Ġof Ġtheir Ġown Ġand Ġyou Ġknow Ġthings Ġlike Ġthat\n",
      "\t Tokenized LLAMA3:with Ġlittle Ġback Ġpacks Ġof Ġtheir Ġown Ġand Ġyou Ġknow Ġthings Ġlike Ġthat\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I'm not sure they're old enough to have back packs.\n",
      "\t Tokenized GPT2:I 'm Ġnot Ġsure Ġthey 're Ġold Ġenough Ġto Ġhave Ġback Ġpacks .\n",
      "\t Tokenized LLAMA3:I 'm Ġnot Ġsure Ġthey 're Ġold Ġenough Ġto Ġhave Ġback Ġpacks .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: In the other sight he saw Adrin's hands cocking back a pair of dragon-hammered pistols.\n",
      "\t Tokenized GPT2:In Ġthe Ġother Ġsight Ġhe Ġsaw ĠAd rin 's Ġhands Ġcock ing Ġback Ġa Ġpair Ġof Ġdragon - ham mered Ġpistol s .\n",
      "\t Tokenized LLAMA3:In Ġthe Ġother Ġsight Ġhe Ġsaw ĠAd rin 's Ġhands Ġcock ing Ġback Ġa Ġpair Ġof Ġdragon -h am mered Ġpist ols .\n",
      "\t Unique Tokens GPT2: {'s', '-', 'Ġpistol', 'ham'}\n",
      "\t Unique Tokens LLAMA3: {'Ġpist', '-h', 'am', 'ols'}\n",
      "Text 2: Adrin fired his machine gun as he watched.\n",
      "\t Tokenized GPT2:Ad rin Ġfired Ġhis Ġmachine Ġgun Ġas Ġhe Ġwatched .\n",
      "\t Tokenized LLAMA3:Ad rin Ġfired Ġhis Ġmachine Ġgun Ġas Ġhe Ġwatched .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Conspiracy theorists  MasterCard is investing in a chip that can store electronic cash, your medical history, and keys to your home and office.\n",
      "\t Tokenized GPT2:Cons pir acy Ġtheor ists Ġ ĠMaster Card Ġis Ġinvesting Ġin Ġa Ġchip Ġthat Ġcan Ġstore Ġelectronic Ġcash , Ġyour Ġmedical Ġhistory , Ġand Ġkeys Ġto Ġyour Ġhome Ġand Ġoffice .\n",
      "\t Tokenized LLAMA3:Cons pir acy Ġthe or ists Ġ ĠMaster Card Ġis Ġinvesting Ġin Ġa Ġchip Ġthat Ġcan Ġstore Ġelectronic Ġcash , Ġyour Ġmedical Ġhistory , Ġand Ġkeys Ġto Ġyour Ġhome Ġand Ġoffice .\n",
      "\t Unique Tokens GPT2: {'Ġtheor'}\n",
      "\t Unique Tokens LLAMA3: {'Ġthe', 'or'}\n",
      "Text 2: Conspiracy theorists believe Mastercard is working on a chip to store all your personal data.\n",
      "\t Tokenized GPT2:Cons pir acy Ġtheor ists Ġbelieve ĠMaster card Ġis Ġworking Ġon Ġa Ġchip Ġto Ġstore Ġall Ġyour Ġpersonal Ġdata .\n",
      "\t Tokenized LLAMA3:Cons pir acy Ġthe or ists Ġbelieve ĠMaster card Ġis Ġworking Ġon Ġa Ġchip Ġto Ġstore Ġall Ġyour Ġpersonal Ġdata .\n",
      "\t Unique Tokens GPT2: {'Ġtheor'}\n",
      "\t Unique Tokens LLAMA3: {'Ġthe', 'or'}\n",
      "==contradiction==\n",
      "Text 1: This call to play fortuneteller is not easily refused.\n",
      "\t Tokenized GPT2:This Ġcall Ġto Ġplay Ġfort un et eller Ġis Ġnot Ġeasily Ġrefused .\n",
      "\t Tokenized LLAMA3:This Ġcall Ġto Ġplay Ġfort un et eller Ġis Ġnot Ġeasily Ġrefused .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It's easily refused the call to play fortuneteller.\n",
      "\t Tokenized GPT2:It 's Ġeasily Ġrefused Ġthe Ġcall Ġto Ġplay Ġfort un et eller .\n",
      "\t Tokenized LLAMA3:It 's Ġeasily Ġrefused Ġthe Ġcall Ġto Ġplay Ġfort un et eller .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: After the high emotion of de Gaulle's march down the Champs-Elys??es, the business of post-war reconstruction, though boosted by the generous aid of the Americans' Mar?­shall Plan, proved arduous, and the wartime alliance of de Gaulle's conservatives and the Communist Party soon broke down.\n",
      "\t Tokenized GPT2:After Ġthe Ġhigh Ġemotion Ġof Ġde ĠG aul le 's Ġmarch Ġdown Ġthe ĠCh amps - E ly s ?? es , Ġthe Ġbusiness Ġof Ġpost - war Ġreconstruction , Ġthough Ġboost ed Ġby Ġthe Ġgenerous Ġaid Ġof Ġthe ĠAmericans ' ĠMar ? ÂŃ sh all ĠPlan , Ġproved Ġar du ous , Ġand Ġthe Ġw art ime Ġalliance Ġof Ġde ĠG aul le 's Ġconservatives Ġand Ġthe ĠCommun ist ĠParty Ġsoon Ġbroke Ġdown .\n",
      "\t Tokenized LLAMA3:After Ġthe Ġhigh Ġemotion Ġof Ġde ĠG aul le 's Ġmarch Ġdown Ġthe ĠCh amps -E ly s ?? es , Ġthe Ġbusiness Ġof Ġpost -war Ġreconstruction , Ġthough Ġboost ed Ġby Ġthe Ġgenerous Ġaid Ġof Ġthe ĠAmericans ' ĠMar ? ÂŃ sh all ĠPlan , Ġproved Ġar du ous , Ġand Ġthe Ġw art ime Ġalliance Ġof Ġde ĠG aul le 's Ġconservatives Ġand Ġthe ĠCommun ist ĠParty Ġsoon Ġbroke Ġdown .\n",
      "\t Unique Tokens GPT2: {'-', 'E', 'war'}\n",
      "\t Unique Tokens LLAMA3: {'-E', '-war'}\n",
      "Text 2: Though the Marshall Plan was designed to help other countries, it failed to fulfill its purpose with the Communists.\n",
      "\t Tokenized GPT2:Though Ġthe ĠMarshall ĠPlan Ġwas Ġdesigned Ġto Ġhelp Ġother Ġcountries , Ġit Ġfailed Ġto Ġfulfill Ġits Ġpurpose Ġwith Ġthe ĠCommun ists .\n",
      "\t Tokenized LLAMA3:Though Ġthe ĠMarshall ĠPlan Ġwas Ġdesigned Ġto Ġhelp Ġother Ġcountries , Ġit Ġfailed Ġto Ġfulfill Ġits Ġpurpose Ġwith Ġthe ĠCommun ists .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: The main gate of the churchyard leads out to Greyfriars Place, and across the street you will find an excellent view of one of Scotland's newest museums.\n",
      "\t Tokenized GPT2:The Ġmain Ġgate Ġof Ġthe Ġchurch yard Ġleads Ġout Ġto ĠGrey f ri ars ĠPlace , Ġand Ġacross Ġthe Ġstreet Ġyou Ġwill Ġfind Ġan Ġexcellent Ġview Ġof Ġone Ġof ĠScotland 's Ġnewest Ġmuse ums .\n",
      "\t Tokenized LLAMA3:The Ġmain Ġgate Ġof Ġthe Ġchurch yard Ġleads Ġout Ġto ĠGrey f ri ars ĠPlace , Ġand Ġacross Ġthe Ġstreet Ġyou Ġwill Ġfind Ġan Ġexcellent Ġview Ġof Ġone Ġof ĠScotland 's Ġnewest Ġmuse ums .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Near the church you can see Greyfriars Place and a new museum. \n",
      "\t Tokenized GPT2:N ear Ġthe Ġchurch Ġyou Ġcan Ġsee ĠGrey f ri ars ĠPlace Ġand Ġa Ġnew Ġmuseum . Ġ\n",
      "\t Tokenized LLAMA3:N ear Ġthe Ġchurch Ġyou Ġcan Ġsee ĠGrey f ri ars ĠPlace Ġand Ġa Ġnew Ġmuseum . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: To see the desert at its best, go out at dawn and at sunset.\n",
      "\t Tokenized GPT2:To Ġsee Ġthe Ġdesert Ġat Ġits Ġbest , Ġgo Ġout Ġat Ġdawn Ġand Ġat Ġsunset .\n",
      "\t Tokenized LLAMA3:To Ġsee Ġthe Ġdesert Ġat Ġits Ġbest , Ġgo Ġout Ġat Ġdawn Ġand Ġat Ġsunset .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Go at noon to see the desert for the best view.\n",
      "\t Tokenized GPT2:Go Ġat Ġnoon Ġto Ġsee Ġthe Ġdesert Ġfor Ġthe Ġbest Ġview .\n",
      "\t Tokenized LLAMA3:Go Ġat Ġnoon Ġto Ġsee Ġthe Ġdesert Ġfor Ġthe Ġbest Ġview .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: This was the site of the Bateau-Lavoir studio, an unprepossessing glass-roofed loft reconstructed since a 1970 fire.\n",
      "\t Tokenized GPT2:This Ġwas Ġthe Ġsite Ġof Ġthe ĠB ate au - L av oir Ġstudio , Ġan Ġun pre poss ess ing Ġglass - ro of ed Ġlo ft Ġreconstruct ed Ġsince Ġa Ġ1970 Ġfire .\n",
      "\t Tokenized LLAMA3:This Ġwas Ġthe Ġsite Ġof Ġthe ĠB ate au -L av oir Ġstudio , Ġan Ġun pre poss ess ing Ġglass -ro of ed Ġlo ft Ġreconstruct ed Ġsince Ġa Ġ 197 0 Ġfire .\n",
      "\t Unique Tokens GPT2: {'Ġ1970', 'ro', 'L', '-'}\n",
      "\t Unique Tokens LLAMA3: {'-ro', '-L', 'Ġ', '197', '0'}\n",
      "Text 2: The glass roof was shattered in 1990 as a result of having debris fall on top of it.\n",
      "\t Tokenized GPT2:The Ġglass Ġroof Ġwas Ġshattered Ġin Ġ1990 Ġas Ġa Ġresult Ġof Ġhaving Ġdebris Ġfall Ġon Ġtop Ġof Ġit .\n",
      "\t Tokenized LLAMA3:The Ġglass Ġroof Ġwas Ġshattered Ġin Ġ 199 0 Ġas Ġa Ġresult Ġof Ġhaving Ġdebris Ġfall Ġon Ġtop Ġof Ġit .\n",
      "\t Unique Tokens GPT2: {'Ġ1990'}\n",
      "\t Unique Tokens LLAMA3: {'199', '0', 'Ġ'}\n",
      "==contradiction==\n",
      "Text 1: He dismounted and Ca'daan saw he was smaller than the rest.\n",
      "\t Tokenized GPT2:He Ġdis mount ed Ġand ĠCa 'd aan Ġsaw Ġhe Ġwas Ġsmaller Ġthan Ġthe Ġrest .\n",
      "\t Tokenized LLAMA3:He Ġdis mount ed Ġand ĠCa 'd aan Ġsaw Ġhe Ġwas Ġsmaller Ġthan Ġthe Ġrest .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He was very tall.\n",
      "\t Tokenized GPT2:He Ġwas Ġvery Ġtall .\n",
      "\t Tokenized LLAMA3:He Ġwas Ġvery Ġtall .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Tommy had a healthy and vigorous appetite.\n",
      "\t Tokenized GPT2:Tom my Ġhad Ġa Ġhealthy Ġand Ġvig orous Ġappetite .\n",
      "\t Tokenized LLAMA3:Tom my Ġhad Ġa Ġhealthy Ġand Ġvig orous Ġappetite .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Tommy hadn't eaten all day.\n",
      "\t Tokenized GPT2:Tom my Ġhadn 't Ġeaten Ġall Ġday .\n",
      "\t Tokenized LLAMA3:Tom my Ġhadn 't Ġeaten Ġall Ġday .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: so you um-hum so you think it comes down to education or or something like that\n",
      "\t Tokenized GPT2:so Ġyou Ġum - hum Ġso Ġyou Ġthink Ġit Ġcomes Ġdown Ġto Ġeducation Ġor Ġor Ġsomething Ġlike Ġthat\n",
      "\t Tokenized LLAMA3:so Ġyou Ġum -h um Ġso Ġyou Ġthink Ġit Ġcomes Ġdown Ġto Ġeducation Ġor Ġor Ġsomething Ġlike Ġthat\n",
      "\t Unique Tokens GPT2: {'-', 'hum'}\n",
      "\t Unique Tokens LLAMA3: {'um', '-h'}\n",
      "Text 2: IT all boils down to how much education you have. \n",
      "\t Tokenized GPT2:IT Ġall Ġbo ils Ġdown Ġto Ġhow Ġmuch Ġeducation Ġyou Ġhave . Ġ\n",
      "\t Tokenized LLAMA3:IT Ġall Ġbo ils Ġdown Ġto Ġhow Ġmuch Ġeducation Ġyou Ġhave . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: As Russell points out, some 400,000 legal aid cases go unassisted each year.\n",
      "\t Tokenized GPT2:As ĠRussell Ġpoints Ġout , Ġsome Ġ400 , 000 Ġlegal Ġaid Ġcases Ġgo Ġun ass isted Ġeach Ġyear .\n",
      "\t Tokenized LLAMA3:As ĠRussell Ġpoints Ġout , Ġsome Ġ 400 , 000 Ġlegal Ġaid Ġcases Ġgo Ġun ass isted Ġeach Ġyear .\n",
      "\t Unique Tokens GPT2: {'Ġ400'}\n",
      "\t Unique Tokens LLAMA3: {'400', 'Ġ'}\n",
      "Text 2: Zero legal aid cases go unassisted each year.\n",
      "\t Tokenized GPT2:Zero Ġlegal Ġaid Ġcases Ġgo Ġun ass isted Ġeach Ġyear .\n",
      "\t Tokenized LLAMA3:Zero Ġlegal Ġaid Ġcases Ġgo Ġun ass isted Ġeach Ġyear .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: We need to be sure of our going.\" But Tuppence, for once, seemed tongue-tied.\n",
      "\t Tokenized GPT2:We Ġneed Ġto Ġbe Ġsure Ġof Ġour Ġgoing .\" ĠBut ĠT upp ence , Ġfor Ġonce , Ġseemed Ġtongue - t ied .\n",
      "\t Tokenized LLAMA3:We Ġneed Ġto Ġbe Ġsure Ġof Ġour Ġgoing .\" ĠBut ĠT upp ence , Ġfor Ġonce , Ġseemed Ġtongue -t ied .\n",
      "\t Unique Tokens GPT2: {'-', 't'}\n",
      "\t Unique Tokens LLAMA3: {'-t'}\n",
      "Text 2: Tuppence was shocked.\n",
      "\t Tokenized GPT2:T upp ence Ġwas Ġshocked .\n",
      "\t Tokenized LLAMA3:T upp ence Ġwas Ġshocked .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The strychnine had been found in a drawer in the prisoner's room. \n",
      "\t Tokenized GPT2:The Ġst ry chn ine Ġhad Ġbeen Ġfound Ġin Ġa Ġdrawer Ġin Ġthe Ġprisoner 's Ġroom . Ġ\n",
      "\t Tokenized LLAMA3:The Ġst ry chn ine Ġhad Ġbeen Ġfound Ġin Ġa Ġdrawer Ġin Ġthe Ġprisoner 's Ġroom . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: They found the strychnine under the prisoner's bed. \n",
      "\t Tokenized GPT2:They Ġfound Ġthe Ġst ry chn ine Ġunder Ġthe Ġprisoner 's Ġbed . Ġ\n",
      "\t Tokenized LLAMA3:They Ġfound Ġthe Ġst ry chn ine Ġunder Ġthe Ġprisoner 's Ġbed . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: and uh you know it's like they they consider that but it would be the same way here you know it's like if if you had to do it you know you have a big sign i'm sorry i don't get paid you know\n",
      "\t Tokenized GPT2:and Ġuh Ġyou Ġknow Ġit 's Ġlike Ġthey Ġthey Ġconsider Ġthat Ġbut Ġit Ġwould Ġbe Ġthe Ġsame Ġway Ġhere Ġyou Ġknow Ġit 's Ġlike Ġif Ġif Ġyou Ġhad Ġto Ġdo Ġit Ġyou Ġknow Ġyou Ġhave Ġa Ġbig Ġsign Ġi 'm Ġsorry Ġi Ġdon 't Ġget Ġpaid Ġyou Ġknow\n",
      "\t Tokenized LLAMA3:and Ġuh Ġyou Ġknow Ġit 's Ġlike Ġthey Ġthey Ġconsider Ġthat Ġbut Ġit Ġwould Ġbe Ġthe Ġsame Ġway Ġhere Ġyou Ġknow Ġit 's Ġlike Ġif Ġif Ġyou Ġhad Ġto Ġdo Ġit Ġyou Ġknow Ġyou Ġhave Ġa Ġbig Ġsign Ġi 'm Ġsorry Ġi Ġdon 't Ġget Ġpaid Ġyou Ġknow\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: If I were somewhere else I would be getting paid. \n",
      "\t Tokenized GPT2:If ĠI Ġwere Ġsomewhere Ġelse ĠI Ġwould Ġbe Ġgetting Ġpaid . Ġ\n",
      "\t Tokenized LLAMA3:If ĠI Ġwere Ġsomewhere Ġelse ĠI Ġwould Ġbe Ġgetting Ġpaid . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: well the floor was uneven you know\n",
      "\t Tokenized GPT2:well Ġthe Ġfloor Ġwas Ġuneven Ġyou Ġknow\n",
      "\t Tokenized LLAMA3:well Ġthe Ġfloor Ġwas Ġuneven Ġyou Ġknow\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: well, you're aware that the floor wasn't even\n",
      "\t Tokenized GPT2:well , Ġyou 're Ġaware Ġthat Ġthe Ġfloor Ġwasn 't Ġeven\n",
      "\t Tokenized LLAMA3:well , Ġyou 're Ġaware Ġthat Ġthe Ġfloor Ġwasn 't Ġeven\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Never trust a Sather, Bork said softly.\n",
      "\t Tokenized GPT2:Never Ġtrust Ġa ĠS ather , ĠB ork Ġsaid Ġsoftly .\n",
      "\t Tokenized LLAMA3:Never Ġtrust Ġa ĠS ather , ĠB ork Ġsaid Ġsoftly .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Borker said to never trust a Sather.\n",
      "\t Tokenized GPT2:B ork er Ġsaid Ġto Ġnever Ġtrust Ġa ĠS ather .\n",
      "\t Tokenized LLAMA3:B ork er Ġsaid Ġto Ġnever Ġtrust Ġa ĠS ather .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Like Arabs and Jews, Diamond warns, Koreans and Japanese are joined by blood yet locked in traditional enmity.\n",
      "\t Tokenized GPT2:Like ĠAr abs Ġand ĠJews , ĠDiamond Ġwar ns , ĠKore ans Ġand ĠJapanese Ġare Ġjoined Ġby Ġblood Ġyet Ġlocked Ġin Ġtraditional Ġen m ity .\n",
      "\t Tokenized LLAMA3:Like ĠAr abs Ġand ĠJews , ĠDiamond Ġwar ns , ĠKore ans Ġand ĠJapanese Ġare Ġjoined Ġby Ġblood Ġyet Ġlocked Ġin Ġtraditional Ġen m ity .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Koreans and Japanese have tension between them because of a war long ago.\n",
      "\t Tokenized GPT2:K ore ans Ġand ĠJapanese Ġhave Ġtension Ġbetween Ġthem Ġbecause Ġof Ġa Ġwar Ġlong Ġago .\n",
      "\t Tokenized LLAMA3:K ore ans Ġand ĠJapanese Ġhave Ġtension Ġbetween Ġthem Ġbecause Ġof Ġa Ġwar Ġlong Ġago .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: South Carolina has no referendum right, so the Supreme Court canceled the vote and upheld the ban.\n",
      "\t Tokenized GPT2:South ĠCarolina Ġhas Ġno Ġreferendum Ġright , Ġso Ġthe ĠSupreme ĠCourt Ġcanceled Ġthe Ġvote Ġand Ġup held Ġthe Ġban .\n",
      "\t Tokenized LLAMA3:South ĠCarolina Ġhas Ġno Ġreferendum Ġright , Ġso Ġthe ĠSupreme ĠCourt Ġcanceled Ġthe Ġvote Ġand Ġup held Ġthe Ġban .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: South Carolina has a referendum right, so the Supreme Court was powerless over the state.\n",
      "\t Tokenized GPT2:South ĠCarolina Ġhas Ġa Ġreferendum Ġright , Ġso Ġthe ĠSupreme ĠCourt Ġwas Ġpower less Ġover Ġthe Ġstate .\n",
      "\t Tokenized LLAMA3:South ĠCarolina Ġhas Ġa Ġreferendum Ġright , Ġso Ġthe ĠSupreme ĠCourt Ġwas Ġpower less Ġover Ġthe Ġstate .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Arafat is also ailing and has no clear successor.\n",
      "\t Tokenized GPT2:A ra fat Ġis Ġalso Ġa iling Ġand Ġhas Ġno Ġclear Ġsuccessor .\n",
      "\t Tokenized LLAMA3:A raf at Ġis Ġalso Ġa iling Ġand Ġhas Ġno Ġclear Ġsuccessor .\n",
      "\t Unique Tokens GPT2: {'ra', 'fat'}\n",
      "\t Unique Tokens LLAMA3: {'at', 'raf'}\n",
      "Text 2: Arafat is in bad health and does not have a person chosen to take his place.\n",
      "\t Tokenized GPT2:A ra fat Ġis Ġin Ġbad Ġhealth Ġand Ġdoes Ġnot Ġhave Ġa Ġperson Ġchosen Ġto Ġtake Ġhis Ġplace .\n",
      "\t Tokenized LLAMA3:A raf at Ġis Ġin Ġbad Ġhealth Ġand Ġdoes Ġnot Ġhave Ġa Ġperson Ġchosen Ġto Ġtake Ġhis Ġplace .\n",
      "\t Unique Tokens GPT2: {'ra', 'fat'}\n",
      "\t Unique Tokens LLAMA3: {'at', 'raf'}\n",
      "==entailment==\n",
      "Text 1: but uh i've always enjoyed uh the train and you know fooling with it and all\n",
      "\t Tokenized GPT2:but Ġuh Ġi 've Ġalways Ġenjoyed Ġuh Ġthe Ġtrain Ġand Ġyou Ġknow Ġfool ing Ġwith Ġit Ġand Ġall\n",
      "\t Tokenized LLAMA3:but Ġuh Ġi 've Ġalways Ġenjoyed Ġuh Ġthe Ġtrain Ġand Ġyou Ġknow Ġfool ing Ġwith Ġit Ġand Ġall\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I have always had a love for trains.\n",
      "\t Tokenized GPT2:I Ġhave Ġalways Ġhad Ġa Ġlove Ġfor Ġtrains .\n",
      "\t Tokenized LLAMA3:I Ġhave Ġalways Ġhad Ġa Ġlove Ġfor Ġtrains .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: and they're fairly close to the water aren't they i mean they're right on the late\n",
      "\t Tokenized GPT2:and Ġthey 're Ġfairly Ġclose Ġto Ġthe Ġwater Ġaren 't Ġthey Ġi Ġmean Ġthey 're Ġright Ġon Ġthe Ġlate\n",
      "\t Tokenized LLAMA3:and Ġthey 're Ġfairly Ġclose Ġto Ġthe Ġwater Ġaren 't Ġthey Ġi Ġmean Ġthey 're Ġright Ġon Ġthe Ġlate\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: They're a distance from the water aren't they.\n",
      "\t Tokenized GPT2:They 're Ġa Ġdistance Ġfrom Ġthe Ġwater Ġaren 't Ġthey .\n",
      "\t Tokenized LLAMA3:They 're Ġa Ġdistance Ġfrom Ġthe Ġwater Ġaren 't Ġthey .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Bien! he said at last. \n",
      "\t Tokenized GPT2:B ien ! Ġhe Ġsaid Ġat Ġlast . Ġ\n",
      "\t Tokenized LLAMA3:B ien ! Ġhe Ġsaid Ġat Ġlast . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He wasted no time speaking. \n",
      "\t Tokenized GPT2:He Ġwasted Ġno Ġtime Ġspeaking . Ġ\n",
      "\t Tokenized LLAMA3:He Ġwasted Ġno Ġtime Ġspeaking . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Lie back, and DON'T THINK.\n",
      "\t Tokenized GPT2:L ie Ġback , Ġand ĠDON ' T ĠTHINK .\n",
      "\t Tokenized LLAMA3:L ie Ġback , Ġand ĠDON 'T ĠTH INK .\n",
      "\t Unique Tokens GPT2: {'ĠTHINK', 'T', \"'\"}\n",
      "\t Unique Tokens LLAMA3: {\"'T\", 'ĠTH', 'INK'}\n",
      "Text 2: Stand up and start thinking.\n",
      "\t Tokenized GPT2:Stand Ġup Ġand Ġstart Ġthinking .\n",
      "\t Tokenized LLAMA3:Stand Ġup Ġand Ġstart Ġthinking .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: It vibrated under his hand.\n",
      "\t Tokenized GPT2:It Ġvibr ated Ġunder Ġhis Ġhand .\n",
      "\t Tokenized LLAMA3:It Ġvibr ated Ġunder Ġhis Ġhand .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It hummed quietly in his hand.\n",
      "\t Tokenized GPT2:It Ġhummed Ġquietly Ġin Ġhis Ġhand .\n",
      "\t Tokenized LLAMA3:It Ġhummed Ġquietly Ġin Ġhis Ġhand .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: This whole unsavory episode brings back memories of skits with Monty Python ! One of my favorite lines was, You are guilty of six--no, seven--charges of heresy.\n",
      "\t Tokenized GPT2:This Ġwhole Ġuns av ory Ġepisode Ġbrings Ġback Ġmemories Ġof Ġsk its Ġwith ĠMon ty ĠPython Ġ! ĠOne Ġof Ġmy Ġfavorite Ġlines Ġwas , ĠYou Ġare Ġguilty Ġof Ġsix -- no , Ġseven -- char ges Ġof Ġhe res y .\n",
      "\t Tokenized LLAMA3:This Ġwhole Ġuns av ory Ġepisode Ġbrings Ġback Ġmemories Ġof Ġsk its Ġwith ĠMon ty ĠPython Ġ! ĠOne Ġof Ġmy Ġfavorite Ġlines Ġwas , ĠYou Ġare Ġguilty Ġof Ġsix -- no , Ġseven -- char ges Ġof Ġhe res y .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: This episode reminds me of skits with Monthy Python.\n",
      "\t Tokenized GPT2:This Ġepisode Ġreminds Ġme Ġof Ġsk its Ġwith ĠMon thy ĠPython .\n",
      "\t Tokenized LLAMA3:This Ġepisode Ġreminds Ġme Ġof Ġsk its Ġwith ĠMon thy ĠPython .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: In the short term, U.S. consumers will benefit from cheap imports (as will U.S. multinationals that use parts made in East Asian factories).\n",
      "\t Tokenized GPT2:In Ġthe Ġshort Ġterm , ĠU . S . Ġconsumers Ġwill Ġbenefit Ġfrom Ġcheap Ġimports Ġ( as Ġwill ĠU . S . Ġmult ination als Ġthat Ġuse Ġparts Ġmade Ġin ĠEast ĠAsian Ġfactories ).\n",
      "\t Tokenized LLAMA3:In Ġthe Ġshort Ġterm , ĠU .S . Ġconsumers Ġwill Ġbenefit Ġfrom Ġcheap Ġimports Ġ( as Ġwill ĠU .S . Ġmult ination als Ġthat Ġuse Ġparts Ġmade Ġin ĠEast ĠAsian Ġfactories ).\n",
      "\t Unique Tokens GPT2: {'S'}\n",
      "\t Unique Tokens LLAMA3: {'.S'}\n",
      "Text 2: U.S. consumers and factories in East Asia benefit from imports.\n",
      "\t Tokenized GPT2:U . S . Ġconsumers Ġand Ġfactories Ġin ĠEast ĠAsia Ġbenefit Ġfrom Ġimports .\n",
      "\t Tokenized LLAMA3:U .S . Ġconsumers Ġand Ġfactories Ġin ĠEast ĠAsia Ġbenefit Ġfrom Ġimports .\n",
      "\t Unique Tokens GPT2: {'S'}\n",
      "\t Unique Tokens LLAMA3: {'.S'}\n",
      "==entailment==\n",
      "Text 1: I noticed that there was a long branch running out from the tree in the right direction.\n",
      "\t Tokenized GPT2:I Ġnoticed Ġthat Ġthere Ġwas Ġa Ġlong Ġbranch Ġrunning Ġout Ġfrom Ġthe Ġtree Ġin Ġthe Ġright Ġdirection .\n",
      "\t Tokenized LLAMA3:I Ġnoticed Ġthat Ġthere Ġwas Ġa Ġlong Ġbranch Ġrunning Ġout Ġfrom Ġthe Ġtree Ġin Ġthe Ġright Ġdirection .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: There was a rather lengthy branch that was pointing in the right direction.  \n",
      "\t Tokenized GPT2:There Ġwas Ġa Ġrather Ġlengthy Ġbranch Ġthat Ġwas Ġpointing Ġin Ġthe Ġright Ġdirection . ĠĠ\n",
      "\t Tokenized LLAMA3:There Ġwas Ġa Ġrather Ġlengthy Ġbranch Ġthat Ġwas Ġpointing Ġin Ġthe Ġright Ġdirection . ĠĠ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Figure 1:  Delivery Points to Stops\n",
      "\t Tokenized GPT2:Figure Ġ1 : Ġ ĠDel ivery ĠPoints Ġto ĠSt ops\n",
      "\t Tokenized LLAMA3:Figure Ġ 1 : Ġ ĠDel ivery ĠPoint s Ġto ĠSt ops\n",
      "\t Unique Tokens GPT2: {'ĠPoints', 'Ġ1'}\n",
      "\t Unique Tokens LLAMA3: {'s', 'ĠPoint', '1'}\n",
      "Text 2: The third figure covers delivery points to stops\n",
      "\t Tokenized GPT2:The Ġthird Ġfigure Ġcovers Ġdelivery Ġpoints Ġto Ġstops\n",
      "\t Tokenized LLAMA3:The Ġthird Ġfigure Ġcovers Ġdelivery Ġpoints Ġto Ġstops\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: So unlike people who are fortunate enough to be able to afford attorneys and can go to another lawyer, our clients are simply lost in the legal system if they cannot get access to it from us.\n",
      "\t Tokenized GPT2:So Ġunlike Ġpeople Ġwho Ġare Ġfortunate Ġenough Ġto Ġbe Ġable Ġto Ġafford Ġattorneys Ġand Ġcan Ġgo Ġto Ġanother Ġlawyer , Ġour Ġclients Ġare Ġsimply Ġlost Ġin Ġthe Ġlegal Ġsystem Ġif Ġthey Ġcannot Ġget Ġaccess Ġto Ġit Ġfrom Ġus .\n",
      "\t Tokenized LLAMA3:So Ġunlike Ġpeople Ġwho Ġare Ġfortunate Ġenough Ġto Ġbe Ġable Ġto Ġafford Ġattorneys Ġand Ġcan Ġgo Ġto Ġanother Ġlawyer , Ġour Ġclients Ġare Ġsimply Ġlost Ġin Ġthe Ġlegal Ġsystem Ġif Ġthey Ġcannot Ġget Ġaccess Ġto Ġit Ġfrom Ġus .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Our clients can afford attorneys and bouncing between lawyers.\n",
      "\t Tokenized GPT2:Our Ġclients Ġcan Ġafford Ġattorneys Ġand Ġbouncing Ġbetween Ġlawyers .\n",
      "\t Tokenized LLAMA3:Our Ġclients Ġcan Ġafford Ġattorneys Ġand Ġbouncing Ġbetween Ġlawyers .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: uh i really i miss college i had a good time\n",
      "\t Tokenized GPT2:uh Ġi Ġreally Ġi Ġmiss Ġcollege Ġi Ġhad Ġa Ġgood Ġtime\n",
      "\t Tokenized LLAMA3:uh Ġi Ġreally Ġi Ġmiss Ġcollege Ġi Ġhad Ġa Ġgood Ġtime\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I enjoyed my time in university. \n",
      "\t Tokenized GPT2:I Ġenjoyed Ġmy Ġtime Ġin Ġuniversity . Ġ\n",
      "\t Tokenized LLAMA3:I Ġenjoyed Ġmy Ġtime Ġin Ġuniversity . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: The baker was not jolly.\n",
      "\t Tokenized GPT2:The Ġb aker Ġwas Ġnot Ġj olly .\n",
      "\t Tokenized LLAMA3:The Ġb aker Ġwas Ġnot Ġj olly .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The baker wasn't happy.\n",
      "\t Tokenized GPT2:The Ġb aker Ġwasn 't Ġhappy .\n",
      "\t Tokenized LLAMA3:The Ġb aker Ġwasn 't Ġhappy .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Poor Dave, she said.\n",
      "\t Tokenized GPT2:Poor ĠDave , Ġshe Ġsaid .\n",
      "\t Tokenized LLAMA3:P oor ĠDave , Ġshe Ġsaid .\n",
      "\t Unique Tokens GPT2: {'Poor'}\n",
      "\t Unique Tokens LLAMA3: {'P', 'oor'}\n",
      "Text 2: She was happy for Dave.\n",
      "\t Tokenized GPT2:She Ġwas Ġhappy Ġfor ĠDave .\n",
      "\t Tokenized LLAMA3:She Ġwas Ġhappy Ġfor ĠDave .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: yeah well i'm a hot weather person i'm i can take the heat but i don't like the cold\n",
      "\t Tokenized GPT2:yeah Ġwell Ġi 'm Ġa Ġhot Ġweather Ġperson Ġi 'm Ġi Ġcan Ġtake Ġthe Ġheat Ġbut Ġi Ġdon 't Ġlike Ġthe Ġcold\n",
      "\t Tokenized LLAMA3:yeah Ġwell Ġi 'm Ġa Ġhot Ġweather Ġperson Ġi 'm Ġi Ġcan Ġtake Ġthe Ġheat Ġbut Ġi Ġdon 't Ġlike Ġthe Ġcold\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I do not like warm weather at all.  \n",
      "\t Tokenized GPT2:I Ġdo Ġnot Ġlike Ġwarm Ġweather Ġat Ġall . ĠĠ\n",
      "\t Tokenized LLAMA3:I Ġdo Ġnot Ġlike Ġwarm Ġweather Ġat Ġall . ĠĠ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: In the vaults of the Bank.\n",
      "\t Tokenized GPT2:In Ġthe Ġvault s Ġof Ġthe ĠBank .\n",
      "\t Tokenized LLAMA3:In Ġthe Ġvault s Ġof Ġthe ĠBank .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: In the cash register at the bank.\n",
      "\t Tokenized GPT2:In Ġthe Ġcash Ġregister Ġat Ġthe Ġbank .\n",
      "\t Tokenized LLAMA3:In Ġthe Ġcash Ġregister Ġat Ġthe Ġbank .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: There followed the Balkan Wars, in which Turkey lost western Thrace and Macedonia, then World War I, into which Turkey entered on Germany's side.\n",
      "\t Tokenized GPT2:There Ġfollowed Ġthe ĠB alk an ĠWars , Ġin Ġwhich ĠTurkey Ġlost Ġwestern ĠThr ace Ġand ĠM aced onia , Ġthen ĠWorld ĠWar ĠI , Ġinto Ġwhich ĠTurkey Ġentered Ġon ĠGermany 's Ġside .\n",
      "\t Tokenized LLAMA3:There Ġfollowed Ġthe ĠB alk an ĠWars , Ġin Ġwhich ĠTurkey Ġlost Ġwestern ĠThr ace Ġand ĠM aced onia , Ġthen ĠWorld ĠWar ĠI , Ġinto Ġwhich ĠTurkey Ġentered Ġon ĠGermany 's Ġside .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Turkey entered the first world war fighting against Germany.\n",
      "\t Tokenized GPT2:Tur key Ġentered Ġthe Ġfirst Ġworld Ġwar Ġfighting Ġagainst ĠGermany .\n",
      "\t Tokenized LLAMA3:Tur key Ġentered Ġthe Ġfirst Ġworld Ġwar Ġfighting Ġagainst ĠGermany .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The spear missed Vrenna by only a hand-span.\n",
      "\t Tokenized GPT2:The Ġspear Ġmissed ĠV ren na Ġby Ġonly Ġa Ġhand - span .\n",
      "\t Tokenized LLAMA3:The Ġspear Ġmissed ĠV ren na Ġby Ġonly Ġa Ġhand -s pan .\n",
      "\t Unique Tokens GPT2: {'-', 'span'}\n",
      "\t Unique Tokens LLAMA3: {'-s', 'pan'}\n",
      "Text 2: The spear smacked the man in the face. \n",
      "\t Tokenized GPT2:The Ġspear Ġsm acked Ġthe Ġman Ġin Ġthe Ġface . Ġ\n",
      "\t Tokenized LLAMA3:The Ġspear Ġsm acked Ġthe Ġman Ġin Ġthe Ġface . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: There never will be.\n",
      "\t Tokenized GPT2:There Ġnever Ġwill Ġbe .\n",
      "\t Tokenized LLAMA3:There Ġnever Ġwill Ġbe .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It should happen soon.\n",
      "\t Tokenized GPT2:It Ġshould Ġhappen Ġsoon .\n",
      "\t Tokenized LLAMA3:It Ġshould Ġhappen Ġsoon .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: yeah maybe the maybe they'll bring their good schools with them  you know if the industry comes\n",
      "\t Tokenized GPT2:yeah Ġmaybe Ġthe Ġmaybe Ġthey 'll Ġbring Ġtheir Ġgood Ġschools Ġwith Ġthem Ġ Ġyou Ġknow Ġif Ġthe Ġindustry Ġcomes\n",
      "\t Tokenized LLAMA3:yeah Ġmaybe Ġthe Ġmaybe Ġthey 'll Ġbring Ġtheir Ġgood Ġschools Ġwith Ġthem Ġ Ġyou Ġknow Ġif Ġthe Ġindustry Ġcomes\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: If the industry comes, it will foster the creation of better schools.\n",
      "\t Tokenized GPT2:If Ġthe Ġindustry Ġcomes , Ġit Ġwill Ġfoster Ġthe Ġcreation Ġof Ġbetter Ġschools .\n",
      "\t Tokenized LLAMA3:If Ġthe Ġindustry Ġcomes , Ġit Ġwill Ġfoster Ġthe Ġcreation Ġof Ġbetter Ġschools .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: He was a pilot, not a platoon leader.\n",
      "\t Tokenized GPT2:He Ġwas Ġa Ġpilot , Ġnot Ġa Ġplat oon Ġleader .\n",
      "\t Tokenized LLAMA3:He Ġwas Ġa Ġpilot , Ġnot Ġa Ġplat oon Ġleader .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He was no platoon leader, but a lowly pilot.\n",
      "\t Tokenized GPT2:He Ġwas Ġno Ġplat oon Ġleader , Ġbut Ġa Ġlow ly Ġpilot .\n",
      "\t Tokenized LLAMA3:He Ġwas Ġno Ġplat oon Ġleader , Ġbut Ġa Ġlow ly Ġpilot .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Despite their 17th-century origins, these gardens avoid the rigid geometry of the Tuileries and Ver?­sailles.\n",
      "\t Tokenized GPT2:Despite Ġtheir Ġ17 th - century Ġorigins , Ġthese Ġgardens Ġavoid Ġthe Ġrigid Ġgeometry Ġof Ġthe ĠTu iler ies Ġand ĠVer ? ÂŃ sa illes .\n",
      "\t Tokenized LLAMA3:Despite Ġtheir Ġ 17 th -century Ġorigins , Ġthese Ġgardens Ġavoid Ġthe Ġrigid Ġgeometry Ġof Ġthe ĠTu iler ies Ġand ĠVer ? ÂŃ sa illes .\n",
      "\t Unique Tokens GPT2: {'-', 'century', 'Ġ17'}\n",
      "\t Unique Tokens LLAMA3: {'-century', 'Ġ', '17'}\n",
      "Text 2: The gardens are not shaped like the Tuileries or Versailles.\n",
      "\t Tokenized GPT2:The Ġgardens Ġare Ġnot Ġshaped Ġlike Ġthe ĠTu iler ies Ġor ĠVers a illes .\n",
      "\t Tokenized LLAMA3:The Ġgardens Ġare Ġnot Ġshaped Ġlike Ġthe ĠTu iler ies Ġor ĠVers a illes .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: He writes that it's the first time he's added such a track.\n",
      "\t Tokenized GPT2:He Ġwrites Ġthat Ġit 's Ġthe Ġfirst Ġtime Ġhe 's Ġadded Ġsuch Ġa Ġtrack .\n",
      "\t Tokenized LLAMA3:He Ġwrites Ġthat Ġit 's Ġthe Ġfirst Ġtime Ġhe 's Ġadded Ġsuch Ġa Ġtrack .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He creates tracks like this all the time.\n",
      "\t Tokenized GPT2:He Ġcreates Ġtracks Ġlike Ġthis Ġall Ġthe Ġtime .\n",
      "\t Tokenized LLAMA3:He Ġcreates Ġtracks Ġlike Ġthis Ġall Ġthe Ġtime .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: which they probably Mexican people don't even know what a taco salad is but i think it's now it's moving up too because uh just a change you know just something different\n",
      "\t Tokenized GPT2:which Ġthey Ġprobably ĠMexican Ġpeople Ġdon 't Ġeven Ġknow Ġwhat Ġa Ġt aco Ġsalad Ġis Ġbut Ġi Ġthink Ġit 's Ġnow Ġit 's Ġmoving Ġup Ġtoo Ġbecause Ġuh Ġjust Ġa Ġchange Ġyou Ġknow Ġjust Ġsomething Ġdifferent\n",
      "\t Tokenized LLAMA3:which Ġthey Ġprobably ĠMexican Ġpeople Ġdon 't Ġeven Ġknow Ġwhat Ġa Ġt aco Ġsalad Ġis Ġbut Ġi Ġthink Ġit 's Ġnow Ġit 's Ġmoving Ġup Ġtoo Ġbecause Ġuh Ġjust Ġa Ġchange Ġyou Ġknow Ġjust Ġsomething Ġdifferent\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Taco salad isn't a traditional Mexican dish, but it's becoming one because of cultural movement.\n",
      "\t Tokenized GPT2:T aco Ġsalad Ġisn 't Ġa Ġtraditional ĠMexican Ġdish , Ġbut Ġit 's Ġbecoming Ġone Ġbecause Ġof Ġcultural Ġmovement .\n",
      "\t Tokenized LLAMA3:T aco Ġsalad Ġisn 't Ġa Ġtraditional ĠMexican Ġdish , Ġbut Ġit 's Ġbecoming Ġone Ġbecause Ġof Ġcultural Ġmovement .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Sir James's presence in Manchester was not accidental.\n",
      "\t Tokenized GPT2:Sir ĠJames 's Ġpresence Ġin ĠManchester Ġwas Ġnot Ġaccidental .\n",
      "\t Tokenized LLAMA3:Sir ĠJames 's Ġpresence Ġin ĠManchester Ġwas Ġnot Ġaccident al .\n",
      "\t Unique Tokens GPT2: {'Ġaccidental'}\n",
      "\t Unique Tokens LLAMA3: {'al', 'Ġaccident'}\n",
      "Text 2: Sir James was present in Manchester on purpose.\n",
      "\t Tokenized GPT2:Sir ĠJames Ġwas Ġpresent Ġin ĠManchester Ġon Ġpurpose .\n",
      "\t Tokenized LLAMA3:Sir ĠJames Ġwas Ġpresent Ġin ĠManchester Ġon Ġpurpose .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Reportedly the biggest payment made in such a case, it is hardly a nick in Texaco's annual revenue of more than $30 billion.\n",
      "\t Tokenized GPT2:Rep orted ly Ġthe Ġbiggest Ġpayment Ġmade Ġin Ġsuch Ġa Ġcase , Ġit Ġis Ġhardly Ġa Ġnick Ġin ĠTex aco 's Ġannual Ġrevenue Ġof Ġmore Ġthan Ġ$ 30 Ġbillion .\n",
      "\t Tokenized LLAMA3:Rep orted ly Ġthe Ġbiggest Ġpayment Ġmade Ġin Ġsuch Ġa Ġcase , Ġit Ġis Ġhardly Ġa Ġnick Ġin ĠTex aco 's Ġannual Ġrevenue Ġof Ġmore Ġthan Ġ$ 30 Ġbillion .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The biggest payment bankrupted the company.\n",
      "\t Tokenized GPT2:The Ġbiggest Ġpayment Ġbankrupt ed Ġthe Ġcompany .\n",
      "\t Tokenized LLAMA3:The Ġbiggest Ġpayment Ġbankrupt ed Ġthe Ġcompany .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: A spark of annoyance lit Lincoln's eyes; the smallest hint of Natalia's Russian fire.\n",
      "\t Tokenized GPT2:A Ġspark Ġof Ġannoyance Ġlit ĠLincoln 's Ġeyes ; Ġthe Ġsmallest Ġhint Ġof ĠNatal ia 's ĠRussian Ġfire .\n",
      "\t Tokenized LLAMA3:A Ġspark Ġof Ġannoyance Ġlit ĠLincoln 's Ġeyes ; Ġthe Ġsmallest Ġhint Ġof ĠNatal ia 's ĠRussian Ġfire .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Lincoln wanted to kill Natalia with his bare hands in that precise moment.\n",
      "\t Tokenized GPT2:Lin coln Ġwanted Ġto Ġkill ĠNatal ia Ġwith Ġhis Ġbare Ġhands Ġin Ġthat Ġprecise Ġmoment .\n",
      "\t Tokenized LLAMA3:Lin coln Ġwanted Ġto Ġkill ĠNatal ia Ġwith Ġhis Ġbare Ġhands Ġin Ġthat Ġprecise Ġmoment .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Perhaps a further password would be required, or, at any rate, some proof of identity.\n",
      "\t Tokenized GPT2:Perhaps Ġa Ġfurther Ġpassword Ġwould Ġbe Ġrequired , Ġor , Ġat Ġany Ġrate , Ġsome Ġproof Ġof Ġidentity .\n",
      "\t Tokenized LLAMA3:Perhaps Ġa Ġfurther Ġpassword Ġwould Ġbe Ġrequired , Ġor , Ġat Ġany Ġrate , Ġsome Ġproof Ġof Ġidentity .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Passwords are unnecessary as they waste additional time.\n",
      "\t Tokenized GPT2:Pass words Ġare Ġunnecessary Ġas Ġthey Ġwaste Ġadditional Ġtime .\n",
      "\t Tokenized LLAMA3:Password s Ġare Ġunnecessary Ġas Ġthey Ġwaste Ġadditional Ġtime .\n",
      "\t Unique Tokens GPT2: {'words', 'Pass'}\n",
      "\t Unique Tokens LLAMA3: {'s', 'Password'}\n",
      "==entailment==\n",
      "Text 1: H-2A agricultural workers are required to maintain a foreign residence which they have no intention of abandoning.\n",
      "\t Tokenized GPT2:H - 2 A Ġagricultural Ġworkers Ġare Ġrequired Ġto Ġmaintain Ġa Ġforeign Ġresidence Ġwhich Ġthey Ġhave Ġno Ġintention Ġof Ġabandon ing .\n",
      "\t Tokenized LLAMA3:H - 2 A Ġagricultural Ġworkers Ġare Ġrequired Ġto Ġmaintain Ġa Ġforeign Ġresidence Ġwhich Ġthey Ġhave Ġno Ġintention Ġof Ġabandon ing .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Permanent foreign residence is required for some types of agricultural work visas.\n",
      "\t Tokenized GPT2:P erman ent Ġforeign Ġresidence Ġis Ġrequired Ġfor Ġsome Ġtypes Ġof Ġagricultural Ġwork Ġvis as .\n",
      "\t Tokenized LLAMA3:P erman ent Ġforeign Ġresidence Ġis Ġrequired Ġfor Ġsome Ġtypes Ġof Ġagricultural Ġwork Ġvis as .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Angry consumers would complain about cheapo car care.\n",
      "\t Tokenized GPT2:Ang ry Ġconsumers Ġwould Ġcomplain Ġabout Ġcheap o Ġcar Ġcare .\n",
      "\t Tokenized LLAMA3:Ang ry Ġconsumers Ġwould Ġcomplain Ġabout Ġcheap o Ġcar Ġcare .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Cheapo car care is a complaint angry consumers have.\n",
      "\t Tokenized GPT2:Che ap o Ġcar Ġcare Ġis Ġa Ġcomplaint Ġangry Ġconsumers Ġhave .\n",
      "\t Tokenized LLAMA3:Che ap o Ġcar Ġcare Ġis Ġa Ġcomplaint Ġangry Ġconsumers Ġhave .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: While AILA has joined the ACLU and other organizations in a Freedom of Information Act request to find out who is being detained where and why, Mohammed notes that the reasons for the immigrants' detention were not immediately clear and sometimes had dire consequences.\n",
      "\t Tokenized GPT2:While ĠA IL A Ġhas Ġjoined Ġthe ĠACL U Ġand Ġother Ġorganizations Ġin Ġa ĠFreedom Ġof ĠInformation ĠAct Ġrequest Ġto Ġfind Ġout Ġwho Ġis Ġbeing Ġdet ained Ġwhere Ġand Ġwhy , ĠMoh ammed Ġnotes Ġthat Ġthe Ġreasons Ġfor Ġthe Ġimmigrants ' Ġdetention Ġwere Ġnot Ġimmediately Ġclear Ġand Ġsometimes Ġhad Ġdire Ġconsequences .\n",
      "\t Tokenized LLAMA3:While ĠA IL A Ġhas Ġjoined Ġthe ĠACL U Ġand Ġother Ġorganizations Ġin Ġa ĠFreedom Ġof ĠInformation ĠAct Ġrequest Ġto Ġfind Ġout Ġwho Ġis Ġbeing Ġdet ained Ġwhere Ġand Ġwhy , ĠMoh ammed Ġnotes Ġthat Ġthe Ġreasons Ġfor Ġthe Ġimmigrants ' Ġdetention Ġwere Ġnot Ġimmediately Ġclear Ġand Ġsometimes Ġhad Ġdire Ġconsequences .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The EPA joined the ACLU in requesting the information.\n",
      "\t Tokenized GPT2:The ĠE PA Ġjoined Ġthe ĠACL U Ġin Ġrequesting Ġthe Ġinformation .\n",
      "\t Tokenized LLAMA3:The ĠE PA Ġjoined Ġthe ĠACL U Ġin Ġrequesting Ġthe Ġinformation .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: HCFA published a Notice of Proposed Rulemaking on March 28, 1997 (62 Fed.\n",
      "\t Tokenized GPT2:HC FA Ġpublished Ġa ĠNotice Ġof ĠPro posed ĠRule making Ġon ĠMarch Ġ28 , Ġ1997 Ġ( 62 ĠFed .\n",
      "\t Tokenized LLAMA3:HC FA Ġpublished Ġa ĠNotice Ġof ĠPro posed ĠRule making Ġon ĠMarch Ġ 28 , Ġ 199 7 Ġ( 62 ĠFed .\n",
      "\t Unique Tokens GPT2: {'Ġ1997', 'Ġ28'}\n",
      "\t Unique Tokens LLAMA3: {'199', '7', '28', 'Ġ'}\n",
      "Text 2: HCFA decided to keep it a secret when they proposed rules.\n",
      "\t Tokenized GPT2:HC FA Ġdecided Ġto Ġkeep Ġit Ġa Ġsecret Ġwhen Ġthey Ġproposed Ġrules .\n",
      "\t Tokenized LLAMA3:HC FA Ġdecided Ġto Ġkeep Ġit Ġa Ġsecret Ġwhen Ġthey Ġproposed Ġrules .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: evaluation questions.\n",
      "\t Tokenized GPT2:e valuation Ġquestions .\n",
      "\t Tokenized LLAMA3:e valuation Ġquestions .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: There are evaluation questions on the topic.\n",
      "\t Tokenized GPT2:There Ġare Ġevaluation Ġquestions Ġon Ġthe Ġtopic .\n",
      "\t Tokenized LLAMA3:There Ġare Ġevaluation Ġquestions Ġon Ġthe Ġtopic .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: oh yeah IBM uh i mean uh a lot of people use human factors folks but IBM is what i'm looking at right now\n",
      "\t Tokenized GPT2:oh Ġyeah ĠIBM Ġuh Ġi Ġmean Ġuh Ġa Ġlot Ġof Ġpeople Ġuse Ġhuman Ġfactors Ġfolks Ġbut ĠIBM Ġis Ġwhat Ġi 'm Ġlooking Ġat Ġright Ġnow\n",
      "\t Tokenized LLAMA3:oh Ġyeah ĠIBM Ġuh Ġi Ġmean Ġuh Ġa Ġlot Ġof Ġpeople Ġuse Ġhuman Ġfactors Ġfolks Ġbut ĠIBM Ġis Ġwhat Ġi 'm Ġlooking Ġat Ġright Ġnow\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I'm looking at IBM right now but I've looked at tons of other things.\n",
      "\t Tokenized GPT2:I 'm Ġlooking Ġat ĠIBM Ġright Ġnow Ġbut ĠI 've Ġlooked Ġat Ġtons Ġof Ġother Ġthings .\n",
      "\t Tokenized LLAMA3:I 'm Ġlooking Ġat ĠIBM Ġright Ġnow Ġbut ĠI 've Ġlooked Ġat Ġtons Ġof Ġother Ġthings .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Pro-Microsoft analysts spin this as a heroic sacrifice, removing the lightning rod whose seemingly disingenuous testimony has ostensibly driven the DOJ to the verge of demanding the company's breakup.\n",
      "\t Tokenized GPT2:Pro - Microsoft Ġanalysts Ġspin Ġthis Ġas Ġa Ġheroic Ġsacrifice , Ġremoving Ġthe Ġlightning Ġrod Ġwhose Ġseemingly Ġdis ing enu ous Ġtestimony Ġhas Ġo st ens ibly Ġdriven Ġthe ĠDO J Ġto Ġthe Ġverge Ġof Ġdemanding Ġthe Ġcompany 's Ġbreak up .\n",
      "\t Tokenized LLAMA3:Pro -M icrosoft Ġanalysts Ġspin Ġthis Ġas Ġa Ġheroic Ġsacrifice , Ġremoving Ġthe Ġlightning Ġrod Ġwhose Ġseemingly Ġdis ing enu ous Ġtestimony Ġhas Ġo st ens ibly Ġdriven Ġthe ĠDO J Ġto Ġthe Ġverge Ġof Ġdemanding Ġthe Ġcompany 's Ġbreak up .\n",
      "\t Unique Tokens GPT2: {'-', 'Microsoft'}\n",
      "\t Unique Tokens LLAMA3: {'-M', 'icrosoft'}\n",
      "Text 2: Pro-Microsoft analysts say that was a sacrifice for the company, risking their future.\n",
      "\t Tokenized GPT2:Pro - Microsoft Ġanalysts Ġsay Ġthat Ġwas Ġa Ġsacrifice Ġfor Ġthe Ġcompany , Ġrisk ing Ġtheir Ġfuture .\n",
      "\t Tokenized LLAMA3:Pro -M icrosoft Ġanalysts Ġsay Ġthat Ġwas Ġa Ġsacrifice Ġfor Ġthe Ġcompany , Ġrisk ing Ġtheir Ġfuture .\n",
      "\t Unique Tokens GPT2: {'-', 'Microsoft'}\n",
      "\t Unique Tokens LLAMA3: {'-M', 'icrosoft'}\n",
      "==entailment==\n",
      "Text 1: someone else noticed it and i said well i guess that's true and it was somewhat melodio us in other words it wasn't just you know it was really funny\n",
      "\t Tokenized GPT2:someone Ġelse Ġnoticed Ġit Ġand Ġi Ġsaid Ġwell Ġi Ġguess Ġthat 's Ġtrue Ġand Ġit Ġwas Ġsomewhat Ġmel od io Ġus Ġin Ġother Ġwords Ġit Ġwasn 't Ġjust Ġyou Ġknow Ġit Ġwas Ġreally Ġfunny\n",
      "\t Tokenized LLAMA3:someone Ġelse Ġnoticed Ġit Ġand Ġi Ġsaid Ġwell Ġi Ġguess Ġthat 's Ġtrue Ġand Ġit Ġwas Ġsomewhat Ġmel od io Ġus Ġin Ġother Ġwords Ġit Ġwasn 't Ġjust Ġyou Ġknow Ġit Ġwas Ġreally Ġfunny\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Someone else paid attention to it and it was really funny. \n",
      "\t Tokenized GPT2:Someone Ġelse Ġpaid Ġattention Ġto Ġit Ġand Ġit Ġwas Ġreally Ġfunny . Ġ\n",
      "\t Tokenized LLAMA3:Someone Ġelse Ġpaid Ġattention Ġto Ġit Ġand Ġit Ġwas Ġreally Ġfunny . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The mansions have been downgraded to consulates since the capital was transferred to Ankara in 1923, and modern shops and restaurants have sprung up.\n",
      "\t Tokenized GPT2:The Ġmans ions Ġhave Ġbeen Ġdown grad ed Ġto Ġcons ulates Ġsince Ġthe Ġcapital Ġwas Ġtransferred Ġto ĠAn k ara Ġin Ġ19 23 , Ġand Ġmodern Ġshops Ġand Ġrestaurants Ġhave Ġspr ung Ġup .\n",
      "\t Tokenized LLAMA3:The Ġmans ions Ġhave Ġbeen Ġdown grad ed Ġto Ġcons ulates Ġsince Ġthe Ġcapital Ġwas Ġtransferred Ġto ĠAn k ara Ġin Ġ 192 3 , Ġand Ġmodern Ġshops Ġand Ġrestaurants Ġhave Ġspr ung Ġup .\n",
      "\t Unique Tokens GPT2: {'Ġ19', '23'}\n",
      "\t Unique Tokens LLAMA3: {'3', '192', 'Ġ'}\n",
      "Text 2: The city of Ankara has always been the capital of the nation.\n",
      "\t Tokenized GPT2:The Ġcity Ġof ĠAn k ara Ġhas Ġalways Ġbeen Ġthe Ġcapital Ġof Ġthe Ġnation .\n",
      "\t Tokenized LLAMA3:The Ġcity Ġof ĠAn k ara Ġhas Ġalways Ġbeen Ġthe Ġcapital Ġof Ġthe Ġnation .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Still, it would be interesting to know. 109 Poirot looked at me very earnestly, and again shook his head. \n",
      "\t Tokenized GPT2:Still , Ġit Ġwould Ġbe Ġinteresting Ġto Ġknow . Ġ109 ĠP oir ot Ġlooked Ġat Ġme Ġvery Ġearnest ly , Ġand Ġagain Ġshook Ġhis Ġhead . Ġ\n",
      "\t Tokenized LLAMA3:Still , Ġit Ġwould Ġbe Ġinteresting Ġto Ġknow . Ġ 109 ĠP oir ot Ġlooked Ġat Ġme Ġvery Ġearnest ly , Ġand Ġagain Ġshook Ġhis Ġhead . Ġ\n",
      "\t Unique Tokens GPT2: {'Ġ109'}\n",
      "\t Unique Tokens LLAMA3: {'109'}\n",
      "Text 2: Poirot was disappointed with me.\n",
      "\t Tokenized GPT2:P oir ot Ġwas Ġdisappointed Ġwith Ġme .\n",
      "\t Tokenized LLAMA3:P oir ot Ġwas Ġdisappointed Ġwith Ġme .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Also downtown is the Flower Market, on Wall and 8th streets; fresh-cut flowers and a variety of plants can be had for bargain prices, but the best selections are found before dawn.\n",
      "\t Tokenized GPT2:Also Ġdowntown Ġis Ġthe ĠFl ower ĠMarket , Ġon ĠWall Ġand Ġ8 th Ġstreets ; Ġfresh - cut Ġflowers Ġand Ġa Ġvariety Ġof Ġplants Ġcan Ġbe Ġhad Ġfor Ġbargain Ġprices , Ġbut Ġthe Ġbest Ġselections Ġare Ġfound Ġbefore Ġdawn .\n",
      "\t Tokenized LLAMA3:Also Ġdowntown Ġis Ġthe ĠFl ower ĠMarket , Ġon ĠWall Ġand Ġ 8 th Ġstreets ; Ġfresh -cut Ġflowers Ġand Ġa Ġvariety Ġof Ġplants Ġcan Ġbe Ġhad Ġfor Ġbargain Ġprices , Ġbut Ġthe Ġbest Ġse lections Ġare Ġfound Ġbefore Ġdawn .\n",
      "\t Unique Tokens GPT2: {'-', 'Ġselections', 'cut', 'Ġ8'}\n",
      "\t Unique Tokens LLAMA3: {'-cut', 'lections', 'Ġ', '8', 'Ġse'}\n",
      "Text 2: Fresh-cut flowers available at the market range from cheap to expensive.\n",
      "\t Tokenized GPT2:F resh - cut Ġflowers Ġavailable Ġat Ġthe Ġmarket Ġrange Ġfrom Ġcheap Ġto Ġexpensive .\n",
      "\t Tokenized LLAMA3:F resh -cut Ġflowers Ġavailable Ġat Ġthe Ġmarket Ġrange Ġfrom Ġcheap Ġto Ġexpensive .\n",
      "\t Unique Tokens GPT2: {'-', 'cut'}\n",
      "\t Unique Tokens LLAMA3: {'-cut'}\n",
      "==contradiction==\n",
      "Text 1: you know like CODA comes out of your out of your pay and the credit union comes out of your pay so we don't have to do anything there and the rest of it as far as my salary goes i just have it automatically deposited in into our bank\n",
      "\t Tokenized GPT2:you Ġknow Ġlike ĠC OD A Ġcomes Ġout Ġof Ġyour Ġout Ġof Ġyour Ġpay Ġand Ġthe Ġcredit Ġunion Ġcomes Ġout Ġof Ġyour Ġpay Ġso Ġwe Ġdon 't Ġhave Ġto Ġdo Ġanything Ġthere Ġand Ġthe Ġrest Ġof Ġit Ġas Ġfar Ġas Ġmy Ġsalary Ġgoes Ġi Ġjust Ġhave Ġit Ġautomatically Ġdeposited Ġin Ġinto Ġour Ġbank\n",
      "\t Tokenized LLAMA3:you Ġknow Ġlike ĠC OD A Ġcomes Ġout Ġof Ġyour Ġout Ġof Ġyour Ġpay Ġand Ġthe Ġcredit Ġunion Ġcomes Ġout Ġof Ġyour Ġpay Ġso Ġwe Ġdon 't Ġhave Ġto Ġdo Ġanything Ġthere Ġand Ġthe Ġrest Ġof Ġit Ġas Ġfar Ġas Ġmy Ġsalary Ġgoes Ġi Ġjust Ġhave Ġit Ġautomatically Ġdeposited Ġin Ġinto Ġour Ġbank\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: After CODA and credit union, nothing is left of my salary.\n",
      "\t Tokenized GPT2:After ĠC OD A Ġand Ġcredit Ġunion , Ġnothing Ġis Ġleft Ġof Ġmy Ġsalary .\n",
      "\t Tokenized LLAMA3:After ĠC OD A Ġand Ġcredit Ġunion , Ġnothing Ġis Ġleft Ġof Ġmy Ġsalary .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The spot does leave the viewer wondering about the rest of the story, and what tale the condom could tell.\n",
      "\t Tokenized GPT2:The Ġspot Ġdoes Ġleave Ġthe Ġviewer Ġwondering Ġabout Ġthe Ġrest Ġof Ġthe Ġstory , Ġand Ġwhat Ġtale Ġthe Ġcond om Ġcould Ġtell .\n",
      "\t Tokenized LLAMA3:The Ġspot Ġdoes Ġleave Ġthe Ġviewer Ġwondering Ġabout Ġthe Ġrest Ġof Ġthe Ġstory , Ġand Ġwhat Ġtale Ġthe Ġcond om Ġcould Ġtell .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The spot resolves the storyline neatly for viewers, especially regarding the condom.\n",
      "\t Tokenized GPT2:The Ġspot Ġres olves Ġthe Ġstoryline Ġneatly Ġfor Ġviewers , Ġespecially Ġregarding Ġthe Ġcond om .\n",
      "\t Tokenized LLAMA3:The Ġspot Ġres olves Ġthe Ġstoryline Ġneatly Ġfor Ġviewers , Ġespecially Ġregarding Ġthe Ġcond om .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: and uh you know once you start up at the top and try to get those dollars on down to the hands that need them you know there's a lot of places the money stops and disappears along the way\n",
      "\t Tokenized GPT2:and Ġuh Ġyou Ġknow Ġonce Ġyou Ġstart Ġup Ġat Ġthe Ġtop Ġand Ġtry Ġto Ġget Ġthose Ġdollars Ġon Ġdown Ġto Ġthe Ġhands Ġthat Ġneed Ġthem Ġyou Ġknow Ġthere 's Ġa Ġlot Ġof Ġplaces Ġthe Ġmoney Ġstops Ġand Ġdisappears Ġalong Ġthe Ġway\n",
      "\t Tokenized LLAMA3:and Ġuh Ġyou Ġknow Ġonce Ġyou Ġstart Ġup Ġat Ġthe Ġtop Ġand Ġtry Ġto Ġget Ġthose Ġdollars Ġon Ġdown Ġto Ġthe Ġhands Ġthat Ġneed Ġthem Ġyou Ġknow Ġthere 's Ġa Ġlot Ġof Ġplaces Ġthe Ġmoney Ġstops Ġand Ġdisappears Ġalong Ġthe Ġway\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: All the money always gets into the hands of those who need it.\n",
      "\t Tokenized GPT2:All Ġthe Ġmoney Ġalways Ġgets Ġinto Ġthe Ġhands Ġof Ġthose Ġwho Ġneed Ġit .\n",
      "\t Tokenized LLAMA3:All Ġthe Ġmoney Ġalways Ġgets Ġinto Ġthe Ġhands Ġof Ġthose Ġwho Ġneed Ġit .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: yeah i've i wish they'd split that bowling season up into uh three seasons\n",
      "\t Tokenized GPT2:yeah Ġi 've Ġi Ġwish Ġthey 'd Ġsplit Ġthat Ġbow ling Ġseason Ġup Ġinto Ġuh Ġthree Ġseasons\n",
      "\t Tokenized LLAMA3:yeah Ġi 've Ġi Ġwish Ġthey 'd Ġsplit Ġthat Ġbow ling Ġseason Ġup Ġinto Ġuh Ġthree Ġseasons\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I am happy with the bowling season schedule as is.\n",
      "\t Tokenized GPT2:I Ġam Ġhappy Ġwith Ġthe Ġbow ling Ġseason Ġschedule Ġas Ġis .\n",
      "\t Tokenized LLAMA3:I Ġam Ġhappy Ġwith Ġthe Ġbow ling Ġseason Ġschedule Ġas Ġis .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Some travelers add Molokai and Lanai to their itineraries.\n",
      "\t Tokenized GPT2:Some Ġtravelers Ġadd ĠMol ok ai Ġand ĠLan ai Ġto Ġtheir Ġit iner aries .\n",
      "\t Tokenized LLAMA3:Some Ġtravel ers Ġadd ĠMol ok ai Ġand ĠLan ai Ġto Ġtheir Ġit iner aries .\n",
      "\t Unique Tokens GPT2: {'Ġtravelers'}\n",
      "\t Unique Tokens LLAMA3: {'Ġtravel', 'ers'}\n",
      "Text 2: Several tourists decide to plan for traveling to Molokai and Lanai.\n",
      "\t Tokenized GPT2:Several Ġtourists Ġdecide Ġto Ġplan Ġfor Ġtraveling Ġto ĠMol ok ai Ġand ĠLan ai .\n",
      "\t Tokenized LLAMA3:Several Ġtourists Ġdecide Ġto Ġplan Ġfor Ġtraveling Ġto ĠMol ok ai Ġand ĠLan ai .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: TIG funds support the Technology Evaluation Project, an initiative of the Legal Aid Society of Cincinnati.\n",
      "\t Tokenized GPT2:T IG Ġfunds Ġsupport Ġthe ĠTechnology ĠE valuation ĠProject , Ġan Ġinitiative Ġof Ġthe ĠLe gal ĠA id ĠSociety Ġof ĠC incinnati .\n",
      "\t Tokenized LLAMA3:T IG Ġfunds Ġsupport Ġthe ĠTechnology ĠE valuation ĠProject , Ġan Ġinitiative Ġof Ġthe ĠLe gal ĠA id ĠSociety Ġof ĠC incinnati .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: TIG funds are used to support the Technology Evolution project, a legal aid society in Cincinnati. \n",
      "\t Tokenized GPT2:T IG Ġfunds Ġare Ġused Ġto Ġsupport Ġthe ĠTechnology ĠEvolution Ġproject , Ġa Ġlegal Ġaid Ġsociety Ġin ĠC incinnati . Ġ\n",
      "\t Tokenized LLAMA3:T IG Ġfunds Ġare Ġused Ġto Ġsupport Ġthe ĠTechnology ĠEvolution Ġproject , Ġa Ġlegal Ġaid Ġsociety Ġin ĠC incinnati . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: oh i enjoyed it i mean it was just more for my money\n",
      "\t Tokenized GPT2:oh Ġi Ġenjoyed Ġit Ġi Ġmean Ġit Ġwas Ġjust Ġmore Ġfor Ġmy Ġmoney\n",
      "\t Tokenized LLAMA3:oh Ġi Ġenjoyed Ġit Ġi Ġmean Ġit Ġwas Ġjust Ġmore Ġfor Ġmy Ġmoney\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It was worth the money for the time.\n",
      "\t Tokenized GPT2:It Ġwas Ġworth Ġthe Ġmoney Ġfor Ġthe Ġtime .\n",
      "\t Tokenized LLAMA3:It Ġwas Ġworth Ġthe Ġmoney Ġfor Ġthe Ġtime .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: His mother died when he was young, and he was adopted by the Brodkeys.\n",
      "\t Tokenized GPT2:His Ġmother Ġdied Ġwhen Ġhe Ġwas Ġyoung , Ġand Ġhe Ġwas Ġadopted Ġby Ġthe ĠBro d keys .\n",
      "\t Tokenized LLAMA3:His Ġmother Ġdied Ġwhen Ġhe Ġwas Ġyoung , Ġand Ġhe Ġwas Ġadopted Ġby Ġthe ĠBro d keys .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He changed his name to Brodkey when he was adopted.\n",
      "\t Tokenized GPT2:He Ġchanged Ġhis Ġname Ġto ĠBro d key Ġwhen Ġhe Ġwas Ġadopted .\n",
      "\t Tokenized LLAMA3:He Ġchanged Ġhis Ġname Ġto ĠBro d key Ġwhen Ġhe Ġwas Ġadopted .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: 4 million homes watch the evening news on CBS, ABC, and NBC.\n",
      "\t Tokenized GPT2:4 Ġmillion Ġhomes Ġwatch Ġthe Ġevening Ġnews Ġon ĠCBS , ĠABC , Ġand ĠNBC .\n",
      "\t Tokenized LLAMA3:4 Ġmillion Ġhomes Ġwatch Ġthe Ġevening Ġnews Ġon ĠCBS , ĠABC , Ġand ĠNBC .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: CBS, ABC and NBC are the leaders in news. \n",
      "\t Tokenized GPT2:C BS , ĠABC Ġand ĠNBC Ġare Ġthe Ġleaders Ġin Ġnews . Ġ\n",
      "\t Tokenized LLAMA3:C BS , ĠABC Ġand ĠNBC Ġare Ġthe Ġleaders Ġin Ġnews . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: And he claimed she earned $11,000 a month - or $132,000 a year - from a home quilting business she had owned for 22 years.\n",
      "\t Tokenized GPT2:And Ġhe Ġclaimed Ġshe Ġearned Ġ$ 11 , 000 Ġa Ġmonth Ġ- Ġor Ġ$ 132 , 000 Ġa Ġyear Ġ- Ġfrom Ġa Ġhome Ġqu il ting Ġbusiness Ġshe Ġhad Ġowned Ġfor Ġ22 Ġyears .\n",
      "\t Tokenized LLAMA3:And Ġhe Ġclaimed Ġshe Ġearned Ġ$ 11 , 000 Ġa Ġmonth Ġ- Ġor Ġ$ 132 , 000 Ġa Ġyear Ġ- Ġfrom Ġa Ġhome Ġqu il ting Ġbusiness Ġshe Ġhad Ġowned Ġfor Ġ 22 Ġyears .\n",
      "\t Unique Tokens GPT2: {'Ġ22'}\n",
      "\t Unique Tokens LLAMA3: {'22', 'Ġ'}\n",
      "Text 2: He said she had a business that she started 10 years ago.\n",
      "\t Tokenized GPT2:He Ġsaid Ġshe Ġhad Ġa Ġbusiness Ġthat Ġshe Ġstarted Ġ10 Ġyears Ġago .\n",
      "\t Tokenized LLAMA3:He Ġsaid Ġshe Ġhad Ġa Ġbusiness Ġthat Ġshe Ġstarted Ġ 10 Ġyears Ġago .\n",
      "\t Unique Tokens GPT2: {'Ġ10'}\n",
      "\t Unique Tokens LLAMA3: {'10', 'Ġ'}\n",
      "==neutral==\n",
      "Text 1: yeah well Rochester's like right on the shores isn't it\n",
      "\t Tokenized GPT2:yeah Ġwell ĠRoche ster 's Ġlike Ġright Ġon Ġthe Ġsh ores Ġisn 't Ġit\n",
      "\t Tokenized LLAMA3:yeah Ġwell ĠR oche ster 's Ġlike Ġright Ġon Ġthe Ġsh ores Ġisn 't Ġit\n",
      "\t Unique Tokens GPT2: {'ĠRoche'}\n",
      "\t Unique Tokens LLAMA3: {'oche', 'ĠR'}\n",
      "Text 2: Rochester is right on the shores of the great lakes.\n",
      "\t Tokenized GPT2:R oche ster Ġis Ġright Ġon Ġthe Ġsh ores Ġof Ġthe Ġgreat Ġlakes .\n",
      "\t Tokenized LLAMA3:R oche ster Ġis Ġright Ġon Ġthe Ġsh ores Ġof Ġthe Ġgreat Ġl akes .\n",
      "\t Unique Tokens GPT2: {'Ġlakes'}\n",
      "\t Unique Tokens LLAMA3: {'akes', 'Ġl'}\n",
      "==entailment==\n",
      "Text 1: yeah that's probably a  a little bit under what it is for this time of year i i think i haven't seen the weather the news the weather on the news in the evening lately but i think the average high would be it should be about seventy\n",
      "\t Tokenized GPT2:yeah Ġthat 's Ġprobably Ġa Ġ Ġa Ġlittle Ġbit Ġunder Ġwhat Ġit Ġis Ġfor Ġthis Ġtime Ġof Ġyear Ġi Ġi Ġthink Ġi Ġhaven 't Ġseen Ġthe Ġweather Ġthe Ġnews Ġthe Ġweather Ġon Ġthe Ġnews Ġin Ġthe Ġevening Ġlately Ġbut Ġi Ġthink Ġthe Ġaverage Ġhigh Ġwould Ġbe Ġit Ġshould Ġbe Ġabout Ġsevent y\n",
      "\t Tokenized LLAMA3:yeah Ġthat 's Ġprobably Ġa Ġ Ġa Ġlittle Ġbit Ġunder Ġwhat Ġit Ġis Ġfor Ġthis Ġtime Ġof Ġyear Ġi Ġi Ġthink Ġi Ġhaven 't Ġseen Ġthe Ġweather Ġthe Ġnews Ġthe Ġweather Ġon Ġthe Ġnews Ġin Ġthe Ġevening Ġlately Ġbut Ġi Ġthink Ġthe Ġaverage Ġhigh Ġwould Ġbe Ġit Ġshould Ġbe Ġabout Ġsevent y\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I have not viewed the weather lately on the evening news.\n",
      "\t Tokenized GPT2:I Ġhave Ġnot Ġviewed Ġthe Ġweather Ġlately Ġon Ġthe Ġevening Ġnews .\n",
      "\t Tokenized LLAMA3:I Ġhave Ġnot Ġviewed Ġthe Ġweather Ġlately Ġon Ġthe Ġevening Ġnews .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: uh wasn't that Jane Eyre no he wrote Jane Eyre too\n",
      "\t Tokenized GPT2:uh Ġwasn 't Ġthat ĠJane ĠE y re Ġno Ġhe Ġwrote ĠJane ĠE y re Ġtoo\n",
      "\t Tokenized LLAMA3:uh Ġwasn 't Ġthat ĠJane ĠE y re Ġno Ġhe Ġwrote ĠJane ĠE y re Ġtoo\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He did not write Jane Eyre or any other book.\n",
      "\t Tokenized GPT2:He Ġdid Ġnot Ġwrite ĠJane ĠE y re Ġor Ġany Ġother Ġbook .\n",
      "\t Tokenized LLAMA3:He Ġdid Ġnot Ġwrite ĠJane ĠE y re Ġor Ġany Ġother Ġbook .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Hardly catering to locals, Universal Citys Cityalk attempts to snag tourist dollars with its extensive collection of retail wonders, including magic shops, toy stores, sports shops, and a host of science fiction memorabilia.\n",
      "\t Tokenized GPT2:Hard ly Ġcater ing Ġto Ġlocals , ĠUniversal ĠCity s ĠCity alk Ġattempts Ġto Ġsn ag Ġtourist Ġdollars Ġwith Ġits Ġextensive Ġcollection Ġof Ġretail Ġwonders , Ġincluding Ġmagic Ġshops , Ġtoy Ġstores , Ġsports Ġshops , Ġand Ġa Ġhost Ġof Ġscience Ġfiction Ġmemor abil ia .\n",
      "\t Tokenized LLAMA3:Hard ly Ġcater ing Ġto Ġlocals , ĠUniversal ĠCity s ĠCity alk Ġattempts Ġto Ġsn ag Ġtourist Ġdollars Ġwith Ġits Ġextensive Ġcollection Ġof Ġretail Ġwonders , Ġincluding Ġmagic Ġshops , Ġtoy Ġstores , Ġsports Ġshops , Ġand Ġa Ġhost Ġof Ġscience Ġfiction Ġmemor abil ia .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Locals aren't the primary target of the many magic shops, toy stores, and sports shops.\n",
      "\t Tokenized GPT2:Loc als Ġaren 't Ġthe Ġprimary Ġtarget Ġof Ġthe Ġmany Ġmagic Ġshops , Ġtoy Ġstores , Ġand Ġsports Ġshops .\n",
      "\t Tokenized LLAMA3:Loc als Ġaren 't Ġthe Ġprimary Ġtarget Ġof Ġthe Ġmany Ġmagic Ġshops , Ġtoy Ġstores , Ġand Ġsports Ġshops .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Normally, these discussions are kept secret.\n",
      "\t Tokenized GPT2:Norm ally , Ġthese Ġdiscussions Ġare Ġkept Ġsecret .\n",
      "\t Tokenized LLAMA3:Norm ally , Ġthese Ġdiscussions Ġare Ġkept Ġsecret .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: In usual circumstances, what is said is not to be shared..\n",
      "\t Tokenized GPT2:In Ġusual Ġcircumstances , Ġwhat Ġis Ġsaid Ġis Ġnot Ġto Ġbe Ġshared ..\n",
      "\t Tokenized LLAMA3:In Ġusual Ġcircumstances , Ġwhat Ġis Ġsaid Ġis Ġnot Ġto Ġbe Ġshared ..\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Participate in the postaward audit for assessing thedegree of success of the acquisition.\n",
      "\t Tokenized GPT2:Part icip ate Ġin Ġthe Ġpost a ward Ġaudit Ġfor Ġassessing Ġthe degree Ġof Ġsuccess Ġof Ġthe Ġacquisition .\n",
      "\t Tokenized LLAMA3:Part icip ate Ġin Ġthe Ġpost a ward Ġaudit Ġfor Ġassessing Ġthe de gree Ġof Ġsuccess Ġof Ġthe Ġacquisition .\n",
      "\t Unique Tokens GPT2: {'degree'}\n",
      "\t Unique Tokens LLAMA3: {'de', 'gree'}\n",
      "Text 2: An audit after the award has been presented.\n",
      "\t Tokenized GPT2:An Ġaudit Ġafter Ġthe Ġaward Ġhas Ġbeen Ġpresented .\n",
      "\t Tokenized LLAMA3:An Ġaudit Ġafter Ġthe Ġaward Ġhas Ġbeen Ġpresented .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: He fled in his car when cops arrived and led them on a chase that ended in the massive crash.\n",
      "\t Tokenized GPT2:He Ġfled Ġin Ġhis Ġcar Ġwhen Ġcops Ġarrived Ġand Ġled Ġthem Ġon Ġa Ġchase Ġthat Ġended Ġin Ġthe Ġmassive Ġcrash .\n",
      "\t Tokenized LLAMA3:He Ġfled Ġin Ġhis Ġcar Ġwhen Ġcops Ġarrived Ġand Ġled Ġthem Ġon Ġa Ġchase Ġthat Ġended Ġin Ġthe Ġmassive Ġcrash .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The cops set off in pursuit until a traffic accident happened.\n",
      "\t Tokenized GPT2:The Ġcops Ġset Ġoff Ġin Ġpursuit Ġuntil Ġa Ġtraffic Ġaccident Ġhappened .\n",
      "\t Tokenized LLAMA3:The Ġcops Ġset Ġoff Ġin Ġpursuit Ġuntil Ġa Ġtraffic Ġaccident Ġhappened .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The National Football League semifinals are set.\n",
      "\t Tokenized GPT2:The ĠNational ĠFootball ĠLeague Ġsem if inals Ġare Ġset .\n",
      "\t Tokenized LLAMA3:The ĠNational ĠFootball ĠLeague Ġsem if inals Ġare Ġset .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: They were unable to disclose when the dates would be set.\n",
      "\t Tokenized GPT2:They Ġwere Ġunable Ġto Ġdisclose Ġwhen Ġthe Ġdates Ġwould Ġbe Ġset .\n",
      "\t Tokenized LLAMA3:They Ġwere Ġunable Ġto Ġdisclose Ġwhen Ġthe Ġdates Ġwould Ġbe Ġset .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1:  Most menu prices include taxes and a service charge, but it's customary to leave a tip if you were served satisfactorily.\n",
      "\t Tokenized GPT2:ĠMost Ġmenu Ġprices Ġinclude Ġtaxes Ġand Ġa Ġservice Ġcharge , Ġbut Ġit 's Ġcustom ary Ġto Ġleave Ġa Ġtip Ġif Ġyou Ġwere Ġserved Ġsatisf actor ily .\n",
      "\t Tokenized LLAMA3:ĠMost Ġmenu Ġprices Ġinclude Ġtaxes Ġand Ġa Ġservice Ġcharge , Ġbut Ġit 's Ġcustom ary Ġto Ġleave Ġa Ġtip Ġif Ġyou Ġwere Ġserved Ġsatisf actor ily .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Tips are not accepted at most restaurants, as there is already a sales tax.\n",
      "\t Tokenized GPT2:T ips Ġare Ġnot Ġaccepted Ġat Ġmost Ġrestaurants , Ġas Ġthere Ġis Ġalready Ġa Ġsales Ġtax .\n",
      "\t Tokenized LLAMA3:T ips Ġare Ġnot Ġaccepted Ġat Ġmost Ġrestaurants , Ġas Ġthere Ġis Ġalready Ġa Ġsales Ġtax .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: But they also don't seem to mind when the tranquillity of a Zen temple rock garden is shattered by recorded announcements blaring from loudspeakers parroting the information already contained in the leaflets provided at the ticket office; when heavy-metal pop music loudly emanates from the radio of \n",
      "\t Tokenized GPT2:But Ġthey Ġalso Ġdon 't Ġseem Ġto Ġmind Ġwhen Ġthe Ġtran qu ill ity Ġof Ġa ĠZen Ġtemple Ġrock Ġgarden Ġis Ġshattered Ġby Ġrecorded Ġannouncements Ġbl aring Ġfrom Ġloud spe akers Ġpar r oting Ġthe Ġinformation Ġalready Ġcontained Ġin Ġthe Ġleaf lets Ġprovided Ġat Ġthe Ġticket Ġoffice ; Ġwhen Ġheavy - met al Ġpop Ġmusic Ġloudly Ġem an ates Ġfrom Ġthe Ġradio Ġof Ġ\n",
      "\t Tokenized LLAMA3:But Ġthey Ġalso Ġdon 't Ġseem Ġto Ġmind Ġwhen Ġthe Ġtran qu ill ity Ġof Ġa ĠZen Ġtemple Ġrock Ġgarden Ġis Ġshattered Ġby Ġrecorded Ġannounce ments Ġbl aring Ġfrom Ġloud spe akers Ġpar r oting Ġthe Ġinformation Ġalready Ġcontained Ġin Ġthe Ġleaf lets Ġprovided Ġat Ġthe Ġticket Ġoffice ; Ġwhen Ġheavy -m etal Ġpop Ġmusic Ġloudly Ġem an ates Ġfrom Ġthe Ġradio Ġof Ġ\n",
      "\t Unique Tokens GPT2: {'al', 'Ġannouncements', 'met'}\n",
      "\t Unique Tokens LLAMA3: {'etal', 'Ġannounce', 'ments', '-m'}\n",
      "Text 2: A Zen temple rock garden is a zen place.\n",
      "\t Tokenized GPT2:A ĠZen Ġtemple Ġrock Ġgarden Ġis Ġa Ġz en Ġplace .\n",
      "\t Tokenized LLAMA3:A ĠZen Ġtemple Ġrock Ġgarden Ġis Ġa Ġz en Ġplace .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: She will step down from the court in December 2002.\n",
      "\t Tokenized GPT2:She Ġwill Ġstep Ġdown Ġfrom Ġthe Ġcourt Ġin ĠDecember Ġ2002 .\n",
      "\t Tokenized LLAMA3:She Ġwill Ġstep Ġdown Ġfrom Ġthe Ġcourt Ġin ĠDecember Ġ 200 2 .\n",
      "\t Unique Tokens GPT2: {'Ġ2002'}\n",
      "\t Unique Tokens LLAMA3: {'2', 'Ġ', '200'}\n",
      "Text 2: She's going to step down from the court in the winter of 2020.\n",
      "\t Tokenized GPT2:She 's Ġgoing Ġto Ġstep Ġdown Ġfrom Ġthe Ġcourt Ġin Ġthe Ġwinter Ġof Ġ2020 .\n",
      "\t Tokenized LLAMA3:She 's Ġgoing Ġto Ġstep Ġdown Ġfrom Ġthe Ġcourt Ġin Ġthe Ġwinter Ġof Ġ 202 0 .\n",
      "\t Unique Tokens GPT2: {'Ġ2020'}\n",
      "\t Unique Tokens LLAMA3: {'0', '202', 'Ġ'}\n",
      "==contradiction==\n",
      "Text 1: I took to him at once.\n",
      "\t Tokenized GPT2:I Ġtook Ġto Ġhim Ġat Ġonce .\n",
      "\t Tokenized LLAMA3:I Ġtook Ġto Ġhim Ġat Ġonce .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I was immediately repulsed by him, and still feel the same way about him. \n",
      "\t Tokenized GPT2:I Ġwas Ġimmediately Ġrep uls ed Ġby Ġhim , Ġand Ġstill Ġfeel Ġthe Ġsame Ġway Ġabout Ġhim . Ġ\n",
      "\t Tokenized LLAMA3:I Ġwas Ġimmediately Ġrep uls ed Ġby Ġhim , Ġand Ġstill Ġfeel Ġthe Ġsame Ġway Ġabout Ġhim . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Act Accounting the Great Management Reform Act\n",
      "\t Tokenized GPT2:Act ĠAccount ing Ġthe ĠGreat ĠManagement ĠRe form ĠAct\n",
      "\t Tokenized LLAMA3:Act ĠAccount ing Ġthe ĠGreat ĠManagement ĠRe form ĠAct\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Act accounting great management reform act \n",
      "\t Tokenized GPT2:Act Ġaccounting Ġgreat Ġmanagement Ġreform Ġact Ġ\n",
      "\t Tokenized LLAMA3:Act Ġaccounting Ġgreat Ġmanagement Ġreform Ġact Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: The importer pays duties that are required by law\n",
      "\t Tokenized GPT2:The Ġim porter Ġpays Ġduties Ġthat Ġare Ġrequired Ġby Ġlaw\n",
      "\t Tokenized LLAMA3:The Ġim porter Ġpays Ġduties Ġthat Ġare Ġrequired Ġby Ġlaw\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Importer pays taxed that law requires\n",
      "\t Tokenized GPT2:Im porter Ġpays Ġtax ed Ġthat Ġlaw Ġrequires\n",
      "\t Tokenized LLAMA3:Im porter Ġpays Ġtax ed Ġthat Ġlaw Ġrequires\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: The spear missed Vrenna by only a hand-span.\n",
      "\t Tokenized GPT2:The Ġspear Ġmissed ĠV ren na Ġby Ġonly Ġa Ġhand - span .\n",
      "\t Tokenized LLAMA3:The Ġspear Ġmissed ĠV ren na Ġby Ġonly Ġa Ġhand -s pan .\n",
      "\t Unique Tokens GPT2: {'-', 'span'}\n",
      "\t Unique Tokens LLAMA3: {'-s', 'pan'}\n",
      "Text 2: It was a short distance from the person to the weapon.\n",
      "\t Tokenized GPT2:It Ġwas Ġa Ġshort Ġdistance Ġfrom Ġthe Ġperson Ġto Ġthe Ġweapon .\n",
      "\t Tokenized LLAMA3:It Ġwas Ġa Ġshort Ġdistance Ġfrom Ġthe Ġperson Ġto Ġthe Ġweapon .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The strychnine had been found in a drawer in the prisoner's room. \n",
      "\t Tokenized GPT2:The Ġst ry chn ine Ġhad Ġbeen Ġfound Ġin Ġa Ġdrawer Ġin Ġthe Ġprisoner 's Ġroom . Ġ\n",
      "\t Tokenized LLAMA3:The Ġst ry chn ine Ġhad Ġbeen Ġfound Ġin Ġa Ġdrawer Ġin Ġthe Ġprisoner 's Ġroom . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The strychnine that was in the drawer was powdered. \n",
      "\t Tokenized GPT2:The Ġst ry chn ine Ġthat Ġwas Ġin Ġthe Ġdrawer Ġwas Ġpow dered . Ġ\n",
      "\t Tokenized LLAMA3:The Ġst ry chn ine Ġthat Ġwas Ġin Ġthe Ġdrawer Ġwas Ġpow dered . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Whether you drink beer or alcohol or not, a trip to Dublin isn't complete without a visit to some of its pubs don't miss this experience.\n",
      "\t Tokenized GPT2:Whether Ġyou Ġdrink Ġbeer Ġor Ġalcohol Ġor Ġnot , Ġa Ġtrip Ġto ĠDublin Ġisn 't Ġcomplete Ġwithout Ġa Ġvisit Ġto Ġsome Ġof Ġits Ġpub s Ġdon 't Ġmiss Ġthis Ġexperience .\n",
      "\t Tokenized LLAMA3:Whether Ġyou Ġdrink Ġbeer Ġor Ġalcohol Ġor Ġnot , Ġa Ġtrip Ġto ĠDublin Ġisn 't Ġcomplete Ġwithout Ġa Ġvisit Ġto Ġsome Ġof Ġits Ġpub s Ġdon 't Ġmiss Ġthis Ġexperience .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Dublin's pubs are beautiful and evocative, worth a trip even if you don't drink\n",
      "\t Tokenized GPT2:D ublin 's Ġpub s Ġare Ġbeautiful Ġand Ġev oc ative , Ġworth Ġa Ġtrip Ġeven Ġif Ġyou Ġdon 't Ġdrink\n",
      "\t Tokenized LLAMA3:D ublin 's Ġpub s Ġare Ġbeautiful Ġand Ġev oc ative , Ġworth Ġa Ġtrip Ġeven Ġif Ġyou Ġdon 't Ġdrink\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: While documenting the basis for judgments can be more difficult than documenting nonjudgmental information, overall the chain of evidence or audit trail techniques should not pose any greater difficulty for GAO evaluators than our documentation procedures for other evaluation methods.\n",
      "\t Tokenized GPT2:While Ġdocument ing Ġthe Ġbasis Ġfor Ġjud gments Ġcan Ġbe Ġmore Ġdifficult Ġthan Ġdocument ing Ġnon jud gment al Ġinformation , Ġoverall Ġthe Ġchain Ġof Ġevidence Ġor Ġaudit Ġtrail Ġtechniques Ġshould Ġnot Ġpose Ġany Ġgreater Ġdifficulty Ġfor ĠGA O Ġevalu ators Ġthan Ġour Ġdocumentation Ġprocedures Ġfor Ġother Ġevaluation Ġmethods .\n",
      "\t Tokenized LLAMA3:While Ġdocument ing Ġthe Ġbasis Ġfor Ġjud gments Ġcan Ġbe Ġmore Ġdifficult Ġthan Ġdocument ing Ġnon jud gment al Ġinformation , Ġoverall Ġthe Ġchain Ġof Ġevidence Ġor Ġaudit Ġtrail Ġtechniques Ġshould Ġnot Ġpose Ġany Ġgreater Ġdifficulty Ġfor ĠGA O Ġevalu ators Ġthan Ġour Ġdocumentation Ġprocedures Ġfor Ġother Ġevaluation Ġmethods .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: GAO evaluators are trained to analyze and document the chain of evidence.\n",
      "\t Tokenized GPT2:GA O Ġevalu ators Ġare Ġtrained Ġto Ġanalyze Ġand Ġdocument Ġthe Ġchain Ġof Ġevidence .\n",
      "\t Tokenized LLAMA3:GA O Ġevalu ators Ġare Ġtrained Ġto Ġanalyze Ġand Ġdocument Ġthe Ġchain Ġof Ġevidence .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: um-hum um-hum yeah well uh i can see you know it's it's it's it's kind of funny because we it seems like we loan money you know we money with strings attached and if the government changes and the country that we loan the money to um i can see why the might have a different attitude towards paying i\n",
      "\t Tokenized GPT2:um - hum Ġum - hum Ġyeah Ġwell Ġuh Ġi Ġcan Ġsee Ġyou Ġknow Ġit 's Ġit 's Ġit 's Ġit 's Ġkind Ġof Ġfunny Ġbecause Ġwe Ġit Ġseems Ġlike Ġwe Ġloan Ġmoney Ġyou Ġknow Ġwe Ġmoney Ġwith Ġstrings Ġattached Ġand Ġif Ġthe Ġgovernment Ġchanges Ġand Ġthe Ġcountry Ġthat Ġwe Ġloan Ġthe Ġmoney Ġto Ġum Ġi Ġcan Ġsee Ġwhy Ġthe Ġmight Ġhave Ġa Ġdifferent Ġattitude Ġtowards Ġpaying Ġi\n",
      "\t Tokenized LLAMA3:um -h um Ġum -h um Ġyeah Ġwell Ġuh Ġi Ġcan Ġsee Ġyou Ġknow Ġit 's Ġit 's Ġit 's Ġit 's Ġkind Ġof Ġfunny Ġbecause Ġwe Ġit Ġseems Ġlike Ġwe Ġloan Ġmoney Ġyou Ġknow Ġwe Ġmoney Ġwith Ġstrings Ġattached Ġand Ġif Ġthe Ġgovernment Ġchanges Ġand Ġthe Ġcountry Ġthat Ġwe Ġloan Ġthe Ġmoney Ġto Ġum Ġi Ġcan Ġsee Ġwhy Ġthe Ġmight Ġhave Ġa Ġdifferent Ġattitude Ġtowards Ġpaying Ġi\n",
      "\t Unique Tokens GPT2: {'-', 'hum'}\n",
      "\t Unique Tokens LLAMA3: {'-h'}\n",
      "Text 2: We loan a lot of money with strings attached and I feel bad about it.\n",
      "\t Tokenized GPT2:We Ġloan Ġa Ġlot Ġof Ġmoney Ġwith Ġstrings Ġattached Ġand ĠI Ġfeel Ġbad Ġabout Ġit .\n",
      "\t Tokenized LLAMA3:We Ġloan Ġa Ġlot Ġof Ġmoney Ġwith Ġstrings Ġattached Ġand ĠI Ġfeel Ġbad Ġabout Ġit .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: At the end of the Wars of Spanish, Austrian, and Polish Succession, the Austrians had taken over northern Italy from the Spanish.\n",
      "\t Tokenized GPT2:At Ġthe Ġend Ġof Ġthe ĠWars Ġof ĠSpanish , ĠAust rian , Ġand ĠPolish ĠSuccess ion , Ġthe ĠAust rians Ġhad Ġtaken Ġover Ġnorthern ĠItaly Ġfrom Ġthe ĠSpanish .\n",
      "\t Tokenized LLAMA3:At Ġthe Ġend Ġof Ġthe ĠWars Ġof ĠSpanish , ĠAust rian , Ġand ĠPolish ĠSuccess ion , Ġthe ĠAust rians Ġhad Ġtaken Ġover Ġnorthern ĠItaly Ġfrom Ġthe ĠSpanish .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Northern Italy was not easily given up to the Austrians at the end of the Wars of Spanish, Austrian and Polish Succession.\n",
      "\t Tokenized GPT2:N orthern ĠItaly Ġwas Ġnot Ġeasily Ġgiven Ġup Ġto Ġthe ĠAust rians Ġat Ġthe Ġend Ġof Ġthe ĠWars Ġof ĠSpanish , ĠAust rian Ġand ĠPolish ĠSuccess ion .\n",
      "\t Tokenized LLAMA3:N orthern ĠItaly Ġwas Ġnot Ġeasily Ġgiven Ġup Ġto Ġthe ĠAust rians Ġat Ġthe Ġend Ġof Ġthe ĠWars Ġof ĠSpanish , ĠAust rian Ġand ĠPolish ĠSuccess ion .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Although claims data provide the most accurate information about health care use, ensuring adequate follow-up for purposes of obtaining information from patient self-report is important because many people do not report alcohol-related events to insurance compa-nies.\n",
      "\t Tokenized GPT2:Although Ġclaims Ġdata Ġprovide Ġthe Ġmost Ġaccurate Ġinformation Ġabout Ġhealth Ġcare Ġuse , Ġensuring Ġadequate Ġfollow - up Ġfor Ġpurposes Ġof Ġobtaining Ġinformation Ġfrom Ġpatient Ġself - report Ġis Ġimportant Ġbecause Ġmany Ġpeople Ġdo Ġnot Ġreport Ġalcohol - related Ġevents Ġto Ġinsurance Ġcomp a - n ies .\n",
      "\t Tokenized LLAMA3:Although Ġclaims Ġdata Ġprovide Ġthe Ġmost Ġaccurate Ġinformation Ġabout Ġhealth Ġcare Ġuse , Ġensuring Ġadequate Ġfollow -up Ġfor Ġpurposes Ġof Ġobtaining Ġinformation Ġfrom Ġpatient Ġself -re port Ġis Ġimportant Ġbecause Ġmany Ġpeople Ġdo Ġnot Ġreport Ġalcohol -related Ġevents Ġto Ġinsurance Ġcomp a -n ies .\n",
      "\t Unique Tokens GPT2: {'n', 'up', 'report', '-', 'related'}\n",
      "\t Unique Tokens LLAMA3: {'-n', 'port', '-related', '-re', '-up'}\n",
      "Text 2: Patients naturally always report to insurance companies when health problems may be a direct result of alcohol. \n",
      "\t Tokenized GPT2:Pat ients Ġnaturally Ġalways Ġreport Ġto Ġinsurance Ġcompanies Ġwhen Ġhealth Ġproblems Ġmay Ġbe Ġa Ġdirect Ġresult Ġof Ġalcohol . Ġ\n",
      "\t Tokenized LLAMA3:Pat ients Ġnaturally Ġalways Ġreport Ġto Ġinsurance Ġcompanies Ġwhen Ġhealth Ġproblems Ġmay Ġbe Ġa Ġdirect Ġresult Ġof Ġalcohol . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The Committee intends that LSC consult with appropriate stakeholders in developing this proposal.\n",
      "\t Tokenized GPT2:The ĠCommittee Ġintends Ġthat ĠL SC Ġconsult Ġwith Ġappropriate Ġstakeholders Ġin Ġdeveloping Ġthis Ġproposal .\n",
      "\t Tokenized LLAMA3:The ĠCommittee Ġintend s Ġthat ĠL SC Ġconsult Ġwith Ġappropriate Ġstakeholders Ġin Ġdeveloping Ġthis Ġproposal .\n",
      "\t Unique Tokens GPT2: {'Ġintends'}\n",
      "\t Unique Tokens LLAMA3: {'Ġintend', 's'}\n",
      "Text 2: The Committee will cover all consultation expenses incurred by LSC.\n",
      "\t Tokenized GPT2:The ĠCommittee Ġwill Ġcover Ġall Ġconsultation Ġexpenses Ġincur red Ġby ĠL SC .\n",
      "\t Tokenized LLAMA3:The ĠCommittee Ġwill Ġcover Ġall Ġconsultation Ġexpenses Ġinc urred Ġby ĠL SC .\n",
      "\t Unique Tokens GPT2: {'red', 'Ġincur'}\n",
      "\t Unique Tokens LLAMA3: {'urred', 'Ġinc'}\n",
      "==neutral==\n",
      "Text 1: I smiled vaguely.\n",
      "\t Tokenized GPT2:I Ġsmiled Ġvaguely .\n",
      "\t Tokenized LLAMA3:I Ġsmiled Ġvaguely .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I was feeling sure of myself.\n",
      "\t Tokenized GPT2:I Ġwas Ġfeeling Ġsure Ġof Ġmyself .\n",
      "\t Tokenized LLAMA3:I Ġwas Ġfeeling Ġsure Ġof Ġmyself .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: This is arguably starting to distort the practice of science itself.\n",
      "\t Tokenized GPT2:This Ġis Ġarguably Ġstarting Ġto Ġdist ort Ġthe Ġpractice Ġof Ġscience Ġitself .\n",
      "\t Tokenized LLAMA3:This Ġis Ġarguably Ġstarting Ġto Ġdist ort Ġthe Ġpractice Ġof Ġscience Ġitself .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: This began to distort scientific practice. \n",
      "\t Tokenized GPT2:This Ġbegan Ġto Ġdist ort Ġscientific Ġpractice . Ġ\n",
      "\t Tokenized LLAMA3:This Ġbegan Ġto Ġdist ort Ġscientific Ġpractice . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The disorder hardly seemed to exist before the stimulant Ritalin came along.\n",
      "\t Tokenized GPT2:The Ġdisorder Ġhardly Ġseemed Ġto Ġexist Ġbefore Ġthe Ġstim ul ant ĠR ital in Ġcame Ġalong .\n",
      "\t Tokenized LLAMA3:The Ġdisorder Ġhardly Ġseemed Ġto Ġexist Ġbefore Ġthe Ġstim ul ant ĠR ital in Ġcame Ġalong .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The only time the disorder seemed to exist was before Ritalin came around.\n",
      "\t Tokenized GPT2:The Ġonly Ġtime Ġthe Ġdisorder Ġseemed Ġto Ġexist Ġwas Ġbefore ĠR ital in Ġcame Ġaround .\n",
      "\t Tokenized LLAMA3:The Ġonly Ġtime Ġthe Ġdisorder Ġseemed Ġto Ġexist Ġwas Ġbefore ĠR ital in Ġcame Ġaround .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: The end is near!  Then a shout went up, and Hanson jerked his eyes from the gears to focus on a group of rocs that were landing at the far end of the camp.\n",
      "\t Tokenized GPT2:The Ġend Ġis Ġnear ! Ġ ĠThen Ġa Ġshout Ġwent Ġup , Ġand ĠHans on Ġjer ked Ġhis Ġeyes Ġfrom Ġthe Ġgears Ġto Ġfocus Ġon Ġa Ġgroup Ġof Ġro cs Ġthat Ġwere Ġlanding Ġat Ġthe Ġfar Ġend Ġof Ġthe Ġcamp .\n",
      "\t Tokenized LLAMA3:The Ġend Ġis Ġnear ! Ġ ĠThen Ġa Ġshout Ġwent Ġup , Ġand ĠHans on Ġjer ked Ġhis Ġeyes Ġfrom Ġthe Ġgears Ġto Ġfocus Ġon Ġa Ġgroup Ġof Ġro cs Ġthat Ġwere Ġlanding Ġat Ġthe Ġfar Ġend Ġof Ġthe Ġcamp .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Hanson redirected his gaze from the gears to the group of rocs.\n",
      "\t Tokenized GPT2:H anson Ġredirected Ġhis Ġgaze Ġfrom Ġthe Ġgears Ġto Ġthe Ġgroup Ġof Ġro cs .\n",
      "\t Tokenized LLAMA3:H anson Ġredirect ed Ġhis Ġgaze Ġfrom Ġthe Ġgears Ġto Ġthe Ġgroup Ġof Ġro cs .\n",
      "\t Unique Tokens GPT2: {'Ġredirected'}\n",
      "\t Unique Tokens LLAMA3: {'Ġredirect', 'ed'}\n",
      "==neutral==\n",
      "Text 1: On Menorca, search for more elusive prehistoric sites, or take the cliff paths of the northwest or south coasts.\n",
      "\t Tokenized GPT2:On ĠMen or ca , Ġsearch Ġfor Ġmore Ġel usive Ġpre histor ic Ġsites , Ġor Ġtake Ġthe Ġcliff Ġpaths Ġof Ġthe Ġnorth west Ġor Ġsouth Ġcoast s .\n",
      "\t Tokenized LLAMA3:On ĠMen or ca , Ġsearch Ġfor Ġmore Ġel usive Ġpre h istor ic Ġsites , Ġor Ġtake Ġthe Ġcliff Ġpaths Ġof Ġthe Ġnorth west Ġor Ġsouth Ġcoast s .\n",
      "\t Unique Tokens GPT2: {'histor'}\n",
      "\t Unique Tokens LLAMA3: {'h', 'istor'}\n",
      "Text 2: The cliff paths are a pleasant place to walk of Menorca.\n",
      "\t Tokenized GPT2:The Ġcliff Ġpaths Ġare Ġa Ġpleasant Ġplace Ġto Ġwalk Ġof ĠMen or ca .\n",
      "\t Tokenized LLAMA3:The Ġcliff Ġpaths Ġare Ġa Ġpleasant Ġplace Ġto Ġwalk Ġof ĠMen or ca .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: 4) Clinton's job rating fell from 60 to 55 points in a Washington Post poll, apparently because pollees disapproved of his use of the White House for fund raising.\n",
      "\t Tokenized GPT2:4 ) ĠClinton 's Ġjob Ġrating Ġfell Ġfrom Ġ60 Ġto Ġ55 Ġpoints Ġin Ġa ĠWashington ĠPost Ġpoll , Ġapparently Ġbecause Ġpol le es Ġdisappro ved Ġof Ġhis Ġuse Ġof Ġthe ĠWhite ĠHouse Ġfor Ġfund Ġraising .\n",
      "\t Tokenized LLAMA3:4 ) ĠClinton 's Ġjob Ġrating Ġfell Ġfrom Ġ 60 Ġto Ġ 55 Ġpoints Ġin Ġa ĠWashington ĠPost Ġpoll , Ġapparently Ġbecause Ġpol le es Ġdisappro ved Ġof Ġhis Ġuse Ġof Ġthe ĠWhite ĠHouse Ġfor Ġfund Ġraising .\n",
      "\t Unique Tokens GPT2: {'Ġ55', 'Ġ60'}\n",
      "\t Unique Tokens LLAMA3: {'55', 'Ġ', '60'}\n",
      "Text 2: Clinton's job ratings fell to an all-time low.\n",
      "\t Tokenized GPT2:Cl inton 's Ġjob Ġratings Ġfell Ġto Ġan Ġall - time Ġlow .\n",
      "\t Tokenized LLAMA3:Cl inton 's Ġjob Ġratings Ġfell Ġto Ġan Ġall -time Ġlow .\n",
      "\t Unique Tokens GPT2: {'time', '-'}\n",
      "\t Unique Tokens LLAMA3: {'-time'}\n",
      "==contradiction==\n",
      "Text 1: FEC Chairman Scott Thomas, a Democrat who was also at the conference, noted that the Federal Election Campaign Act of 1971 outlined three principles that need to be preserved on the  1) disclosure of how money is raised and spent to influence elections; 2) limits on the amount that any one person ca\n",
      "\t Tokenized GPT2:F EC ĠChairman ĠScott ĠThomas , Ġa ĠDemocrat Ġwho Ġwas Ġalso Ġat Ġthe Ġconference , Ġnoted Ġthat Ġthe ĠFederal ĠElection ĠCampaign ĠAct Ġof Ġ19 71 Ġoutlined Ġthree Ġprinciples Ġthat Ġneed Ġto Ġbe Ġpreserved Ġon Ġthe Ġ Ġ1 ) Ġdisclosure Ġof Ġhow Ġmoney Ġis Ġraised Ġand Ġspent Ġto Ġinfluence Ġelections ; Ġ2 ) Ġlimits Ġon Ġthe Ġamount Ġthat Ġany Ġone Ġperson Ġca\n",
      "\t Tokenized LLAMA3:F EC ĠChairman ĠScott ĠThomas , Ġa ĠDemocrat Ġwho Ġwas Ġalso Ġat Ġthe Ġconference , Ġnoted Ġthat Ġthe ĠFederal ĠE lection ĠCampaign ĠAct Ġof Ġ 197 1 Ġoutlined Ġthree Ġprinciples Ġthat Ġneed Ġto Ġbe Ġpreserved Ġon Ġthe Ġ Ġ 1 ) Ġdisclosure Ġof Ġhow Ġmoney Ġis Ġraised Ġand Ġspent Ġto Ġinfluence Ġelections ; Ġ 2 ) Ġlimits Ġon Ġthe Ġamount Ġthat Ġany Ġone Ġperson Ġca\n",
      "\t Unique Tokens GPT2: {'Ġ3', 'Ġ2', 'ĠElection', 'Ġ19', '71', 'Ġ1'}\n",
      "\t Unique Tokens LLAMA3: {'3', 'ĠE', 'lection', '197', '2', '1'}\n",
      "Text 2: Scott Thomas was the EPA Chairman.\n",
      "\t Tokenized GPT2:Scott ĠThomas Ġwas Ġthe ĠE PA ĠChairman .\n",
      "\t Tokenized LLAMA3:Scott ĠThomas Ġwas Ġthe ĠE PA ĠChairman .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: There is simply no historical precedent for a large empire calling it quits because it could not compete economically or technologically.\n",
      "\t Tokenized GPT2:There Ġis Ġsimply Ġno Ġhistorical Ġprecedent Ġfor Ġa Ġlarge Ġempire Ġcalling Ġit Ġqu its Ġbecause Ġit Ġcould Ġnot Ġcompete Ġeconomically Ġor Ġtechn ologically .\n",
      "\t Tokenized LLAMA3:There Ġis Ġsimply Ġno Ġhistorical Ġpreced ent Ġfor Ġa Ġlarge Ġempire Ġcalling Ġit Ġqu its Ġbecause Ġit Ġcould Ġnot Ġcompete Ġeconomically Ġor Ġtechn ologically .\n",
      "\t Unique Tokens GPT2: {'Ġprecedent'}\n",
      "\t Unique Tokens LLAMA3: {'ent', 'Ġpreced'}\n",
      "Text 2: Empires do not quit because they couldn't compete economically.\n",
      "\t Tokenized GPT2:E mp ires Ġdo Ġnot Ġquit Ġbecause Ġthey Ġcouldn 't Ġcompete Ġeconomically .\n",
      "\t Tokenized LLAMA3:E mp ires Ġdo Ġnot Ġquit Ġbecause Ġthey Ġcouldn 't Ġcompete Ġeconomically .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: i don't know um do you do a lot of camping\n",
      "\t Tokenized GPT2:i Ġdon 't Ġknow Ġum Ġdo Ġyou Ġdo Ġa Ġlot Ġof Ġcamping\n",
      "\t Tokenized LLAMA3:i Ġdon 't Ġknow Ġum Ġdo Ġyou Ġdo Ġa Ġlot Ġof Ġcamping\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I enjoy camping.\n",
      "\t Tokenized GPT2:I Ġenjoy Ġcamping .\n",
      "\t Tokenized LLAMA3:I Ġenjoy Ġcamping .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: But a list of who's better than other people in some aspect or another is not inevitable and does not make the economy any more prosperous or society any richer in other ways.\n",
      "\t Tokenized GPT2:But Ġa Ġlist Ġof Ġwho 's Ġbetter Ġthan Ġother Ġpeople Ġin Ġsome Ġaspect Ġor Ġanother Ġis Ġnot Ġinevitable Ġand Ġdoes Ġnot Ġmake Ġthe Ġeconomy Ġany Ġmore Ġprosper ous Ġor Ġsociety Ġany Ġricher Ġin Ġother Ġways .\n",
      "\t Tokenized LLAMA3:But Ġa Ġlist Ġof Ġwho 's Ġbetter Ġthan Ġother Ġpeople Ġin Ġsome Ġaspect Ġor Ġanother Ġis Ġnot Ġinevitable Ġand Ġdoes Ġnot Ġmake Ġthe Ġeconomy Ġany Ġmore Ġprosper ous Ġor Ġsociety Ġany Ġric her Ġin Ġother Ġways .\n",
      "\t Unique Tokens GPT2: {'Ġricher'}\n",
      "\t Unique Tokens LLAMA3: {'Ġric', 'her'}\n",
      "Text 2: A listing of those better than others is very helpful to the economy.\n",
      "\t Tokenized GPT2:A Ġlisting Ġof Ġthose Ġbetter Ġthan Ġothers Ġis Ġvery Ġhelpful Ġto Ġthe Ġeconomy .\n",
      "\t Tokenized LLAMA3:A Ġlisting Ġof Ġthose Ġbetter Ġthan Ġothers Ġis Ġvery Ġhelpful Ġto Ġthe Ġeconomy .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: 'I saw him get aboard myself.\n",
      "\t Tokenized GPT2:' I Ġsaw Ġhim Ġget Ġaboard Ġmyself .\n",
      "\t Tokenized LLAMA3:'I Ġsaw Ġhim Ġget Ġaboard Ġmyself .\n",
      "\t Unique Tokens GPT2: {'I', \"'\"}\n",
      "\t Unique Tokens LLAMA3: {\"'I\"}\n",
      "Text 2: I saw him get on the train.\n",
      "\t Tokenized GPT2:I Ġsaw Ġhim Ġget Ġon Ġthe Ġtrain .\n",
      "\t Tokenized LLAMA3:I Ġsaw Ġhim Ġget Ġon Ġthe Ġtrain .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: During his disastrous campaign in Russia, he found time in Moscow to draw up a new statute for the Com??die-Francaise (the national theater), which had been dissolved during the Revolution.\n",
      "\t Tokenized GPT2:During Ġhis Ġdis astrous Ġcampaign Ġin ĠRussia , Ġhe Ġfound Ġtime Ġin ĠMoscow Ġto Ġdraw Ġup Ġa Ġnew Ġstat ute Ġfor Ġthe ĠCom ?? die - Fr anc a ise Ġ( the Ġnational Ġtheater ), Ġwhich Ġhad Ġbeen Ġdissolved Ġduring Ġthe ĠRevolution .\n",
      "\t Tokenized LLAMA3:During Ġhis Ġdis ast rous Ġcampaign Ġin ĠRussia , Ġhe Ġfound Ġtime Ġin ĠMoscow Ġto Ġdraw Ġup Ġa Ġnew Ġstat ute Ġfor Ġthe ĠCom ?? die - Fr anc a ise Ġ( the Ġnational Ġtheater ), Ġwhich Ġhad Ġbeen Ġdissolved Ġduring Ġthe ĠRevolution .\n",
      "\t Unique Tokens GPT2: {'astrous'}\n",
      "\t Unique Tokens LLAMA3: {'ast', 'rous'}\n",
      "Text 2: Russia has been successfully invaded hundreds of times.\n",
      "\t Tokenized GPT2:Russia Ġhas Ġbeen Ġsuccessfully Ġinvaded Ġhundreds Ġof Ġtimes .\n",
      "\t Tokenized LLAMA3:Russia Ġhas Ġbeen Ġsuccessfully Ġinv aded Ġhundreds Ġof Ġtimes .\n",
      "\t Unique Tokens GPT2: {'Ġinvaded'}\n",
      "\t Unique Tokens LLAMA3: {'Ġinv', 'aded'}\n",
      "==neutral==\n",
      "Text 1:  Ibiza's seven-bulwark defences are almost completely intact.\n",
      "\t Tokenized GPT2:ĠI b iza 's Ġseven - bul w ark Ġdef ences Ġare Ġalmost Ġcompletely Ġintact .\n",
      "\t Tokenized LLAMA3:ĠI b iza 's Ġseven -b ul w ark Ġdef ences Ġare Ġalmost Ġcompletely Ġintact .\n",
      "\t Unique Tokens GPT2: {'-', 'bul'}\n",
      "\t Unique Tokens LLAMA3: {'ul', '-b'}\n",
      "Text 2: Ibiza was never attacked so the walls are in great condition.\n",
      "\t Tokenized GPT2:I b iza Ġwas Ġnever Ġattacked Ġso Ġthe Ġwalls Ġare Ġin Ġgreat Ġcondition .\n",
      "\t Tokenized LLAMA3:I b iza Ġwas Ġnever Ġattacked Ġso Ġthe Ġwalls Ġare Ġin Ġgreat Ġcondition .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: So unlike people who are fortunate enough to be able to afford attorneys and can go to another lawyer, our clients are simply lost in the legal system if they cannot get access to it from us.\n",
      "\t Tokenized GPT2:So Ġunlike Ġpeople Ġwho Ġare Ġfortunate Ġenough Ġto Ġbe Ġable Ġto Ġafford Ġattorneys Ġand Ġcan Ġgo Ġto Ġanother Ġlawyer , Ġour Ġclients Ġare Ġsimply Ġlost Ġin Ġthe Ġlegal Ġsystem Ġif Ġthey Ġcannot Ġget Ġaccess Ġto Ġit Ġfrom Ġus .\n",
      "\t Tokenized LLAMA3:So Ġunlike Ġpeople Ġwho Ġare Ġfortunate Ġenough Ġto Ġbe Ġable Ġto Ġafford Ġattorneys Ġand Ġcan Ġgo Ġto Ġanother Ġlawyer , Ġour Ġclients Ġare Ġsimply Ġlost Ġin Ġthe Ġlegal Ġsystem Ġif Ġthey Ġcannot Ġget Ġaccess Ġto Ġit Ġfrom Ġus .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Our clients can barely afford our legal assistance.\n",
      "\t Tokenized GPT2:Our Ġclients Ġcan Ġbarely Ġafford Ġour Ġlegal Ġassistance .\n",
      "\t Tokenized LLAMA3:Our Ġclients Ġcan Ġbarely Ġafford Ġour Ġlegal Ġassistance .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: bGross national saving is held constant as a share of GDP at 18.\n",
      "\t Tokenized GPT2:b G ross Ġnational Ġsaving Ġis Ġheld Ġconstant Ġas Ġa Ġshare Ġof ĠGDP Ġat Ġ18 .\n",
      "\t Tokenized LLAMA3:b G ross Ġnational Ġsaving Ġis Ġheld Ġconstant Ġas Ġa Ġshare Ġof ĠGDP Ġat Ġ 18 .\n",
      "\t Unique Tokens GPT2: {'Ġ18'}\n",
      "\t Unique Tokens LLAMA3: {'18', 'Ġ'}\n",
      "Text 2: bGross national saving represents a national bank.\n",
      "\t Tokenized GPT2:b G ross Ġnational Ġsaving Ġrepresents Ġa Ġnational Ġbank .\n",
      "\t Tokenized LLAMA3:b G ross Ġnational Ġsaving Ġrepresents Ġa Ġnational Ġbank .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Even if the entire unified surplus were saved, GDP per capita would fall somewhat short of the U.S. historical average of doubling every 35 years.\n",
      "\t Tokenized GPT2:Even Ġif Ġthe Ġentire Ġunified Ġsurplus Ġwere Ġsaved , ĠGDP Ġper Ġcap ita Ġwould Ġfall Ġsomewhat Ġshort Ġof Ġthe ĠU . S . Ġhistorical Ġaverage Ġof Ġdoub ling Ġevery Ġ35 Ġyears .\n",
      "\t Tokenized LLAMA3:Even Ġif Ġthe Ġentire Ġunified Ġsurplus Ġwere Ġsaved , ĠGDP Ġper Ġcap ita Ġwould Ġfall Ġsomewhat Ġshort Ġof Ġthe ĠU .S . Ġhistorical Ġaverage Ġof Ġdoub ling Ġevery Ġ 35 Ġyears .\n",
      "\t Unique Tokens GPT2: {'Ġ35', 'S'}\n",
      "\t Unique Tokens LLAMA3: {'.S', '35', 'Ġ'}\n",
      "Text 2: GDP would still be above 50,000 even given the conditions.\n",
      "\t Tokenized GPT2:G DP Ġwould Ġstill Ġbe Ġabove Ġ50 , 000 Ġeven Ġgiven Ġthe Ġconditions .\n",
      "\t Tokenized LLAMA3:G DP Ġwould Ġstill Ġbe Ġabove Ġ 50 , 000 Ġeven Ġgiven Ġthe Ġconditions .\n",
      "\t Unique Tokens GPT2: {'Ġ50'}\n",
      "\t Unique Tokens LLAMA3: {'50', 'Ġ'}\n",
      "==contradiction==\n",
      "Text 1: His fantastic body could heal itself against whatever they did to him, and his mind refused to accept the torture supinely.\n",
      "\t Tokenized GPT2:His Ġfantastic Ġbody Ġcould Ġheal Ġitself Ġagainst Ġwhatever Ġthey Ġdid Ġto Ġhim , Ġand Ġhis Ġmind Ġrefused Ġto Ġaccept Ġthe Ġtorture Ġsu pine ly .\n",
      "\t Tokenized LLAMA3:His Ġfantastic Ġbody Ġcould Ġheal Ġitself Ġagainst Ġwhatever Ġthey Ġdid Ġto Ġhim , Ġand Ġhis Ġmind Ġrefused Ġto Ġaccept Ġthe Ġtorture Ġsup ine ly .\n",
      "\t Unique Tokens GPT2: {'Ġsu', 'pine'}\n",
      "\t Unique Tokens LLAMA3: {'ine', 'Ġsup'}\n",
      "Text 2: His weak body could not heal itself against the tiniest scratch.\n",
      "\t Tokenized GPT2:His Ġweak Ġbody Ġcould Ġnot Ġheal Ġitself Ġagainst Ġthe Ġtin iest Ġscratch .\n",
      "\t Tokenized LLAMA3:His Ġweak Ġbody Ġcould Ġnot Ġheal Ġitself Ġagainst Ġthe Ġtin iest Ġscratch .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: It started with The Wild Bunch : We sexualized violence, we made it beautiful.\n",
      "\t Tokenized GPT2:It Ġstarted Ġwith ĠThe ĠWild ĠB unch Ġ: ĠWe Ġsexual ized Ġviolence , Ġwe Ġmade Ġit Ġbeautiful .\n",
      "\t Tokenized LLAMA3:It Ġstarted Ġwith ĠThe ĠWild ĠB unch Ġ: ĠWe Ġsexual ized Ġviolence , Ġwe Ġmade Ġit Ġbeautiful .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Violence is now look at in the positive due to The Wild Bunch.\n",
      "\t Tokenized GPT2:V iol ence Ġis Ġnow Ġlook Ġat Ġin Ġthe Ġpositive Ġdue Ġto ĠThe ĠWild ĠB unch .\n",
      "\t Tokenized LLAMA3:V iol ence Ġis Ġnow Ġlook Ġat Ġin Ġthe Ġpositive Ġdue Ġto ĠThe ĠWild ĠB unch .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: The much-previewed profile of Michael Huffington reveals that he is--surprise, surprise--gay.\n",
      "\t Tokenized GPT2:The Ġmuch - preview ed Ġprofile Ġof ĠMichael ĠH uff ington Ġreveals Ġthat Ġhe Ġis -- sur prise , Ġsurprise -- gay .\n",
      "\t Tokenized LLAMA3:The Ġmuch -p review ed Ġprofile Ġof ĠMichael ĠH uff ington Ġreveals Ġthat Ġhe Ġis -- sur prise , Ġsurprise -- g ay .\n",
      "\t Unique Tokens GPT2: {'-', 'preview', 'gay'}\n",
      "\t Unique Tokens LLAMA3: {'review', 'g', '-p', 'ay'}\n",
      "Text 2: Michael Huffington is gay.\n",
      "\t Tokenized GPT2:Michael ĠH uff ington Ġis Ġgay .\n",
      "\t Tokenized LLAMA3:Michael ĠH uff ington Ġis Ġgay .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The disputes among nobles were not the first concern of ordinary French citizens.\n",
      "\t Tokenized GPT2:The Ġdisput es Ġamong Ġno bles Ġwere Ġnot Ġthe Ġfirst Ġconcern Ġof Ġordinary ĠFrench Ġcitizens .\n",
      "\t Tokenized LLAMA3:The Ġdisput es Ġamong Ġnob les Ġwere Ġnot Ġthe Ġfirst Ġconcern Ġof Ġordinary ĠFrench Ġcitizens .\n",
      "\t Unique Tokens GPT2: {'Ġno', 'bles'}\n",
      "\t Unique Tokens LLAMA3: {'les', 'Ġnob'}\n",
      "Text 2: One of the first concerns of the ordinary French citizens were the disputes among nobles.\n",
      "\t Tokenized GPT2:One Ġof Ġthe Ġfirst Ġconcerns Ġof Ġthe Ġordinary ĠFrench Ġcitizens Ġwere Ġthe Ġdisput es Ġamong Ġno bles .\n",
      "\t Tokenized LLAMA3:One Ġof Ġthe Ġfirst Ġconcerns Ġof Ġthe Ġordinary ĠFrench Ġcitizens Ġwere Ġthe Ġdisput es Ġamong Ġnob les .\n",
      "\t Unique Tokens GPT2: {'Ġno', 'bles'}\n",
      "\t Unique Tokens LLAMA3: {'les', 'Ġnob'}\n",
      "==entailment==\n",
      "Text 1: The tree-lined avenue extends less than three blocks to the sea.\n",
      "\t Tokenized GPT2:The Ġtree - lined Ġa venue Ġextends Ġless Ġthan Ġthree Ġblocks Ġto Ġthe Ġsea .\n",
      "\t Tokenized LLAMA3:The Ġtree -l ined Ġa venue Ġextends Ġless Ġthan Ġthree Ġblocks Ġto Ġthe Ġsea .\n",
      "\t Unique Tokens GPT2: {'-', 'lined'}\n",
      "\t Unique Tokens LLAMA3: {'ined', '-l'}\n",
      "Text 2: The sea isn't even three blocks away.\n",
      "\t Tokenized GPT2:The Ġsea Ġisn 't Ġeven Ġthree Ġblocks Ġaway .\n",
      "\t Tokenized LLAMA3:The Ġsea Ġisn 't Ġeven Ġthree Ġblocks Ġaway .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The statue was beheaded several years ago by islanders, who blame Josephine for her role in the slavery in Martinique.\n",
      "\t Tokenized GPT2:The Ġstatue Ġwas Ġbe headed Ġseveral Ġyears Ġago Ġby Ġisland ers , Ġwho Ġblame ĠJoseph ine Ġfor Ġher Ġrole Ġin Ġthe Ġslavery Ġin ĠMartin ique .\n",
      "\t Tokenized LLAMA3:The Ġstatue Ġwas Ġbe head ed Ġseveral Ġyears Ġago Ġby Ġisland ers , Ġwho Ġblame ĠJoseph ine Ġfor Ġher Ġrole Ġin Ġthe Ġslavery Ġin ĠMartin ique .\n",
      "\t Unique Tokens GPT2: {'headed'}\n",
      "\t Unique Tokens LLAMA3: {'ed', 'head'}\n",
      "Text 2: The statue was erected to remind the populace to stay obedient to their masters.\n",
      "\t Tokenized GPT2:The Ġstatue Ġwas Ġerect ed Ġto Ġremind Ġthe Ġpopul ace Ġto Ġstay Ġobed ient Ġto Ġtheir Ġmasters .\n",
      "\t Tokenized LLAMA3:The Ġstatue Ġwas Ġerect ed Ġto Ġremind Ġthe Ġpopul ace Ġto Ġstay Ġobed ient Ġto Ġtheir Ġmasters .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: The thing started to grow brighter.\n",
      "\t Tokenized GPT2:The Ġthing Ġstarted Ġto Ġgrow Ġbrighter .\n",
      "\t Tokenized LLAMA3:The Ġthing Ġstarted Ġto Ġgrow Ġbrighter .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It started to illuminate more and more.\n",
      "\t Tokenized GPT2:It Ġstarted Ġto Ġill uminate Ġmore Ġand Ġmore .\n",
      "\t Tokenized LLAMA3:It Ġstarted Ġto Ġill uminate Ġmore Ġand Ġmore .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: I still didn't trust the little buggers.\n",
      "\t Tokenized GPT2:I Ġstill Ġdidn 't Ġtrust Ġthe Ġlittle Ġbu gg ers .\n",
      "\t Tokenized LLAMA3:I Ġstill Ġdidn 't Ġtrust Ġthe Ġlittle Ġbu gg ers .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I had a distrust for the tiny things.\n",
      "\t Tokenized GPT2:I Ġhad Ġa Ġdistr ust Ġfor Ġthe Ġtiny Ġthings .\n",
      "\t Tokenized LLAMA3:I Ġhad Ġa Ġdistr ust Ġfor Ġthe Ġtiny Ġthings .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: A martini should be gin and vermouth and a twist.\n",
      "\t Tokenized GPT2:A Ġmart ini Ġshould Ġbe Ġg in Ġand Ġver mouth Ġand Ġa Ġtwist .\n",
      "\t Tokenized LLAMA3:A Ġmart ini Ġshould Ġbe Ġg in Ġand Ġver mouth Ġand Ġa Ġtwist .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: A martini must be composed by vodka and vermouth.\n",
      "\t Tokenized GPT2:A Ġmart ini Ġmust Ġbe Ġcomposed Ġby Ġv odka Ġand Ġver mouth .\n",
      "\t Tokenized LLAMA3:A Ġmart ini Ġmust Ġbe Ġcomposed Ġby Ġv od ka Ġand Ġver mouth .\n",
      "\t Unique Tokens GPT2: {'odka'}\n",
      "\t Unique Tokens LLAMA3: {'ka', 'od'}\n",
      "==contradiction==\n",
      "Text 1: see too much crime on TV and they think it's way to go i don't know what do you think\n",
      "\t Tokenized GPT2:see Ġtoo Ġmuch Ġcrime Ġon ĠTV Ġand Ġthey Ġthink Ġit 's Ġway Ġto Ġgo Ġi Ġdon 't Ġknow Ġwhat Ġdo Ġyou Ġthink\n",
      "\t Tokenized LLAMA3:see Ġtoo Ġmuch Ġcrime Ġon ĠTV Ġand Ġthey Ġthink Ġit 's Ġway Ġto Ġgo Ġi Ġdon 't Ġknow Ġwhat Ġdo Ġyou Ġthink\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: They don't see crime on TV.\n",
      "\t Tokenized GPT2:They Ġdon 't Ġsee Ġcrime Ġon ĠTV .\n",
      "\t Tokenized LLAMA3:They Ġdon 't Ġsee Ġcrime Ġon ĠTV .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: i don't know what kind of a summer we're expecting this year i imagine it's going to be hot again\n",
      "\t Tokenized GPT2:i Ġdon 't Ġknow Ġwhat Ġkind Ġof Ġa Ġsummer Ġwe 're Ġexpecting Ġthis Ġyear Ġi Ġimagine Ġit 's Ġgoing Ġto Ġbe Ġhot Ġagain\n",
      "\t Tokenized LLAMA3:i Ġdon 't Ġknow Ġwhat Ġkind Ġof Ġa Ġsummer Ġwe 're Ġexpecting Ġthis Ġyear Ġi Ġimagine Ġit 's Ġgoing Ġto Ġbe Ġhot Ġagain\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I guess this summer will be another warm one; we'll see.\n",
      "\t Tokenized GPT2:I Ġguess Ġthis Ġsummer Ġwill Ġbe Ġanother Ġwarm Ġone ; Ġwe 'll Ġsee .\n",
      "\t Tokenized LLAMA3:I Ġguess Ġthis Ġsummer Ġwill Ġbe Ġanother Ġwarm Ġone ; Ġwe 'll Ġsee .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: He seemed too self-assured.\n",
      "\t Tokenized GPT2:He Ġseemed Ġtoo Ġself - ass ured .\n",
      "\t Tokenized LLAMA3:He Ġseemed Ġtoo Ġself -ass ured .\n",
      "\t Unique Tokens GPT2: {'-', 'ass'}\n",
      "\t Unique Tokens LLAMA3: {'-ass'}\n",
      "Text 2: He is very cocky.\n",
      "\t Tokenized GPT2:He Ġis Ġvery Ġcock y .\n",
      "\t Tokenized LLAMA3:He Ġis Ġvery Ġcock y .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: that's cool kind of like Pink Floyd or something uh yeah basketball's cool but football kind of after a while\n",
      "\t Tokenized GPT2:that 's Ġcool Ġkind Ġof Ġlike ĠPink ĠFloyd Ġor Ġsomething Ġuh Ġyeah Ġbasketball 's Ġcool Ġbut Ġfootball Ġkind Ġof Ġafter Ġa Ġwhile\n",
      "\t Tokenized LLAMA3:that 's Ġcool Ġkind Ġof Ġlike ĠPink ĠFl oyd Ġor Ġsomething Ġuh Ġyeah Ġbasketball 's Ġcool Ġbut Ġfootball Ġkind Ġof Ġafter Ġa Ġwhile\n",
      "\t Unique Tokens GPT2: {'ĠFloyd'}\n",
      "\t Unique Tokens LLAMA3: {'ĠFl', 'oyd'}\n",
      "Text 2: Yeah, I love basketball and football.\n",
      "\t Tokenized GPT2:Yeah , ĠI Ġlove Ġbasketball Ġand Ġfootball .\n",
      "\t Tokenized LLAMA3:Yeah , ĠI Ġlove Ġbasketball Ġand Ġfootball .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Sometimes more than one denomination shares one church.\n",
      "\t Tokenized GPT2:Sometimes Ġmore Ġthan Ġone Ġden om ination Ġshares Ġone Ġchurch .\n",
      "\t Tokenized LLAMA3:Sometimes Ġmore Ġthan Ġone Ġden om ination Ġshares Ġone Ġchurch .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: At times a church is used by multiple faiths.\n",
      "\t Tokenized GPT2:At Ġtimes Ġa Ġchurch Ġis Ġused Ġby Ġmultiple Ġfaith s .\n",
      "\t Tokenized LLAMA3:At Ġtimes Ġa Ġchurch Ġis Ġused Ġby Ġmultiple Ġfaith s .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Horwitz makes us see that the pinched circumstances of their lives are not so different from the conditions of their ancestors, dirt-poor yeoman farmers who seldom saw, much less owned, a slave.\n",
      "\t Tokenized GPT2:Hor w itz Ġmakes Ġus Ġsee Ġthat Ġthe Ġpin ched Ġcircumstances Ġof Ġtheir Ġlives Ġare Ġnot Ġso Ġdifferent Ġfrom Ġthe Ġconditions Ġof Ġtheir Ġancestors , Ġdirt - poor Ġye oman Ġfarmers Ġwho Ġseldom Ġsaw , Ġmuch Ġless Ġowned , Ġa Ġslave .\n",
      "\t Tokenized LLAMA3:H or w itz Ġmakes Ġus Ġsee Ġthat Ġthe Ġpin ched Ġcircumstances Ġof Ġtheir Ġlives Ġare Ġnot Ġso Ġdifferent Ġfrom Ġthe Ġconditions Ġof Ġtheir Ġancestors , Ġdirt -p oor Ġye oman Ġfarmers Ġwho Ġseldom Ġsaw , Ġmuch Ġless Ġowned , Ġa Ġslave .\n",
      "\t Unique Tokens GPT2: {'-', 'Hor', 'poor'}\n",
      "\t Unique Tokens LLAMA3: {'oor', 'H', '-p', 'or'}\n",
      "Text 2: Their lives are much better than their ancestors.\n",
      "\t Tokenized GPT2:Their Ġlives Ġare Ġmuch Ġbetter Ġthan Ġtheir Ġancestors .\n",
      "\t Tokenized LLAMA3:Their Ġlives Ġare Ġmuch Ġbetter Ġthan Ġtheir Ġancestors .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: She was alone at last with the president!\n",
      "\t Tokenized GPT2:She Ġwas Ġalone Ġat Ġlast Ġwith Ġthe Ġpresident !\n",
      "\t Tokenized LLAMA3:She Ġwas Ġalone Ġat Ġlast Ġwith Ġthe Ġpresident !\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: At last, she has not been alone with the president!\n",
      "\t Tokenized GPT2:At Ġlast , Ġshe Ġhas Ġnot Ġbeen Ġalone Ġwith Ġthe Ġpresident !\n",
      "\t Tokenized LLAMA3:At Ġlast , Ġshe Ġhas Ġnot Ġbeen Ġalone Ġwith Ġthe Ġpresident !\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: i can't do any jumping up and down because it makes it hurt\n",
      "\t Tokenized GPT2:i Ġcan 't Ġdo Ġany Ġjumping Ġup Ġand Ġdown Ġbecause Ġit Ġmakes Ġit Ġhurt\n",
      "\t Tokenized LLAMA3:i Ġcan 't Ġdo Ġany Ġjumping Ġup Ġand Ġdown Ġbecause Ġit Ġmakes Ġit Ġhurt\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: There is no pain from jumping.\n",
      "\t Tokenized GPT2:There Ġis Ġno Ġpain Ġfrom Ġjumping .\n",
      "\t Tokenized LLAMA3:There Ġis Ġno Ġpain Ġfrom Ġjumping .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: um i know that i had heard that uh McDonald's has gotten so much flack about sending their hot foods out in the Styrofoam that they are going to work on something\n",
      "\t Tokenized GPT2:um Ġi Ġknow Ġthat Ġi Ġhad Ġheard Ġthat Ġuh ĠMcDonald 's Ġhas Ġgotten Ġso Ġmuch Ġfl ack Ġabout Ġsending Ġtheir Ġhot Ġfoods Ġout Ġin Ġthe ĠS ty ro fo am Ġthat Ġthey Ġare Ġgoing Ġto Ġwork Ġon Ġsomething\n",
      "\t Tokenized LLAMA3:um Ġi Ġknow Ġthat Ġi Ġhad Ġheard Ġthat Ġuh ĠMcDonald 's Ġhas Ġgotten Ġso Ġmuch Ġfl ack Ġabout Ġsending Ġtheir Ġhot Ġfoods Ġout Ġin Ġthe ĠS ty ro fo am Ġthat Ġthey Ġare Ġgoing Ġto Ġwork Ġon Ġsomething\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Styrofoam is not a safe thing to have near food.\n",
      "\t Tokenized GPT2:S ty ro fo am Ġis Ġnot Ġa Ġsafe Ġthing Ġto Ġhave Ġnear Ġfood .\n",
      "\t Tokenized LLAMA3:S ty ro fo am Ġis Ġnot Ġa Ġsafe Ġthing Ġto Ġhave Ġnear Ġfood .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Others watched them with cold eyes and expressionless faces.\n",
      "\t Tokenized GPT2:Other s Ġwatched Ġthem Ġwith Ġcold Ġeyes Ġand Ġexpression less Ġfaces .\n",
      "\t Tokenized LLAMA3:Other s Ġwatched Ġthem Ġwith Ġcold Ġeyes Ġand Ġexpression less Ġfaces .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Everyone was cheering or cursing as they watched.\n",
      "\t Tokenized GPT2:Everyone Ġwas Ġcheering Ġor Ġcur sing Ġas Ġthey Ġwatched .\n",
      "\t Tokenized LLAMA3:Everyone Ġwas Ġcheering Ġor Ġcur sing Ġas Ġthey Ġwatched .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The island's burgeoning economic significance propelled population growth, and by the middle of the 15th century Madeira was home to 800 families.\n",
      "\t Tokenized GPT2:The Ġisland 's Ġbur geon ing Ġeconomic Ġsignificance Ġpro pelled Ġpopulation Ġgrowth , Ġand Ġby Ġthe Ġmiddle Ġof Ġthe Ġ15 th Ġcentury ĠMade ira Ġwas Ġhome Ġto Ġ800 Ġfamilies .\n",
      "\t Tokenized LLAMA3:The Ġisland 's Ġbur geon ing Ġeconomic Ġsignificance Ġpro pelled Ġpopulation Ġgrowth , Ġand Ġby Ġthe Ġmiddle Ġof Ġthe Ġ 15 th Ġcentury ĠMade ira Ġwas Ġhome Ġto Ġ 800 Ġfamilies .\n",
      "\t Unique Tokens GPT2: {'Ġ800', 'Ġ15'}\n",
      "\t Unique Tokens LLAMA3: {'800', '15', 'Ġ'}\n",
      "Text 2: Madeira proved to be uninhabitable.\n",
      "\t Tokenized GPT2:M ade ira Ġproved Ġto Ġbe Ġun in hab itable .\n",
      "\t Tokenized LLAMA3:M ade ira Ġproved Ġto Ġbe Ġun in hab itable .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: The anthropologist Napoleon Chagnon has shown that Yanomamo men who have killed other men have more wives and more offspring than average guys.\n",
      "\t Tokenized GPT2:The Ġanthrop ologist ĠNapoleon ĠCh agn on Ġhas Ġshown Ġthat ĠYan om amo Ġmen Ġwho Ġhave Ġkilled Ġother Ġmen Ġhave Ġmore Ġwives Ġand Ġmore Ġoffspring Ġthan Ġaverage Ġguys .\n",
      "\t Tokenized LLAMA3:The Ġanthrop ologist ĠNap oleon ĠCh agn on Ġhas Ġshown Ġthat ĠYan om amo Ġmen Ġwho Ġhave Ġkilled Ġother Ġmen Ġhave Ġmore Ġwives Ġand Ġmore Ġoffspring Ġthan Ġaverage Ġguys .\n",
      "\t Unique Tokens GPT2: {'ĠNapoleon'}\n",
      "\t Unique Tokens LLAMA3: {'oleon', 'ĠNap'}\n",
      "Text 2: Yanomamo men who kill other men have better chances at getting more wives.\n",
      "\t Tokenized GPT2:Y an om amo Ġmen Ġwho Ġkill Ġother Ġmen Ġhave Ġbetter Ġchances Ġat Ġgetting Ġmore Ġwives .\n",
      "\t Tokenized LLAMA3:Y an om amo Ġmen Ġwho Ġkill Ġother Ġmen Ġhave Ġbetter Ġchances Ġat Ġgetting Ġmore Ġwives .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: i mean that's a real attractive option if you have the the technology for it all it was was you know i mean she just used a phone modem and she was like she was sitting in the office\n",
      "\t Tokenized GPT2:i Ġmean Ġthat 's Ġa Ġreal Ġattractive Ġoption Ġif Ġyou Ġhave Ġthe Ġthe Ġtechnology Ġfor Ġit Ġall Ġit Ġwas Ġwas Ġyou Ġknow Ġi Ġmean Ġshe Ġjust Ġused Ġa Ġphone Ġmodem Ġand Ġshe Ġwas Ġlike Ġshe Ġwas Ġsitting Ġin Ġthe Ġoffice\n",
      "\t Tokenized LLAMA3:i Ġmean Ġthat 's Ġa Ġreal Ġattractive Ġoption Ġif Ġyou Ġhave Ġthe Ġthe Ġtechnology Ġfor Ġit Ġall Ġit Ġwas Ġwas Ġyou Ġknow Ġi Ġmean Ġshe Ġjust Ġused Ġa Ġphone Ġmodem Ġand Ġshe Ġwas Ġlike Ġshe Ġwas Ġsitting Ġin Ġthe Ġoffice\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: She used a phone modem but it was very different than if she were in the office. \n",
      "\t Tokenized GPT2:She Ġused Ġa Ġphone Ġmodem Ġbut Ġit Ġwas Ġvery Ġdifferent Ġthan Ġif Ġshe Ġwere Ġin Ġthe Ġoffice . Ġ\n",
      "\t Tokenized LLAMA3:She Ġused Ġa Ġphone Ġmodem Ġbut Ġit Ġwas Ġvery Ġdifferent Ġthan Ġif Ġshe Ġwere Ġin Ġthe Ġoffice . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Not only must capital goods be replaced as they depreciate, but new generations of workers must be comparably\n",
      "\t Tokenized GPT2:Not Ġonly Ġmust Ġcapital Ġgoods Ġbe Ġreplaced Ġas Ġthey Ġdep reci ate , Ġbut Ġnew Ġgenerations Ġof Ġworkers Ġmust Ġbe Ġcompar ably\n",
      "\t Tokenized LLAMA3:Not Ġonly Ġmust Ġcapital Ġgoods Ġbe Ġreplaced Ġas Ġthey Ġdep reci ate , Ġbut Ġnew Ġgenerations Ġof Ġworkers Ġmust Ġbe Ġcompar ably\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Capital goods are able to last for eternity.\n",
      "\t Tokenized GPT2:Cap ital Ġgoods Ġare Ġable Ġto Ġlast Ġfor Ġeternity .\n",
      "\t Tokenized LLAMA3:Cap ital Ġgoods Ġare Ġable Ġto Ġlast Ġfor Ġeternity .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: This provides insight into the important Japanese concept of katachi (form), the rough equivalent of  It isn't what you do; it's the way that you do it.  \n",
      "\t Tokenized GPT2:This Ġprovides Ġinsight Ġinto Ġthe Ġimportant ĠJapanese Ġconcept Ġof Ġk at achi Ġ( form ), Ġthe Ġrough Ġequivalent Ġof Ġ ĠIt Ġisn 't Ġwhat Ġyou Ġdo ; Ġit 's Ġthe Ġway Ġthat Ġyou Ġdo Ġit . ĠĠ\n",
      "\t Tokenized LLAMA3:This Ġprovides Ġinsight Ġinto Ġthe Ġimportant ĠJapanese Ġconcept Ġof Ġk at achi Ġ( form ), Ġthe Ġrough Ġequivalent Ġof Ġ ĠIt Ġisn 't Ġwhat Ġyou Ġdo ; Ġit 's Ġthe Ġway Ġthat Ġyou Ġdo Ġit . ĠĠ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: All Japanese people abide by the concept of katachi.\n",
      "\t Tokenized GPT2:All ĠJapanese Ġpeople Ġab ide Ġby Ġthe Ġconcept Ġof Ġk at achi .\n",
      "\t Tokenized LLAMA3:All ĠJapanese Ġpeople Ġab ide Ġby Ġthe Ġconcept Ġof Ġk at achi .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Outside, set in manicured gardens, are the remains of the Abbey of Holyrood.\n",
      "\t Tokenized GPT2:Out side , Ġset Ġin Ġman ic ured Ġgardens , Ġare Ġthe Ġremains Ġof Ġthe ĠAb bey Ġof ĠHoly ro od .\n",
      "\t Tokenized LLAMA3:Out side , Ġset Ġin Ġman ic ured Ġgardens , Ġare Ġthe Ġremains Ġof Ġthe ĠAb bey Ġof ĠHoly ro od .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The gardens containing the remains of the Abbey of Holyrood are in disarray and not well-kept.\n",
      "\t Tokenized GPT2:The Ġgardens Ġcontaining Ġthe Ġremains Ġof Ġthe ĠAb bey Ġof ĠHoly ro od Ġare Ġin Ġdis array Ġand Ġnot Ġwell - ke pt .\n",
      "\t Tokenized LLAMA3:The Ġgardens Ġcontaining Ġthe Ġremains Ġof Ġthe ĠAb bey Ġof ĠHoly ro od Ġare Ġin Ġdis array Ġand Ġnot Ġwell - ke pt .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: It cannot be outlawed.\n",
      "\t Tokenized GPT2:It Ġcannot Ġbe Ġout law ed .\n",
      "\t Tokenized LLAMA3:It Ġcannot Ġbe Ġout law ed .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Abortion cannot be outlawed.\n",
      "\t Tokenized GPT2:Ab ortion Ġcannot Ġbe Ġout law ed .\n",
      "\t Tokenized LLAMA3:Ab ortion Ġcannot Ġbe Ġout law ed .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1:  The leaves of the papyrus were dried and used by Ancient Egyptians as a form of paper.\n",
      "\t Tokenized GPT2:ĠThe Ġleaves Ġof Ġthe Ġpap yrus Ġwere Ġdried Ġand Ġused Ġby ĠAncient ĠEgypt ians Ġas Ġa Ġform Ġof Ġpaper .\n",
      "\t Tokenized LLAMA3:ĠThe Ġleaves Ġof Ġthe Ġpap yrus Ġwere Ġdried Ġand Ġused Ġby ĠAn cient ĠEgypt ians Ġas Ġa Ġform Ġof Ġpaper .\n",
      "\t Unique Tokens GPT2: {'ĠAncient'}\n",
      "\t Unique Tokens LLAMA3: {'cient', 'ĠAn'}\n",
      "Text 2: Papyrus paper was only used by wealthy Egyptians because it was so expensive.\n",
      "\t Tokenized GPT2:P ap yrus Ġpaper Ġwas Ġonly Ġused Ġby Ġwealthy ĠEgypt ians Ġbecause Ġit Ġwas Ġso Ġexpensive .\n",
      "\t Tokenized LLAMA3:P ap yrus Ġpaper Ġwas Ġonly Ġused Ġby Ġwealthy ĠEgypt ians Ġbecause Ġit Ġwas Ġso Ġexpensive .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: One opportunist who stayed was Octavius Decatur Gass.\n",
      "\t Tokenized GPT2:One Ġopport un ist Ġwho Ġstayed Ġwas ĠOct av ius ĠDec atur ĠG ass .\n",
      "\t Tokenized LLAMA3:One Ġopport un ist Ġwho Ġstayed Ġwas ĠOct av ius ĠDec atur ĠG ass .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Octavius described himself as an opportunist often. \n",
      "\t Tokenized GPT2:Oct av ius Ġdescribed Ġhimself Ġas Ġan Ġopport un ist Ġoften . Ġ\n",
      "\t Tokenized LLAMA3:Oct av ius Ġdescribed Ġhimself Ġas Ġan Ġopport un ist Ġoften . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Many users commented on the effectiveness of the new technology in promoting closer relationships among providers.\n",
      "\t Tokenized GPT2:Many Ġusers Ġcommented Ġon Ġthe Ġeffectiveness Ġof Ġthe Ġnew Ġtechnology Ġin Ġpromoting Ġcloser Ġrelationships Ġamong Ġproviders .\n",
      "\t Tokenized LLAMA3:Many Ġusers Ġcommented Ġon Ġthe Ġeffectiveness Ġof Ġthe Ġnew Ġtechnology Ġin Ġpromoting Ġcloser Ġrelationships Ġamong Ġproviders .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: They were disappointed to hear the consumer complaints.\n",
      "\t Tokenized GPT2:They Ġwere Ġdisappointed Ġto Ġhear Ġthe Ġconsumer Ġcomplaints .\n",
      "\t Tokenized LLAMA3:They Ġwere Ġdisappointed Ġto Ġhear Ġthe Ġconsumer Ġcomplaints .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Recent SAB deliberations on mortality and morbidity valuation approaches suggest that some adjustments to unit values are appropriate to reflect economic theory (EPA-SAB-EEAC-00-013, 2000).\n",
      "\t Tokenized GPT2:Recent ĠS AB Ġdeliber ations Ġon Ġmortality Ġand Ġmorbid ity Ġvaluation Ġapproaches Ġsuggest Ġthat Ġsome Ġadjustments Ġto Ġunit Ġvalues Ġare Ġappropriate Ġto Ġreflect Ġeconomic Ġtheory Ġ( E PA - S AB - EE AC - 00 - 013 , Ġ2000 ).\n",
      "\t Tokenized LLAMA3:Rec ent ĠS AB Ġdeliber ations Ġon Ġmortality Ġand Ġmorbid ity Ġval uation Ġapproaches Ġsuggest Ġthat Ġsome Ġadjustments Ġto Ġunit Ġvalues Ġare Ġappropriate Ġto Ġreflect Ġeconomic Ġtheory Ġ( E PA -S AB - EE AC - 00 - 013 , Ġ 200 0 ).\n",
      "\t Unique Tokens GPT2: {'Ġvaluation', 'S', 'Recent', 'Ġ2000'}\n",
      "\t Unique Tokens LLAMA3: {'ent', 'Ġ', 'Ġval', '-S', '0', 'Rec', 'uation', '200'}\n",
      "Text 2: Economic theory is the deciding factor when it comes to valuation.\n",
      "\t Tokenized GPT2:E conomic Ġtheory Ġis Ġthe Ġdeciding Ġfactor Ġwhen Ġit Ġcomes Ġto Ġvaluation .\n",
      "\t Tokenized LLAMA3:E conomic Ġtheory Ġis Ġthe Ġdeciding Ġfactor Ġwhen Ġit Ġcomes Ġto Ġval uation .\n",
      "\t Unique Tokens GPT2: {'Ġvaluation'}\n",
      "\t Unique Tokens LLAMA3: {'uation', 'Ġval'}\n",
      "==entailment==\n",
      "Text 1: Monday's Question (No.\n",
      "\t Tokenized GPT2:Monday 's ĠQuestion Ġ( No .\n",
      "\t Tokenized LLAMA3:Monday 's ĠQuestion Ġ( No .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: There was a question on Monday.\n",
      "\t Tokenized GPT2:There Ġwas Ġa Ġquestion Ġon ĠMonday .\n",
      "\t Tokenized LLAMA3:There Ġwas Ġa Ġquestion Ġon ĠMonday .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: yeah so it's easy to do i'm actually interested in getting one of those kind of my wife has been talking about this in the past couple of years one of those kind of campers that pop-up so it's about uh maybe eight foot square and but only about two feet tall and when you get to where you're going it\n",
      "\t Tokenized GPT2:yeah Ġso Ġit 's Ġeasy Ġto Ġdo Ġi 'm Ġactually Ġinterested Ġin Ġgetting Ġone Ġof Ġthose Ġkind Ġof Ġmy Ġwife Ġhas Ġbeen Ġtalking Ġabout Ġthis Ġin Ġthe Ġpast Ġcouple Ġof Ġyears Ġone Ġof Ġthose Ġkind Ġof Ġcamp ers Ġthat Ġpop - up Ġso Ġit 's Ġabout Ġuh Ġmaybe Ġeight Ġfoot Ġsquare Ġand Ġbut Ġonly Ġabout Ġtwo Ġfeet Ġtall Ġand Ġwhen Ġyou Ġget Ġto Ġwhere Ġyou 're Ġgoing Ġit\n",
      "\t Tokenized LLAMA3:yeah Ġso Ġit 's Ġeasy Ġto Ġdo Ġi 'm Ġactually Ġinterested Ġin Ġgetting Ġone Ġof Ġthose Ġkind Ġof Ġmy Ġwife Ġhas Ġbeen Ġtalking Ġabout Ġthis Ġin Ġthe Ġpast Ġcouple Ġof Ġyears Ġone Ġof Ġthose Ġkind Ġof Ġcamp ers Ġthat Ġpop -up Ġso Ġit 's Ġabout Ġuh Ġmaybe Ġeight Ġfoot Ġsquare Ġand Ġbut Ġonly Ġabout Ġtwo Ġfeet Ġtall Ġand Ġwhen Ġyou Ġget Ġto Ġwhere Ġyou 're Ġgoing Ġit\n",
      "\t Unique Tokens GPT2: {'-', 'up'}\n",
      "\t Unique Tokens LLAMA3: {'-up'}\n",
      "Text 2: I don't want a camper like that.\n",
      "\t Tokenized GPT2:I Ġdon 't Ġwant Ġa Ġcam per Ġlike Ġthat .\n",
      "\t Tokenized LLAMA3:I Ġdon 't Ġwant Ġa Ġcam per Ġlike Ġthat .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Larger boats for up to 20 people, plus crew, offer organized gourmet cruises.\n",
      "\t Tokenized GPT2:L ar ger Ġboats Ġfor Ġup Ġto Ġ20 Ġpeople , Ġplus Ġcrew , Ġoffer Ġorganized Ġg our met Ġcru ises .\n",
      "\t Tokenized LLAMA3:L ar ger Ġboats Ġfor Ġup Ġto Ġ 20 Ġpeople , Ġplus Ġcrew , Ġoffer Ġorganized Ġg our met Ġcru ises .\n",
      "\t Unique Tokens GPT2: {'Ġ20'}\n",
      "\t Unique Tokens LLAMA3: {'20', 'Ġ'}\n",
      "Text 2: Larger boats that can fit up to 20 people (not including the crew), have gourmet cruises.\n",
      "\t Tokenized GPT2:L ar ger Ġboats Ġthat Ġcan Ġfit Ġup Ġto Ġ20 Ġpeople Ġ( not Ġincluding Ġthe Ġcrew ), Ġhave Ġg our met Ġcru ises .\n",
      "\t Tokenized LLAMA3:L ar ger Ġboats Ġthat Ġcan Ġfit Ġup Ġto Ġ 20 Ġpeople Ġ( not Ġincluding Ġthe Ġcrew ), Ġhave Ġg our met Ġcru ises .\n",
      "\t Unique Tokens GPT2: {'Ġ20'}\n",
      "\t Unique Tokens LLAMA3: {'20', 'Ġ'}\n",
      "==neutral==\n",
      "Text 1: Don't forget to take a change of clothing and a towel.\n",
      "\t Tokenized GPT2:Don 't Ġforget Ġto Ġtake Ġa Ġchange Ġof Ġclothing Ġand Ġa Ġtowel .\n",
      "\t Tokenized LLAMA3:Don 't Ġforget Ġto Ġtake Ġa Ġchange Ġof Ġclothing Ġand Ġa Ġtowel .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: You should buy new clothes and other stuff.\n",
      "\t Tokenized GPT2:You Ġshould Ġbuy Ġnew Ġclothes Ġand Ġother Ġstuff .\n",
      "\t Tokenized LLAMA3:You Ġshould Ġbuy Ġnew Ġclothes Ġand Ġother Ġstuff .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: farmworkers conducted by the U.S.\n",
      "\t Tokenized GPT2:f arm workers Ġconducted Ġby Ġthe ĠU . S .\n",
      "\t Tokenized LLAMA3:f arm work ers Ġconducted Ġby Ġthe ĠU .S .\n",
      "\t Unique Tokens GPT2: {'S', 'workers'}\n",
      "\t Unique Tokens LLAMA3: {'work', '.S', 'ers'}\n",
      "Text 2: Some farm laborers were sampled.\n",
      "\t Tokenized GPT2:Some Ġfarm Ġlab ore rs Ġwere Ġsampled .\n",
      "\t Tokenized LLAMA3:Some Ġfarm Ġlab ore rs Ġwere Ġsampl ed .\n",
      "\t Unique Tokens GPT2: {'Ġsampled'}\n",
      "\t Unique Tokens LLAMA3: {'ed', 'Ġsampl'}\n",
      "==neutral==\n",
      "Text 1: there and they uh they in fact they had this was in uh the late twenties and they in fact used some of the equipment that had been left over and uh he turned them down it it's interesting that that most people don't realize how small the canal is have you ever been there\n",
      "\t Tokenized GPT2:there Ġand Ġthey Ġuh Ġthey Ġin Ġfact Ġthey Ġhad Ġthis Ġwas Ġin Ġuh Ġthe Ġlate Ġtwent ies Ġand Ġthey Ġin Ġfact Ġused Ġsome Ġof Ġthe Ġequipment Ġthat Ġhad Ġbeen Ġleft Ġover Ġand Ġuh Ġhe Ġturned Ġthem Ġdown Ġit Ġit 's Ġinteresting Ġthat Ġthat Ġmost Ġpeople Ġdon 't Ġrealize Ġhow Ġsmall Ġthe Ġcanal Ġis Ġhave Ġyou Ġever Ġbeen Ġthere\n",
      "\t Tokenized LLAMA3:there Ġand Ġthey Ġuh Ġthey Ġin Ġfact Ġthey Ġhad Ġthis Ġwas Ġin Ġuh Ġthe Ġlate Ġtwent ies Ġand Ġthey Ġin Ġfact Ġused Ġsome Ġof Ġthe Ġequipment Ġthat Ġhad Ġbeen Ġleft Ġover Ġand Ġuh Ġhe Ġturned Ġthem Ġdown Ġit Ġit 's Ġinteresting Ġthat Ġthat Ġmost Ġpeople Ġdon 't Ġrealize Ġhow Ġsmall Ġthe Ġcanal Ġis Ġhave Ġyou Ġever Ġbeen Ġthere\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: This was in 1928\n",
      "\t Tokenized GPT2:This Ġwas Ġin Ġ19 28\n",
      "\t Tokenized LLAMA3:This Ġwas Ġin Ġ 192 8\n",
      "\t Unique Tokens GPT2: {'28', 'Ġ19'}\n",
      "\t Unique Tokens LLAMA3: {'192', 'Ġ', '8'}\n",
      "==neutral==\n",
      "Text 1: Carmel Man, a relation of the Neanderthal family, lived here 600,000 years ago.\n",
      "\t Tokenized GPT2:C arm el ĠMan , Ġa Ġrelation Ġof Ġthe ĠNe ander thal Ġfamily , Ġlived Ġhere Ġ600 , 000 Ġyears Ġago .\n",
      "\t Tokenized LLAMA3:C arm el ĠMan , Ġa Ġrelation Ġof Ġthe ĠNe ander thal Ġfamily , Ġlived Ġhere Ġ 600 , 000 Ġyears Ġago .\n",
      "\t Unique Tokens GPT2: {'Ġ600'}\n",
      "\t Unique Tokens LLAMA3: {'600', 'Ġ'}\n",
      "Text 2: Carmel Man is still relatively well-preserved today, which is how we were able to estimate his age.\n",
      "\t Tokenized GPT2:C arm el ĠMan Ġis Ġstill Ġrelatively Ġwell - pres erved Ġtoday , Ġwhich Ġis Ġhow Ġwe Ġwere Ġable Ġto Ġestimate Ġhis Ġage .\n",
      "\t Tokenized LLAMA3:C arm el ĠMan Ġis Ġstill Ġrelatively Ġwell -pres erved Ġtoday , Ġwhich Ġis Ġhow Ġwe Ġwere Ġable Ġto Ġestimate Ġhis Ġage .\n",
      "\t Unique Tokens GPT2: {'-', 'pres'}\n",
      "\t Unique Tokens LLAMA3: {'-pres'}\n",
      "==neutral==\n",
      "Text 1: The WP says that the Paula Jones trial judge has had an interesting prior run-in with Bill Clinton.\n",
      "\t Tokenized GPT2:The ĠWP Ġsays Ġthat Ġthe ĠPaul a ĠJones Ġtrial Ġjudge Ġhas Ġhad Ġan Ġinteresting Ġprior Ġrun - in Ġwith ĠBill ĠClinton .\n",
      "\t Tokenized LLAMA3:The ĠWP Ġsays Ġthat Ġthe ĠPaul a ĠJones Ġtrial Ġjudge Ġhas Ġhad Ġan Ġinteresting Ġprior Ġrun -in Ġwith ĠBill ĠClinton .\n",
      "\t Unique Tokens GPT2: {'-', 'in'}\n",
      "\t Unique Tokens LLAMA3: {'-in'}\n",
      "Text 2: The man did not know he was a judge.\n",
      "\t Tokenized GPT2:The Ġman Ġdid Ġnot Ġknow Ġhe Ġwas Ġa Ġjudge .\n",
      "\t Tokenized LLAMA3:The Ġman Ġdid Ġnot Ġknow Ġhe Ġwas Ġa Ġjudge .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Also, other sorbent-based approaches in development may prove in time to be preferable to ACI, making the use of ACI only a conservative assumption.\n",
      "\t Tokenized GPT2:Also , Ġother Ġsor b ent - based Ġapproaches Ġin Ġdevelopment Ġmay Ġprove Ġin Ġtime Ġto Ġbe Ġpreferable Ġto ĠAC I , Ġmaking Ġthe Ġuse Ġof ĠAC I Ġonly Ġa Ġconservative Ġassumption .\n",
      "\t Tokenized LLAMA3:Also , Ġother Ġsor b ent -based Ġapproaches Ġin Ġdevelopment Ġmay Ġprove Ġin Ġtime Ġto Ġbe Ġprefer able Ġto ĠA CI , Ġmaking Ġthe Ġuse Ġof ĠA CI Ġonly Ġa Ġconservative Ġassumption .\n",
      "\t Unique Tokens GPT2: {'ĠAC', 'Ġpreferable', 'based', '-', 'I'}\n",
      "\t Unique Tokens LLAMA3: {'-based', 'CI', 'able', 'ĠA', 'Ġprefer'}\n",
      "Text 2: Sorbent-based approaches in development may be preferable to ACl.\n",
      "\t Tokenized GPT2:S orb ent - based Ġapproaches Ġin Ġdevelopment Ġmay Ġbe Ġpreferable Ġto ĠA Cl .\n",
      "\t Tokenized LLAMA3:S or b ent -based Ġapproaches Ġin Ġdevelopment Ġmay Ġbe Ġprefer able Ġto ĠA Cl .\n",
      "\t Unique Tokens GPT2: {'Ġpreferable', 'based', 'orb', '-'}\n",
      "\t Unique Tokens LLAMA3: {'-based', 'or', 'able', 'b', 'Ġprefer'}\n",
      "==entailment==\n",
      "Text 1: oh that might be kind of interesting is it\n",
      "\t Tokenized GPT2:oh Ġthat Ġmight Ġbe Ġkind Ġof Ġinteresting Ġis Ġit\n",
      "\t Tokenized LLAMA3:oh Ġthat Ġmight Ġbe Ġkind Ġof Ġinteresting Ġis Ġit\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: That sounds kinda interesting to me.\n",
      "\t Tokenized GPT2:That Ġsounds Ġkinda Ġinteresting Ġto Ġme .\n",
      "\t Tokenized LLAMA3:That Ġsounds Ġkinda Ġinteresting Ġto Ġme .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: They were so sure of themselves that they took it for granted he had made a mistake.\"\n",
      "\t Tokenized GPT2:They Ġwere Ġso Ġsure Ġof Ġthemselves Ġthat Ġthey Ġtook Ġit Ġfor Ġgranted Ġhe Ġhad Ġmade Ġa Ġmistake .\"\n",
      "\t Tokenized LLAMA3:They Ġwere Ġso Ġsure Ġof Ġthemselves Ġthat Ġthey Ġtook Ġit Ġfor Ġgranted Ġhe Ġhad Ġmade Ġa Ġmistake .\"\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He made a mistake but concealed it so it wasn't obvious.\n",
      "\t Tokenized GPT2:He Ġmade Ġa Ġmistake Ġbut Ġconcealed Ġit Ġso Ġit Ġwasn 't Ġobvious .\n",
      "\t Tokenized LLAMA3:He Ġmade Ġa Ġmistake Ġbut Ġconce aled Ġit Ġso Ġit Ġwasn 't Ġobvious .\n",
      "\t Unique Tokens GPT2: {'Ġconcealed'}\n",
      "\t Unique Tokens LLAMA3: {'aled', 'Ġconce'}\n",
      "==contradiction==\n",
      "Text 1: hi Mary have you gone visiting uh any new restaurants lately\n",
      "\t Tokenized GPT2:hi ĠMary Ġhave Ġyou Ġgone Ġvisiting Ġuh Ġany Ġnew Ġrestaurants Ġlately\n",
      "\t Tokenized LLAMA3:hi ĠMary Ġhave Ġyou Ġgone Ġvisiting Ġuh Ġany Ġnew Ġrestaurants Ġlately\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Hi Mary, thanks for going to the restaurant with me yesterday, it was fun.\n",
      "\t Tokenized GPT2:Hi ĠMary , Ġthanks Ġfor Ġgoing Ġto Ġthe Ġrestaurant Ġwith Ġme Ġyesterday , Ġit Ġwas Ġfun .\n",
      "\t Tokenized LLAMA3:Hi ĠMary , Ġthanks Ġfor Ġgoing Ġto Ġthe Ġrestaurant Ġwith Ġme Ġyesterday , Ġit Ġwas Ġfun .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Politically, it's anti-democratic, replacing congressional and executive branch decision-making.\n",
      "\t Tokenized GPT2:Pol it ically , Ġit 's Ġanti - dem ocratic , Ġreplacing Ġcongressional Ġand Ġexecutive Ġbranch Ġdecision - making .\n",
      "\t Tokenized LLAMA3:Pol it ically , Ġit 's Ġanti -dem ocratic , Ġreplacing Ġcongressional Ġand Ġexecutive Ġbranch Ġdecision -making .\n",
      "\t Unique Tokens GPT2: {'making', 'dem', '-'}\n",
      "\t Unique Tokens LLAMA3: {'-making', '-dem'}\n",
      "Text 2: It's anti-democratic and takes the decision-making away from the executive branch in DC.\n",
      "\t Tokenized GPT2:It 's Ġanti - dem ocratic Ġand Ġtakes Ġthe Ġdecision - making Ġaway Ġfrom Ġthe Ġexecutive Ġbranch Ġin ĠDC .\n",
      "\t Tokenized LLAMA3:It 's Ġanti -dem ocratic Ġand Ġtakes Ġthe Ġdecision -making Ġaway Ġfrom Ġthe Ġexecutive Ġbranch Ġin ĠDC .\n",
      "\t Unique Tokens GPT2: {'making', 'dem', '-'}\n",
      "\t Unique Tokens LLAMA3: {'-making', '-dem'}\n",
      "==neutral==\n",
      "Text 1: Hey, no problem, a fine policy.\n",
      "\t Tokenized GPT2:Hey , Ġno Ġproblem , Ġa Ġfine Ġpolicy .\n",
      "\t Tokenized LLAMA3:Hey , Ġno Ġproblem , Ġa Ġfine Ġpolicy .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: No trouble, the best policy.\n",
      "\t Tokenized GPT2:No Ġtrouble , Ġthe Ġbest Ġpolicy .\n",
      "\t Tokenized LLAMA3:No Ġtrouble , Ġthe Ġbest Ġpolicy .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: South Along the Caribbean\n",
      "\t Tokenized GPT2:South ĠAlong Ġthe ĠCaribbean\n",
      "\t Tokenized LLAMA3:South ĠAlong Ġthe ĠCaribbean\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Opposite of North along the Caribbean.\n",
      "\t Tokenized GPT2:O pp os ite Ġof ĠNorth Ġalong Ġthe ĠCaribbean .\n",
      "\t Tokenized LLAMA3:O pp os ite Ġof ĠNorth Ġalong Ġthe ĠCaribbean .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: On the northwestern Alpine frontier, a new state had appeared on the scene, destined to lead the movement to a united Italy.\n",
      "\t Tokenized GPT2:On Ġthe Ġnorth western ĠAl pine Ġfront ier , Ġa Ġnew Ġstate Ġhad Ġappeared Ġon Ġthe Ġscene , Ġdestined Ġto Ġlead Ġthe Ġmovement Ġto Ġa Ġunited ĠItaly .\n",
      "\t Tokenized LLAMA3:On Ġthe Ġnorth western ĠAl pine Ġfront ier , Ġa Ġnew Ġstate Ġhad Ġappeared Ġon Ġthe Ġscene , Ġdestined Ġto Ġlead Ġthe Ġmovement Ġto Ġa Ġunited ĠItaly .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The alpine frontier was separated from Italy by glaciers.\n",
      "\t Tokenized GPT2:The Ġal pine Ġfront ier Ġwas Ġseparated Ġfrom ĠItaly Ġby Ġgl ac iers .\n",
      "\t Tokenized LLAMA3:The Ġal pine Ġfront ier Ġwas Ġseparated Ġfrom ĠItaly Ġby Ġgl ac iers .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: And she came to you?\n",
      "\t Tokenized GPT2:And Ġshe Ġcame Ġto Ġyou ?\n",
      "\t Tokenized LLAMA3:And Ġshe Ġcame Ġto Ġyou ?\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The person asked if the woman came to him.\n",
      "\t Tokenized GPT2:The Ġperson Ġasked Ġif Ġthe Ġwoman Ġcame Ġto Ġhim .\n",
      "\t Tokenized LLAMA3:The Ġperson Ġasked Ġif Ġthe Ġwoman Ġcame Ġto Ġhim .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The river-beds are mostly too shallow for anything but flat-bottomed boats.\n",
      "\t Tokenized GPT2:The Ġriver - bed s Ġare Ġmostly Ġtoo Ġshallow Ġfor Ġanything Ġbut Ġflat - bottom ed Ġboats .\n",
      "\t Tokenized LLAMA3:The Ġriver -b eds Ġare Ġmostly Ġtoo Ġshallow Ġfor Ġanything Ġbut Ġflat -bottom ed Ġboats .\n",
      "\t Unique Tokens GPT2: {'bed', '-', 'bottom', 's'}\n",
      "\t Unique Tokens LLAMA3: {'-b', 'eds', '-bottom'}\n",
      "Text 2: Boats that are not flat-bottomed are illegal on the river.\n",
      "\t Tokenized GPT2:Bo ats Ġthat Ġare Ġnot Ġflat - bottom ed Ġare Ġillegal Ġon Ġthe Ġriver .\n",
      "\t Tokenized LLAMA3:Bo ats Ġthat Ġare Ġnot Ġflat -bottom ed Ġare Ġillegal Ġon Ġthe Ġriver .\n",
      "\t Unique Tokens GPT2: {'-', 'bottom'}\n",
      "\t Unique Tokens LLAMA3: {'-bottom'}\n",
      "==neutral==\n",
      "Text 1: Cases in Comparative\n",
      "\t Tokenized GPT2:C ases Ġin ĠCompar ative\n",
      "\t Tokenized LLAMA3:C ases Ġin ĠCompar ative\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Cases can be part of a legal matter.\n",
      "\t Tokenized GPT2:C ases Ġcan Ġbe Ġpart Ġof Ġa Ġlegal Ġmatter .\n",
      "\t Tokenized LLAMA3:C ases Ġcan Ġbe Ġpart Ġof Ġa Ġlegal Ġmatter .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The credibility of the United States working with its European partners in NATO is on the line.\n",
      "\t Tokenized GPT2:The Ġcredibility Ġof Ġthe ĠUnited ĠStates Ġworking Ġwith Ġits ĠEuropean Ġpartners Ġin ĠNATO Ġis Ġon Ġthe Ġline .\n",
      "\t Tokenized LLAMA3:The Ġcredibility Ġof Ġthe ĠUnited ĠStates Ġworking Ġwith Ġits ĠEuropean Ġpartners Ġin ĠNATO Ġis Ġon Ġthe Ġline .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The United States currently enjoys high favorability among its European allies who trust it completely.\n",
      "\t Tokenized GPT2:The ĠUnited ĠStates Ġcurrently Ġenjoys Ġhigh Ġfavor ability Ġamong Ġits ĠEuropean Ġallies Ġwho Ġtrust Ġit Ġcompletely .\n",
      "\t Tokenized LLAMA3:The ĠUnited ĠStates Ġcurrently Ġenjoys Ġhigh Ġfavor ability Ġamong Ġits ĠEuropean Ġallies Ġwho Ġtrust Ġit Ġcompletely .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Perhaps we should prepare a militia.\n",
      "\t Tokenized GPT2:Perhaps Ġwe Ġshould Ġprepare Ġa Ġmilit ia .\n",
      "\t Tokenized LLAMA3:Perhaps Ġwe Ġshould Ġprepare Ġa Ġmilit ia .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Maybe it would be a good idea if we prepared a militia.\n",
      "\t Tokenized GPT2:Maybe Ġit Ġwould Ġbe Ġa Ġgood Ġidea Ġif Ġwe Ġprepared Ġa Ġmilit ia .\n",
      "\t Tokenized LLAMA3:Maybe Ġit Ġwould Ġbe Ġa Ġgood Ġidea Ġif Ġwe Ġprepared Ġa Ġmilit ia .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: In this respect, bringing Steve Jobs back to save Apple is like bringing Gen.\n",
      "\t Tokenized GPT2:In Ġthis Ġrespect , Ġbringing ĠSteve ĠJobs Ġback Ġto Ġsave ĠApple Ġis Ġlike Ġbringing ĠGen .\n",
      "\t Tokenized LLAMA3:In Ġthis Ġrespect , Ġbringing ĠSteve ĠJobs Ġback Ġto Ġsave ĠApple Ġis Ġlike Ġbringing ĠGen .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Steve Jobs unretired in 2002.\n",
      "\t Tokenized GPT2:Steve ĠJobs Ġun ret ired Ġin Ġ2002 .\n",
      "\t Tokenized LLAMA3:Steve ĠJobs Ġun ret ired Ġin Ġ 200 2 .\n",
      "\t Unique Tokens GPT2: {'Ġ2002'}\n",
      "\t Unique Tokens LLAMA3: {'2', 'Ġ', '200'}\n",
      "==neutral==\n",
      "Text 1: Smart men make good thieves, as long as they're desperate.\n",
      "\t Tokenized GPT2:Smart Ġmen Ġmake Ġgood Ġth ieves , Ġas Ġlong Ġas Ġthey 're Ġdesperate .\n",
      "\t Tokenized LLAMA3:Sm art Ġmen Ġmake Ġgood Ġth ieves , Ġas Ġlong Ġas Ġthey 're Ġdesperate .\n",
      "\t Unique Tokens GPT2: {'Smart'}\n",
      "\t Unique Tokens LLAMA3: {'Sm', 'art'}\n",
      "Text 2: Most thieves are desperate.\n",
      "\t Tokenized GPT2:Most Ġth ieves Ġare Ġdesperate .\n",
      "\t Tokenized LLAMA3:Most Ġth ieves Ġare Ġdesperate .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Other pundits beam their opinions at us as through a time warp, from the hazy days of past administrations.\n",
      "\t Tokenized GPT2:Other Ġp und its Ġbeam Ġtheir Ġopinions Ġat Ġus Ġas Ġthrough Ġa Ġtime Ġwar p , Ġfrom Ġthe Ġha zy Ġdays Ġof Ġpast Ġadministr ations .\n",
      "\t Tokenized LLAMA3:Other Ġp und its Ġbeam Ġtheir Ġopinions Ġat Ġus Ġas Ġthrough Ġa Ġtime Ġwar p , Ġfrom Ġthe Ġha zy Ġdays Ġof Ġpast Ġadministr ations .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Other experts give their opinions through time warp from past administrations.\n",
      "\t Tokenized GPT2:Other Ġexperts Ġgive Ġtheir Ġopinions Ġthrough Ġtime Ġwar p Ġfrom Ġpast Ġadministr ations .\n",
      "\t Tokenized LLAMA3:Other Ġexperts Ġgive Ġtheir Ġopinions Ġthrough Ġtime Ġwar p Ġfrom Ġpast Ġadministr ations .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: uh it's in Georgia it's yeah it's right outside of Macon and and it's just a i like the way that i like the way that idea of the south is\n",
      "\t Tokenized GPT2:uh Ġit 's Ġin ĠGeorgia Ġit 's Ġyeah Ġit 's Ġright Ġoutside Ġof ĠMac on Ġand Ġand Ġit 's Ġjust Ġa Ġi Ġlike Ġthe Ġway Ġthat Ġi Ġlike Ġthe Ġway Ġthat Ġidea Ġof Ġthe Ġsouth Ġis\n",
      "\t Tokenized LLAMA3:uh Ġit 's Ġin ĠGeorgia Ġit 's Ġyeah Ġit 's Ġright Ġoutside Ġof ĠMac on Ġand Ġand Ġit 's Ġjust Ġa Ġi Ġlike Ġthe Ġway Ġthat Ġi Ġlike Ġthe Ġway Ġthat Ġidea Ġof Ġthe Ġsouth Ġis\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It's in Georgia but it's a very long distance from Macon.\n",
      "\t Tokenized GPT2:It 's Ġin ĠGeorgia Ġbut Ġit 's Ġa Ġvery Ġlong Ġdistance Ġfrom ĠMac on .\n",
      "\t Tokenized LLAMA3:It 's Ġin ĠGeorgia Ġbut Ġit 's Ġa Ġvery Ġlong Ġdistance Ġfrom ĠMac on .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Coast Guard rules establishing bridgeopening schedules).\n",
      "\t Tokenized GPT2:Co ast ĠGuard Ġrules Ġestablishing Ġbridge opening Ġschedules ).\n",
      "\t Tokenized LLAMA3:Co ast ĠGuard Ġrules Ġestablishing Ġbridge open ing Ġschedules ).\n",
      "\t Unique Tokens GPT2: {'opening'}\n",
      "\t Unique Tokens LLAMA3: {'open', 'ing'}\n",
      "Text 2: The Coast Guard is in charge of opening bridges.\n",
      "\t Tokenized GPT2:The ĠCoast ĠGuard Ġis Ġin Ġcharge Ġof Ġopening Ġbridges .\n",
      "\t Tokenized LLAMA3:The ĠCoast ĠGuard Ġis Ġin Ġcharge Ġof Ġopening Ġbridges .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: did oh they're they are everywhere they\n",
      "\t Tokenized GPT2:did Ġoh Ġthey 're Ġthey Ġare Ġeverywhere Ġthey\n",
      "\t Tokenized LLAMA3:did Ġoh Ġthey 're Ġthey Ġare Ġeverywhere Ġthey\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: They are all over.\n",
      "\t Tokenized GPT2:They Ġare Ġall Ġover .\n",
      "\t Tokenized LLAMA3:They Ġare Ġall Ġover .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: In the other sight he saw Adrin's hands cocking back a pair of dragon-hammered pistols.\n",
      "\t Tokenized GPT2:In Ġthe Ġother Ġsight Ġhe Ġsaw ĠAd rin 's Ġhands Ġcock ing Ġback Ġa Ġpair Ġof Ġdragon - ham mered Ġpistol s .\n",
      "\t Tokenized LLAMA3:In Ġthe Ġother Ġsight Ġhe Ġsaw ĠAd rin 's Ġhands Ġcock ing Ġback Ġa Ġpair Ġof Ġdragon -h am mered Ġpist ols .\n",
      "\t Unique Tokens GPT2: {'s', '-', 'Ġpistol', 'ham'}\n",
      "\t Unique Tokens LLAMA3: {'Ġpist', '-h', 'am', 'ols'}\n",
      "Text 2: Adrin was seen cocking a pistol in each hand out of the corner of his eye.\n",
      "\t Tokenized GPT2:Ad rin Ġwas Ġseen Ġcock ing Ġa Ġpistol Ġin Ġeach Ġhand Ġout Ġof Ġthe Ġcorner Ġof Ġhis Ġeye .\n",
      "\t Tokenized LLAMA3:Ad rin Ġwas Ġseen Ġcock ing Ġa Ġpistol Ġin Ġeach Ġhand Ġout Ġof Ġthe Ġcorner Ġof Ġhis Ġeye .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: In an effort to more thoroughly explore this topic, we expanded our discussions beyond the eight organizations that were the primary subjects of our study by requesting the Computer Security Institute to informally poll its most active members on this subject.\n",
      "\t Tokenized GPT2:In Ġan Ġeffort Ġto Ġmore Ġthoroughly Ġexplore Ġthis Ġtopic , Ġwe Ġexpanded Ġour Ġdiscussions Ġbeyond Ġthe Ġeight Ġorganizations Ġthat Ġwere Ġthe Ġprimary Ġsubjects Ġof Ġour Ġstudy Ġby Ġrequesting Ġthe ĠComputer ĠSecurity ĠInstitute Ġto Ġinform ally Ġpoll Ġits Ġmost Ġactive Ġmembers Ġon Ġthis Ġsubject .\n",
      "\t Tokenized LLAMA3:In Ġan Ġeffort Ġto Ġmore Ġthoroughly Ġexplore Ġthis Ġtopic , Ġwe Ġexpanded Ġour Ġdiscussions Ġbeyond Ġthe Ġeight Ġorganizations Ġthat Ġwere Ġthe Ġprimary Ġsubjects Ġof Ġour Ġstudy Ġby Ġrequesting Ġthe ĠComputer ĠSecurity ĠInstitute Ġto Ġinform ally Ġpoll Ġits Ġmost Ġactive Ġmembers Ġon Ġthis Ġsubject .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: We are discussing this topic with more than just the original organizations.  \n",
      "\t Tokenized GPT2:We Ġare Ġdiscussing Ġthis Ġtopic Ġwith Ġmore Ġthan Ġjust Ġthe Ġoriginal Ġorganizations . ĠĠ\n",
      "\t Tokenized LLAMA3:We Ġare Ġdiscussing Ġthis Ġtopic Ġwith Ġmore Ġthan Ġjust Ġthe Ġoriginal Ġorganizations . ĠĠ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: After four years, Clinton has learned how to avoid looking unpresidential.\n",
      "\t Tokenized GPT2:After Ġfour Ġyears , ĠClinton Ġhas Ġlearned Ġhow Ġto Ġavoid Ġlooking Ġun pres ident ial .\n",
      "\t Tokenized LLAMA3:After Ġfour Ġyears , ĠClinton Ġhas Ġlearned Ġhow Ġto Ġavoid Ġlooking Ġun pres ident ial .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Four years later, Clinton understands how to look presidential.\n",
      "\t Tokenized GPT2:Four Ġyears Ġlater , ĠClinton Ġunderstands Ġhow Ġto Ġlook Ġpresidential .\n",
      "\t Tokenized LLAMA3:Four Ġyears Ġlater , ĠClinton Ġunderstands Ġhow Ġto Ġlook Ġpresidential .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: During the Crimean War (1854 56) she set up a hospital in the huge Selimiye Barracks (Selimiye Kelase).\n",
      "\t Tokenized GPT2:During Ġthe ĠCrime an ĠWar Ġ( 18 54 Ġ56 ) Ġshe Ġset Ġup Ġa Ġhospital Ġin Ġthe Ġhuge ĠSel imi ye ĠBarr acks Ġ( Sel imi ye ĠKel ase ).\n",
      "\t Tokenized LLAMA3:During Ġthe ĠCrime an ĠWar Ġ( 185 4 Ġ 56 ) Ġshe Ġset Ġup Ġa Ġhospital Ġin Ġthe Ġhuge ĠSel imi ye ĠBarr acks Ġ( S el imi ye ĠKel ase ).\n",
      "\t Unique Tokens GPT2: {'54', 'Sel', '18', 'Ġ56'}\n",
      "\t Unique Tokens LLAMA3: {'S', '185', 'Ġ', '56', 'el', '4'}\n",
      "Text 2: The hospital set up in the Selimiye Barracks is no longer standing.\n",
      "\t Tokenized GPT2:The Ġhospital Ġset Ġup Ġin Ġthe ĠSel imi ye ĠBarr acks Ġis Ġno Ġlonger Ġstanding .\n",
      "\t Tokenized LLAMA3:The Ġhospital Ġset Ġup Ġin Ġthe ĠSel imi ye ĠBarr acks Ġis Ġno Ġlonger Ġstanding .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: As a result, their services may be more effective when conducted in the emergency department environment.\n",
      "\t Tokenized GPT2:As Ġa Ġresult , Ġtheir Ġservices Ġmay Ġbe Ġmore Ġeffective Ġwhen Ġconducted Ġin Ġthe Ġemergency Ġdepartment Ġenvironment .\n",
      "\t Tokenized LLAMA3:As Ġa Ġresult , Ġtheir Ġservices Ġmay Ġbe Ġmore Ġeffective Ġwhen Ġconducted Ġin Ġthe Ġemergency Ġdepartment Ġenvironment .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Their services might be more effective if they're done in the ED.\n",
      "\t Tokenized GPT2:Their Ġservices Ġmight Ġbe Ġmore Ġeffective Ġif Ġthey 're Ġdone Ġin Ġthe ĠED .\n",
      "\t Tokenized LLAMA3:Their Ġservices Ġmight Ġbe Ġmore Ġeffective Ġif Ġthey 're Ġdone Ġin Ġthe ĠED .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: i know that you know the further we go from Adam the worse the food is for you but God still somehow makes us all be able to still live i think it's a miracle we're all still alive after so many generations well the last couple of processed foods you know i mean but i don't know i like to i like to \n",
      "\t Tokenized GPT2:i Ġknow Ġthat Ġyou Ġknow Ġthe Ġfurther Ġwe Ġgo Ġfrom ĠAdam Ġthe Ġworse Ġthe Ġfood Ġis Ġfor Ġyou Ġbut ĠGod Ġstill Ġsomehow Ġmakes Ġus Ġall Ġbe Ġable Ġto Ġstill Ġlive Ġi Ġthink Ġit 's Ġa Ġmiracle Ġwe 're Ġall Ġstill Ġalive Ġafter Ġso Ġmany Ġgenerations Ġwell Ġthe Ġlast Ġcouple Ġof Ġprocessed Ġfoods Ġyou Ġknow Ġi Ġmean Ġbut Ġi Ġdon 't Ġknow Ġi Ġlike Ġto Ġi Ġlike Ġto Ġ\n",
      "\t Tokenized LLAMA3:i Ġknow Ġthat Ġyou Ġknow Ġthe Ġfurther Ġwe Ġgo Ġfrom ĠAdam Ġthe Ġworse Ġthe Ġfood Ġis Ġfor Ġyou Ġbut ĠGod Ġstill Ġsomehow Ġmakes Ġus Ġall Ġbe Ġable Ġto Ġstill Ġlive Ġi Ġthink Ġit 's Ġa Ġmiracle Ġwe 're Ġall Ġstill Ġalive Ġafter Ġso Ġmany Ġgenerations Ġwell Ġthe Ġlast Ġcouple Ġof Ġprocessed Ġfoods Ġyou Ġknow Ġi Ġmean Ġbut Ġi Ġdon 't Ġknow Ġi Ġlike Ġto Ġi Ġlike Ġto Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It is miraculous God still provides for us to this day.\n",
      "\t Tokenized GPT2:It Ġis Ġmir ac ulous ĠGod Ġstill Ġprovides Ġfor Ġus Ġto Ġthis Ġday .\n",
      "\t Tokenized LLAMA3:It Ġis Ġmir ac ulous ĠGod Ġstill Ġprovides Ġfor Ġus Ġto Ġthis Ġday .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: As a counterweight to the Singapore Chinese, he would bring in the North Borneo states of Sabah and Sarawak, granting them special privileges for their indigenous populations and funds for the development of their backward economies.\n",
      "\t Tokenized GPT2:As Ġa Ġcounter weight Ġto Ġthe ĠSingapore ĠChinese , Ġhe Ġwould Ġbring Ġin Ġthe ĠNorth ĠB orne o Ġstates Ġof ĠSab ah Ġand ĠSar aw ak , Ġgrant ing Ġthem Ġspecial Ġprivileges Ġfor Ġtheir Ġindigenous Ġpopulations Ġand Ġfunds Ġfor Ġthe Ġdevelopment Ġof Ġtheir Ġbackward Ġeconomies .\n",
      "\t Tokenized LLAMA3:As Ġa Ġcounter weight Ġto Ġthe ĠSingapore ĠChinese , Ġhe Ġwould Ġbring Ġin Ġthe ĠNorth ĠB orne o Ġstates Ġof ĠSab ah Ġand ĠSar aw ak , Ġgrant ing Ġthem Ġspecial Ġprivileges Ġfor Ġtheir Ġindigenous Ġpopulations Ġand Ġfunds Ġfor Ġthe Ġdevelopment Ġof Ġtheir Ġbackward Ġeconomies .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The states of Sabah and Sarawak committed mass genocide of their indigenous populations.\n",
      "\t Tokenized GPT2:The Ġstates Ġof ĠSab ah Ġand ĠSar aw ak Ġcommitted Ġmass Ġgenocide Ġof Ġtheir Ġindigenous Ġpopulations .\n",
      "\t Tokenized LLAMA3:The Ġstates Ġof ĠSab ah Ġand ĠSar aw ak Ġcommitted Ġmass Ġgen ocide Ġof Ġtheir Ġindigenous Ġpopulations .\n",
      "\t Unique Tokens GPT2: {'Ġgenocide'}\n",
      "\t Unique Tokens LLAMA3: {'Ġgen', 'ocide'}\n",
      "==entailment==\n",
      "Text 1: Tell me, how did those scribbled words on the envelope help you to discover that a will was made yesterday afternoon?\" Poirot smiled. \n",
      "\t Tokenized GPT2:Tell Ġme , Ġhow Ġdid Ġthose Ġscri bb led Ġwords Ġon Ġthe Ġenvelope Ġhelp Ġyou Ġto Ġdiscover Ġthat Ġa Ġwill Ġwas Ġmade Ġyesterday Ġafternoon ?\" ĠP oir ot Ġsmiled . Ġ\n",
      "\t Tokenized LLAMA3:Tell Ġme , Ġhow Ġdid Ġthose Ġscri bb led Ġwords Ġon Ġthe Ġenvelope Ġhelp Ġyou Ġto Ġdiscover Ġthat Ġa Ġwill Ġwas Ġmade Ġyesterday Ġafternoon ?\" ĠP oir ot Ġsmiled . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: How did you work out from that text that there was a new will?\n",
      "\t Tokenized GPT2:How Ġdid Ġyou Ġwork Ġout Ġfrom Ġthat Ġtext Ġthat Ġthere Ġwas Ġa Ġnew Ġwill ?\n",
      "\t Tokenized LLAMA3:How Ġdid Ġyou Ġwork Ġout Ġfrom Ġthat Ġtext Ġthat Ġthere Ġwas Ġa Ġnew Ġwill ?\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: A detailed English explanation of the plot is always provided, and wireless recorded commentary units are sometimes available.\n",
      "\t Tokenized GPT2:A Ġdetailed ĠEnglish Ġexplanation Ġof Ġthe Ġplot Ġis Ġalways Ġprovided , Ġand Ġwireless Ġrecorded Ġcommentary Ġunits Ġare Ġsometimes Ġavailable .\n",
      "\t Tokenized LLAMA3:A Ġdetailed ĠEnglish Ġexplanation Ġof Ġthe Ġplot Ġis Ġalways Ġprovided , Ġand Ġwireless Ġrecorded Ġcommentary Ġunits Ġare Ġsometimes Ġavailable .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: A detailed plot, written in English, is always available and an audio commentary is sometimes available.\n",
      "\t Tokenized GPT2:A Ġdetailed Ġplot , Ġwritten Ġin ĠEnglish , Ġis Ġalways Ġavailable Ġand Ġan Ġaudio Ġcommentary Ġis Ġsometimes Ġavailable .\n",
      "\t Tokenized LLAMA3:A Ġdetailed Ġplot , Ġwritten Ġin ĠEnglish , Ġis Ġalways Ġavailable Ġand Ġan Ġaudio Ġcommentary Ġis Ġsometimes Ġavailable .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Or to judge by the  Failing to nurse at night can lead to painful engorgement or even breast infection.\n",
      "\t Tokenized GPT2:Or Ġto Ġjudge Ġby Ġthe Ġ ĠF ailing Ġto Ġnurse Ġat Ġnight Ġcan Ġlead Ġto Ġpainful Ġeng org ement Ġor Ġeven Ġbreast Ġinfection .\n",
      "\t Tokenized LLAMA3:Or Ġto Ġjudge Ġby Ġthe Ġ ĠF ailing Ġto Ġnurse Ġat Ġnight Ġcan Ġlead Ġto Ġpainful Ġeng org ement Ġor Ġeven Ġbreast Ġinfection .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Mothers can go many hours without nursing.\n",
      "\t Tokenized GPT2:M others Ġcan Ġgo Ġmany Ġhours Ġwithout Ġnursing .\n",
      "\t Tokenized LLAMA3:M others Ġcan Ġgo Ġmany Ġhours Ġwithout Ġnursing .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Your man wouldn't have remained conscious after the first blow.\n",
      "\t Tokenized GPT2:Your Ġman Ġwouldn 't Ġhave Ġremained Ġconscious Ġafter Ġthe Ġfirst Ġblow .\n",
      "\t Tokenized LLAMA3:Your Ġman Ġwouldn 't Ġhave Ġremained Ġconscious Ġafter Ġthe Ġfirst Ġblow .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Your man stayed conscious even after the first blow.\n",
      "\t Tokenized GPT2:Your Ġman Ġstayed Ġconscious Ġeven Ġafter Ġthe Ġfirst Ġblow .\n",
      "\t Tokenized LLAMA3:Your Ġman Ġstayed Ġconscious Ġeven Ġafter Ġthe Ġfirst Ġblow .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: With most plants needing to install control equipment to meet these requirements, it is likely that this approach would lead to installation of controls that become obsolete and stranded capital investments as additional requirements are promulgated.\n",
      "\t Tokenized GPT2:With Ġmost Ġplants Ġneeding Ġto Ġinstall Ġcontrol Ġequipment Ġto Ġmeet Ġthese Ġrequirements , Ġit Ġis Ġlikely Ġthat Ġthis Ġapproach Ġwould Ġlead Ġto Ġinstallation Ġof Ġcontrols Ġthat Ġbecome Ġobsolete Ġand Ġstr anded Ġcapital Ġinvestments Ġas Ġadditional Ġrequirements Ġare Ġprom ul g ated .\n",
      "\t Tokenized LLAMA3:With Ġmost Ġplants Ġneeding Ġto Ġinstall Ġcontrol Ġequipment Ġto Ġmeet Ġthese Ġrequirements , Ġit Ġis Ġlikely Ġthat Ġthis Ġapproach Ġwould Ġlead Ġto Ġinstallation Ġof Ġcontrols Ġthat Ġbecome Ġobsolete Ġand Ġstr anded Ġcapital Ġinvestments Ġas Ġadditional Ġrequirements Ġare Ġprom ul g ated .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Most plants are already up to code.\n",
      "\t Tokenized GPT2:Most Ġplants Ġare Ġalready Ġup Ġto Ġcode .\n",
      "\t Tokenized LLAMA3:Most Ġplants Ġare Ġalready Ġup Ġto Ġcode .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Do you want to see historic sights and tour museums and art galleries?\n",
      "\t Tokenized GPT2:Do Ġyou Ġwant Ġto Ġsee Ġhistoric Ġsights Ġand Ġtour Ġmuse ums Ġand Ġart Ġgall eries ?\n",
      "\t Tokenized LLAMA3:Do Ġyou Ġwant Ġto Ġsee Ġhistoric Ġsights Ġand Ġtour Ġmuse ums Ġand Ġart Ġgall eries ?\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Since you love learning new things, would you like to visit some historic places, museums, and art galleries?\n",
      "\t Tokenized GPT2:Since Ġyou Ġlove Ġlearning Ġnew Ġthings , Ġwould Ġyou Ġlike Ġto Ġvisit Ġsome Ġhistoric Ġplaces , Ġmuse ums , Ġand Ġart Ġgall eries ?\n",
      "\t Tokenized LLAMA3:Since Ġyou Ġlove Ġlearning Ġnew Ġthings , Ġwould Ġyou Ġlike Ġto Ġvisit Ġsome Ġhistoric Ġplaces , Ġmuse ums , Ġand Ġart Ġgall eries ?\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Miller claimed the First Amendment (right to freedom of speech and association) rather than taking the Fifth (right against self-incrimination).\n",
      "\t Tokenized GPT2:M iller Ġclaimed Ġthe ĠFirst ĠAmendment Ġ( right Ġto Ġfreedom Ġof Ġspeech Ġand Ġassociation ) Ġrather Ġthan Ġtaking Ġthe ĠFifth Ġ( right Ġagainst Ġself - inc rimination ).\n",
      "\t Tokenized LLAMA3:M iller Ġclaimed Ġthe ĠFirst ĠAmendment Ġ( right Ġto Ġfreedom Ġof Ġspeech Ġand Ġassociation ) Ġrather Ġthan Ġtaking Ġthe ĠFifth Ġ( right Ġagainst Ġself -in cr imination ).\n",
      "\t Unique Tokens GPT2: {'-', 'inc', 'rimination'}\n",
      "\t Unique Tokens LLAMA3: {'-in', 'cr', 'imination'}\n",
      "Text 2: The man did not plead the Fifth.\n",
      "\t Tokenized GPT2:The Ġman Ġdid Ġnot Ġple ad Ġthe ĠFifth .\n",
      "\t Tokenized LLAMA3:The Ġman Ġdid Ġnot Ġple ad Ġthe ĠFifth .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The primary screen must be integrated into the standard intake procedure of the emergency setting and must be the responsibility of the staff to administer to all patients.\n",
      "\t Tokenized GPT2:The Ġprimary Ġscreen Ġmust Ġbe Ġintegrated Ġinto Ġthe Ġstandard Ġintake Ġprocedure Ġof Ġthe Ġemergency Ġsetting Ġand Ġmust Ġbe Ġthe Ġresponsibility Ġof Ġthe Ġstaff Ġto Ġadmin ister Ġto Ġall Ġpatients .\n",
      "\t Tokenized LLAMA3:The Ġprimary Ġscreen Ġmust Ġbe Ġintegrated Ġinto Ġthe Ġstandard Ġintake Ġprocedure Ġof Ġthe Ġemergency Ġsetting Ġand Ġmust Ġbe Ġthe Ġresponsibility Ġof Ġthe Ġstaff Ġto Ġadmin ister Ġto Ġall Ġpatients .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Integration of primary screens will prevent patients from leaving early.\n",
      "\t Tokenized GPT2:Integr ation Ġof Ġprimary Ġscreens Ġwill Ġprevent Ġpatients Ġfrom Ġleaving Ġearly .\n",
      "\t Tokenized LLAMA3:Integr ation Ġof Ġprimary Ġscreens Ġwill Ġprevent Ġpatients Ġfrom Ġleaving Ġearly .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: General Motors, for instance, lost $460 million to strikes in 1997, but investors treated the costs as a kind of extraordinary charge and valued the company as if the losses had never happened.\n",
      "\t Tokenized GPT2:General ĠMot ors , Ġfor Ġinstance , Ġlost Ġ$ 4 60 Ġmillion Ġto Ġstrikes Ġin Ġ1997 , Ġbut Ġinvestors Ġtreated Ġthe Ġcosts Ġas Ġa Ġkind Ġof Ġextraordinary Ġcharge Ġand Ġvalued Ġthe Ġcompany Ġas Ġif Ġthe Ġlosses Ġhad Ġnever Ġhappened .\n",
      "\t Tokenized LLAMA3:General ĠMot ors , Ġfor Ġinstance , Ġlost Ġ$ 460 Ġmillion Ġto Ġstrikes Ġin Ġ 199 7 , Ġbut Ġinvestors Ġtreated Ġthe Ġcosts Ġas Ġa Ġkind Ġof Ġextraordinary Ġcharge Ġand Ġvalued Ġthe Ġcompany Ġas Ġif Ġthe Ġlosses Ġhad Ġnever Ġhappened .\n",
      "\t Unique Tokens GPT2: {'4', 'Ġ1997', '60'}\n",
      "\t Unique Tokens LLAMA3: {'460', '7', '199', 'Ġ'}\n",
      "Text 2: GM lost a lot of money in labor disputes but was victorious in the end.\n",
      "\t Tokenized GPT2:GM Ġlost Ġa Ġlot Ġof Ġmoney Ġin Ġlabor Ġdisput es Ġbut Ġwas Ġvict orious Ġin Ġthe Ġend .\n",
      "\t Tokenized LLAMA3:GM Ġlost Ġa Ġlot Ġof Ġmoney Ġin Ġlabor Ġdisput es Ġbut Ġwas Ġvict orious Ġin Ġthe Ġend .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: the only problem is it's not large enough it only holds about i think they squeezed when Ryan struck out his five thousandth player they they squeezed about forty thousand people in there\n",
      "\t Tokenized GPT2:the Ġonly Ġproblem Ġis Ġit 's Ġnot Ġlarge Ġenough Ġit Ġonly Ġholds Ġabout Ġi Ġthink Ġthey Ġsqueezed Ġwhen ĠRyan Ġstruck Ġout Ġhis Ġfive Ġthousand th Ġplayer Ġthey Ġthey Ġsqueezed Ġabout Ġforty Ġthousand Ġpeople Ġin Ġthere\n",
      "\t Tokenized LLAMA3:the Ġonly Ġproblem Ġis Ġit 's Ġnot Ġlarge Ġenough Ġit Ġonly Ġholds Ġabout Ġi Ġthink Ġthey Ġsqueezed Ġwhen ĠRyan Ġstruck Ġout Ġhis Ġfive Ġthousand th Ġplayer Ġthey Ġthey Ġsqueezed Ġabout Ġforty Ġthousand Ġpeople Ġin Ġthere\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It doesn't hold many people so it was standing room only.\n",
      "\t Tokenized GPT2:It Ġdoesn 't Ġhold Ġmany Ġpeople Ġso Ġit Ġwas Ġstanding Ġroom Ġonly .\n",
      "\t Tokenized LLAMA3:It Ġdoesn 't Ġhold Ġmany Ġpeople Ġso Ġit Ġwas Ġstanding Ġroom Ġonly .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Paris and its immediate surroundings are a magnet for tourists, students, businessmen, artists, inventors ' in short, everyone except perhaps the farmer and fisherman, who may well come to the city to protest government policies.\n",
      "\t Tokenized GPT2:Par is Ġand Ġits Ġimmediate Ġsurroundings Ġare Ġa Ġmagnet Ġfor Ġtourists , Ġstudents , Ġbusiness men , Ġartists , Ġinvent ors Ġ' Ġin Ġshort , Ġeveryone Ġexcept Ġperhaps Ġthe Ġfarmer Ġand Ġfisher man , Ġwho Ġmay Ġwell Ġcome Ġto Ġthe Ġcity Ġto Ġprotest Ġgovernment Ġpolicies .\n",
      "\t Tokenized LLAMA3:Par is Ġand Ġits Ġimmediate Ġsurroundings Ġare Ġa Ġmagnet Ġfor Ġtourists , Ġstudents , Ġbusiness men , Ġartists , Ġinvent ors Ġ' Ġin Ġshort , Ġeveryone Ġexcept Ġperhaps Ġthe Ġfarmer Ġand Ġfisher man , Ġwho Ġmay Ġwell Ġcome Ġto Ġthe Ġcity Ġto Ġprotest Ġgovernment Ġpolicies .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Businessmen spend more time in Paris and its immediate surroundings than inventors do.\n",
      "\t Tokenized GPT2:Business men Ġspend Ġmore Ġtime Ġin ĠParis Ġand Ġits Ġimmediate Ġsurroundings Ġthan Ġinvent ors Ġdo .\n",
      "\t Tokenized LLAMA3:Business men Ġspend Ġmore Ġtime Ġin ĠParis Ġand Ġits Ġimmediate Ġsurroundings Ġthan Ġinvent ors Ġdo .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Then he turned to Tommy.\n",
      "\t Tokenized GPT2:Then Ġhe Ġturned Ġto ĠTommy .\n",
      "\t Tokenized LLAMA3:Then Ġhe Ġturned Ġto ĠTommy .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He talked to Tommy.\n",
      "\t Tokenized GPT2:He Ġtalked Ġto ĠTommy .\n",
      "\t Tokenized LLAMA3:He Ġtalked Ġto ĠTommy .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The seven grants flow from a new Nonprofit Capacity Building program at the foundation, part of a trend among philanthropists to give money to help organizations grow stronger, rather than to the program services they provide.\n",
      "\t Tokenized GPT2:The Ġseven Ġgrants Ġflow Ġfrom Ġa Ġnew ĠNon profit ĠCap acity ĠBuilding Ġprogram Ġat Ġthe Ġfoundation , Ġpart Ġof Ġa Ġtrend Ġamong Ġphil anthrop ists Ġto Ġgive Ġmoney Ġto Ġhelp Ġorganizations Ġgrow Ġstronger , Ġrather Ġthan Ġto Ġthe Ġprogram Ġservices Ġthey Ġprovide .\n",
      "\t Tokenized LLAMA3:The Ġseven Ġgrants Ġflow Ġfrom Ġa Ġnew ĠNon pro fit ĠCap acity ĠBuilding Ġprogram Ġat Ġthe Ġfoundation , Ġpart Ġof Ġa Ġtrend Ġamong Ġphil an throp ists Ġto Ġgive Ġmoney Ġto Ġhelp Ġorganizations Ġgrow Ġstronger , Ġrather Ġthan Ġto Ġthe Ġprogram Ġservices Ġthey Ġprovide .\n",
      "\t Unique Tokens GPT2: {'profit', 'anthrop'}\n",
      "\t Unique Tokens LLAMA3: {'throp', 'pro', 'fit', 'an'}\n",
      "Text 2: The grants flow from a Nonprofit Capacity Building program at the foundation, exemplifying a trend among philanthropists to give money to grow organizations and then to take them over.\n",
      "\t Tokenized GPT2:The Ġgrants Ġflow Ġfrom Ġa ĠNon profit ĠCap acity ĠBuilding Ġprogram Ġat Ġthe Ġfoundation , Ġexempl ifying Ġa Ġtrend Ġamong Ġphil anthrop ists Ġto Ġgive Ġmoney Ġto Ġgrow Ġorganizations Ġand Ġthen Ġto Ġtake Ġthem Ġover .\n",
      "\t Tokenized LLAMA3:The Ġgrants Ġflow Ġfrom Ġa ĠNon pro fit ĠCap acity ĠBuilding Ġprogram Ġat Ġthe Ġfoundation , Ġexempl ifying Ġa Ġtrend Ġamong Ġphil an throp ists Ġto Ġgive Ġmoney Ġto Ġgrow Ġorganizations Ġand Ġthen Ġto Ġtake Ġthem Ġover .\n",
      "\t Unique Tokens GPT2: {'profit', 'anthrop'}\n",
      "\t Unique Tokens LLAMA3: {'throp', 'pro', 'fit', 'an'}\n",
      "==contradiction==\n",
      "Text 1: Yet, in the mouths of the white townsfolk of Salisbury, N.C., it sounds convincing.\n",
      "\t Tokenized GPT2:Yet , Ġin Ġthe Ġmouths Ġof Ġthe Ġwhite Ġtowns folk Ġof ĠSal is bury , ĠN . C ., Ġit Ġsounds Ġconvincing .\n",
      "\t Tokenized LLAMA3:Yet , Ġin Ġthe Ġmouths Ġof Ġthe Ġwhite Ġtowns fol k Ġof ĠSal is bury , ĠN .C ., Ġit Ġsounds Ġconvincing .\n",
      "\t Unique Tokens GPT2: {'folk', 'C'}\n",
      "\t Unique Tokens LLAMA3: {'fol', '.C', 'k'}\n",
      "Text 2: White people in Salisbury, N.C. don't believe it. \n",
      "\t Tokenized GPT2:White Ġpeople Ġin ĠSal is bury , ĠN . C . Ġdon 't Ġbelieve Ġit . Ġ\n",
      "\t Tokenized LLAMA3:White Ġpeople Ġin ĠSal is bury , ĠN .C . Ġdon 't Ġbelieve Ġit . Ġ\n",
      "\t Unique Tokens GPT2: {'C'}\n",
      "\t Unique Tokens LLAMA3: {'.C'}\n",
      "==contradiction==\n",
      "Text 1: As the road climbs toward the entrance, you'll pass fields full of Santorini's famed tomatoes growing on the steep slopes.\n",
      "\t Tokenized GPT2:As Ġthe Ġroad Ġclim bs Ġtoward Ġthe Ġentrance , Ġyou 'll Ġpass Ġfields Ġfull Ġof ĠSant or ini 's Ġfam ed Ġtomatoes Ġgrowing Ġon Ġthe Ġsteep Ġsl opes .\n",
      "\t Tokenized LLAMA3:As Ġthe Ġroad Ġclim bs Ġtoward Ġthe Ġentrance , Ġyou 'll Ġpass Ġfields Ġfull Ġof ĠSant or ini 's Ġfam ed Ġtomatoes Ġgrowing Ġon Ġthe Ġsteep Ġsl opes .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The climate in Santorini is not conducive to tomato farming.\n",
      "\t Tokenized GPT2:The Ġclimate Ġin ĠSant or ini Ġis Ġnot Ġcon duc ive Ġto Ġtomato Ġfarming .\n",
      "\t Tokenized LLAMA3:The Ġclimate Ġin ĠSant or ini Ġis Ġnot Ġcon duc ive Ġto Ġtomato Ġfarming .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: for the direct sunlight and stuff right but uh but i i haven't really found it too bad we've lived in our house about uh oh thirteen years i suppose and and really really only painted once and you know it was new when we bought it and we painted one time since then but you know it's probably going t\n",
      "\t Tokenized GPT2:for Ġthe Ġdirect Ġsunlight Ġand Ġstuff Ġright Ġbut Ġuh Ġbut Ġi Ġi Ġhaven 't Ġreally Ġfound Ġit Ġtoo Ġbad Ġwe 've Ġlived Ġin Ġour Ġhouse Ġabout Ġuh Ġoh Ġthirteen Ġyears Ġi Ġsuppose Ġand Ġand Ġreally Ġreally Ġonly Ġpainted Ġonce Ġand Ġyou Ġknow Ġit Ġwas Ġnew Ġwhen Ġwe Ġbought Ġit Ġand Ġwe Ġpainted Ġone Ġtime Ġsince Ġthen Ġbut Ġyou Ġknow Ġit 's Ġprobably Ġgoing Ġt\n",
      "\t Tokenized LLAMA3:for Ġthe Ġdirect Ġsunlight Ġand Ġstuff Ġright Ġbut Ġuh Ġbut Ġi Ġi Ġhaven 't Ġreally Ġfound Ġit Ġtoo Ġbad Ġwe 've Ġlived Ġin Ġour Ġhouse Ġabout Ġuh Ġoh Ġth irteen Ġyears Ġi Ġsuppose Ġand Ġand Ġreally Ġreally Ġonly Ġpainted Ġonce Ġand Ġyou Ġknow Ġit Ġwas Ġnew Ġwhen Ġwe Ġbought Ġit Ġand Ġwe Ġpainted Ġone Ġtime Ġsince Ġthen Ġbut Ġyou Ġknow Ġit 's Ġprobably Ġgoing Ġt\n",
      "\t Unique Tokens GPT2: {'Ġthirteen'}\n",
      "\t Unique Tokens LLAMA3: {'irteen', 'Ġth'}\n",
      "Text 2: I only had to paint the house once.\n",
      "\t Tokenized GPT2:I Ġonly Ġhad Ġto Ġpaint Ġthe Ġhouse Ġonce .\n",
      "\t Tokenized LLAMA3:I Ġonly Ġhad Ġto Ġpaint Ġthe Ġhouse Ġonce .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: She leaned back in her chair.\n",
      "\t Tokenized GPT2:She Ġleaned Ġback Ġin Ġher Ġchair .\n",
      "\t Tokenized LLAMA3:She Ġleaned Ġback Ġin Ġher Ġchair .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: She was sitting on a chair. \n",
      "\t Tokenized GPT2:She Ġwas Ġsitting Ġon Ġa Ġchair . Ġ\n",
      "\t Tokenized LLAMA3:She Ġwas Ġsitting Ġon Ġa Ġchair . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Tommy realized perfectly that in his own wits lay the only chance of escape, and behind his casual manner he was racking his brains furiously.\n",
      "\t Tokenized GPT2:Tom my Ġrealized Ġperfectly Ġthat Ġin Ġhis Ġown Ġw its Ġlay Ġthe Ġonly Ġchance Ġof Ġescape , Ġand Ġbehind Ġhis Ġcasual Ġmanner Ġhe Ġwas Ġr acking Ġhis Ġbrains Ġfur iously .\n",
      "\t Tokenized LLAMA3:Tom my Ġrealized Ġperfectly Ġthat Ġin Ġhis Ġown Ġw its Ġlay Ġthe Ġonly Ġchance Ġof Ġescape , Ġand Ġbehind Ġhis Ġcasual Ġmanner Ġhe Ġwas Ġr acking Ġhis Ġbrains Ġfur iously .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Tommy was keeping a calm demeanor, though his mind was racing with thoughts of escaping.\n",
      "\t Tokenized GPT2:Tom my Ġwas Ġkeeping Ġa Ġcalm Ġdem eanor , Ġthough Ġhis Ġmind Ġwas Ġracing Ġwith Ġthoughts Ġof Ġescaping .\n",
      "\t Tokenized LLAMA3:Tom my Ġwas Ġkeeping Ġa Ġcalm Ġdem eanor , Ġthough Ġhis Ġmind Ġwas Ġracing Ġwith Ġthoughts Ġof Ġescaping .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1:  The Romans never really infiltrated Ibiza, and even after the defeat of Hannibal in 202 b.c. during the Second Punic War their influence was restrained.\n",
      "\t Tokenized GPT2:ĠThe ĠRomans Ġnever Ġreally Ġinfiltr ated ĠI b iza , Ġand Ġeven Ġafter Ġthe Ġdefeat Ġof ĠHannibal Ġin Ġ20 2 Ġb . c . Ġduring Ġthe ĠSecond ĠPun ic ĠWar Ġtheir Ġinfluence Ġwas Ġrestr ained .\n",
      "\t Tokenized LLAMA3:ĠThe ĠRom ans Ġnever Ġreally Ġinfiltr ated ĠI b iza , Ġand Ġeven Ġafter Ġthe Ġdefeat Ġof ĠHannibal Ġin Ġ 202 Ġb .c . Ġduring Ġthe ĠSecond ĠPun ic ĠWar Ġtheir Ġinfluence Ġwas Ġrestr ained .\n",
      "\t Unique Tokens GPT2: {'ĠRomans', 'c', 'Ġ20', '2'}\n",
      "\t Unique Tokens LLAMA3: {'Ġ', 'ĠRom', 'ans', '.c', '202'}\n",
      "Text 2: The Romans infiltrated Ibiza.\n",
      "\t Tokenized GPT2:The ĠRomans Ġinfiltr ated ĠI b iza .\n",
      "\t Tokenized LLAMA3:The ĠRom ans Ġinfiltr ated ĠI b iza .\n",
      "\t Unique Tokens GPT2: {'ĠRomans'}\n",
      "\t Unique Tokens LLAMA3: {'ans', 'ĠRom'}\n",
      "==entailment==\n",
      "Text 1: Such a knowledgebased process enables decision makers to be reasonably certain about critical facets of the product under development when they need this knowledge.\n",
      "\t Tokenized GPT2:Such Ġa Ġknowledge based Ġprocess Ġenables Ġdecision Ġmakers Ġto Ġbe Ġreasonably Ġcertain Ġabout Ġcritical Ġfac ets Ġof Ġthe Ġproduct Ġunder Ġdevelopment Ġwhen Ġthey Ġneed Ġthis Ġknowledge .\n",
      "\t Tokenized LLAMA3:Such Ġa Ġknowledge based Ġprocess Ġenables Ġdecision Ġmakers Ġto Ġbe Ġreasonably Ġcertain Ġabout Ġcritical Ġfac ets Ġof Ġthe Ġproduct Ġunder Ġdevelopment Ġwhen Ġthey Ġneed Ġthis Ġknowledge .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The knowledge base will give them information needed for the product to be developed.\n",
      "\t Tokenized GPT2:The Ġknowledge Ġbase Ġwill Ġgive Ġthem Ġinformation Ġneeded Ġfor Ġthe Ġproduct Ġto Ġbe Ġdeveloped .\n",
      "\t Tokenized LLAMA3:The Ġknowledge Ġbase Ġwill Ġgive Ġthem Ġinformation Ġneeded Ġfor Ġthe Ġproduct Ġto Ġbe Ġdeveloped .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The Drawing Room was partially destroyed by fire in 1941, and its furnishings are faithful reproductions; the huge (repaired) Ming punch bowl is striking.\n",
      "\t Tokenized GPT2:The ĠDraw ing ĠRoom Ġwas Ġpartially Ġdestroyed Ġby Ġfire Ġin Ġ19 41 , Ġand Ġits Ġfurn ishing s Ġare Ġfaithful Ġreprodu ctions ; Ġthe Ġhuge Ġ( rep aired ) ĠMing Ġpunch Ġbowl Ġis Ġstriking .\n",
      "\t Tokenized LLAMA3:The ĠDraw ing ĠRoom Ġwas Ġpartially Ġdestroyed Ġby Ġfire Ġin Ġ 194 1 , Ġand Ġits Ġfurn ishing s Ġare Ġfaithful Ġreprodu ctions ; Ġthe Ġhuge Ġ( rep aired ) ĠM ing Ġpunch Ġbowl Ġis Ġstriking .\n",
      "\t Unique Tokens GPT2: {'41', 'ĠMing', 'Ġ19'}\n",
      "\t Unique Tokens LLAMA3: {'ĠM', '194', '1', 'Ġ'}\n",
      "Text 2: The 1941 fire spared the Drawing Room.\n",
      "\t Tokenized GPT2:The Ġ19 41 Ġfire Ġsp ared Ġthe ĠDraw ing ĠRoom .\n",
      "\t Tokenized LLAMA3:The Ġ 194 1 Ġfire Ġsp ared Ġthe ĠDraw ing ĠRoom .\n",
      "\t Unique Tokens GPT2: {'41', 'Ġ19'}\n",
      "\t Unique Tokens LLAMA3: {'1', '194', 'Ġ'}\n",
      "==contradiction==\n",
      "Text 1: Total electricity expenditures increase by about 15% to 30% depending on the year and the scenario (see Table 3, below, and the tables in Appendix 5.2 for more detail on the changing pattern of expenditures).\n",
      "\t Tokenized GPT2:Total Ġelectricity Ġexpend it ures Ġincrease Ġby Ġabout Ġ15 % Ġto Ġ30 % Ġdepending Ġon Ġthe Ġyear Ġand Ġthe Ġscenario Ġ( see ĠTable Ġ3 , Ġbelow , Ġand Ġthe Ġtables Ġin ĠAppend ix Ġ5 . 2 Ġfor Ġmore Ġdetail Ġon Ġthe Ġchanging Ġpattern Ġof Ġexpend it ures ).\n",
      "\t Tokenized LLAMA3:Total Ġelectricity Ġexpend it ures Ġincrease Ġby Ġabout Ġ 15 % Ġto Ġ 30 % Ġdepending Ġon Ġthe Ġyear Ġand Ġthe Ġscenario Ġ( see ĠTable Ġ 3 , Ġbelow , Ġand Ġthe Ġtables Ġin ĠAppend ix Ġ 5 . 2 Ġfor Ġmore Ġdetail Ġon Ġthe Ġchanging Ġpattern Ġof Ġexpend it ures ).\n",
      "\t Unique Tokens GPT2: {'Ġ3', 'Ġ5', 'Ġ30', 'Ġ15'}\n",
      "\t Unique Tokens LLAMA3: {'3', 'Ġ', '15', '30', '5'}\n",
      "Text 2: They were sad to see the costs go up 80% in a year.\n",
      "\t Tokenized GPT2:They Ġwere Ġsad Ġto Ġsee Ġthe Ġcosts Ġgo Ġup Ġ80 % Ġin Ġa Ġyear .\n",
      "\t Tokenized LLAMA3:They Ġwere Ġsad Ġto Ġsee Ġthe Ġcosts Ġgo Ġup Ġ 80 % Ġin Ġa Ġyear .\n",
      "\t Unique Tokens GPT2: {'Ġ80'}\n",
      "\t Unique Tokens LLAMA3: {'80', 'Ġ'}\n",
      "==entailment==\n",
      "Text 1: Or else it was administered in the brandy you gave her.\n",
      "\t Tokenized GPT2:Or Ġelse Ġit Ġwas Ġadministered Ġin Ġthe Ġbrand y Ġyou Ġgave Ġher .\n",
      "\t Tokenized LLAMA3:Or Ġelse Ġit Ġwas Ġadministered Ġin Ġthe Ġbrand y Ġyou Ġgave Ġher .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Otherwise it was put in the alcohol you gave her.\n",
      "\t Tokenized GPT2:Otherwise Ġit Ġwas Ġput Ġin Ġthe Ġalcohol Ġyou Ġgave Ġher .\n",
      "\t Tokenized LLAMA3:Otherwise Ġit Ġwas Ġput Ġin Ġthe Ġalcohol Ġyou Ġgave Ġher .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: My article does not say or imply that real earnings growth only reflects retentions and that dividend growth must be zero or that all valuation techniques are out the window for firms that don't pay dividends.\n",
      "\t Tokenized GPT2:My Ġarticle Ġdoes Ġnot Ġsay Ġor Ġimply Ġthat Ġreal Ġearnings Ġgrowth Ġonly Ġreflects Ġret ent ions Ġand Ġthat Ġdividend Ġgrowth Ġmust Ġbe Ġzero Ġor Ġthat Ġall Ġvaluation Ġtechniques Ġare Ġout Ġthe Ġwindow Ġfor Ġfirms Ġthat Ġdon 't Ġpay Ġdividend s .\n",
      "\t Tokenized LLAMA3:My Ġarticle Ġdoes Ġnot Ġsay Ġor Ġimply Ġthat Ġreal Ġearnings Ġgrowth Ġonly Ġreflects Ġret ent ions Ġand Ġthat Ġdividend Ġgrowth Ġmust Ġbe Ġzero Ġor Ġthat Ġall Ġval uation Ġtechniques Ġare Ġout Ġthe Ġwindow Ġfor Ġfirms Ġthat Ġdon 't Ġpay Ġdividend s .\n",
      "\t Unique Tokens GPT2: {'Ġvaluation'}\n",
      "\t Unique Tokens LLAMA3: {'uation', 'Ġval'}\n",
      "Text 2: My article simply implies that real earnings growth reflects only retentions and that dividend growth must be zero or that valuation techniques are unused for firms which don't pay dividends.\n",
      "\t Tokenized GPT2:My Ġarticle Ġsimply Ġimplies Ġthat Ġreal Ġearnings Ġgrowth Ġreflects Ġonly Ġret ent ions Ġand Ġthat Ġdividend Ġgrowth Ġmust Ġbe Ġzero Ġor Ġthat Ġvaluation Ġtechniques Ġare Ġunused Ġfor Ġfirms Ġwhich Ġdon 't Ġpay Ġdividend s .\n",
      "\t Tokenized LLAMA3:My Ġarticle Ġsimply Ġimplies Ġthat Ġreal Ġearnings Ġgrowth Ġreflects Ġonly Ġret ent ions Ġand Ġthat Ġdividend Ġgrowth Ġmust Ġbe Ġzero Ġor Ġthat Ġval uation Ġtechniques Ġare Ġunused Ġfor Ġfirms Ġwhich Ġdon 't Ġpay Ġdividend s .\n",
      "\t Unique Tokens GPT2: {'Ġvaluation'}\n",
      "\t Unique Tokens LLAMA3: {'uation', 'Ġval'}\n",
      "==neutral==\n",
      "Text 1: and the like a guy does it and he has his own pigs\n",
      "\t Tokenized GPT2:and Ġthe Ġlike Ġa Ġguy Ġdoes Ġit Ġand Ġhe Ġhas Ġhis Ġown Ġpigs\n",
      "\t Tokenized LLAMA3:and Ġthe Ġlike Ġa Ġguy Ġdoes Ġit Ġand Ġhe Ġhas Ġhis Ġown Ġpigs\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The guy is a pig farmer in Iowa.\n",
      "\t Tokenized GPT2:The Ġguy Ġis Ġa Ġpig Ġfarmer Ġin ĠIowa .\n",
      "\t Tokenized LLAMA3:The Ġguy Ġis Ġa Ġpig Ġfarmer Ġin ĠIowa .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Then Shuman claims that Linux provides no graphical user interface.\n",
      "\t Tokenized GPT2:Then ĠSh uman Ġclaims Ġthat ĠLinux Ġprovides Ġno Ġgraphical Ġuser Ġinterface .\n",
      "\t Tokenized LLAMA3:Then ĠSh uman Ġclaims Ġthat ĠLinux Ġprovides Ġno Ġgraphical Ġuser Ġinterface .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: They knew what they were talking about.\n",
      "\t Tokenized GPT2:They Ġknew Ġwhat Ġthey Ġwere Ġtalking Ġabout .\n",
      "\t Tokenized LLAMA3:They Ġknew Ġwhat Ġthey Ġwere Ġtalking Ġabout .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The universal credibility problem with polling is that wordsmithing and mathematics don't mix, and never will.\n",
      "\t Tokenized GPT2:The Ġuniversal Ġcredibility Ġproblem Ġwith Ġpolling Ġis Ġthat Ġwords mith ing Ġand Ġmathematics Ġdon 't Ġmix , Ġand Ġnever Ġwill .\n",
      "\t Tokenized LLAMA3:The Ġuniversal Ġcredibility Ġproblem Ġwith Ġpolling Ġis Ġthat Ġwords mith ing Ġand Ġmathematics Ġdon 't Ġmix , Ġand Ġnever Ġwill .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Mathematics is the most important aspect of polling.\n",
      "\t Tokenized GPT2:Mat hemat ics Ġis Ġthe Ġmost Ġimportant Ġaspect Ġof Ġpolling .\n",
      "\t Tokenized LLAMA3:Mat hematics Ġis Ġthe Ġmost Ġimportant Ġaspect Ġof Ġpolling .\n",
      "\t Unique Tokens GPT2: {'hemat', 'ics'}\n",
      "\t Unique Tokens LLAMA3: {'hematics'}\n",
      "==contradiction==\n",
      "Text 1: you don't think it's a deterrent\n",
      "\t Tokenized GPT2:you Ġdon 't Ġthink Ġit 's Ġa Ġdeter rent\n",
      "\t Tokenized LLAMA3:you Ġdon 't Ġthink Ġit 's Ġa Ġdeter rent\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: You have absolutely no doubt whatsoever that it will not be a deterrent\n",
      "\t Tokenized GPT2:You Ġhave Ġabsolutely Ġno Ġdoubt Ġwhatsoever Ġthat Ġit Ġwill Ġnot Ġbe Ġa Ġdeter rent\n",
      "\t Tokenized LLAMA3:You Ġhave Ġabsolutely Ġno Ġdoubt Ġwhatsoever Ġthat Ġit Ġwill Ġnot Ġbe Ġa Ġdeter rent\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: If you missed the two top stories in yesterday's USAT --the government's first post-deregulation attempt to preserve competitiveness among airlines and the emergence of a drug that can prevent breast cancer--they are on the NYT 's front today.\n",
      "\t Tokenized GPT2:If Ġyou Ġmissed Ġthe Ġtwo Ġtop Ġstories Ġin Ġyesterday 's ĠUS AT Ġ-- the Ġgovernment 's Ġfirst Ġpost - d ere g ulation Ġattempt Ġto Ġpreserve Ġcompet it iveness Ġamong Ġairlines Ġand Ġthe Ġemergence Ġof Ġa Ġdrug Ġthat Ġcan Ġprevent Ġbreast Ġcancer -- they Ġare Ġon Ġthe ĠNYT Ġ' s Ġfront Ġtoday .\n",
      "\t Tokenized LLAMA3:If Ġyou Ġmissed Ġthe Ġtwo Ġtop Ġstories Ġin Ġyesterday 's ĠUS AT Ġ-- the Ġgovernment 's Ġfirst Ġpost -d ere g ulation Ġattempt Ġto Ġpreserve Ġcompet it iveness Ġamong Ġa irlines Ġand Ġthe Ġemergence Ġof Ġa Ġdrug Ġthat Ġcan Ġprevent Ġbreast Ġcancer -- they Ġare Ġon Ġthe ĠNYT Ġ' s Ġfront Ġtoday .\n",
      "\t Unique Tokens GPT2: {'-', 'Ġairlines', 'd'}\n",
      "\t Unique Tokens LLAMA3: {'-d', 'irlines'}\n",
      "Text 2: These two story are causing an uproar among wide audiences.\n",
      "\t Tokenized GPT2:These Ġtwo Ġstory Ġare Ġcausing Ġan Ġup ro ar Ġamong Ġwide Ġaudiences .\n",
      "\t Tokenized LLAMA3:These Ġtwo Ġstory Ġare Ġcausing Ġan Ġup ro ar Ġamong Ġwide Ġaudiences .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: He felt the off-hand dagger's weight in the small of his back.\n",
      "\t Tokenized GPT2:He Ġfelt Ġthe Ġoff - hand Ġd agger 's Ġweight Ġin Ġthe Ġsmall Ġof Ġhis Ġback .\n",
      "\t Tokenized LLAMA3:He Ġfelt Ġthe Ġoff -hand Ġd agger 's Ġweight Ġin Ġthe Ġsmall Ġof Ġhis Ġback .\n",
      "\t Unique Tokens GPT2: {'-', 'hand'}\n",
      "\t Unique Tokens LLAMA3: {'-hand'}\n",
      "Text 2: The knife was on his back.\n",
      "\t Tokenized GPT2:The Ġknife Ġwas Ġon Ġhis Ġback .\n",
      "\t Tokenized LLAMA3:The Ġknife Ġwas Ġon Ġhis Ġback .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: so well i think we've taken up at least five minutes\n",
      "\t Tokenized GPT2:so Ġwell Ġi Ġthink Ġwe 've Ġtaken Ġup Ġat Ġleast Ġfive Ġminutes\n",
      "\t Tokenized LLAMA3:so Ġwell Ġi Ġthink Ġwe 've Ġtaken Ġup Ġat Ġleast Ġfive Ġminutes\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I have taken up the last 5 minutes.\n",
      "\t Tokenized GPT2:I Ġhave Ġtaken Ġup Ġthe Ġlast Ġ5 Ġminutes .\n",
      "\t Tokenized LLAMA3:I Ġhave Ġtaken Ġup Ġthe Ġlast Ġ 5 Ġminutes .\n",
      "\t Unique Tokens GPT2: {'Ġ5'}\n",
      "\t Unique Tokens LLAMA3: {'5', 'Ġ'}\n",
      "==contradiction==\n",
      "Text 1: The tree-lined avenue extends less than three blocks to the sea.\n",
      "\t Tokenized GPT2:The Ġtree - lined Ġa venue Ġextends Ġless Ġthan Ġthree Ġblocks Ġto Ġthe Ġsea .\n",
      "\t Tokenized LLAMA3:The Ġtree -l ined Ġa venue Ġextends Ġless Ġthan Ġthree Ġblocks Ġto Ġthe Ġsea .\n",
      "\t Unique Tokens GPT2: {'-', 'lined'}\n",
      "\t Unique Tokens LLAMA3: {'ined', '-l'}\n",
      "Text 2: You must travel two miles via the avenue to the sea.\n",
      "\t Tokenized GPT2:You Ġmust Ġtravel Ġtwo Ġmiles Ġvia Ġthe Ġa venue Ġto Ġthe Ġsea .\n",
      "\t Tokenized LLAMA3:You Ġmust Ġtravel Ġtwo Ġmiles Ġvia Ġthe Ġa venue Ġto Ġthe Ġsea .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Growth &amp\n",
      "\t Tokenized GPT2:G row th Ġ& amp\n",
      "\t Tokenized LLAMA3:G row th Ġ& amp\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Shrinking.\n",
      "\t Tokenized GPT2:Sh r inking .\n",
      "\t Tokenized LLAMA3:Sh r inking .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Many Greeks in Asia Minor were forced to leave their homes and brought an influence of eastern cadences with them.\n",
      "\t Tokenized GPT2:Many ĠGree ks Ġin ĠAsia ĠMin or Ġwere Ġforced Ġto Ġleave Ġtheir Ġhomes Ġand Ġbrought Ġan Ġinfluence Ġof Ġeastern Ġcad ences Ġwith Ġthem .\n",
      "\t Tokenized LLAMA3:Many ĠGree ks Ġin ĠAsia ĠMin or Ġwere Ġforced Ġto Ġleave Ġtheir Ġhomes Ġand Ġbrought Ġan Ġinfluence Ġof Ġeastern Ġcad ences Ġwith Ġthem .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The greens in Asia minor were able to stay in their homes. \n",
      "\t Tokenized GPT2:The Ġg reens Ġin ĠAsia Ġminor Ġwere Ġable Ġto Ġstay Ġin Ġtheir Ġhomes . Ġ\n",
      "\t Tokenized LLAMA3:The Ġg reens Ġin ĠAsia Ġminor Ġwere Ġable Ġto Ġstay Ġin Ġtheir Ġhomes . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The author began with a set of hunches or hypotheses about what can go wrong in agency management, and what would be evidence supporting-or contradicting-these hypotheses.\n",
      "\t Tokenized GPT2:The Ġauthor Ġbegan Ġwith Ġa Ġset Ġof Ġhun ches Ġor Ġhypot heses Ġabout Ġwhat Ġcan Ġgo Ġwrong Ġin Ġagency Ġmanagement , Ġand Ġwhat Ġwould Ġbe Ġevidence Ġsupporting - or Ġcontrad icting - these Ġhypot heses .\n",
      "\t Tokenized LLAMA3:The Ġauthor Ġbegan Ġwith Ġa Ġset Ġof Ġhun ches Ġor Ġhypot heses Ġabout Ġwhat Ġcan Ġgo Ġwrong Ġin Ġagency Ġmanagement , Ġand Ġwhat Ġwould Ġbe Ġevidence Ġsupporting -or Ġcontrad icting -the se Ġhypot heses .\n",
      "\t Unique Tokens GPT2: {'-', 'these', 'or'}\n",
      "\t Unique Tokens LLAMA3: {'se', '-the', '-or'}\n",
      "Text 2: The author began with a set of theories about the ways in which agency management can go right.\n",
      "\t Tokenized GPT2:The Ġauthor Ġbegan Ġwith Ġa Ġset Ġof Ġtheories Ġabout Ġthe Ġways Ġin Ġwhich Ġagency Ġmanagement Ġcan Ġgo Ġright .\n",
      "\t Tokenized LLAMA3:The Ġauthor Ġbegan Ġwith Ġa Ġset Ġof Ġtheories Ġabout Ġthe Ġways Ġin Ġwhich Ġagency Ġmanagement Ġcan Ġgo Ġright .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: It was going to be a hot day. \n",
      "\t Tokenized GPT2:It Ġwas Ġgoing Ġto Ġbe Ġa Ġhot Ġday . Ġ\n",
      "\t Tokenized LLAMA3:It Ġwas Ġgoing Ġto Ġbe Ġa Ġhot Ġday . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It was already hot and was going to get hotter.\n",
      "\t Tokenized GPT2:It Ġwas Ġalready Ġhot Ġand Ġwas Ġgoing Ġto Ġget Ġhot ter .\n",
      "\t Tokenized LLAMA3:It Ġwas Ġalready Ġhot Ġand Ġwas Ġgoing Ġto Ġget Ġhot ter .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: In the stock market, however, the damage can get much worse.\n",
      "\t Tokenized GPT2:In Ġthe Ġstock Ġmarket , Ġhowever , Ġthe Ġdamage Ġcan Ġget Ġmuch Ġworse .\n",
      "\t Tokenized LLAMA3:In Ġthe Ġstock Ġmarket , Ġhowever , Ġthe Ġdamage Ġcan Ġget Ġmuch Ġworse .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The damage to the stock market can get much worse when prices increase. \n",
      "\t Tokenized GPT2:The Ġdamage Ġto Ġthe Ġstock Ġmarket Ġcan Ġget Ġmuch Ġworse Ġwhen Ġprices Ġincrease . Ġ\n",
      "\t Tokenized LLAMA3:The Ġdamage Ġto Ġthe Ġstock Ġmarket Ġcan Ġget Ġmuch Ġworse Ġwhen Ġprices Ġincrease . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: i mean that's a real attractive option if you have the the technology for it all it was was you know i mean she just used a phone modem and she was like she was sitting in the office\n",
      "\t Tokenized GPT2:i Ġmean Ġthat 's Ġa Ġreal Ġattractive Ġoption Ġif Ġyou Ġhave Ġthe Ġthe Ġtechnology Ġfor Ġit Ġall Ġit Ġwas Ġwas Ġyou Ġknow Ġi Ġmean Ġshe Ġjust Ġused Ġa Ġphone Ġmodem Ġand Ġshe Ġwas Ġlike Ġshe Ġwas Ġsitting Ġin Ġthe Ġoffice\n",
      "\t Tokenized LLAMA3:i Ġmean Ġthat 's Ġa Ġreal Ġattractive Ġoption Ġif Ġyou Ġhave Ġthe Ġthe Ġtechnology Ġfor Ġit Ġall Ġit Ġwas Ġwas Ġyou Ġknow Ġi Ġmean Ġshe Ġjust Ġused Ġa Ġphone Ġmodem Ġand Ġshe Ġwas Ġlike Ġshe Ġwas Ġsitting Ġin Ġthe Ġoffice\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The phone modem was easy to set up and use. \n",
      "\t Tokenized GPT2:The Ġphone Ġmodem Ġwas Ġeasy Ġto Ġset Ġup Ġand Ġuse . Ġ\n",
      "\t Tokenized LLAMA3:The Ġphone Ġmodem Ġwas Ġeasy Ġto Ġset Ġup Ġand Ġuse . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Small boats tie up here with batches of crayfish, fresh fish, and eel, and housewives clamor for the fishermen to weigh their choices on rudimentary scales.\n",
      "\t Tokenized GPT2:Small Ġboats Ġtie Ġup Ġhere Ġwith Ġbat ches Ġof Ġcr ay fish , Ġfresh Ġfish , Ġand Ġe el , Ġand Ġhouse w ives Ġclam or Ġfor Ġthe Ġfisher men Ġto Ġweigh Ġtheir Ġchoices Ġon Ġrud iment ary Ġscales .\n",
      "\t Tokenized LLAMA3:Small Ġboats Ġtie Ġup Ġhere Ġwith Ġbat ches Ġof Ġcr ay fish , Ġfresh Ġfish , Ġand Ġe el , Ġand Ġhouse w ives Ġclam or Ġfor Ġthe Ġfisher men Ġto Ġweigh Ġtheir Ġchoices Ġon Ġr ud iment ary Ġscales .\n",
      "\t Unique Tokens GPT2: {'Ġrud'}\n",
      "\t Unique Tokens LLAMA3: {'Ġr', 'ud'}\n",
      "Text 2: The fish are weighed using high tech digital scales.\n",
      "\t Tokenized GPT2:The Ġfish Ġare Ġweighed Ġusing Ġhigh Ġtech Ġdigital Ġscales .\n",
      "\t Tokenized LLAMA3:The Ġfish Ġare Ġweighed Ġusing Ġhigh Ġtech Ġdigital Ġscales .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: and once we came here it was like gosh i just miss that because it really is exciting to be around people of different\n",
      "\t Tokenized GPT2:and Ġonce Ġwe Ġcame Ġhere Ġit Ġwas Ġlike Ġgosh Ġi Ġjust Ġmiss Ġthat Ġbecause Ġit Ġreally Ġis Ġexciting Ġto Ġbe Ġaround Ġpeople Ġof Ġdifferent\n",
      "\t Tokenized LLAMA3:and Ġonce Ġwe Ġcame Ġhere Ġit Ġwas Ġlike Ġgosh Ġi Ġjust Ġmiss Ġthat Ġbecause Ġit Ġreally Ġis Ġexciting Ġto Ġbe Ġaround Ġpeople Ġof Ġdifferent\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: This place does not surprise me anymore.  \n",
      "\t Tokenized GPT2:This Ġplace Ġdoes Ġnot Ġsurprise Ġme Ġanymore . ĠĠ\n",
      "\t Tokenized LLAMA3:This Ġplace Ġdoes Ġnot Ġsurprise Ġme Ġanymore . ĠĠ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Too bad it chose to use McIntyre instead.\n",
      "\t Tokenized GPT2:Too Ġbad Ġit Ġchose Ġto Ġuse ĠMc Int y re Ġinstead .\n",
      "\t Tokenized LLAMA3:Too Ġbad Ġit Ġchose Ġto Ġuse ĠMc Int y re Ġinstead .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: McIntyre was not picked to be used.\n",
      "\t Tokenized GPT2:Mc Int y re Ġwas Ġnot Ġpicked Ġto Ġbe Ġused .\n",
      "\t Tokenized LLAMA3:Mc Int y re Ġwas Ġnot Ġpicked Ġto Ġbe Ġused .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: right just get you away from the everyday things that are going on we when the children were smaller we used to go to uh Delaware along the ocean ocean most every year and that was fun we stayed mostly in state parks and uh we really enjoyed that\n",
      "\t Tokenized GPT2:right Ġjust Ġget Ġyou Ġaway Ġfrom Ġthe Ġeveryday Ġthings Ġthat Ġare Ġgoing Ġon Ġwe Ġwhen Ġthe Ġchildren Ġwere Ġsmaller Ġwe Ġused Ġto Ġgo Ġto Ġuh ĠDel aware Ġalong Ġthe Ġocean Ġocean Ġmost Ġevery Ġyear Ġand Ġthat Ġwas Ġfun Ġwe Ġstayed Ġmostly Ġin Ġstate Ġparks Ġand Ġuh Ġwe Ġreally Ġenjoyed Ġthat\n",
      "\t Tokenized LLAMA3:right Ġjust Ġget Ġyou Ġaway Ġfrom Ġthe Ġeveryday Ġthings Ġthat Ġare Ġgoing Ġon Ġwe Ġwhen Ġthe Ġchildren Ġwere Ġsmaller Ġwe Ġused Ġto Ġgo Ġto Ġuh ĠDel aware Ġalong Ġthe Ġocean Ġocean Ġmost Ġevery Ġyear Ġand Ġthat Ġwas Ġfun Ġwe Ġstayed Ġmostly Ġin Ġstate Ġparks Ġand Ġuh Ġwe Ġreally Ġenjoyed Ġthat\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The kids used to prefer the ocean over state parks.\n",
      "\t Tokenized GPT2:The Ġkids Ġused Ġto Ġprefer Ġthe Ġocean Ġover Ġstate Ġparks .\n",
      "\t Tokenized LLAMA3:The Ġkids Ġused Ġto Ġprefer Ġthe Ġocean Ġover Ġstate Ġparks .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Robust  came in third among words and phrases submitted (220 citations in the CR ), and unlike the previous two, it seems to be a genuinely new cliche; at any rate, Chatterbox hadn't previously been aware of its overuse.\n",
      "\t Tokenized GPT2:Rob ust Ġ Ġcame Ġin Ġthird Ġamong Ġwords Ġand Ġphrases Ġsubmitted Ġ( 220 Ġcit ations Ġin Ġthe ĠCR Ġ), Ġand Ġunlike Ġthe Ġprevious Ġtwo , Ġit Ġseems Ġto Ġbe Ġa Ġgenuinely Ġnew Ġcliche ; Ġat Ġany Ġrate , ĠChat ter box Ġhadn 't Ġpreviously Ġbeen Ġaware Ġof Ġits Ġover use .\n",
      "\t Tokenized LLAMA3:Rob ust Ġ Ġcame Ġin Ġthird Ġamong Ġwords Ġand Ġphrases Ġsubmitted Ġ( 220 Ġcit ations Ġin Ġthe ĠCR Ġ), Ġand Ġunlike Ġthe Ġprevious Ġtwo , Ġit Ġseems Ġto Ġbe Ġa Ġgenuinely Ġnew Ġcl iche ; Ġat Ġany Ġrate , ĠChat ter box Ġhadn 't Ġpreviously Ġbeen Ġaware Ġof Ġits Ġover use .\n",
      "\t Unique Tokens GPT2: {'Ġcliche'}\n",
      "\t Unique Tokens LLAMA3: {'iche', 'Ġcl'}\n",
      "Text 2: Chatterbox was surprised by all of the phrases and words that were submitted.\n",
      "\t Tokenized GPT2:Chat ter box Ġwas Ġsurprised Ġby Ġall Ġof Ġthe Ġphrases Ġand Ġwords Ġthat Ġwere Ġsubmitted .\n",
      "\t Tokenized LLAMA3:Chat ter box Ġwas Ġsurprised Ġby Ġall Ġof Ġthe Ġphrases Ġand Ġwords Ġthat Ġwere Ġsubmitted .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Some predict the jokes will wear thin soon, while others call it definitively depraved (Tom Shales, the Washington Post ). (Download a clip from South Park here.)\n",
      "\t Tokenized GPT2:Some Ġpredict Ġthe Ġjokes Ġwill Ġwear Ġthin Ġsoon , Ġwhile Ġothers Ġcall Ġit Ġdefinitive ly Ġde pr aved Ġ( Tom ĠSh ales , Ġthe ĠWashington ĠPost Ġ). Ġ( Download Ġa Ġclip Ġfrom ĠSouth ĠPark Ġhere .)\n",
      "\t Tokenized LLAMA3:Some Ġpredict Ġthe Ġjokes Ġwill Ġwear Ġthin Ġsoon , Ġwhile Ġothers Ġcall Ġit Ġdefinitive ly Ġde pr aved Ġ( Tom ĠSh ales , Ġthe ĠWashington ĠPost Ġ). Ġ( Download Ġa Ġclip Ġfrom ĠSouth ĠPark Ġhere .)\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Everyone thinks the jokes will always be funny.\n",
      "\t Tokenized GPT2:Everyone Ġthinks Ġthe Ġjokes Ġwill Ġalways Ġbe Ġfunny .\n",
      "\t Tokenized LLAMA3:Everyone Ġthinks Ġthe Ġjokes Ġwill Ġalways Ġbe Ġfunny .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: There are a number of expensive jewelry and other duty-free shops, all with goods priced in US dollars (duty-free goods must always be paid for in foreign currency).\n",
      "\t Tokenized GPT2:There Ġare Ġa Ġnumber Ġof Ġexpensive Ġjewelry Ġand Ġother Ġduty - free Ġshops , Ġall Ġwith Ġgoods Ġpriced Ġin ĠUS Ġdollars Ġ( d uty - free Ġgoods Ġmust Ġalways Ġbe Ġpaid Ġfor Ġin Ġforeign Ġcurrency ).\n",
      "\t Tokenized LLAMA3:There Ġare Ġa Ġnumber Ġof Ġexpensive Ġjewelry Ġand Ġother Ġduty -free Ġshops , Ġall Ġwith Ġgoods Ġpriced Ġin ĠUS Ġdollars Ġ( d uty -free Ġgoods Ġmust Ġalways Ġbe Ġpaid Ġfor Ġin Ġforeign Ġcurrency ).\n",
      "\t Unique Tokens GPT2: {'-', 'free'}\n",
      "\t Unique Tokens LLAMA3: {'-free'}\n",
      "Text 2: Be sure to bring currencies other than the US dollar when buying goods from the duty-free shops.\n",
      "\t Tokenized GPT2:Be Ġsure Ġto Ġbring Ġcur rencies Ġother Ġthan Ġthe ĠUS Ġdollar Ġwhen Ġbuying Ġgoods Ġfrom Ġthe Ġduty - free Ġshops .\n",
      "\t Tokenized LLAMA3:Be Ġsure Ġto Ġbring Ġcur rencies Ġother Ġthan Ġthe ĠUS Ġdollar Ġwhen Ġbuying Ġgoods Ġfrom Ġthe Ġduty -free Ġshops .\n",
      "\t Unique Tokens GPT2: {'-', 'free'}\n",
      "\t Unique Tokens LLAMA3: {'-free'}\n",
      "==entailment==\n",
      "Text 1: Some rooms have balconies.\n",
      "\t Tokenized GPT2:Some Ġrooms Ġhave Ġbalcon ies .\n",
      "\t Tokenized LLAMA3:Some Ġrooms Ġhave Ġbalcon ies .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Some rooms have balconies off of them.\n",
      "\t Tokenized GPT2:Some Ġrooms Ġhave Ġbalcon ies Ġoff Ġof Ġthem .\n",
      "\t Tokenized LLAMA3:Some Ġrooms Ġhave Ġbalcon ies Ġoff Ġof Ġthem .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: But you would not trust me.\"\n",
      "\t Tokenized GPT2:But Ġyou Ġwould Ġnot Ġtrust Ġme .\"\n",
      "\t Tokenized LLAMA3:But Ġyou Ġwould Ġnot Ġtrust Ġme .\"\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: You trust me implicitly. \n",
      "\t Tokenized GPT2:You Ġtrust Ġme Ġimplicitly . Ġ\n",
      "\t Tokenized LLAMA3:You Ġtrust Ġme Ġimplicitly . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Second, reducing the rate of HIV transmission is in any event not the only social goal worth  If it were, we'd outlaw sex entirely.\n",
      "\t Tokenized GPT2:Second , Ġreducing Ġthe Ġrate Ġof ĠHIV Ġtransmission Ġis Ġin Ġany Ġevent Ġnot Ġthe Ġonly Ġsocial Ġgoal Ġworth Ġ ĠIf Ġit Ġwere , Ġwe 'd Ġout law Ġsex Ġentirely .\n",
      "\t Tokenized LLAMA3:Second , Ġreducing Ġthe Ġrate Ġof ĠHIV Ġtransmission Ġis Ġin Ġany Ġevent Ġnot Ġthe Ġonly Ġsocial Ġgoal Ġworth Ġ ĠIf Ġit Ġwere , Ġwe 'd Ġout law Ġsex Ġentirely .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Reducing the transmission of HIV is just as important as reducing drug abuse.\n",
      "\t Tokenized GPT2:Red ucing Ġthe Ġtransmission Ġof ĠHIV Ġis Ġjust Ġas Ġimportant Ġas Ġreducing Ġdrug Ġabuse .\n",
      "\t Tokenized LLAMA3:Red uc ing Ġthe Ġtransmission Ġof ĠHIV Ġis Ġjust Ġas Ġimportant Ġas Ġreducing Ġdrug Ġabuse .\n",
      "\t Unique Tokens GPT2: {'ucing'}\n",
      "\t Unique Tokens LLAMA3: {'uc', 'ing'}\n",
      "==contradiction==\n",
      "Text 1: um-hum with the ice yeah\n",
      "\t Tokenized GPT2:um - hum Ġwith Ġthe Ġice Ġyeah\n",
      "\t Tokenized LLAMA3:um -h um Ġwith Ġthe Ġice Ġyeah\n",
      "\t Unique Tokens GPT2: {'-', 'hum'}\n",
      "\t Unique Tokens LLAMA3: {'-h'}\n",
      "Text 2: With the sunshine and heat wave yes.\n",
      "\t Tokenized GPT2:With Ġthe Ġsunshine Ġand Ġheat Ġwave Ġyes .\n",
      "\t Tokenized LLAMA3:With Ġthe Ġsunshine Ġand Ġheat Ġwave Ġyes .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Since there is no airport on the island, all visitors must arrive at the port, Skala, where most of the hotels are located and all commercial activity is carried out.\n",
      "\t Tokenized GPT2:Since Ġthere Ġis Ġno Ġairport Ġon Ġthe Ġisland , Ġall Ġvisitors Ġmust Ġarrive Ġat Ġthe Ġport , ĠSk ala , Ġwhere Ġmost Ġof Ġthe Ġhotels Ġare Ġlocated Ġand Ġall Ġcommercial Ġactivity Ġis Ġcarried Ġout .\n",
      "\t Tokenized LLAMA3:Since Ġthere Ġis Ġno Ġairport Ġon Ġthe Ġisland , Ġall Ġvisitors Ġmust Ġarrive Ġat Ġthe Ġport , ĠSk ala , Ġwhere Ġmost Ġof Ġthe Ġhotels Ġare Ġlocated Ġand Ġall Ġcommercial Ġactivity Ġis Ġcarried Ġout .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The best way to get onto the island is by plane.\n",
      "\t Tokenized GPT2:The Ġbest Ġway Ġto Ġget Ġonto Ġthe Ġisland Ġis Ġby Ġplane .\n",
      "\t Tokenized LLAMA3:The Ġbest Ġway Ġto Ġget Ġonto Ġthe Ġisland Ġis Ġby Ġplane .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: yeah well i was surprised at the the way they drafted last year they didn't really didn't go for the uh big offensive lineman or the defensive lineman they're going for the skilled positions so quarterbacks they really\n",
      "\t Tokenized GPT2:yeah Ġwell Ġi Ġwas Ġsurprised Ġat Ġthe Ġthe Ġway Ġthey Ġdrafted Ġlast Ġyear Ġthey Ġdidn 't Ġreally Ġdidn 't Ġgo Ġfor Ġthe Ġuh Ġbig Ġoffensive Ġl inem an Ġor Ġthe Ġdefensive Ġl inem an Ġthey 're Ġgoing Ġfor Ġthe Ġskilled Ġpositions Ġso Ġquarter backs Ġthey Ġreally\n",
      "\t Tokenized LLAMA3:yeah Ġwell Ġi Ġwas Ġsurprised Ġat Ġthe Ġthe Ġway Ġthey Ġdrafted Ġlast Ġyear Ġthey Ġdidn 't Ġreally Ġdidn 't Ġgo Ġfor Ġthe Ġuh Ġbig Ġoffensive Ġl inem an Ġor Ġthe Ġdefensive Ġl inem an Ġthey 're Ġgoing Ġfor Ġthe Ġskilled Ġpositions Ġso Ġquarter backs Ġthey Ġreally\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I thought they should focus more on big linemen.\n",
      "\t Tokenized GPT2:I Ġthought Ġthey Ġshould Ġfocus Ġmore Ġon Ġbig Ġl inem en .\n",
      "\t Tokenized LLAMA3:I Ġthought Ġthey Ġshould Ġfocus Ġmore Ġon Ġbig Ġl inem en .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: If you have any questions regarding this report, please call me at (202) 512-4841.\n",
      "\t Tokenized GPT2:If Ġyou Ġhave Ġany Ġquestions Ġregarding Ġthis Ġreport , Ġplease Ġcall Ġme Ġat Ġ( 202 ) Ġ512 - 48 41 .\n",
      "\t Tokenized LLAMA3:If Ġyou Ġhave Ġany Ġquestions Ġregarding Ġthis Ġreport , Ġplease Ġcall Ġme Ġat Ġ( 202 ) Ġ 512 - 484 1 .\n",
      "\t Unique Tokens GPT2: {'Ġ512', '48', '41'}\n",
      "\t Unique Tokens LLAMA3: {'484', '1', '512', 'Ġ'}\n",
      "Text 2: My phone number is  (202) 512-4841.\n",
      "\t Tokenized GPT2:My Ġphone Ġnumber Ġis Ġ Ġ( 202 ) Ġ512 - 48 41 .\n",
      "\t Tokenized LLAMA3:My Ġphone Ġnumber Ġis Ġ Ġ( 202 ) Ġ 512 - 484 1 .\n",
      "\t Unique Tokens GPT2: {'Ġ512', '48', '41'}\n",
      "\t Unique Tokens LLAMA3: {'484', '512', '1'}\n",
      "==entailment==\n",
      "Text 1: Finish it, someone yelled.\n",
      "\t Tokenized GPT2:Finish Ġit , Ġsomeone Ġyelled .\n",
      "\t Tokenized LLAMA3:Fin ish Ġit , Ġsomeone Ġyelled .\n",
      "\t Unique Tokens GPT2: {'Finish'}\n",
      "\t Unique Tokens LLAMA3: {'Fin', 'ish'}\n",
      "Text 2: Someone yelled to finish it.\n",
      "\t Tokenized GPT2:Someone Ġyelled Ġto Ġfinish Ġit .\n",
      "\t Tokenized LLAMA3:Someone Ġyelled Ġto Ġfinish Ġit .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: do you really romance\n",
      "\t Tokenized GPT2:do Ġyou Ġreally Ġromance\n",
      "\t Tokenized LLAMA3:do Ġyou Ġreally Ġromance\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Do you really have an affair?\n",
      "\t Tokenized GPT2:Do Ġyou Ġreally Ġhave Ġan Ġaffair ?\n",
      "\t Tokenized LLAMA3:Do Ġyou Ġreally Ġhave Ġan Ġaffair ?\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: i can believe i can believe that\n",
      "\t Tokenized GPT2:i Ġcan Ġbelieve Ġi Ġcan Ġbelieve Ġthat\n",
      "\t Tokenized LLAMA3:i Ġcan Ġbelieve Ġi Ġcan Ġbelieve Ġthat\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: That's something that's believable.\n",
      "\t Tokenized GPT2:That 's Ġsomething Ġthat 's Ġbelievable .\n",
      "\t Tokenized LLAMA3:That 's Ġsomething Ġthat 's Ġbelievable .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Extremely limited exceptions to the authority are established in 31 U.S.C.\n",
      "\t Tokenized GPT2:Ext remely Ġlimited Ġexceptions Ġto Ġthe Ġauthority Ġare Ġestablished Ġin Ġ31 ĠU . S . C .\n",
      "\t Tokenized LLAMA3:Ext remely Ġlimited Ġexceptions Ġto Ġthe Ġauthority Ġare Ġestablished Ġin Ġ 31 ĠU .S .C .\n",
      "\t Unique Tokens GPT2: {'S', 'Ġ31', 'C'}\n",
      "\t Unique Tokens LLAMA3: {'.S', '.C', '31', 'Ġ'}\n",
      "Text 2: There are only a selected few exceptions.\n",
      "\t Tokenized GPT2:There Ġare Ġonly Ġa Ġselected Ġfew Ġexceptions .\n",
      "\t Tokenized LLAMA3:There Ġare Ġonly Ġa Ġselected Ġfew Ġexceptions .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: He seemed too self-assured.\n",
      "\t Tokenized GPT2:He Ġseemed Ġtoo Ġself - ass ured .\n",
      "\t Tokenized LLAMA3:He Ġseemed Ġtoo Ġself -ass ured .\n",
      "\t Unique Tokens GPT2: {'-', 'ass'}\n",
      "\t Unique Tokens LLAMA3: {'-ass'}\n",
      "Text 2: He is insecure.\n",
      "\t Tokenized GPT2:He Ġis Ġinsecure .\n",
      "\t Tokenized LLAMA3:He Ġis Ġinsecure .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: No one would ever think of sentiment in connection with you.\n",
      "\t Tokenized GPT2:No Ġone Ġwould Ġever Ġthink Ġof Ġsentiment Ġin Ġconnection Ġwith Ġyou .\n",
      "\t Tokenized LLAMA3:No Ġone Ġwould Ġever Ġthink Ġof Ġsentiment Ġin Ġconnection Ġwith Ġyou .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Sentiment when connecting with you is beyond everyone's expectations.\n",
      "\t Tokenized GPT2:Sent iment Ġwhen Ġconnecting Ġwith Ġyou Ġis Ġbeyond Ġeveryone 's Ġexpectations .\n",
      "\t Tokenized LLAMA3:S ent iment Ġwhen Ġconnecting Ġwith Ġyou Ġis Ġbeyond Ġeveryone 's Ġexpectations .\n",
      "\t Unique Tokens GPT2: {'Sent'}\n",
      "\t Unique Tokens LLAMA3: {'S', 'ent'}\n",
      "==contradiction==\n",
      "Text 1: It's conceivable that some of these allegations are true, and there's no harm in checking them out, as long as the decedent's family agrees to participate.\n",
      "\t Tokenized GPT2:It 's Ġconce ivable Ġthat Ġsome Ġof Ġthese Ġallegations Ġare Ġtrue , Ġand Ġthere 's Ġno Ġharm Ġin Ġchecking Ġthem Ġout , Ġas Ġlong Ġas Ġthe Ġde ced ent 's Ġfamily Ġagrees Ġto Ġparticipate .\n",
      "\t Tokenized LLAMA3:It 's Ġconce ivable Ġthat Ġsome Ġof Ġthese Ġallegations Ġare Ġtrue , Ġand Ġthere 's Ġno Ġharm Ġin Ġchecking Ġthem Ġout , Ġas Ġlong Ġas Ġthe Ġde ced ent 's Ġfamily Ġagrees Ġto Ġparticipate .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: None  of the allegations are true.\n",
      "\t Tokenized GPT2:None Ġ Ġof Ġthe Ġallegations Ġare Ġtrue .\n",
      "\t Tokenized LLAMA3:None Ġ Ġof Ġthe Ġallegations Ġare Ġtrue .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: and the wind started blowing and it was one of my earlier trips to be really out in the middle of\n",
      "\t Tokenized GPT2:and Ġthe Ġwind Ġstarted Ġblowing Ġand Ġit Ġwas Ġone Ġof Ġmy Ġearlier Ġtrips Ġto Ġbe Ġreally Ġout Ġin Ġthe Ġmiddle Ġof\n",
      "\t Tokenized LLAMA3:and Ġthe Ġwind Ġstarted Ġblowing Ġand Ġit Ġwas Ġone Ġof Ġmy Ġearlier Ġtrips Ġto Ġbe Ġreally Ġout Ġin Ġthe Ġmiddle Ġof\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I was glad that the wind was calm.\n",
      "\t Tokenized GPT2:I Ġwas Ġglad Ġthat Ġthe Ġwind Ġwas Ġcalm .\n",
      "\t Tokenized LLAMA3:I Ġwas Ġglad Ġthat Ġthe Ġwind Ġwas Ġcalm .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Where would he be today without American commercial know-how?\n",
      "\t Tokenized GPT2:Where Ġwould Ġhe Ġbe Ġtoday Ġwithout ĠAmerican Ġcommercial Ġknow - how ?\n",
      "\t Tokenized LLAMA3:Where Ġwould Ġhe Ġbe Ġtoday Ġwithout ĠAmerican Ġcommercial Ġknow -h ow ?\n",
      "\t Unique Tokens GPT2: {'-', 'how'}\n",
      "\t Unique Tokens LLAMA3: {'ow', '-h'}\n",
      "Text 2: Americans lack important commercial know-how.\n",
      "\t Tokenized GPT2:Americans Ġlack Ġimportant Ġcommercial Ġknow - how .\n",
      "\t Tokenized LLAMA3:Americ ans Ġlack Ġimportant Ġcommercial Ġknow -h ow .\n",
      "\t Unique Tokens GPT2: {'Americans', 'how', '-'}\n",
      "\t Unique Tokens LLAMA3: {'Americ', 'ans', 'ow', '-h'}\n",
      "==neutral==\n",
      "Text 1: Larger ski resorts are 90 minutes away.\n",
      "\t Tokenized GPT2:L ar ger Ġski Ġres orts Ġare Ġ90 Ġminutes Ġaway .\n",
      "\t Tokenized LLAMA3:L ar ger Ġski Ġres orts Ġare Ġ 90 Ġminutes Ġaway .\n",
      "\t Unique Tokens GPT2: {'Ġ90'}\n",
      "\t Unique Tokens LLAMA3: {'90', 'Ġ'}\n",
      "Text 2: The largest resort is actually 100 minutes away.\n",
      "\t Tokenized GPT2:The Ġlargest Ġresort Ġis Ġactually Ġ100 Ġminutes Ġaway .\n",
      "\t Tokenized LLAMA3:The Ġlargest Ġresort Ġis Ġactually Ġ 100 Ġminutes Ġaway .\n",
      "\t Unique Tokens GPT2: {'Ġ100'}\n",
      "\t Unique Tokens LLAMA3: {'100', 'Ġ'}\n",
      "==neutral==\n",
      "Text 1: 'These are human lives.\n",
      "\t Tokenized GPT2:' These Ġare Ġhuman Ġlives .\n",
      "\t Tokenized LLAMA3:'T he se Ġare Ġhuman Ġlives .\n",
      "\t Unique Tokens GPT2: {'These', \"'\"}\n",
      "\t Unique Tokens LLAMA3: {\"'T\", 'se', 'he'}\n",
      "Text 2: Human lives are at stake \n",
      "\t Tokenized GPT2:Human Ġlives Ġare Ġat Ġstake Ġ\n",
      "\t Tokenized LLAMA3:Human Ġlives Ġare Ġat Ġstake Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: so uh i hope you like your office\n",
      "\t Tokenized GPT2:so Ġuh Ġi Ġhope Ġyou Ġlike Ġyour Ġoffice\n",
      "\t Tokenized LLAMA3:so Ġuh Ġi Ġhope Ġyou Ġlike Ġyour Ġoffice\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I hope you like your new office.\n",
      "\t Tokenized GPT2:I Ġhope Ġyou Ġlike Ġyour Ġnew Ġoffice .\n",
      "\t Tokenized LLAMA3:I Ġhope Ġyou Ġlike Ġyour Ġnew Ġoffice .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "first_tok = gpt2_tok\n",
    "first_name = \"GPT2\"\n",
    "second_tok = llama3_tok\n",
    "second_name = \"LLAMA3\"\n",
    "for index, row in df_mnli_gpt2_correct_llama3_wrong.iterrows():\n",
    "    if row[\"predictions\"] == 0:\n",
    "        print(\"==entailment==\")\n",
    "    elif row[\"predictions\"] == 1:\n",
    "        print(\"==neutral==\")\n",
    "    elif row[\"predictions\"] == 2:\n",
    "        print(\"==contradiction==\")\n",
    "    print(f\"Text 1: {row['premise'][:300]}\")\n",
    "    print(f\"\\t Tokenized {first_name}:{' '.join(first_tok.tokenize(row['premise'][:300]))}\")\n",
    "    print(f\"\\t Tokenized {second_name}:{' '.join(second_tok.tokenize(row['premise'][:300]))}\")\n",
    "    print(f\"\\t Unique Tokens {first_name}: {set(first_tok.tokenize(row['premise'])) - set(second_tok.tokenize(row['premise']))}\")\n",
    "    print(f\"\\t Unique Tokens {second_name}: {set(second_tok.tokenize(row['premise'])) - set(first_tok.tokenize(row['premise']))}\")\n",
    "    print(f\"Text 2: {row['hypothesis'][:300]}\")   \n",
    "    print(f\"\\t Tokenized {first_name}:{' '.join(first_tok.tokenize(row['hypothesis'][:300]))}\")\n",
    "    print(f\"\\t Tokenized {second_name}:{' '.join(second_tok.tokenize(row['hypothesis'][:300]))}\")\n",
    "    print(f\"\\t Unique Tokens {first_name}: {set(first_tok.tokenize(row['hypothesis'])) - set(second_tok.tokenize(row['hypothesis']))}\")\n",
    "    print(f\"\\t Unique Tokens {second_name}: {set(second_tok.tokenize(row['hypothesis'])) - set(first_tok.tokenize(row['hypothesis']))}\")\n",
    "    i += 1\n",
    "    \n",
    "    if 20 > i > 50: break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T17:08:35.263847Z",
     "start_time": "2025-02-06T17:08:34.983967Z"
    }
   },
   "id": "b1f8e021331e341e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c5a98a66521e40c9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
