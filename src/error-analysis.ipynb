{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-06T15:35:17.262793Z",
     "start_time": "2025-02-06T15:35:17.259882Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "local_finder_addition = \"/Users/anna/sftp_mount/hpc_disk/02-awegmann/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "path_gpt2_PAN = os.path.join(local_finder_addition, \"TOKENIZER/output/VAR/train-mixed/base-BERT/mixed-gpt2-32000/749M/steps-45000/seed-42/42/PAN\")\n",
    "path_llama3_PAN = os.path.join(local_finder_addition, \"TOKENIZER/output/VAR/train-mixed/base-BERT/mixed-llama3-32000/749M/steps-45000/seed-42/42/PAN\")\n",
    "path_wsorg_PAN = os.path.join(local_finder_addition, \"TOKENIZER/output/VAR/train-mixed/base-BERT/mixed-wsorg-32000/749M/steps-45000/seed-42/42/PAN\")\n",
    "path_ws_PAN = os.path.join(local_finder_addition, \"TOKENIZER/output/VAR/train-mixed/base-BERT/mixed-ws-32000/749M/steps-45000/seed-42/42/PAN\")\n",
    "tokenizer_gpt2 = os.path.join(local_finder_addition, \"TOKENIZER/tokenizer/mixed-gpt2-32000\")\n",
    "tokenizer_llama3 = os.path.join(local_finder_addition, \"TOKENIZER/tokenizer/mixed-llama3-32000\")\n",
    "tokenizer_wsorg = os.path.join(local_finder_addition, \"TOKENIZER/tokenizer/mixed-wsorg-32000\")\n",
    "tokenizer_ws = os.path.join(local_finder_addition, \"TOKENIZER/tokenizer/mixed-ws-32000\")\n",
    "path_gpt2_webbook_PAN = os.path.join(local_finder_addition, \"TOKENIZER/output/VAR/base-BERT/mixed-gpt2-32000/749M/steps-45000/seed-42/42/PAN\")\n",
    "path_llama3_webbook_PAN = os.path.join(local_finder_addition, \"TOKENIZER/output/VAR/base-BERT/mixed-llama3-32000/749M/steps-45000/seed-42/42/PAN\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T16:34:38.220080Z",
     "start_time": "2025-02-06T16:34:38.211686Z"
    }
   },
   "id": "8ae404da341aa907"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-06 17:34:38,407 - styletokenizer.utility.custom_logger - DEBUG - Loading previously fitted tokenizer from local path: /Users/anna/sftp_mount/hpc_disk/02-awegmann/TOKENIZER/tokenizer/mixed-gpt2-32000/tokenizer.json\n",
      "2025-02-06 17:34:38,865 - styletokenizer.utility.custom_logger - DEBUG - Loading previously fitted tokenizer from local path: /Users/anna/sftp_mount/hpc_disk/02-awegmann/TOKENIZER/tokenizer/mixed-llama3-32000/tokenizer.json\n",
      "2025-02-06 17:34:39,291 - styletokenizer.utility.custom_logger - DEBUG - Loading previously fitted tokenizer from local path: /Users/anna/sftp_mount/hpc_disk/02-awegmann/TOKENIZER/tokenizer/mixed-wsorg-32000/tokenizer.json\n",
      "2025-02-06 17:34:39,754 - styletokenizer.utility.custom_logger - DEBUG - Loading previously fitted tokenizer from local path: /Users/anna/sftp_mount/hpc_disk/02-awegmann/TOKENIZER/tokenizer/mixed-ws-32000/tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "# laod tokeinzer\n",
    "from train_bert import load_tokenizer\n",
    "gpt2_tok, _ = load_tokenizer(tokenizer_gpt2)\n",
    "llama3_tok, _ = load_tokenizer(tokenizer_llama3)\n",
    "wsorg_tok, _ = load_tokenizer(tokenizer_wsorg)\n",
    "ws_tok, _ = load_tokenizer(tokenizer_ws)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T16:34:40.313028Z",
     "start_time": "2025-02-06T16:34:38.388857Z"
    }
   },
   "id": "fc4951e844e8f379"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_gpt2 = pd.read_csv(os.path.join(path_gpt2_PAN, \"2025-01-21_eval_dataset.tsv\"), sep=\"\\t\")\n",
    "df_wb_gpt2 = pd.read_csv(os.path.join(path_gpt2_webbook_PAN, \"2025-01-10_eval_dataset.tsv\"), sep=\"\\t\")\n",
    "df_llama3 = pd.read_csv(os.path.join(path_llama3_PAN, \"2025-01-22_eval_dataset.tsv\"), sep=\"\\t\")\n",
    "df_wb_llama3 = pd.read_csv(os.path.join(path_llama3_webbook_PAN, \"2025-01-10_eval_dataset.tsv\"), sep=\"\\t\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T16:14:19.400578Z",
     "start_time": "2025-02-06T16:13:57.386651Z"
    }
   },
   "id": "b4033b7b5d4980b4"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "df_wsorg = pd.read_csv(os.path.join(path_wsorg_PAN, \"2025-01-25_eval_dataset.tsv\"), sep=\"\\t\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T16:23:49.180750Z",
     "start_time": "2025-02-06T16:23:41.893960Z"
    }
   },
   "id": "77e428ce4f9ca9df"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "df_ws = pd.read_csv(os.path.join(path_ws_PAN, \"2025-01-26_eval_dataset.tsv\"), sep=\"\\t\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T16:35:24.801213Z",
     "start_time": "2025-02-06T16:35:18.136244Z"
    }
   },
   "id": "27a22a42316897a0"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "# find cases where llama3 predicted correctly, but gpt2 did not\n",
    "df_llama3_correct_gpt2_wrong = df_llama3[(df_llama3[\"label\"] == df_llama3[\"predictions\"]) & (df_gpt2[\"label\"] != df_gpt2[\"predictions\"])]\n",
    "df_wb_gpt2_correct_llama3_wrong = df_wb_gpt2[(df_wb_gpt2[\"label\"] == df_wb_gpt2[\"predictions\"]) & (df_llama3[\"label\"] != df_llama3[\"predictions\"])]\n",
    "df_ws_correct_gpt2_wrong = df_ws[(df_ws[\"label\"] == df_ws[\"predictions\"]) & (df_gpt2[\"label\"] != df_gpt2[\"predictions\"])]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T16:35:30.327999Z",
     "start_time": "2025-02-06T16:35:30.319364Z"
    }
   },
   "id": "ce34de363b917860"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "531\n",
      "434\n",
      "447\n"
     ]
    }
   ],
   "source": [
    "print(len(df_llama3_correct_gpt2_wrong))\n",
    "print(len(df_wb_gpt2_correct_llama3_wrong))\n",
    "print(len(df_ws_correct_gpt2_wrong))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T16:35:33.265965Z",
     "start_time": "2025-02-06T16:35:33.263046Z"
    }
   },
   "id": "2d3b9c1dd8bf229d"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==Different Author==\n",
      "Text 1: Yeah but use your brain for a second and look into what created the circumstances for those genocides or mass executions to even be possible. Killing 15,000 people is a big ask, who is going to dish it out? If you are using soldiers and bullets you have the nazi problem: How do you handle the moral \n",
      "\t Tokenized GPT2:Yeah Ġbut Ġuse Ġyour Ġbrain Ġfor Ġa Ġsecond Ġand Ġlook Ġinto Ġwhat Ġcreated Ġthe Ġcircumstances Ġfor Ġthose Ġgen oc ides Ġor Ġmass Ġexecut ions Ġto Ġeven Ġbe Ġpossible . ĠK illing Ġ15 , 000 Ġpeople Ġis Ġa Ġbig Ġask , Ġwho Ġis Ġgoing Ġto Ġdish Ġit Ġout ? ĠIf Ġyou Ġare Ġusing Ġsoldiers Ġand Ġbullets Ġyou Ġhave Ġthe Ġn azi Ġproblem : ĠHow Ġdo Ġyou Ġhandle Ġthe Ġmoral Ġ\n",
      "\t Tokenized ws:Yeah Ġbut Ġuse Ġyour Ġbrain Ġfor Ġa Ġsecond Ġand Ġlook Ġinto Ġwhat Ġcreated Ġthe Ġcircumstances Ġfor Ġthose Ġgen oc ides Ġor Ġmass Ġexecut ions Ġto Ġeven Ġbe Ġpossible. ĠK illing Ġ15 ,000 Ġpeople Ġis Ġa Ġbig Ġask, Ġwho Ġis Ġgoing Ġto Ġdish Ġit Ġout? ĠIf Ġyou Ġare Ġusing Ġsoldiers Ġand Ġbullets Ġyou Ġhave Ġthe Ġn az i Ġproblem: ĠHow Ġdo Ġyou Ġhandle Ġthe Ġmoral Ġ\n",
      "\t Unique Tokens GPT2: {':', 'Ġprisoners', '.', 'Ġask', ',', 'Ġpossible', '?', 'Ġcircumstance', 'azi', 'Ġout', 'Ġproblem'}\n",
      "\t Unique Tokens ws: {'Ġpossible.', 'Ġproblem:', ',000', 'Ġcircumst', 'ance.', 'Ġout?', 'Ġask,', 'i', 'Ġprison', 'ers.'}\n",
      "Text 2: Yeah like, the nazis were fighting almost the entire world yet still had the time and ressourcen to commit the holocaust and other crimes. As awful as it sounds but 15000 people is probably on the \"easier\" end of mass murder commited by regimes....\n",
      "\t Tokenized GPT2:Yeah Ġlike , Ġthe Ġn az is Ġwere Ġfighting Ġalmost Ġthe Ġentire Ġworld Ġyet Ġstill Ġhad Ġthe Ġtime Ġand Ġres s our c en Ġto Ġcommit Ġthe Ġhol ocaust Ġand Ġother Ġcrimes . ĠAs Ġawful Ġas Ġit Ġsounds Ġbut Ġ15 000 Ġpeople Ġis Ġprobably Ġon Ġthe Ġ\" eas ier \" Ġend Ġof Ġmass Ġmurder Ġcomm ited Ġby Ġreg imes ....\n",
      "\t Tokenized ws:Yeah Ġlike, Ġthe Ġn az is Ġwere Ġfighting Ġalmost Ġthe Ġentire Ġworld Ġyet Ġstill Ġhad Ġthe Ġtime Ġand Ġres s ourc en Ġto Ġcommit Ġthe Ġhol oca ust Ġand Ġother Ġcrim es. ĠAs Ġawful Ġas Ġit Ġsounds Ġbut Ġ15 000 Ġpeople Ġis Ġprobably Ġon Ġthe Ġ\"e as ier \" Ġend Ġof Ġmass Ġmurder Ġcomm ited Ġby Ġreg imes ....\n",
      "\t Unique Tokens GPT2: {'Ġ\"', 'our', '.', ',', 'ocaust', 'c', 'Ġcrimes', 'Ġlike', 'eas'}\n",
      "\t Unique Tokens ws: {'ust', 'oca', 'Ġlike,', 'Ġcrim', 'es.', 'ourc', 'as', 'Ġ\"e'}\n",
      "==Same Author==\n",
      "Text 1: I'm a German and my strong gut instincts tell me that these are only stalling tactic and Scholz ultimately will say 'No'. Maybe Scholz tries to stall the process only until our new secretary of defense has taken his oath and afterwards Scholz will blame him for saying 'No'. Or maybe he thinks the wa\n",
      "\t Tokenized GPT2:I 'm Ġa ĠGerman Ġand Ġmy Ġstrong Ġgut Ġinstincts Ġtell Ġme Ġthat Ġthese Ġare Ġonly Ġst alling Ġtactic Ġand ĠSch ol z Ġultimately Ġwill Ġsay Ġ' No '. ĠMaybe ĠSch ol z Ġtries Ġto Ġstall Ġthe Ġprocess Ġonly Ġuntil Ġour Ġnew Ġsecretary Ġof Ġdefense Ġhas Ġtaken Ġhis Ġoath Ġand Ġafterwards ĠSch ol z Ġwill Ġblame Ġhim Ġfor Ġsaying Ġ' No '. ĠOr Ġmaybe Ġhe Ġthinks Ġthe Ġwa\n",
      "\t Tokenized ws:I'm Ġa ĠGerman Ġand Ġmy Ġstrong Ġgut Ġinstinct s Ġtell Ġme Ġthat Ġthese Ġare Ġonly Ġst alling Ġtact ic Ġand ĠSch ol z Ġultimately Ġwill Ġsay Ġ' No '. ĠMaybe ĠSch ol z Ġtries Ġto Ġstall Ġthe Ġprocess Ġonly Ġuntil Ġour Ġnew Ġsecretary Ġof Ġdefense Ġhas Ġtaken Ġhis Ġo ath Ġand Ġafterwards ĠSch ol z Ġwill Ġblame Ġhim Ġfor Ġsaying Ġ' No '. ĠOr Ġmaybe Ġhe Ġthinks Ġthe Ġwa\n",
      "\t Unique Tokens GPT2: {'Ġoath', '.', 'Ġinstincts', \"'t\", 'Ġknow', 'Ġtactic', \"'m\", 'Ġdon', 'I'}\n",
      "\t Unique Tokens ws: {\"I'm\", 'Ġo', 'Ġknow.', \"Ġdon't\", 'ic', 'Ġinstinct', 's', 'ath', 'Ġtact'}\n",
      "Text 2: Yeah, they may be a good alternative for Ukraine concerning the tank-question, but they surely are not a good option for Germany.\n",
      "\t Tokenized GPT2:Yeah , Ġthey Ġmay Ġbe Ġa Ġgood Ġalternative Ġfor ĠUkraine Ġconcerning Ġthe Ġtank - question , Ġbut Ġthey Ġsurely Ġare Ġnot Ġa Ġgood Ġoption Ġfor ĠGermany .\n",
      "\t Tokenized ws:Yeah, Ġthey Ġmay Ġbe Ġa Ġgood Ġalternative Ġfor ĠUkraine Ġconcerning Ġthe Ġtank - quest ion, Ġbut Ġthey Ġsurely Ġare Ġnot Ġa Ġgood Ġoption Ġfor ĠGerman y.\n",
      "\t Unique Tokens GPT2: {'ĠGermany', '.', ',', 'question', 'Yeah'}\n",
      "\t Unique Tokens ws: {'ion,', 'y.', 'Yeah,', 'quest', 'ĠGerman'}\n",
      "==Different Author==\n",
      "Text 1: I take it most of the commenters did not read the article it talks about exposing minors to explicit content not just for exposing someone to trans people.\n",
      "\t Tokenized GPT2:I Ġtake Ġit Ġmost Ġof Ġthe Ġcomment ers Ġdid Ġnot Ġread Ġthe Ġarticle Ġit Ġtalks Ġabout Ġexposing Ġmin ors Ġto Ġexplicit Ġcontent Ġnot Ġjust Ġfor Ġexposing Ġsomeone Ġto Ġtrans Ġpeople .\n",
      "\t Tokenized ws:I Ġtake Ġit Ġmost Ġof Ġthe Ġcomment ers Ġdid Ġnot Ġread Ġthe Ġarticle Ġit Ġtalks Ġabout Ġexposing Ġmin ors Ġto Ġexplicit Ġcontent Ġnot Ġjust Ġfor Ġexposing Ġsomeone Ġto Ġtrans Ġpeople.\n",
      "\t Unique Tokens GPT2: {'.', 'Ġpeople'}\n",
      "\t Unique Tokens ws: {'Ġpeople.'}\n",
      "Text 2: That is to say, the bill specifically defines \"transgender exposure [...] to a minor\" as inherently indecent and sexually explicit. So yes, it does make exposing someone to trans people illegal.\n",
      "\t Tokenized GPT2:That Ġis Ġto Ġsay , Ġthe Ġbill Ġspecifically Ġdefines Ġ\" trans gender Ġexposure Ġ[...] Ġto Ġa Ġminor \" Ġas Ġinherently Ġinde cent Ġand Ġsexually Ġexplicit . ĠSo Ġyes , Ġit Ġdoes Ġmake Ġexposing Ġsomeone Ġto Ġtrans Ġpeople Ġillegal .\n",
      "\t Tokenized ws:That Ġis Ġto Ġsay, Ġthe Ġbill Ġspecifically Ġdefines Ġ\" trans gender Ġexposure Ġ[ ...] Ġto Ġa Ġminor \" Ġas Ġinherently Ġinde cent Ġand Ġsexually Ġexpl ic it. ĠSo Ġyes, Ġit Ġdoes Ġmake Ġexposing Ġsomeone Ġto Ġtrans Ġpeople Ġilleg al.\n",
      "\t Unique Tokens GPT2: {'Ġ[...]', 'Ġsay', '.', ',', 'Ġexplicit', 'Ġillegal', 'Ġyes'}\n",
      "\t Unique Tokens ws: {'...]', 'Ġilleg', 'Ġ[', 'ic', 'Ġexpl', 'al.', 'Ġyes,', 'it.', 'Ġsay,'}\n",
      "==Same Author==\n",
      "Text 1: The only ones they have thousands of are S300s, which have limited range (<120km) and are far older and less reliable.\n",
      "\t Tokenized GPT2:The Ġonly Ġones Ġthey Ġhave Ġthousands Ġof Ġare ĠS 300 s , Ġwhich Ġhave Ġlimited Ġrange Ġ(< 120 km ) Ġand Ġare Ġfar Ġolder Ġand Ġless Ġreliable .\n",
      "\t Tokenized ws:The Ġonly Ġones Ġthey Ġhave Ġthousands Ġof Ġare ĠS 300 s, Ġwhich Ġhave Ġlimited Ġrange Ġ(< 120 k m) Ġand Ġare Ġfar Ġolder Ġand Ġless Ġreli able.\n",
      "\t Unique Tokens GPT2: {'km', '.', ',', 's', 'Ġreliable', ')'}\n",
      "\t Unique Tokens ws: {'m)', 's,', 'Ġreli', 'able.', 'k'}\n",
      "Text 2: So the expectation is that Russia will continue to use cruise missiles and Iranian drones for long range strikes, and they are very limited in their stocks of those while their hit rates are getting worse and worse.\n",
      "\t Tokenized GPT2:So Ġthe Ġexpectation Ġis Ġthat ĠRussia Ġwill Ġcontinue Ġto Ġuse Ġcruise Ġmissiles Ġand ĠIranian Ġdrones Ġfor Ġlong Ġrange Ġstrikes , Ġand Ġthey Ġare Ġvery Ġlimited Ġin Ġtheir Ġstocks Ġof Ġthose Ġwhile Ġtheir Ġhit Ġrates Ġare Ġgetting Ġworse Ġand Ġworse .\n",
      "\t Tokenized ws:So Ġthe Ġexpectation Ġis Ġthat ĠRussia Ġwill Ġcontinue Ġto Ġuse Ġcruise Ġmiss iles Ġand ĠIranian Ġdr ones Ġfor Ġlong Ġrange Ġstri k es, Ġand Ġthey Ġare Ġvery Ġlimited Ġin Ġtheir Ġstocks Ġof Ġthose Ġwhile Ġtheir Ġhit Ġrates Ġare Ġgetting Ġworse Ġand Ġworse.\n",
      "\t Unique Tokens GPT2: {'Ġdrones', '.', 'Ġstrikes', ',', 'Ġmissiles'}\n",
      "\t Unique Tokens ws: {'es,', 'Ġstri', 'ones', 'Ġworse.', 'iles', 'Ġmiss', 'Ġdr', 'k'}\n",
      "==Same Author==\n",
      "Text 1: It's also conveniently being setup so 85% of the coal will be exported , this has no logic in being done as it won't be used for its primary purpose and is just for money.\n",
      "\t Tokenized GPT2:It 's Ġalso Ġconvenient ly Ġbeing Ġsetup Ġso Ġ85 % Ġof Ġthe Ġcoal Ġwill Ġbe Ġexported Ġ, Ġthis Ġhas Ġno Ġlogic Ġin Ġbeing Ġdone Ġas Ġit Ġwon 't Ġbe Ġused Ġfor Ġits Ġprimary Ġpurpose Ġand Ġis Ġjust Ġfor Ġmoney .\n",
      "\t Tokenized ws:It's Ġalso Ġconvenient ly Ġbeing Ġsetup Ġso Ġ8 5% Ġof Ġthe Ġcoal Ġwill Ġbe Ġexported Ġ, Ġthis Ġhas Ġno Ġlogic Ġin Ġbeing Ġdone Ġas Ġit Ġwon't Ġbe Ġused Ġfor Ġits Ġprimary Ġpurpose Ġand Ġis Ġjust Ġfor Ġmoney.\n",
      "\t Unique Tokens GPT2: {'Ġmoney', '%', '.', 'Ġ85', \"'s\", \"'t\", 'It', 'Ġwon'}\n",
      "\t Unique Tokens ws: {\"It's\", 'Ġmoney.', \"Ġwon't\", '5%', 'Ġ8'}\n",
      "Text 2: This coal mine is being said to use for steel in the UK but UK Steel has said no it won't be used as they are moving to green alternatives and actually 85% of the coal will be exported which is purely for money and not a requirement to stop using russian coal.\n",
      "\t Tokenized GPT2:This Ġcoal Ġmine Ġis Ġbeing Ġsaid Ġto Ġuse Ġfor Ġsteel Ġin Ġthe ĠUK Ġbut ĠUK ĠSteel Ġhas Ġsaid Ġno Ġit Ġwon 't Ġbe Ġused Ġas Ġthey Ġare Ġmoving Ġto Ġgreen Ġalternatives Ġand Ġactually Ġ85 % Ġof Ġthe Ġcoal Ġwill Ġbe Ġexported Ġwhich Ġis Ġpurely Ġfor Ġmoney Ġand Ġnot Ġa Ġrequirement Ġto Ġstop Ġusing Ġr ussian Ġcoal .\n",
      "\t Tokenized ws:This Ġcoal Ġmine Ġis Ġbeing Ġsaid Ġto Ġuse Ġfor Ġsteel Ġin Ġthe ĠUK Ġbut ĠUK ĠSteel Ġhas Ġsaid Ġno Ġit Ġwon't Ġbe Ġused Ġas Ġthey Ġare Ġmoving Ġto Ġgreen Ġalternatives Ġand Ġactually Ġ8 5% Ġof Ġthe Ġcoal Ġwill Ġbe Ġexported Ġwhich Ġis Ġpurely Ġfor Ġmoney Ġand Ġnot Ġa Ġrequirement Ġto Ġstop Ġusing Ġr ussian Ġco al.\n",
      "\t Unique Tokens GPT2: {'%', '.', 'Ġ85', \"'t\", 'Ġwon'}\n",
      "\t Unique Tokens ws: {\"Ġwon't\", 'al.', 'Ġco', '5%', 'Ġ8'}\n",
      "==Same Author==\n",
      "Text 1: Thing is, there's plenty of Russians that would answer the same. You don't have to sacrifice yourself and go to prison, you just need to be a decent person to understand what's right and what's wrong. And it doesn't make you a good Russian, it's just make you a compassionate human being. None of us \n",
      "\t Tokenized GPT2:T hing Ġis , Ġthere 's Ġplenty Ġof ĠRussians Ġthat Ġwould Ġanswer Ġthe Ġsame . ĠYou Ġdon 't Ġhave Ġto Ġsacrifice Ġyourself Ġand Ġgo Ġto Ġprison , Ġyou Ġjust Ġneed Ġto Ġbe Ġa Ġdecent Ġperson Ġto Ġunderstand Ġwhat 's Ġright Ġand Ġwhat 's Ġwrong . ĠAnd Ġit Ġdoesn 't Ġmake Ġyou Ġa Ġgood ĠRussian , Ġit 's Ġjust Ġmake Ġyou Ġa Ġcompassion ate Ġhuman Ġbeing . ĠNone Ġof Ġus Ġ\n",
      "\t Tokenized ws:T hing Ġis, Ġthere's Ġplenty Ġof ĠRuss ians Ġthat Ġwould Ġanswer Ġthe Ġsame. ĠYou Ġdon't Ġhave Ġto Ġsacrifice Ġyourself Ġand Ġgo Ġto Ġpr ison, Ġyou Ġjust Ġneed Ġto Ġbe Ġa Ġdecent Ġperson Ġto Ġunderstand Ġwhat's Ġright Ġand Ġwhat's Ġwrong. ĠAnd Ġit Ġdoesn't Ġmake Ġyou Ġa Ġgood ĠRussian , Ġit's Ġjust Ġmake Ġyou Ġa Ġcompassion ate Ġhuman Ġbeing. ĠNone Ġof Ġus Ġ\n",
      "\t Unique Tokens GPT2: {'Ġthere', 'Ġpower', 'Ġbeing', 'Ġwhat', \"'s\", 'Ġwrong', \"'t\", 'ĠRussians', 'ĠPutin', 'Ġdon', 'Ġdoesn', 'Ġwas', 'Ġprison', 'Ġsame'}\n",
      "\t Unique Tokens ws: {'Ġis,', 'ĠPut', \"Ġit's\", \"Ġdon't\", \"Ġwhat's\", 'Ġsame.', 'ison,', \"in's\", 'ĠRuss', \"Ġdoesn't\", 'ians', 'Ġwas,', 'Ġwrong.', 'Ġpr', 'Ġpower.', \"Ġthere's\", 'Ġbeing.'}\n",
      "Text 2: To be clear and honest, I'm half Russian and half Tatar person and I am strongly against the war. I'm not interested in being considered good. But I want to understand your logic.\n",
      "\t Tokenized GPT2:To Ġbe Ġclear Ġand Ġhonest , ĠI 'm Ġhalf ĠRussian Ġand Ġhalf ĠT atar Ġperson Ġand ĠI Ġam Ġstrongly Ġagainst Ġthe Ġwar . ĠI 'm Ġnot Ġinterested Ġin Ġbeing Ġconsidered Ġgood . ĠBut ĠI Ġwant Ġto Ġunderstand Ġyour Ġlogic .\n",
      "\t Tokenized ws:To Ġbe Ġclear Ġand Ġhonest, ĠI'm Ġhalf ĠRussian Ġand Ġhalf ĠT atar Ġperson Ġand ĠI Ġam Ġstrongly Ġagainst Ġthe Ġwar. ĠI'm Ġnot Ġinterested Ġin Ġbeing Ġconsidered Ġgood. ĠBut ĠI Ġwant Ġto Ġunderstand Ġyour Ġlog ic.\n",
      "\t Unique Tokens GPT2: {'Ġwar', 'Ġhonest', '.', ',', 'Ġgood', \"'m\", 'Ġlogic'}\n",
      "\t Unique Tokens ws: {'Ġhonest,', 'ic.', \"ĠI'm\", 'Ġwar.', 'Ġlog', 'Ġgood.'}\n",
      "==Different Author==\n",
      "Text 1: Possible. The Guardian article at the top of the subreddit right now says the pentagon was aware of 3 during the Trump administration, and a 4th one early in the Biden administration. This is at least the 5th detected balloon.\n",
      "\t Tokenized GPT2:Poss ible . ĠThe ĠGuardian Ġarticle Ġat Ġthe Ġtop Ġof Ġthe Ġsubreddit Ġright Ġnow Ġsays Ġthe Ġpent agon Ġwas Ġaware Ġof Ġ3 Ġduring Ġthe ĠTrump Ġadministration , Ġand Ġa Ġ4 th Ġone Ġearly Ġin Ġthe ĠBiden Ġadministration . ĠThis Ġis Ġat Ġleast Ġthe Ġ5 th Ġdetected Ġballoon .\n",
      "\t Tokenized ws:P oss ible. ĠThe ĠGuardian Ġarticle Ġat Ġthe Ġtop Ġof Ġthe Ġsubreddit Ġright Ġnow Ġsays Ġthe Ġpent agon Ġwas Ġaware Ġof Ġ3 Ġduring Ġthe ĠTrump Ġadministr ation, Ġand Ġa Ġ4th Ġone Ġearly Ġin Ġthe ĠBiden Ġadministr ation. ĠThis Ġis Ġat Ġleast Ġthe Ġ5th Ġdetected Ġballoon .\n",
      "\t Unique Tokens GPT2: {'Ġ5', ',', 'Poss', 'ible', 'Ġ4', 'th', 'Ġadministration'}\n",
      "\t Unique Tokens ws: {'Ġ4th', 'ation.', 'P', 'Ġadministr', 'Ġ5th', 'ation,', 'oss', 'ible.'}\n",
      "Text 2: A balloon isn't that easy to detect. The radar return won't be all that strong and a slow target might be missed if Moving Target Indication (MTI) processing is used to avoid detecting birds or ground clutter. There is also the problem of distinguishing a threat from legitimate weather balloons.\n",
      "\t Tokenized GPT2:A Ġballoon Ġisn 't Ġthat Ġeasy Ġto Ġdetect . ĠThe Ġradar Ġreturn Ġwon 't Ġbe Ġall Ġthat Ġstrong Ġand Ġa Ġslow Ġtarget Ġmight Ġbe Ġmissed Ġif ĠMoving ĠTarget ĠInd ication Ġ( MT I ) Ġprocessing Ġis Ġused Ġto Ġavoid Ġdetecting Ġbirds Ġor Ġground Ġcl utter . ĠThere Ġis Ġalso Ġthe Ġproblem Ġof Ġdistingu ishing Ġa Ġthreat Ġfrom Ġlegitimate Ġweather Ġball oons .\n",
      "\t Tokenized ws:A Ġballoon Ġisn't Ġthat Ġeasy Ġto Ġdetect . ĠThe Ġradar Ġreturn Ġwon't Ġbe Ġall Ġthat Ġstrong Ġand Ġa Ġslow Ġtarget Ġmight Ġbe Ġmissed Ġif ĠM oving ĠTarget ĠInd ication Ġ(M T I) Ġprocessing Ġis Ġused Ġto Ġavoid Ġdetecting Ġbirds Ġor Ġground Ġclut ter. ĠThere Ġis Ġalso Ġthe Ġproblem Ġof Ġdistingu ishing Ġa Ġthreat Ġfrom Ġlegitimate Ġweather Ġballoon s.\n",
      "\t Unique Tokens GPT2: {'ĠMoving', 'oons', 'utter', 'Ġball', 'Ġcl', 'Ġ(', \"'t\", 'Ġwon', 'MT', ')', 'I', 'Ġisn'}\n",
      "\t Unique Tokens ws: {'ter.', 'Ġclut', 'Ġ(M', 's.', 'I)', \"Ġwon't\", \"Ġisn't\", 'T', 'ĠM', 'oving'}\n",
      "==Different Author==\n",
      "Text 1: Part of the increasing global population is due to increased life expectancy, not just birth rates, and the increase due to life expectancy doesn't compound in the same way.\n",
      "\t Tokenized GPT2:Part Ġof Ġthe Ġincreasing Ġglobal Ġpopulation Ġis Ġdue Ġto Ġincreased Ġlife Ġexpect ancy , Ġnot Ġjust Ġbirth Ġrates , Ġand Ġthe Ġincrease Ġdue Ġto Ġlife Ġexpect ancy Ġdoesn 't Ġcompound Ġin Ġthe Ġsame Ġway .\n",
      "\t Tokenized ws:Part Ġof Ġthe Ġincreasing Ġglobal Ġpopulation Ġis Ġdue Ġto Ġincreased Ġlife Ġexpect ancy, Ġnot Ġjust Ġbirth Ġrat es, Ġand Ġthe Ġincrease Ġdue Ġto Ġlife Ġexpect ancy Ġdoesn't Ġcompound Ġin Ġthe Ġsame Ġway.\n",
      "\t Unique Tokens GPT2: {'Ġway', '.', ',', \"'t\", 'Ġdoesn', 'Ġrates'}\n",
      "\t Unique Tokens ws: {'es,', 'Ġway.', 'ancy,', \"Ġdoesn't\", 'Ġrat'}\n",
      "Text 2: I don't know if it is a positive or not however you look at it but currently birth rates are in decline in many countries with the median average age increasing.\n",
      "\t Tokenized GPT2:I Ġdon 't Ġknow Ġif Ġit Ġis Ġa Ġpositive Ġor Ġnot Ġhowever Ġyou Ġlook Ġat Ġit Ġbut Ġcurrently Ġbirth Ġrates Ġare Ġin Ġdecline Ġin Ġmany Ġcountries Ġwith Ġthe Ġmedian Ġaverage Ġage Ġincreasing .\n",
      "\t Tokenized ws:I Ġdon't Ġknow Ġif Ġit Ġis Ġa Ġpositive Ġor Ġnot Ġhowever Ġyou Ġlook Ġat Ġit Ġbut Ġcurrently Ġbirth Ġrates Ġare Ġin Ġdecline Ġin Ġmany Ġcountries Ġwith Ġthe Ġmedian Ġaverage Ġage Ġincre as ing.\n",
      "\t Unique Tokens GPT2: {\"'t\", '.', 'Ġincreasing', 'Ġdon'}\n",
      "\t Unique Tokens ws: {'as', \"Ġdon't\", 'ing.', 'Ġincre'}\n",
      "==Different Author==\n",
      "Text 1: “Officers” should be a category on the daily and total losses pictographs a couple sources publish. Right next to “helicopters” and “special equipment”.\n",
      "\t Tokenized GPT2:âĢľ Offic ers âĢĿ Ġshould Ġbe Ġa Ġcategory Ġon Ġthe Ġdaily Ġand Ġtotal Ġlosses Ġpic to graph s Ġa Ġcouple Ġsources Ġpublish . ĠRight Ġnext Ġto ĠâĢľ he lic op ters âĢĿ Ġand ĠâĢľ special Ġequipment âĢĿ.\n",
      "\t Tokenized ws:âĢľ O ffic ers âĢĿ Ġshould Ġbe Ġa Ġcategory Ġon Ġthe Ġdaily Ġand Ġtotal Ġlosses Ġpic to graph s Ġa Ġcouple Ġsources Ġpublish . ĠRight Ġnext Ġto ĠâĢľ he lic op ters âĢĿ Ġand ĠâĢľs pecial Ġequipment âĢĿ.\n",
      "\t Unique Tokens GPT2: {'Offic', 'special'}\n",
      "\t Unique Tokens ws: {'ĠâĢľs', 'pecial', 'ffic', 'O'}\n",
      "Text 2: A small squad with javelins and drones are able to have an outsized influence on a modern battlefield.\n",
      "\t Tokenized GPT2:A Ġsmall Ġsquad Ġwith Ġj avel ins Ġand Ġdrones Ġare Ġable Ġto Ġhave Ġan Ġouts ized Ġinfluence Ġon Ġa Ġmodern Ġbattlefield .\n",
      "\t Tokenized ws:A Ġsmall Ġsquad Ġwith Ġj avel ins Ġand Ġdr ones Ġare Ġable Ġto Ġhave Ġan Ġouts ized Ġinfluence Ġon Ġa Ġmodern Ġbattle field .\n",
      "\t Unique Tokens GPT2: {'Ġdrones', 'Ġbattlefield'}\n",
      "\t Unique Tokens ws: {'Ġbattle', 'Ġdr', 'field', 'ones'}\n",
      "==Same Author==\n",
      "Text 1: The original post is absolutely true. Putin was a joke until he wasn’t. Sure we here in the US can laugh and call him a joke. I bet millions of Ukrainians don’t share that same sentiment.\n",
      "\t Tokenized GPT2:The Ġoriginal Ġpost Ġis Ġabsolutely Ġtrue . ĠPutin Ġwas Ġa Ġjoke Ġuntil Ġhe Ġwasn âĢĻ t . ĠSure Ġwe Ġhere Ġin Ġthe ĠUS Ġcan Ġlaugh Ġand Ġcall Ġhim Ġa Ġjoke . ĠI Ġbet Ġmillions Ġof ĠUkrain ians Ġdon âĢĻ t Ġshare Ġthat Ġsame Ġsentiment .\n",
      "\t Tokenized ws:The Ġoriginal Ġpost Ġis Ġabsolutely Ġtrue. ĠPutin Ġwas Ġa Ġjoke Ġuntil Ġhe ĠwasnâĢĻt . ĠSure Ġwe Ġhere Ġin Ġthe ĠUS Ġcan Ġlaugh Ġand Ġcall Ġhim Ġa Ġjoke. ĠI Ġbet Ġmillions Ġof ĠUkrain ians ĠdonâĢĻt Ġshare Ġthat Ġsame Ġsentiment .\n",
      "\t Unique Tokens GPT2: {'âĢĻ', 't', 'Ġwasn', 'Ġtrue', 'Ġdon'}\n",
      "\t Unique Tokens ws: {'ĠdonâĢĻt', 'ĠwasnâĢĻt', 'Ġtrue.', 'Ġjoke.'}\n",
      "Text 2: You’re missing the point. N Korea could still level S Korea if they wanted to. Sure, that’s about all they could do before we wiped them from the face of the earth. Nevertheless, they could still kill tens of thousands or even hundreds of thousands of S Koreans if they choose to.\n",
      "\t Tokenized GPT2:You âĢĻ re Ġmissing Ġthe Ġpoint . ĠN ĠKorea Ġcould Ġstill Ġlevel ĠS ĠKorea Ġif Ġthey Ġwanted Ġto . ĠSure , Ġthat âĢĻ s Ġabout Ġall Ġthey Ġcould Ġdo Ġbefore Ġwe Ġwiped Ġthem Ġfrom Ġthe Ġface Ġof Ġthe Ġearth . ĠNevertheless , Ġthey Ġcould Ġstill Ġkill Ġtens Ġof Ġthousands Ġor Ġeven Ġhundreds Ġof Ġthousands Ġof ĠS ĠKore ans Ġif Ġthey Ġchoose Ġto .\n",
      "\t Tokenized ws:You âĢĻre Ġmissing Ġthe Ġpoint. ĠN ĠKorea Ġcould Ġstill Ġlevel ĠS ĠKorea Ġif Ġthey Ġwanted Ġto. ĠSure, ĠthatâĢĻs Ġabout Ġall Ġthey Ġcould Ġdo Ġbefore Ġwe Ġwiped Ġthem Ġfrom Ġthe Ġface Ġof Ġthe Ġearth. ĠNevertheless, Ġthey Ġcould Ġstill Ġkill Ġtens Ġof Ġthousands Ġor Ġeven Ġhundreds Ġof Ġthousands Ġof ĠS ĠKore ans Ġif Ġthey Ġchoose Ġto.\n",
      "\t Unique Tokens GPT2: {'Ġpoint', 'ĠSure', 'Ġearth', 'Ġthat', '.', ',', 'âĢĻ', 'ĠNevertheless', 's', 're', 'Ġto'}\n",
      "\t Unique Tokens ws: {'Ġearth.', 'âĢĻre', 'ĠNevertheless,', 'Ġpoint.', 'Ġto.', 'ĠthatâĢĻs', 'ĠSure,'}\n",
      "==Different Author==\n",
      "Text 1: Congressmen and Senators should be definitely banned from trading stocks, and the ban should be taken a step further and prohibit any family members from trading in stocks related to the committees of the members they're related too.\n",
      "\t Tokenized GPT2:Cong ress men Ġand ĠSen ators Ġshould Ġbe Ġdefinitely Ġbanned Ġfrom Ġtrading Ġstocks , Ġand Ġthe Ġban Ġshould Ġbe Ġtaken Ġa Ġstep Ġfurther Ġand Ġprohib it Ġany Ġfamily Ġmembers Ġfrom Ġtrading Ġin Ġstocks Ġrelated Ġto Ġthe Ġcommit tees Ġof Ġthe Ġmembers Ġthey 're Ġrelated Ġtoo .\n",
      "\t Tokenized ws:C ong ress men Ġand ĠSen ators Ġshould Ġbe Ġdefinitely Ġbanned Ġfrom Ġtrading Ġstock s, Ġand Ġthe Ġban Ġshould Ġbe Ġtaken Ġa Ġstep Ġfurther Ġand Ġprohib it Ġany Ġfamily Ġmembers Ġfrom Ġtrading Ġin Ġstocks Ġrelated Ġto Ġthe Ġcommitte es Ġof Ġthe Ġmembers Ġthey're Ġrelated Ġtoo.\n",
      "\t Unique Tokens GPT2: {'tees', 'Ġtoo', '.', \"'re\", ',', 'Ġcommit', 'Ġthey', 'Cong'}\n",
      "\t Unique Tokens ws: {'ong', 'Ġstock', 'Ġtoo.', 'C', 's,', 'es', 'Ġcommitte', \"Ġthey're\"}\n",
      "Text 2: I'm not defending it because I do think they should be banned from trading individual stocks (index funds I'm fine with and blind trusts should be mandatory), but so it's not like they're doing any of their stock investing in secret, it's quite easy to see what legislation is being debated and is go\n",
      "\t Tokenized GPT2:I 'm Ġnot Ġdefending Ġit Ġbecause ĠI Ġdo Ġthink Ġthey Ġshould Ġbe Ġbanned Ġfrom Ġtrading Ġindividual Ġstocks Ġ( index Ġfunds ĠI 'm Ġfine Ġwith Ġand Ġblind Ġtrust s Ġshould Ġbe Ġmandatory ), Ġbut Ġso Ġit 's Ġnot Ġlike Ġthey 're Ġdoing Ġany Ġof Ġtheir Ġstock Ġinvesting Ġin Ġsecret , Ġit 's Ġquite Ġeasy Ġto Ġsee Ġwhat Ġlegislation Ġis Ġbeing Ġdeb ated Ġand Ġis Ġgo\n",
      "\t Tokenized ws:I'm Ġnot Ġdefending Ġit Ġbecause ĠI Ġdo Ġthink Ġthey Ġshould Ġbe Ġbanned Ġfrom Ġtrading Ġindividual Ġstocks Ġ( index Ġfunds ĠI'm Ġfine Ġwith Ġand Ġblind Ġtrust s Ġshould Ġbe Ġmandatory ), Ġbut Ġso Ġit's Ġnot Ġlike Ġthey're Ġdoing Ġany Ġof Ġtheir Ġstock Ġinvesting Ġin Ġsecret , Ġit's Ġquite Ġeasy Ġto Ġsee Ġwhat Ġlegislation Ġis Ġbeing Ġdeb ated Ġand Ġis Ġgo\n",
      "\t Unique Tokens GPT2: {'Ġ\"', \"'re\", 'invest', \"'s\", 'Ġtrends', '.', 'Ġopen', \"'t\", 'Ġjob', \"'m\", 'Ġhappening', 'well', 'I', 'Ġvote', 'Ġisn'}\n",
      "\t Unique Tokens ws: {\"I'm\", 'Ġhappening.', 'Ġ\"in', \"Ġit's\", 'Ġvote,', \"ĠI'm\", 's,', 'Ġopen,', 'well,', 'Ġjob.', \"Ġisn't\", 'vest', 'Ġtrend', \"Ġthey're\"}\n"
     ]
    }
   ],
   "source": [
    "# note that 0 for PAN corresponds to same author or no change, see https://pan.webis.de/clef24/pan24-web/style-change-detection.html\n",
    "i = 0\n",
    "first_tok = gpt2_tok\n",
    "first_name = \"GPT2\"\n",
    "second_tok = llama3_tok\n",
    "second_name = \"LLAMA3\"\n",
    "second_tok = ws_tok\n",
    "second_name = \"ws\"\n",
    "for index, row in df_ws_correct_gpt2_wrong.iterrows():\n",
    "    print(\"==Same Author==\" if row[\"predictions\"] == 0 else \"==Different Author==\")\n",
    "    print(f\"Text 1: {row['text 1'][:300]}\")\n",
    "    print(f\"\\t Tokenized {first_name}:{' '.join(first_tok.tokenize(row['text 1'][:300]))}\")\n",
    "    print(f\"\\t Tokenized {second_name}:{' '.join(second_tok.tokenize(row['text 1'][:300]))}\")\n",
    "    print(f\"\\t Unique Tokens {first_name}: {set(first_tok.tokenize(row['text 1'])) - set(second_tok.tokenize(row['text 1']))}\")\n",
    "    print(f\"\\t Unique Tokens {second_name}: {set(second_tok.tokenize(row['text 1'])) - set(first_tok.tokenize(row['text 1']))}\")\n",
    "    print(f\"Text 2: {row['text 2'][:300]}\")   \n",
    "    print(f\"\\t Tokenized {first_name}:{' '.join(first_tok.tokenize(row['text 2'][:300]))}\")\n",
    "    print(f\"\\t Tokenized {second_name}:{' '.join(second_tok.tokenize(row['text 2'][:300]))}\")\n",
    "    print(f\"\\t Unique Tokens {first_name}: {set(first_tok.tokenize(row['text 2'])) - set(second_tok.tokenize(row['text 2']))}\")\n",
    "    print(f\"\\t Unique Tokens {second_name}: {set(second_tok.tokenize(row['text 2'])) - set(first_tok.tokenize(row['text 2']))}\")\n",
    "    i += 1\n",
    "    \n",
    "    if i > 10: break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T16:35:44.558969Z",
     "start_time": "2025-02-06T16:35:44.533215Z"
    }
   },
   "id": "320d65f4176d6363"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "# load webbook corpus\n",
    "path_webbook = os.path.join(local_finder_addition,\n",
    "                             \"TOKENIZER/data/train-corpora/webbook\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T16:43:09.463129Z",
     "start_time": "2025-02-06T16:43:09.451573Z"
    }
   },
   "id": "368414537d2cbe27"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading dataset from disk:   0%|          | 0/40 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "728bb1a8a439473c861a351e5f99aa5d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "webbook = load_from_disk(path_webbook)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T16:47:47.019130Z",
     "start_time": "2025-02-06T16:43:31.352705Z"
    }
   },
   "id": "c49ce1204cda4d8"
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excerpt 0:  visit and they all swore on a second blood oath that it wasn't them.\n",
      "\n",
      "\"What about Phantom?\" Hell Girl asked. \"Maybe Phantom talked to Campus News.\"\n",
      "\n",
      "Frankenstein shook his head. \"Why? Skeletor has pictures of Phantom like he has of the rest of us.\"\n",
      "\n",
      "Ghost Face looked thoughtful. \"Maybe Phantom want\n",
      "Excerpt 1: The future of SA-affiliated club sports, a cappella, and Greek groups is uncertain after the All-Campus Judicial Council ruled Friday that they may not use gendered language in their constitutions, advertisements, and names, and that they may not participate in gender-exclusive competitions.\n",
      "\n",
      "The ru\n",
      "Excerpt 2:  to support it; then a chunk of Iceland spar was discovered among navigational instruments on an Elizabethan ship that sank in 1592. This wasn't a Viking ship, but it existed almost two hundred years before scientists even understood the concept of using a crystal to determine direction at dawn and \n",
      "Excerpt 3:  in his Trojan maiden. Statius's Antigone, like all the young women of the _Thebaid_ , is distinguished by her sexual purity as well as by the nobility of character which appears in her _pietas_ toward the body of Polynices. We see her first just before the initial joining of battle, looking out fro\n",
      "Excerpt 4:  **Felted Adult Version A:** Cascade 128 Tweed (90% Peruvian wool, 10% Donegal tweed; 128 yd [117 m]/100 g): #7602 dark olive, 256 yd (234 m). Crystal Palace Splash (100% polyester; 85 yd [78 m]/100 g) #7181 jaguar, 85 yd (78 m).\n",
      "\n",
      "**Unfelted Adult Version B:** Cascade 128 Tweed (90% Peruvian wool, 1\n",
      "Excerpt 5: The Kaimukī Shire is a section of urban Honolulu that you can’t necessarily find on a map of O‘ahu, but is a gathering place for creatives, surfers, hippies, designers, and tourists. Although the moniker, coined by younger residents of Kaimukī, conjures images of hipster hobbits, puffing vape pens, \n",
      "Excerpt 6: Looking for a fun way to enjoy books and meet new people? Start a book club! But there is more to starting a book club than you might think. To help you make sure your book club is on point, here is a short guide about how to start a book club to help you make yours perfect.\n",
      "\n",
      "1. Figure Out Who You’r\n",
      "Excerpt 7: “Ohhhhhh! Who lives in a pineapple under the sea? SpongeBob SquarePants! Absorbent and yellow and porous is he! SpongeBob SquarePants!”\n",
      "\n",
      "Sometimes you don’t ask why someone wants a certain case mod, you just take what they ask for and do your best. This kind of happened with a request from EVGA when\n",
      "Excerpt 8:  down to the local hardware store for a replacement part. It takes all the ingenuity, patience, and sense of humor a scientist can muster to cope with the challenges. But it's happening, and it's thrilling.\n",
      "\n",
      "Rick Potts and his team drilled two boreholes at Olorgesailie in 2012. The cores they lifted\n",
      "Excerpt 9:  the WBHG, and the householder identity it represented, in Shanghai's specific cultural context in ways that would have been clear to those in attendance. President Shi's speech, which had in fact been jointly written with Wang Yiting and other WBHG leaders,33 pointed directly to the fundamental set\n"
     ]
    }
   ],
   "source": [
    "# for the dev set print the first 10 texts\n",
    "for i in range(10):\n",
    "    print(f\"Excerpt {i}: {webbook['train'][i]['text'][:300]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T16:49:34.226647Z",
     "start_time": "2025-02-06T16:49:34.219832Z"
    }
   },
   "id": "8116068a6dd35c98"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "path_gpt2_mnli = os.path.join(local_finder_addition, \"TOKENIZER/output/GLUE/train-mixed/base-BERT/mixed-gpt2-32000/749M/steps-45000/seed-42/42/mnli\")\n",
    "path_llama3_mnli = os.path.join(local_finder_addition, \"TOKENIZER/output/GLUE/train-mixed/base-BERT/mixed-llama3-32000/749M/steps-45000/seed-42/42/mnli\")\n",
    "df_mnli_gpt2 = pd.read_csv(os.path.join(path_gpt2_mnli, \"2025-02-01_eval_dataset_mnli.tsv\"), sep=\"\\t\")\n",
    "df_mnli_llama3 = pd.read_csv(os.path.join(path_llama3_mnli, \"2025-01-22_eval_dataset_mnli.tsv\"), sep=\"\\t\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T17:02:44.461842Z",
     "start_time": "2025-02-06T17:02:32.947396Z"
    }
   },
   "id": "55d998d7d656a788"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "df_mnli_gpt2_correct_llama3_wrong = df_mnli_gpt2[(df_mnli_gpt2[\"label\"] == df_mnli_gpt2[\"predictions\"]) & (df_mnli_llama3[\"label\"] != df_mnli_llama3[\"predictions\"])]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T17:03:18.778247Z",
     "start_time": "2025-02-06T17:03:18.759647Z"
    }
   },
   "id": "9d116acb341e7fd3"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "619\n"
     ]
    }
   ],
   "source": [
    "print(len(df_mnli_gpt2_correct_llama3_wrong))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T17:03:21.144882Z",
     "start_time": "2025-02-06T17:03:21.135930Z"
    }
   },
   "id": "b4f8eac9ad99bda4"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==entailment==\n",
      "Text 1: uh i don't know i i have mixed emotions about him uh sometimes i like him but at the same times i love to see somebody beat him\n",
      "\t Tokenized GPT2:uh Ġi Ġdon 't Ġknow Ġi Ġi Ġhave Ġmixed Ġemotions Ġabout Ġhim Ġuh Ġsometimes Ġi Ġlike Ġhim Ġbut Ġat Ġthe Ġsame Ġtimes Ġi Ġlove Ġto Ġsee Ġsomebody Ġbeat Ġhim\n",
      "\t Tokenized LLAMA3:uh Ġi Ġdon 't Ġknow Ġi Ġi Ġhave Ġmixed Ġemotions Ġabout Ġhim Ġuh Ġsometimes Ġi Ġlike Ġhim Ġbut Ġat Ġthe Ġsame Ġtimes Ġi Ġlove Ġto Ġsee Ġsomebody Ġbeat Ġhim\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I like him for the most part, but would still enjoy seeing someone beat him.\n",
      "\t Tokenized GPT2:I Ġlike Ġhim Ġfor Ġthe Ġmost Ġpart , Ġbut Ġwould Ġstill Ġenjoy Ġseeing Ġsomeone Ġbeat Ġhim .\n",
      "\t Tokenized LLAMA3:I Ġlike Ġhim Ġfor Ġthe Ġmost Ġpart , Ġbut Ġwould Ġstill Ġenjoy Ġseeing Ġsomeone Ġbeat Ġhim .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: yeah i i think my favorite restaurant is always been the one closest  you know the closest as long as it's it meets the minimum criteria you know of good food\n",
      "\t Tokenized GPT2:yeah Ġi Ġi Ġthink Ġmy Ġfavorite Ġrestaurant Ġis Ġalways Ġbeen Ġthe Ġone Ġclosest Ġ Ġyou Ġknow Ġthe Ġclosest Ġas Ġlong Ġas Ġit 's Ġit Ġmeets Ġthe Ġminimum Ġcriteria Ġyou Ġknow Ġof Ġgood Ġfood\n",
      "\t Tokenized LLAMA3:yeah Ġi Ġi Ġthink Ġmy Ġfavorite Ġrestaurant Ġis Ġalways Ġbeen Ġthe Ġone Ġclosest Ġ Ġyou Ġknow Ġthe Ġclosest Ġas Ġlong Ġas Ġit 's Ġit Ġmeets Ġthe Ġminimum Ġcriteria Ġyou Ġknow Ġof Ġgood Ġfood\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: My favorite restaurants are always at least a hundred miles away from my house. \n",
      "\t Tokenized GPT2:My Ġfavorite Ġrestaurants Ġare Ġalways Ġat Ġleast Ġa Ġhundred Ġmiles Ġaway Ġfrom Ġmy Ġhouse . Ġ\n",
      "\t Tokenized LLAMA3:My Ġfavorite Ġrestaurants Ġare Ġalways Ġat Ġleast Ġa Ġhundred Ġmiles Ġaway Ġfrom Ġmy Ġhouse . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Calcutta seems to be the only other production center having any pretensions to artistic creativity at all, but ironically you're actually more likely to see the works of Satyajit Ray or Mrinal Sen shown in Europe or North America than in India itself.\n",
      "\t Tokenized GPT2:Cal cut ta Ġseems Ġto Ġbe Ġthe Ġonly Ġother Ġproduction Ġcenter Ġhaving Ġany Ġpret ensions Ġto Ġartistic Ġcreativity Ġat Ġall , Ġbut Ġiron ically Ġyou 're Ġactually Ġmore Ġlikely Ġto Ġsee Ġthe Ġworks Ġof ĠSat y aj it ĠRay Ġor ĠMr inal ĠSen Ġshown Ġin ĠEurope Ġor ĠNorth ĠAmerica Ġthan Ġin ĠIndia Ġitself .\n",
      "\t Tokenized LLAMA3:Cal cut ta Ġseems Ġto Ġbe Ġthe Ġonly Ġother Ġproduction Ġcenter Ġhaving Ġany Ġpret ensions Ġto Ġartistic Ġcreativity Ġat Ġall , Ġbut Ġiron ically Ġyou 're Ġactually Ġmore Ġlikely Ġto Ġsee Ġthe Ġworks Ġof ĠSat y aj it ĠRay Ġor ĠMr inal ĠSen Ġshown Ġin ĠEurope Ġor ĠNorth ĠAmerica Ġthan Ġin ĠIndia Ġitself .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Most of Mrinal Sen's work can be found in European collections.\n",
      "\t Tokenized GPT2:Most Ġof ĠMr inal ĠSen 's Ġwork Ġcan Ġbe Ġfound Ġin ĠEuropean Ġcollections .\n",
      "\t Tokenized LLAMA3:Most Ġof ĠMr inal ĠSen 's Ġwork Ġcan Ġbe Ġfound Ġin ĠEuropean Ġcollections .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: i'm not sure what the overnight low was\n",
      "\t Tokenized GPT2:i 'm Ġnot Ġsure Ġwhat Ġthe Ġovernight Ġlow Ġwas\n",
      "\t Tokenized LLAMA3:i 'm Ġnot Ġsure Ġwhat Ġthe Ġovernight Ġlow Ġwas\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I don't know how cold it got last night.\n",
      "\t Tokenized GPT2:I Ġdon 't Ġknow Ġhow Ġcold Ġit Ġgot Ġlast Ġnight .\n",
      "\t Tokenized LLAMA3:I Ġdon 't Ġknow Ġhow Ġcold Ġit Ġgot Ġlast Ġnight .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: He hadn't seen even pictures of such things since the few silent movies run in some of the little art theaters.\n",
      "\t Tokenized GPT2:He Ġhadn 't Ġseen Ġeven Ġpictures Ġof Ġsuch Ġthings Ġsince Ġthe Ġfew Ġsilent Ġmovies Ġrun Ġin Ġsome Ġof Ġthe Ġlittle Ġart Ġtheaters .\n",
      "\t Tokenized LLAMA3:He Ġhadn 't Ġseen Ġeven Ġpictures Ġof Ġsuch Ġthings Ġsince Ġthe Ġfew Ġsilent Ġmovies Ġrun Ġin Ġsome Ġof Ġthe Ġlittle Ġart Ġtheaters .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He had recently seen pictures depicting those things.\n",
      "\t Tokenized GPT2:He Ġhad Ġrecently Ġseen Ġpictures Ġdep icting Ġthose Ġthings .\n",
      "\t Tokenized LLAMA3:He Ġhad Ġrecently Ġseen Ġpictures Ġdep icting Ġthose Ġthings .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Look, there's a legend here.\n",
      "\t Tokenized GPT2:Look , Ġthere 's Ġa Ġlegend Ġhere .\n",
      "\t Tokenized LLAMA3:Look , Ġthere 's Ġa Ġlegend Ġhere .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: See, there is a well known hero here.\n",
      "\t Tokenized GPT2:See , Ġthere Ġis Ġa Ġwell Ġknown Ġhero Ġhere .\n",
      "\t Tokenized LLAMA3:See , Ġthere Ġis Ġa Ġwell Ġknown Ġhero Ġhere .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: There are no shares of a stock that might someday come back, just piles of options as worthless as those shares of Cook's American Business Alliance.\n",
      "\t Tokenized GPT2:There Ġare Ġno Ġshares Ġof Ġa Ġstock Ġthat Ġmight Ġsomeday Ġcome Ġback , Ġjust Ġp iles Ġof Ġoptions Ġas Ġworthless Ġas Ġthose Ġshares Ġof ĠCook 's ĠAmerican ĠBusiness ĠAlliance .\n",
      "\t Tokenized LLAMA3:There Ġare Ġno Ġshares Ġof Ġa Ġstock Ġthat Ġmight Ġsomeday Ġcome Ġback , Ġjust Ġp iles Ġof Ġoptions Ġas Ġworthless Ġas Ġthose Ġshares Ġof ĠCook 's ĠAmerican ĠBusiness ĠAlliance .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2:  Cook's American Business Alliance caused shares of stock to come back.\n",
      "\t Tokenized GPT2:ĠCook 's ĠAmerican ĠBusiness ĠAlliance Ġcaused Ġshares Ġof Ġstock Ġto Ġcome Ġback .\n",
      "\t Tokenized LLAMA3:ĠCook 's ĠAmerican ĠBusiness ĠAlliance Ġcaused Ġshares Ġof Ġstock Ġto Ġcome Ġback .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: yeah really no kidding\n",
      "\t Tokenized GPT2:yeah Ġreally Ġno Ġkidding\n",
      "\t Tokenized LLAMA3:yeah Ġreally Ġno Ġkidding\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Really? No kidding! \n",
      "\t Tokenized GPT2:Really ? ĠNo Ġkidding ! Ġ\n",
      "\t Tokenized LLAMA3:Really ? ĠNo Ġkidding ! Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Visit at sundown or out of season to get the full flavor of the setting.\n",
      "\t Tokenized GPT2:Vis it Ġat Ġsund own Ġor Ġout Ġof Ġseason Ġto Ġget Ġthe Ġfull Ġflavor Ġof Ġthe Ġsetting .\n",
      "\t Tokenized LLAMA3:Vis it Ġat Ġsund own Ġor Ġout Ġof Ġseason Ġto Ġget Ġthe Ġfull Ġflavor Ġof Ġthe Ġsetting .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The setting is better to visit at sundown or during low season.\n",
      "\t Tokenized GPT2:The Ġsetting Ġis Ġbetter Ġto Ġvisit Ġat Ġsund own Ġor Ġduring Ġlow Ġseason .\n",
      "\t Tokenized LLAMA3:The Ġsetting Ġis Ġbetter Ġto Ġvisit Ġat Ġsund own Ġor Ġduring Ġlow Ġseason .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: While parents may pick up this gay semaphore, kids aren't likely to.\n",
      "\t Tokenized GPT2:While Ġparents Ġmay Ġpick Ġup Ġthis Ġgay Ġsem aph ore , Ġkids Ġaren 't Ġlikely Ġto .\n",
      "\t Tokenized LLAMA3:While Ġparents Ġmay Ġpick Ġup Ġthis Ġgay Ġsem aph ore , Ġkids Ġaren 't Ġlikely Ġto .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Some kids do understand gay signals.\n",
      "\t Tokenized GPT2:Some Ġkids Ġdo Ġunderstand Ġgay Ġsignals .\n",
      "\t Tokenized LLAMA3:Some Ġkids Ġdo Ġunderstand Ġgay Ġsignals .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: As a basic guide, the symbols below have been used to indicate high-season rates in Hong Kong dollars, based on double occupancy, with bath or shower.\n",
      "\t Tokenized GPT2:As Ġa Ġbasic Ġguide , Ġthe Ġsymbols Ġbelow Ġhave Ġbeen Ġused Ġto Ġindicate Ġhigh - season Ġrates Ġin ĠHong ĠKong Ġdollars , Ġbased Ġon Ġdouble Ġoccup ancy , Ġwith Ġbath Ġor Ġshower .\n",
      "\t Tokenized LLAMA3:As Ġa Ġbasic Ġguide , Ġthe Ġsymbols Ġbelow Ġhave Ġbeen Ġused Ġto Ġindicate Ġhigh -season Ġrates Ġin ĠHong ĠKong Ġdollars , Ġbased Ġon Ġdouble Ġoccup ancy , Ġwith Ġbath Ġor Ġshower .\n",
      "\t Unique Tokens GPT2: {'-', 'season'}\n",
      "\t Unique Tokens LLAMA3: {'-season'}\n",
      "Text 2: As you can see, the symbols are of dolphins and octopuses.\n",
      "\t Tokenized GPT2:As Ġyou Ġcan Ġsee , Ġthe Ġsymbols Ġare Ġof Ġd olphins Ġand Ġoct op uses .\n",
      "\t Tokenized LLAMA3:As Ġyou Ġcan Ġsee , Ġthe Ġsymbols Ġare Ġof Ġd olph ins Ġand Ġoct op uses .\n",
      "\t Unique Tokens GPT2: {'olphins'}\n",
      "\t Unique Tokens LLAMA3: {'olph', 'ins'}\n",
      "==entailment==\n",
      "Text 1: This having come to his stepmother's ears, she taxed him with it on the afternoon before her death, and a quarrel ensued, part of which was overheard. \n",
      "\t Tokenized GPT2:This Ġhaving Ġcome Ġto Ġhis Ġstep mother 's Ġears , Ġshe Ġtax ed Ġhim Ġwith Ġit Ġon Ġthe Ġafternoon Ġbefore Ġher Ġdeath , Ġand Ġa Ġqu ar rel Ġens ued , Ġpart Ġof Ġwhich Ġwas Ġover heard . Ġ\n",
      "\t Tokenized LLAMA3:This Ġhaving Ġcome Ġto Ġhis Ġstep mother 's Ġears , Ġshe Ġtax ed Ġhim Ġwith Ġit Ġon Ġthe Ġafternoon Ġbefore Ġher Ġdeath , Ġand Ġa Ġqu ar rel Ġens ued , Ġpart Ġof Ġwhich Ġwas Ġover heard . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: A fight broke out between the stepmother and the man before her death.\n",
      "\t Tokenized GPT2:A Ġfight Ġbroke Ġout Ġbetween Ġthe Ġstep mother Ġand Ġthe Ġman Ġbefore Ġher Ġdeath .\n",
      "\t Tokenized LLAMA3:A Ġfight Ġbroke Ġout Ġbetween Ġthe Ġstep mother Ġand Ġthe Ġman Ġbefore Ġher Ġdeath .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The tomb guardian will unlock the gate to the tunnel and give you a candle to explore the small circular catacomb, but for what little you can see, it is hardly worth the effort.\n",
      "\t Tokenized GPT2:The Ġtomb Ġguardian Ġwill Ġunlock Ġthe Ġgate Ġto Ġthe Ġtunnel Ġand Ġgive Ġyou Ġa Ġcandle Ġto Ġexplore Ġthe Ġsmall Ġcircular Ġcat ac omb , Ġbut Ġfor Ġwhat Ġlittle Ġyou Ġcan Ġsee , Ġit Ġis Ġhardly Ġworth Ġthe Ġeffort .\n",
      "\t Tokenized LLAMA3:The Ġtomb Ġguardian Ġwill Ġunlock Ġthe Ġgate Ġto Ġthe Ġtunnel Ġand Ġgive Ġyou Ġa Ġcandle Ġto Ġexplore Ġthe Ġsmall Ġcircular Ġcat ac omb , Ġbut Ġfor Ġwhat Ġlittle Ġyou Ġcan Ġsee , Ġit Ġis Ġhardly Ġworth Ġthe Ġeffort .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The tomb garden can give you a thorough tour of the catacombs.\n",
      "\t Tokenized GPT2:The Ġtomb Ġgarden Ġcan Ġgive Ġyou Ġa Ġthorough Ġtour Ġof Ġthe Ġcat ac om bs .\n",
      "\t Tokenized LLAMA3:The Ġtomb Ġgarden Ġcan Ġgive Ġyou Ġa Ġthorough Ġtour Ġof Ġthe Ġcat ac om bs .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: at least i'm going to give it a try cause you can see i mean the oil filters i mean you can touch it it's right there\n",
      "\t Tokenized GPT2:at Ġleast Ġi 'm Ġgoing Ġto Ġgive Ġit Ġa Ġtry Ġcause Ġyou Ġcan Ġsee Ġi Ġmean Ġthe Ġoil Ġfilters Ġi Ġmean Ġyou Ġcan Ġtouch Ġit Ġit 's Ġright Ġthere\n",
      "\t Tokenized LLAMA3:at Ġleast Ġi 'm Ġgoing Ġto Ġgive Ġit Ġa Ġtry Ġcause Ġyou Ġcan Ġsee Ġi Ġmean Ġthe Ġoil Ġfilters Ġi Ġmean Ġyou Ġcan Ġtouch Ġit Ġit 's Ġright Ġthere\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It seems like it's worth trying to get the oil filter out.\n",
      "\t Tokenized GPT2:It Ġseems Ġlike Ġit 's Ġworth Ġtrying Ġto Ġget Ġthe Ġoil Ġfilter Ġout .\n",
      "\t Tokenized LLAMA3:It Ġseems Ġlike Ġit 's Ġworth Ġtrying Ġto Ġget Ġthe Ġoil Ġfilter Ġout .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Even the lower limit of that differential compounds to a hefty sum over time.\n",
      "\t Tokenized GPT2:Even Ġthe Ġlower Ġlimit Ġof Ġthat Ġdifferential Ġcompounds Ġto Ġa Ġhe fty Ġsum Ġover Ġtime .\n",
      "\t Tokenized LLAMA3:Even Ġthe Ġlower Ġlimit Ġof Ġthat Ġdifferential Ġcompounds Ġto Ġa Ġhe fty Ġsum Ġover Ġtime .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The differential will not grow.\n",
      "\t Tokenized GPT2:The Ġdifferential Ġwill Ġnot Ġgrow .\n",
      "\t Tokenized LLAMA3:The Ġdifferential Ġwill Ġnot Ġgrow .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Reportedly the biggest payment made in such a case, it is hardly a nick in Texaco's annual revenue of more than $30 billion.\n",
      "\t Tokenized GPT2:Rep orted ly Ġthe Ġbiggest Ġpayment Ġmade Ġin Ġsuch Ġa Ġcase , Ġit Ġis Ġhardly Ġa Ġnick Ġin ĠTex aco 's Ġannual Ġrevenue Ġof Ġmore Ġthan Ġ$ 30 Ġbillion .\n",
      "\t Tokenized LLAMA3:Rep orted ly Ġthe Ġbiggest Ġpayment Ġmade Ġin Ġsuch Ġa Ġcase , Ġit Ġis Ġhardly Ġa Ġnick Ġin ĠTex aco 's Ġannual Ġrevenue Ġof Ġmore Ġthan Ġ$ 30 Ġbillion .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The biggest payment they made barely hurt their profits.\n",
      "\t Tokenized GPT2:The Ġbiggest Ġpayment Ġthey Ġmade Ġbarely Ġhurt Ġtheir Ġprofits .\n",
      "\t Tokenized LLAMA3:The Ġbiggest Ġpayment Ġthey Ġmade Ġbarely Ġhurt Ġtheir Ġprofits .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: They are levied through the power of the Government to compel payment, and the person or entity that pays these fees does not receive anything of value from the Government in exchange.\n",
      "\t Tokenized GPT2:They Ġare Ġle v ied Ġthrough Ġthe Ġpower Ġof Ġthe ĠGovernment Ġto Ġcom pel Ġpayment , Ġand Ġthe Ġperson Ġor Ġentity Ġthat Ġpays Ġthese Ġfees Ġdoes Ġnot Ġreceive Ġanything Ġof Ġvalue Ġfrom Ġthe ĠGovernment Ġin Ġexchange .\n",
      "\t Tokenized LLAMA3:They Ġare Ġle v ied Ġthrough Ġthe Ġpower Ġof Ġthe ĠGovernment Ġto Ġcom pel Ġpayment , Ġand Ġthe Ġperson Ġor Ġentity Ġthat Ġpays Ġthese Ġfees Ġdoes Ġnot Ġreceive Ġanything Ġof Ġvalue Ġfrom Ġthe ĠGovernment Ġin Ġexchange .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: They are not levied through the power of the Government to compel payment.\n",
      "\t Tokenized GPT2:They Ġare Ġnot Ġle v ied Ġthrough Ġthe Ġpower Ġof Ġthe ĠGovernment Ġto Ġcom pel Ġpayment .\n",
      "\t Tokenized LLAMA3:They Ġare Ġnot Ġle v ied Ġthrough Ġthe Ġpower Ġof Ġthe ĠGovernment Ġto Ġcom pel Ġpayment .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: It was replaced in 1910 by the famous old pontoon bridge with its seafood restaurants, which served until the present bridge was opened in 1992.\n",
      "\t Tokenized GPT2:It Ġwas Ġreplaced Ġin Ġ19 10 Ġby Ġthe Ġfamous Ġold Ġp onto on Ġbridge Ġwith Ġits Ġse af ood Ġrestaurants , Ġwhich Ġserved Ġuntil Ġthe Ġpresent Ġbridge Ġwas Ġopened Ġin Ġ1992 .\n",
      "\t Tokenized LLAMA3:It Ġwas Ġreplaced Ġin Ġ 191 0 Ġby Ġthe Ġfamous Ġold Ġp onto on Ġbridge Ġwith Ġits Ġse af ood Ġrestaurants , Ġwhich Ġserved Ġuntil Ġthe Ġpresent Ġbridge Ġwas Ġopened Ġin Ġ 199 2 .\n",
      "\t Unique Tokens GPT2: {'Ġ1992', '10', 'Ġ19'}\n",
      "\t Unique Tokens LLAMA3: {'199', '191', 'Ġ', '0', '2'}\n",
      "Text 2: The pontoon bridge had shops as well as restaurants.\n",
      "\t Tokenized GPT2:The Ġp onto on Ġbridge Ġhad Ġshops Ġas Ġwell Ġas Ġrestaurants .\n",
      "\t Tokenized LLAMA3:The Ġp onto on Ġbridge Ġhad Ġshops Ġas Ġwell Ġas Ġrestaurants .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: it would probably be a lot more work and probably not turn out as good\n",
      "\t Tokenized GPT2:it Ġwould Ġprobably Ġbe Ġa Ġlot Ġmore Ġwork Ġand Ġprobably Ġnot Ġturn Ġout Ġas Ġgood\n",
      "\t Tokenized LLAMA3:it Ġwould Ġprobably Ġbe Ġa Ġlot Ġmore Ġwork Ġand Ġprobably Ġnot Ġturn Ġout Ġas Ġgood\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Oh that way sounds great, it could turn out even better\n",
      "\t Tokenized GPT2:Oh Ġthat Ġway Ġsounds Ġgreat , Ġit Ġcould Ġturn Ġout Ġeven Ġbetter\n",
      "\t Tokenized LLAMA3:Oh Ġthat Ġway Ġsounds Ġgreat , Ġit Ġcould Ġturn Ġout Ġeven Ġbetter\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: I put it to you that, wearing a suit of Mr. Inglethorp's clothes, with a black beard trimmed to resemble his, you were there ”and signed the register in his name!\n",
      "\t Tokenized GPT2:I Ġput Ġit Ġto Ġyou Ġthat , Ġwearing Ġa Ġsuit Ġof ĠMr . ĠIn gle th orp 's Ġclothes , Ġwith Ġa Ġblack Ġbeard Ġtrim med Ġto Ġresem ble Ġhis , Ġyou Ġwere Ġthere ĠâĢĿ and Ġsigned Ġthe Ġregister Ġin Ġhis Ġname !\n",
      "\t Tokenized LLAMA3:I Ġput Ġit Ġto Ġyou Ġthat , Ġwearing Ġa Ġsuit Ġof ĠMr . ĠIn gle th orp 's Ġclothes , Ġwith Ġa Ġblack Ġbeard Ġtrim med Ġto Ġresem ble Ġhis , Ġyou Ġwere Ġthere ĠâĢĿ and Ġsigned Ġthe Ġregister Ġin Ġhis Ġname !\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The green suit that he wore was actually Mr. Inglethorp's which he stole from his closet a few days ago. \n",
      "\t Tokenized GPT2:The Ġgreen Ġsuit Ġthat Ġhe Ġwore Ġwas Ġactually ĠMr . ĠIn gle th orp 's Ġwhich Ġhe Ġstole Ġfrom Ġhis Ġcloset Ġa Ġfew Ġdays Ġago . Ġ\n",
      "\t Tokenized LLAMA3:The Ġgreen Ġsuit Ġthat Ġhe Ġwore Ġwas Ġactually ĠMr . ĠIn gle th orp 's Ġwhich Ġhe Ġstole Ġfrom Ġhis Ġcloset Ġa Ġfew Ġdays Ġago . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The activities included in the Unified Agenda are, in general, those expected to have a regulatory action within the next 12 months, although agencies may include activities with an even longer time frame.\n",
      "\t Tokenized GPT2:The Ġactivities Ġincluded Ġin Ġthe ĠUn ified ĠA gend a Ġare , Ġin Ġgeneral , Ġthose Ġexpected Ġto Ġhave Ġa Ġregulatory Ġaction Ġwithin Ġthe Ġnext Ġ12 Ġmonths , Ġalthough Ġagencies Ġmay Ġinclude Ġactivities Ġwith Ġan Ġeven Ġlonger Ġtime Ġframe .\n",
      "\t Tokenized LLAMA3:The Ġactivities Ġincluded Ġin Ġthe ĠUn ified ĠA gend a Ġare , Ġin Ġgeneral , Ġthose Ġexpected Ġto Ġhave Ġa Ġregulatory Ġaction Ġwithin Ġthe Ġnext Ġ 12 Ġmonths , Ġalthough Ġagencies Ġmay Ġinclude Ġactivities Ġwith Ġan Ġeven Ġlonger Ġtime Ġframe .\n",
      "\t Unique Tokens GPT2: {'Ġ12'}\n",
      "\t Unique Tokens LLAMA3: {'12', 'Ġ'}\n",
      "Text 2: Most of the activities taken under the regulatory actions have been longer that 12 months.\n",
      "\t Tokenized GPT2:Most Ġof Ġthe Ġactivities Ġtaken Ġunder Ġthe Ġregulatory Ġactions Ġhave Ġbeen Ġlonger Ġthat Ġ12 Ġmonths .\n",
      "\t Tokenized LLAMA3:Most Ġof Ġthe Ġactivities Ġtaken Ġunder Ġthe Ġregulatory Ġactions Ġhave Ġbeen Ġlonger Ġthat Ġ 12 Ġmonths .\n",
      "\t Unique Tokens GPT2: {'Ġ12'}\n",
      "\t Unique Tokens LLAMA3: {'Ġ', '12'}\n",
      "==entailment==\n",
      "Text 1: Black professionals braid their hair to display their ethnic pride.\n",
      "\t Tokenized GPT2:Black Ġprofessionals Ġb raid Ġtheir Ġhair Ġto Ġdisplay Ġtheir Ġethnic Ġpride .\n",
      "\t Tokenized LLAMA3:Black Ġprofessionals Ġb raid Ġtheir Ġhair Ġto Ġdisplay Ġtheir Ġethnic Ġpride .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Blacks proudly braid their hair.\n",
      "\t Tokenized GPT2:Bl acks Ġproudly Ġb raid Ġtheir Ġhair .\n",
      "\t Tokenized LLAMA3:Bl acks Ġproudly Ġb raid Ġtheir Ġhair .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: This testing of the marketplace may range from written or telephone contacts with knowledgeable federal and non-federal experts regarding similar or duplicate requirements and the results of any market test recently undertaken, to the more formal sources-sought announcements in pertinent publication\n",
      "\t Tokenized GPT2:This Ġtesting Ġof Ġthe Ġmarketplace Ġmay Ġrange Ġfrom Ġwritten Ġor Ġtelephone Ġcontacts Ġwith Ġknowledgeable Ġfederal Ġand Ġnon - fed eral Ġexperts Ġregarding Ġsimilar Ġor Ġduplicate Ġrequirements Ġand Ġthe Ġresults Ġof Ġany Ġmarket Ġtest Ġrecently Ġundertaken , Ġto Ġthe Ġmore Ġformal Ġsources - s ought Ġannouncements Ġin Ġpert inent Ġpublication\n",
      "\t Tokenized LLAMA3:This Ġtesting Ġof Ġthe Ġmarketplace Ġmay Ġrange Ġfrom Ġwritten Ġor Ġtelephone Ġcontacts Ġwith Ġknowledgeable Ġfederal Ġand Ġnon -f ed eral Ġexperts Ġregarding Ġsimilar Ġor Ġduplicate Ġrequirements Ġand Ġthe Ġresults Ġof Ġany Ġmarket Ġtest Ġrecently Ġundertaken , Ġto Ġthe Ġmore Ġformal Ġsources -s ought Ġannounce ments Ġin Ġpert inent Ġpublication\n",
      "\t Unique Tokens GPT2: {'fed', 's', 'Ġannouncements', '-', 'g'}\n",
      "\t Unique Tokens LLAMA3: {'Ġannounce', '-s', 'ed', '-f', 'ments', '.g'}\n",
      "Text 2: This marketplace testing ranges from informal to formal surveys.\n",
      "\t Tokenized GPT2:This Ġmarketplace Ġtesting Ġranges Ġfrom Ġinformal Ġto Ġformal Ġsurveys .\n",
      "\t Tokenized LLAMA3:This Ġmarketplace Ġtesting Ġranges Ġfrom Ġinformal Ġto Ġformal Ġsurveys .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: They made little effort, despite the Jesuit presence in Asia, to convert local inhabitants to Christianity or to expand their territory into the interior.\n",
      "\t Tokenized GPT2:They Ġmade Ġlittle Ġeffort , Ġdespite Ġthe ĠJ es uit Ġpresence Ġin ĠAsia , Ġto Ġconvert Ġlocal Ġinhabitants Ġto ĠChristianity Ġor Ġto Ġexpand Ġtheir Ġterritory Ġinto Ġthe Ġinterior .\n",
      "\t Tokenized LLAMA3:They Ġmade Ġlittle Ġeffort , Ġdespite Ġthe ĠJ es uit Ġpresence Ġin ĠAsia , Ġto Ġconvert Ġlocal Ġinhabitants Ġto ĠChristianity Ġor Ġto Ġexpand Ġtheir Ġterritory Ġinto Ġthe Ġinterior .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The Jesuit thought that by converting the Asian people to Christianity, it would help them to expand their territory. \n",
      "\t Tokenized GPT2:The ĠJ es uit Ġthought Ġthat Ġby Ġconverting Ġthe ĠAsian Ġpeople Ġto ĠChristianity , Ġit Ġwould Ġhelp Ġthem Ġto Ġexpand Ġtheir Ġterritory . Ġ\n",
      "\t Tokenized LLAMA3:The ĠJ es uit Ġthought Ġthat Ġby Ġconverting Ġthe ĠAsian Ġpeople Ġto ĠChristianity , Ġit Ġwould Ġhelp Ġthem Ġto Ġexpand Ġtheir Ġterritory . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: True to his word to his faithful mare, Ca'daan left Whitebelly in Fena Dim and borrowed Gray Cloud from his uncle.\n",
      "\t Tokenized GPT2:True Ġto Ġhis Ġword Ġto Ġhis Ġfaithful Ġm are , ĠCa 'd aan Ġleft ĠWhite b elly Ġin ĠF ena ĠDim Ġand Ġborrowed ĠGray ĠCloud Ġfrom Ġhis Ġuncle .\n",
      "\t Tokenized LLAMA3:True Ġto Ġhis Ġword Ġto Ġhis Ġfaithful Ġm are , ĠCa 'd aan Ġleft ĠWhite b elly Ġin ĠF ena ĠDim Ġand Ġborrowed ĠGray ĠCloud Ġfrom Ġhis Ġuncle .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Ca'daan kept his word to Gray Cloud and borrowed Whitebelly from his uncle. \n",
      "\t Tokenized GPT2:Ca 'd aan Ġkept Ġhis Ġword Ġto ĠGray ĠCloud Ġand Ġborrowed ĠWhite b elly Ġfrom Ġhis Ġuncle . Ġ\n",
      "\t Tokenized LLAMA3:Ca 'd aan Ġkept Ġhis Ġword Ġto ĠGray ĠCloud Ġand Ġborrowed ĠWhite b elly Ġfrom Ġhis Ġuncle . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: She gave the girl clothes and gifts and took her to her Connecticut estate for weekend pony rides, according to the Star . How was I supposed to compete with that?\n",
      "\t Tokenized GPT2:She Ġgave Ġthe Ġgirl Ġclothes Ġand Ġgifts Ġand Ġtook Ġher Ġto Ġher ĠConnecticut Ġestate Ġfor Ġweekend Ġpony Ġrides , Ġaccording Ġto Ġthe ĠStar Ġ. ĠHow Ġwas ĠI Ġsupposed Ġto Ġcompete Ġwith Ġthat ?\n",
      "\t Tokenized LLAMA3:She Ġgave Ġthe Ġgirl Ġclothes Ġand Ġgifts Ġand Ġtook Ġher Ġto Ġher ĠConnecticut Ġestate Ġfor Ġweekend Ġpony Ġrides , Ġaccording Ġto Ġthe ĠStar Ġ. ĠHow Ġwas ĠI Ġsupposed Ġto Ġcompete Ġwith Ġthat ?\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: She gave the boy clothes, gifts and pony rides. That's hard to compete with.\n",
      "\t Tokenized GPT2:She Ġgave Ġthe Ġboy Ġclothes , Ġgifts Ġand Ġpony Ġrides . ĠThat 's Ġhard Ġto Ġcompete Ġwith .\n",
      "\t Tokenized LLAMA3:She Ġgave Ġthe Ġboy Ġclothes , Ġgifts Ġand Ġpony Ġrides . ĠThat 's Ġhard Ġto Ġcompete Ġwith .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Decline and Decadence\n",
      "\t Tokenized GPT2:Decl ine Ġand ĠDec ad ence\n",
      "\t Tokenized LLAMA3:Decl ine Ġand ĠDec ad ence\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Decline and decadence have a direct correlation. \n",
      "\t Tokenized GPT2:Decl ine Ġand Ġdec ad ence Ġhave Ġa Ġdirect Ġcorrelation . Ġ\n",
      "\t Tokenized LLAMA3:Decl ine Ġand Ġdec ad ence Ġhave Ġa Ġdirect Ġcorrelation . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: and they're more independent and there's things to do then it's good for them to go to different i mean it he goes to a a mother's day out program now once a week both of my kids do\n",
      "\t Tokenized GPT2:and Ġthey 're Ġmore Ġindependent Ġand Ġthere 's Ġthings Ġto Ġdo Ġthen Ġit 's Ġgood Ġfor Ġthem Ġto Ġgo Ġto Ġdifferent Ġi Ġmean Ġit Ġhe Ġgoes Ġto Ġa Ġa Ġmother 's Ġday Ġout Ġprogram Ġnow Ġonce Ġa Ġweek Ġboth Ġof Ġmy Ġkids Ġdo\n",
      "\t Tokenized LLAMA3:and Ġthey 're Ġmore Ġindependent Ġand Ġthere 's Ġthings Ġto Ġdo Ġthen Ġit 's Ġgood Ġfor Ġthem Ġto Ġgo Ġto Ġdifferent Ġi Ġmean Ġit Ġhe Ġgoes Ġto Ġa Ġa Ġmother 's Ġday Ġout Ġprogram Ġnow Ġonce Ġa Ġweek Ġboth Ġof Ġmy Ġkids Ġdo\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Independence does not grant anymore options for them.\n",
      "\t Tokenized GPT2:Ind ependence Ġdoes Ġnot Ġgrant Ġanymore Ġoptions Ġfor Ġthem .\n",
      "\t Tokenized LLAMA3:Ind ependence Ġdoes Ġnot Ġgrant Ġanymore Ġoptions Ġfor Ġthem .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: but but it is peaceful i mean it is relaxing to do once you find the time to do it\n",
      "\t Tokenized GPT2:but Ġbut Ġit Ġis Ġpeaceful Ġi Ġmean Ġit Ġis Ġrelaxing Ġto Ġdo Ġonce Ġyou Ġfind Ġthe Ġtime Ġto Ġdo Ġit\n",
      "\t Tokenized LLAMA3:but Ġbut Ġit Ġis Ġpeaceful Ġi Ġmean Ġit Ġis Ġrelaxing Ġto Ġdo Ġonce Ġyou Ġfind Ġthe Ġtime Ġto Ġdo Ġit\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The time it takes is not very much.\n",
      "\t Tokenized GPT2:The Ġtime Ġit Ġtakes Ġis Ġnot Ġvery Ġmuch .\n",
      "\t Tokenized LLAMA3:The Ġtime Ġit Ġtakes Ġis Ġnot Ġvery Ġmuch .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: so i really i really don't have heart burn at all with doing it myself over four nights tie i tied the car up if four days but we're fortunate we didn't need it\n",
      "\t Tokenized GPT2:so Ġi Ġreally Ġi Ġreally Ġdon 't Ġhave Ġheart Ġburn Ġat Ġall Ġwith Ġdoing Ġit Ġmyself Ġover Ġfour Ġnights Ġtie Ġi Ġtied Ġthe Ġcar Ġup Ġif Ġfour Ġdays Ġbut Ġwe 're Ġfortunate Ġwe Ġdidn 't Ġneed Ġit\n",
      "\t Tokenized LLAMA3:so Ġi Ġreally Ġi Ġreally Ġdon 't Ġhave Ġheart Ġburn Ġat Ġall Ġwith Ġdoing Ġit Ġmyself Ġover Ġfour Ġnights Ġtie Ġi Ġtied Ġthe Ġcar Ġup Ġif Ġfour Ġdays Ġbut Ġwe 're Ġfortunate Ġwe Ġdidn 't Ġneed Ġit\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I tied the car up for four nights but I had heartburn the entire time.\n",
      "\t Tokenized GPT2:I Ġtied Ġthe Ġcar Ġup Ġfor Ġfour Ġnights Ġbut ĠI Ġhad Ġheart burn Ġthe Ġentire Ġtime .\n",
      "\t Tokenized LLAMA3:I Ġtied Ġthe Ġcar Ġup Ġfor Ġfour Ġnights Ġbut ĠI Ġhad Ġheart burn Ġthe Ġentire Ġtime .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Every August young women convene to light joss sticks and some even climb the nine-meter (30-ft) rock to pray for good husbands.\n",
      "\t Tokenized GPT2:Every ĠAugust Ġyoung Ġwomen Ġconven e Ġto Ġlight Ġj oss Ġsticks Ġand Ġsome Ġeven Ġclimb Ġthe Ġnine - meter Ġ( 30 - ft ) Ġrock Ġto Ġpray Ġfor Ġgood Ġhus bands .\n",
      "\t Tokenized LLAMA3:Every ĠAugust Ġyoung Ġwomen Ġconven e Ġto Ġlight Ġj oss Ġsticks Ġand Ġsome Ġeven Ġclimb Ġthe Ġnine -m eter Ġ( 30 - ft ) Ġrock Ġto Ġpray Ġfor Ġgood Ġhus bands .\n",
      "\t Unique Tokens GPT2: {'meter'}\n",
      "\t Unique Tokens LLAMA3: {'eter', '-m'}\n",
      "Text 2: Women converge on this place to light joss sticks and climb the rock.  \n",
      "\t Tokenized GPT2:Women Ġconver ge Ġon Ġthis Ġplace Ġto Ġlight Ġj oss Ġsticks Ġand Ġclimb Ġthe Ġrock . ĠĠ\n",
      "\t Tokenized LLAMA3:W omen Ġconver ge Ġon Ġthis Ġplace Ġto Ġlight Ġj oss Ġsticks Ġand Ġclimb Ġthe Ġrock . ĠĠ\n",
      "\t Unique Tokens GPT2: {'Women'}\n",
      "\t Unique Tokens LLAMA3: {'omen', 'W'}\n",
      "==entailment==\n",
      "Text 1: In Japan, Mainichi Shimbun criticized the new Liberal Democratic Party leader Keizo Obuchi for being devoid of fresh ideas for reviving the Japanese economy.\n",
      "\t Tokenized GPT2:In ĠJapan , ĠMain ichi ĠSh imb un Ġcriticized Ġthe Ġnew ĠLiberal ĠDemocratic ĠParty Ġleader ĠKe iz o ĠOb uchi Ġfor Ġbeing Ġdev oid Ġof Ġfresh Ġideas Ġfor Ġrev iving Ġthe ĠJapanese Ġeconomy .\n",
      "\t Tokenized LLAMA3:In ĠJapan , ĠMain ichi ĠSh imb un Ġcriticized Ġthe Ġnew ĠLiberal ĠDemocratic ĠParty Ġleader ĠKe iz o ĠOb uchi Ġfor Ġbeing Ġdev oid Ġof Ġfresh Ġideas Ġfor Ġrev iving Ġthe ĠJapanese Ġeconomy .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Mainichi Shimbun was critical of Keizo Obuchi, the new Liberal Democratic Party Leader.\n",
      "\t Tokenized GPT2:Main ichi ĠSh imb un Ġwas Ġcritical Ġof ĠKe iz o ĠOb uchi , Ġthe Ġnew ĠLiberal ĠDemocratic ĠParty ĠLeader .\n",
      "\t Tokenized LLAMA3:Main ichi ĠSh imb un Ġwas Ġcritical Ġof ĠKe iz o ĠOb uchi , Ġthe Ġnew ĠLiberal ĠDemocratic ĠParty ĠLeader .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: One bakes Flipper.\n",
      "\t Tokenized GPT2:One Ġb akes ĠF li pper .\n",
      "\t Tokenized LLAMA3:One Ġb akes ĠF li pper .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The flipper was here.\n",
      "\t Tokenized GPT2:The Ġfli pper Ġwas Ġhere .\n",
      "\t Tokenized LLAMA3:The Ġfli pper Ġwas Ġhere .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The purpose of the Diwan-i-Khas is hotly disputed; it is not necessarily the hall of private audience that its name implies.\n",
      "\t Tokenized GPT2:The Ġpurpose Ġof Ġthe ĠDi wan - i - K has Ġis Ġhot ly Ġdisput ed ; Ġit Ġis Ġnot Ġnecessarily Ġthe Ġhall Ġof Ġprivate Ġaudience Ġthat Ġits Ġname Ġimplies .\n",
      "\t Tokenized LLAMA3:The Ġpurpose Ġof Ġthe ĠDi wan -i -K has Ġis Ġhot ly Ġdis puted ; Ġit Ġis Ġnot Ġnecessarily Ġthe Ġhall Ġof Ġprivate Ġaudience Ġthat Ġits Ġname Ġimplies .\n",
      "\t Unique Tokens GPT2: {'Ġdisput', 'ed', 'K', '-', 'i'}\n",
      "\t Unique Tokens LLAMA3: {'Ġdis', '-i', '-K', 'puted'}\n",
      "Text 2: The name suggest that it is open to the public.\n",
      "\t Tokenized GPT2:The Ġname Ġsuggest Ġthat Ġit Ġis Ġopen Ġto Ġthe Ġpublic .\n",
      "\t Tokenized LLAMA3:The Ġname Ġsuggest Ġthat Ġit Ġis Ġopen Ġto Ġthe Ġpublic .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: and ancient coins\n",
      "\t Tokenized GPT2:and Ġancient Ġcoins\n",
      "\t Tokenized LLAMA3:and Ġancient Ġcoins\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: And really old coins.\n",
      "\t Tokenized GPT2:And Ġreally Ġold Ġcoins .\n",
      "\t Tokenized LLAMA3:And Ġreally Ġold Ġcoins .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The Kal tangled both of Adrin's arms, keeping the blades far away.\n",
      "\t Tokenized GPT2:The ĠKal Ġtangled Ġboth Ġof ĠAd rin 's Ġarms , Ġkeeping Ġthe Ġblades Ġfar Ġaway .\n",
      "\t Tokenized LLAMA3:The ĠKal Ġtangled Ġboth Ġof ĠAd rin 's Ġarms , Ġkeeping Ġthe Ġblades Ġfar Ġaway .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Adrin's arms were tangled, keeping his rusty blades away from Kal.\n",
      "\t Tokenized GPT2:Ad rin 's Ġarms Ġwere Ġtangled , Ġkeeping Ġhis Ġrust y Ġblades Ġaway Ġfrom ĠKal .\n",
      "\t Tokenized LLAMA3:Ad rin 's Ġarms Ġwere Ġtangled , Ġkeeping Ġhis Ġrust y Ġblades Ġaway Ġfrom ĠKal .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The burden of his spiritual functions as high priest of Shinto and the tasks of administration led the emperor to welcome an early abdication, frequently to retire to a life of Buddhist meditation and scholarship.\n",
      "\t Tokenized GPT2:The Ġburden Ġof Ġhis Ġspiritual Ġfunctions Ġas Ġhigh Ġpriest Ġof ĠSh into Ġand Ġthe Ġtasks Ġof Ġadministration Ġled Ġthe Ġem peror Ġto Ġwelcome Ġan Ġearly Ġab d ication , Ġfrequently Ġto Ġretire Ġto Ġa Ġlife Ġof ĠBudd hist Ġmeditation Ġand Ġscholarship .\n",
      "\t Tokenized LLAMA3:The Ġburden Ġof Ġhis Ġspiritual Ġfunctions Ġas Ġhigh Ġpriest Ġof ĠSh into Ġand Ġthe Ġtasks Ġof Ġadministration Ġled Ġthe Ġem peror Ġto Ġwelcome Ġan Ġearly Ġab d ication , Ġfrequently Ġto Ġretire Ġto Ġa Ġlife Ġof ĠBudd hist Ġmeditation Ġand Ġscholarship .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: People looked down on the emperor for abandoning his duties and abdicating.\n",
      "\t Tokenized GPT2:People Ġlooked Ġdown Ġon Ġthe Ġem peror Ġfor Ġabandon ing Ġhis Ġduties Ġand Ġab d icating .\n",
      "\t Tokenized LLAMA3:People Ġlooked Ġdown Ġon Ġthe Ġem peror Ġfor Ġabandon ing Ġhis Ġduties Ġand Ġab d icating .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Managing better requires that agencies have, and rely upon, sound financial and program information.\n",
      "\t Tokenized GPT2:Man aging Ġbetter Ġrequires Ġthat Ġagencies Ġhave , Ġand Ġrely Ġupon , Ġsound Ġfinancial Ġand Ġprogram Ġinformation .\n",
      "\t Tokenized LLAMA3:Man aging Ġbetter Ġrequires Ġthat Ġagencies Ġhave , Ġand Ġrely Ġupon , Ġsound Ġfinancial Ġand Ġprogram Ġinformation .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Agencies need sound financial and program information for good management.\n",
      "\t Tokenized GPT2:Ag encies Ġneed Ġsound Ġfinancial Ġand Ġprogram Ġinformation Ġfor Ġgood Ġmanagement .\n",
      "\t Tokenized LLAMA3:Ag encies Ġneed Ġsound Ġfinancial Ġand Ġprogram Ġinformation Ġfor Ġgood Ġmanagement .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Always check with drivers and hotel employees to determine if road conditions are good before you depart.\n",
      "\t Tokenized GPT2:Always Ġcheck Ġwith Ġdrivers Ġand Ġhotel Ġemployees Ġto Ġdetermine Ġif Ġroad Ġconditions Ġare Ġgood Ġbefore Ġyou Ġdepart .\n",
      "\t Tokenized LLAMA3:Always Ġcheck Ġwith Ġdrivers Ġand Ġhotel Ġemployees Ġto Ġdetermine Ġif Ġroad Ġconditions Ġare Ġgood Ġbefore Ġyou Ġdepart .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: If the roads are not in good condition, you will be provided an extra night's stay for free.\n",
      "\t Tokenized GPT2:If Ġthe Ġroads Ġare Ġnot Ġin Ġgood Ġcondition , Ġyou Ġwill Ġbe Ġprovided Ġan Ġextra Ġnight 's Ġstay Ġfor Ġfree .\n",
      "\t Tokenized LLAMA3:If Ġthe Ġroads Ġare Ġnot Ġin Ġgood Ġcondition , Ġyou Ġwill Ġbe Ġprovided Ġan Ġextra Ġnight 's Ġstay Ġfor Ġfree .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: His arm came up over his eyes, cutting off the glare.\n",
      "\t Tokenized GPT2:His Ġarm Ġcame Ġup Ġover Ġhis Ġeyes , Ġcutting Ġoff Ġthe Ġglare .\n",
      "\t Tokenized LLAMA3:His Ġarm Ġcame Ġup Ġover Ġhis Ġeyes , Ġcutting Ġoff Ġthe Ġglare .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Everything was dark, and he couldn't see a thing.\n",
      "\t Tokenized GPT2:Everything Ġwas Ġdark , Ġand Ġhe Ġcouldn 't Ġsee Ġa Ġthing .\n",
      "\t Tokenized LLAMA3:Everything Ġwas Ġdark , Ġand Ġhe Ġcouldn 't Ġsee Ġa Ġthing .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: A piece describes the Learning Channel's new women-targeted reality TV  A Wedding Story , A Baby Story , and A Dating Story , featuring real-life marriages, babies, and dates.\n",
      "\t Tokenized GPT2:A Ġpiece Ġdescribes Ġthe ĠLearning ĠChannel 's Ġnew Ġwomen - target ed Ġreality ĠTV Ġ ĠA ĠWed ding ĠStory Ġ, ĠA ĠBaby ĠStory Ġ, Ġand ĠA ĠD ating ĠStory Ġ, Ġfeaturing Ġreal - life Ġmar riages , Ġbabies , Ġand Ġdates .\n",
      "\t Tokenized LLAMA3:A Ġpiece Ġdescribes Ġthe ĠLearning ĠChannel 's Ġnew Ġwomen -target ed Ġreality ĠTV Ġ ĠA ĠWed ding ĠStory Ġ, ĠA ĠBaby ĠStory Ġ, Ġand ĠA ĠD ating ĠStory Ġ, Ġfeaturing Ġreal -life Ġmar riages , Ġbabies , Ġand Ġdates .\n",
      "\t Unique Tokens GPT2: {'-', 'life', 'target'}\n",
      "\t Unique Tokens LLAMA3: {'-target', '-life'}\n",
      "Text 2: The LEarning Channel focuses on the male audience.\n",
      "\t Tokenized GPT2:The ĠLE ar ning ĠChannel Ġfocuses Ġon Ġthe Ġmale Ġaudience .\n",
      "\t Tokenized LLAMA3:The ĠLE ar ning ĠChannel Ġfocuses Ġon Ġthe Ġmale Ġaudience .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Beatrice and Grace made out OK legally, but some of us will never use their products again without thinking about Travolta losing his shirt in the name of those wasted-away little kids.\n",
      "\t Tokenized GPT2:Be at rice Ġand ĠGrace Ġmade Ġout ĠOK Ġlegally , Ġbut Ġsome Ġof Ġus Ġwill Ġnever Ġuse Ġtheir Ġproducts Ġagain Ġwithout Ġthinking Ġabout ĠTr av ol ta Ġlosing Ġhis Ġshirt Ġin Ġthe Ġname Ġof Ġthose Ġwasted - away Ġlittle Ġkids .\n",
      "\t Tokenized LLAMA3:Be at rice Ġand ĠGrace Ġmade Ġout ĠOK Ġlegally , Ġbut Ġsome Ġof Ġus Ġwill Ġnever Ġuse Ġtheir Ġproducts Ġagain Ġwithout Ġthinking Ġabout ĠTr av ol ta Ġlosing Ġhis Ġshirt Ġin Ġthe Ġname Ġof Ġthose Ġwasted -a way Ġlittle Ġkids .\n",
      "\t Unique Tokens GPT2: {'-', 'away'}\n",
      "\t Unique Tokens LLAMA3: {'-a', 'way'}\n",
      "Text 2: Beatrice and Grace ended up in prison at the end.\n",
      "\t Tokenized GPT2:Be at rice Ġand ĠGrace Ġended Ġup Ġin Ġprison Ġat Ġthe Ġend .\n",
      "\t Tokenized LLAMA3:Be at rice Ġand ĠGrace Ġended Ġup Ġin Ġprison Ġat Ġthe Ġend .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: eThe number of deletions was negligible.\n",
      "\t Tokenized GPT2:e The Ġnumber Ġof Ġdelet ions Ġwas Ġneglig ible .\n",
      "\t Tokenized LLAMA3:e The Ġnumber Ġof Ġdelet ions Ġwas Ġneglig ible .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The precise number of deletions was 71.\n",
      "\t Tokenized GPT2:The Ġprecise Ġnumber Ġof Ġdelet ions Ġwas Ġ71 .\n",
      "\t Tokenized LLAMA3:The Ġprecise Ġnumber Ġof Ġdelet ions Ġwas Ġ 71 .\n",
      "\t Unique Tokens GPT2: {'Ġ71'}\n",
      "\t Unique Tokens LLAMA3: {'71', 'Ġ'}\n",
      "==entailment==\n",
      "Text 1: It takes a deeper fire than most salamanders can stir, Ser Perth.\n",
      "\t Tokenized GPT2:It Ġtakes Ġa Ġdeeper Ġfire Ġthan Ġmost Ġsal am and ers Ġcan Ġstir , ĠSer ĠPer th .\n",
      "\t Tokenized LLAMA3:It Ġtakes Ġa Ġdeeper Ġfire Ġthan Ġmost Ġsal am and ers Ġcan Ġstir , ĠSer ĠPer th .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Most salamanders can't stir a fire that deep.\n",
      "\t Tokenized GPT2:Most Ġsal am and ers Ġcan 't Ġstir Ġa Ġfire Ġthat Ġdeep .\n",
      "\t Tokenized LLAMA3:Most Ġsal am and ers Ġcan 't Ġstir Ġa Ġfire Ġthat Ġdeep .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: We need your help with another new feature that starts next week.\n",
      "\t Tokenized GPT2:We Ġneed Ġyour Ġhelp Ġwith Ġanother Ġnew Ġfeature Ġthat Ġstarts Ġnext Ġweek .\n",
      "\t Tokenized LLAMA3:We Ġneed Ġyour Ġhelp Ġwith Ġanother Ġnew Ġfeature Ġthat Ġstarts Ġnext Ġweek .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: We are able to work with the new feature on our own.\n",
      "\t Tokenized GPT2:We Ġare Ġable Ġto Ġwork Ġwith Ġthe Ġnew Ġfeature Ġon Ġour Ġown .\n",
      "\t Tokenized LLAMA3:We Ġare Ġable Ġto Ġwork Ġwith Ġthe Ġnew Ġfeature Ġon Ġour Ġown .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: She was quite young, not more than eighteen.\n",
      "\t Tokenized GPT2:She Ġwas Ġquite Ġyoung , Ġnot Ġmore Ġthan Ġeighteen .\n",
      "\t Tokenized LLAMA3:She Ġwas Ġquite Ġyoung , Ġnot Ġmore Ġthan Ġeight een .\n",
      "\t Unique Tokens GPT2: {'Ġeighteen'}\n",
      "\t Unique Tokens LLAMA3: {'Ġeight', 'een'}\n",
      "Text 2: The girls was at least eighteen years old, but not much older. \n",
      "\t Tokenized GPT2:The Ġgirls Ġwas Ġat Ġleast Ġeighteen Ġyears Ġold , Ġbut Ġnot Ġmuch Ġolder . Ġ\n",
      "\t Tokenized LLAMA3:The Ġgirls Ġwas Ġat Ġleast Ġeight een Ġyears Ġold , Ġbut Ġnot Ġmuch Ġolder . Ġ\n",
      "\t Unique Tokens GPT2: {'Ġeighteen'}\n",
      "\t Unique Tokens LLAMA3: {'Ġeight', 'een'}\n",
      "==entailment==\n",
      "Text 1: no not it not no it's a it's not something\n",
      "\t Tokenized GPT2:no Ġnot Ġit Ġnot Ġno Ġit 's Ġa Ġit 's Ġnot Ġsomething\n",
      "\t Tokenized LLAMA3:no Ġnot Ġit Ġnot Ġno Ġit 's Ġa Ġit 's Ġnot Ġsomething\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It's not anything\n",
      "\t Tokenized GPT2:It 's Ġnot Ġanything\n",
      "\t Tokenized LLAMA3:It 's Ġnot Ġanything\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: (Read Slate 's  on how Bush flaunts the courage of his cliches.\n",
      "\t Tokenized GPT2:( Read ĠSl ate Ġ' s Ġ Ġon Ġhow ĠBush Ġfl a unts Ġthe Ġcourage Ġof Ġhis Ġcl iches .\n",
      "\t Tokenized LLAMA3:( Read ĠSl ate Ġ' s Ġ Ġon Ġhow ĠBush Ġfl aun ts Ġthe Ġcourage Ġof Ġhis Ġcl iches .\n",
      "\t Unique Tokens GPT2: {'unts', 'a'}\n",
      "\t Unique Tokens LLAMA3: {'ts', 'aun'}\n",
      "Text 2: Slate talks about how Bush is ashamed of his cliches.\n",
      "\t Tokenized GPT2:Sl ate Ġtalks Ġabout Ġhow ĠBush Ġis Ġashamed Ġof Ġhis Ġcl iches .\n",
      "\t Tokenized LLAMA3:Sl ate Ġtalks Ġabout Ġhow ĠBush Ġis Ġashamed Ġof Ġhis Ġcl iches .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: The last 12 years of his life are a blank.\n",
      "\t Tokenized GPT2:The Ġlast Ġ12 Ġyears Ġof Ġhis Ġlife Ġare Ġa Ġblank .\n",
      "\t Tokenized LLAMA3:The Ġlast Ġ 12 Ġyears Ġof Ġhis Ġlife Ġare Ġa Ġblank .\n",
      "\t Unique Tokens GPT2: {'Ġ12'}\n",
      "\t Unique Tokens LLAMA3: {'Ġ', '12'}\n",
      "Text 2: He can't remember the last 12 years of his life\n",
      "\t Tokenized GPT2:He Ġcan 't Ġremember Ġthe Ġlast Ġ12 Ġyears Ġof Ġhis Ġlife\n",
      "\t Tokenized LLAMA3:He Ġcan 't Ġremember Ġthe Ġlast Ġ 12 Ġyears Ġof Ġhis Ġlife\n",
      "\t Unique Tokens GPT2: {'Ġ12'}\n",
      "\t Unique Tokens LLAMA3: {'Ġ', '12'}\n",
      "==neutral==\n",
      "Text 1: they don't i don't i don't work at TI\n",
      "\t Tokenized GPT2:they Ġdon 't Ġi Ġdon 't Ġi Ġdon 't Ġwork Ġat ĠTI\n",
      "\t Tokenized LLAMA3:they Ġdon 't Ġi Ġdon 't Ġi Ġdon 't Ġwork Ġat ĠTI\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I work at Boeing instead of TI.\n",
      "\t Tokenized GPT2:I Ġwork Ġat ĠBoe ing Ġinstead Ġof ĠTI .\n",
      "\t Tokenized LLAMA3:I Ġwork Ġat ĠBoe ing Ġinstead Ġof ĠTI .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: He saw Stark buried under the earth, screaming for a mercy or death that would never come and crawling out of the rock decades later.\n",
      "\t Tokenized GPT2:He Ġsaw ĠStark Ġburied Ġunder Ġthe Ġearth , Ġscreaming Ġfor Ġa Ġmercy Ġor Ġdeath Ġthat Ġwould Ġnever Ġcome Ġand Ġcrawling Ġout Ġof Ġthe Ġrock Ġdecades Ġlater .\n",
      "\t Tokenized LLAMA3:He Ġsaw ĠStark Ġburied Ġunder Ġthe Ġearth , Ġscreaming Ġfor Ġa Ġmercy Ġor Ġdeath Ġthat Ġwould Ġnever Ġcome Ġand Ġcrawling Ġout Ġof Ġthe Ġrock Ġdecades Ġlater .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Stark got buried in a big hole.\n",
      "\t Tokenized GPT2:St ark Ġgot Ġburied Ġin Ġa Ġbig Ġhole .\n",
      "\t Tokenized LLAMA3:St ark Ġgot Ġburied Ġin Ġa Ġbig Ġhole .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: A profile crowns Chris Rock The Funniest Man in America.\n",
      "\t Tokenized GPT2:A Ġprofile Ġcrown s ĠChris ĠRock ĠThe ĠFun n iest ĠMan Ġin ĠAmerica .\n",
      "\t Tokenized LLAMA3:A Ġprofile Ġcrown s ĠChris ĠRock ĠThe ĠFun n iest ĠMan Ġin ĠAmerica .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: A profile crowns Chris Rock the Funniest Man in America, but many disagree. \n",
      "\t Tokenized GPT2:A Ġprofile Ġcrown s ĠChris ĠRock Ġthe ĠFun n iest ĠMan Ġin ĠAmerica , Ġbut Ġmany Ġdisagree . Ġ\n",
      "\t Tokenized LLAMA3:A Ġprofile Ġcrown s ĠChris ĠRock Ġthe ĠFun n iest ĠMan Ġin ĠAmerica , Ġbut Ġmany Ġdisagree . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: A re-created street of colonial Macau is lined with traditional Chinese shops.\n",
      "\t Tokenized GPT2:A Ġre - created Ġstreet Ġof Ġcolonial ĠMac au Ġis Ġlined Ġwith Ġtraditional ĠChinese Ġshops .\n",
      "\t Tokenized LLAMA3:A Ġre - created Ġstreet Ġof Ġcolonial ĠMac au Ġis Ġlined Ġwith Ġtraditional ĠChinese Ġshops .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: You'll find plenty of authentic, old-world restaurants on that street.\n",
      "\t Tokenized GPT2:You 'll Ġfind Ġplenty Ġof Ġauthentic , Ġold - world Ġrestaurants Ġon Ġthat Ġstreet .\n",
      "\t Tokenized LLAMA3:You 'll Ġfind Ġplenty Ġof Ġauthentic , Ġold -world Ġrestaurants Ġon Ġthat Ġstreet .\n",
      "\t Unique Tokens GPT2: {'world', '-'}\n",
      "\t Unique Tokens LLAMA3: {'-world'}\n",
      "==contradiction==\n",
      "Text 1: Similarly, OIM revised the electronic Grant Renewal Application to accommodate new information sought by LSC and to ensure greater ease for users.\n",
      "\t Tokenized GPT2:Similar ly , ĠO IM Ġrevised Ġthe Ġelectronic ĠGrant ĠRen ew al ĠApplication Ġto Ġaccommodate Ġnew Ġinformation Ġsought Ġby ĠL SC Ġand Ġto Ġensure Ġgreater Ġease Ġfor Ġusers .\n",
      "\t Tokenized LLAMA3:Similar ly , ĠO IM Ġrevised Ġthe Ġelectronic ĠGrant ĠRen ew al ĠApplication Ġto Ġaccommodate Ġnew Ġinformation Ġsought Ġby ĠL SC Ġand Ġto Ġensure Ġgreater Ġease Ġfor Ġusers .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The OIM is hoping to revise the Grant Renewal Application to reduce the LSC's ability to request information.\n",
      "\t Tokenized GPT2:The ĠO IM Ġis Ġhoping Ġto Ġrev ise Ġthe ĠGrant ĠRen ew al ĠApplication Ġto Ġreduce Ġthe ĠL SC 's Ġability Ġto Ġrequest Ġinformation .\n",
      "\t Tokenized LLAMA3:The ĠO IM Ġis Ġhoping Ġto Ġrev ise Ġthe ĠGrant ĠRen ew al ĠApplication Ġto Ġreduce Ġthe ĠL SC 's Ġability Ġto Ġrequest Ġinformation .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: guess it didn't last too long at the box office but i thought it was pretty good\n",
      "\t Tokenized GPT2:guess Ġit Ġdidn 't Ġlast Ġtoo Ġlong Ġat Ġthe Ġbox Ġoffice Ġbut Ġi Ġthought Ġit Ġwas Ġpretty Ġgood\n",
      "\t Tokenized LLAMA3:gu ess Ġit Ġdidn 't Ġlast Ġtoo Ġlong Ġat Ġthe Ġbox Ġoffice Ġbut Ġi Ġthought Ġit Ġwas Ġpretty Ġgood\n",
      "\t Unique Tokens GPT2: {'guess'}\n",
      "\t Unique Tokens LLAMA3: {'gu', 'ess'}\n",
      "Text 2: Wow, it lasted in the box office for so long, I'm surprised.\n",
      "\t Tokenized GPT2:Wow , Ġit Ġlasted Ġin Ġthe Ġbox Ġoffice Ġfor Ġso Ġlong , ĠI 'm Ġsurprised .\n",
      "\t Tokenized LLAMA3:Wow , Ġit Ġlasted Ġin Ġthe Ġbox Ġoffice Ġfor Ġso Ġlong , ĠI 'm Ġsurprised .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Built in a.d. 715 to help measure the peak and trough of the Nile flood.\n",
      "\t Tokenized GPT2:Bu ilt Ġin Ġa . d . Ġ7 15 Ġto Ġhelp Ġmeasure Ġthe Ġpeak Ġand Ġtr ough Ġof Ġthe ĠN ile Ġflood .\n",
      "\t Tokenized LLAMA3:B uilt Ġin Ġa .d . Ġ 715 Ġto Ġhelp Ġmeasure Ġthe Ġpeak Ġand Ġtr ough Ġof Ġthe ĠN ile Ġflood .\n",
      "\t Unique Tokens GPT2: {'ilt', '15', 'Ġ7', 'Bu', 'd'}\n",
      "\t Unique Tokens LLAMA3: {'uilt', 'Ġ', 'B', '.d', '715'}\n",
      "Text 2: It said the Nile flood was 12 feet deep.\n",
      "\t Tokenized GPT2:It Ġsaid Ġthe ĠN ile Ġflood Ġwas Ġ12 Ġfeet Ġdeep .\n",
      "\t Tokenized LLAMA3:It Ġsaid Ġthe ĠN ile Ġflood Ġwas Ġ 12 Ġfeet Ġdeep .\n",
      "\t Unique Tokens GPT2: {'Ġ12'}\n",
      "\t Unique Tokens LLAMA3: {'Ġ', '12'}\n",
      "==contradiction==\n",
      "Text 1: All of a sudden I sat down on the edge of the table, and put my face in my hands, sobbing out a 'Mon Dieu! \n",
      "\t Tokenized GPT2:All Ġof Ġa Ġsudden ĠI Ġsat Ġdown Ġon Ġthe Ġedge Ġof Ġthe Ġtable , Ġand Ġput Ġmy Ġface Ġin Ġmy Ġhands , Ġsobbing Ġout Ġa Ġ' Mon ĠDie u ! Ġ\n",
      "\t Tokenized LLAMA3:All Ġof Ġa Ġsudden ĠI Ġsat Ġdown Ġon Ġthe Ġedge Ġof Ġthe Ġtable , Ġand Ġput Ġmy Ġface Ġin Ġmy Ġhands , Ġsobbing Ġout Ġa Ġ' Mon ĠDie u ! Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I sat down on the table and started laughing out loudly.\n",
      "\t Tokenized GPT2:I Ġsat Ġdown Ġon Ġthe Ġtable Ġand Ġstarted Ġlaughing Ġout Ġloudly .\n",
      "\t Tokenized LLAMA3:I Ġsat Ġdown Ġon Ġthe Ġtable Ġand Ġstarted Ġlaughing Ġout Ġloudly .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Very simply. \n",
      "\t Tokenized GPT2:Very Ġsimply . Ġ\n",
      "\t Tokenized LLAMA3:Very Ġsimply . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Only a little explanation was needed.\n",
      "\t Tokenized GPT2:Only Ġa Ġlittle Ġexplanation Ġwas Ġneeded .\n",
      "\t Tokenized LLAMA3:Only Ġa Ġlittle Ġexplanation Ġwas Ġneeded .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: In a new retrospective, the Vienna modernist (1890-1918) wins critics' grudging respect.\n",
      "\t Tokenized GPT2:In Ġa Ġnew Ġretrospective , Ġthe ĠVienna Ġmodern ist Ġ( 18 90 - 19 18 ) Ġwins Ġcritics ' Ġgr udging Ġrespect .\n",
      "\t Tokenized LLAMA3:In Ġa Ġnew Ġretrospective , Ġthe ĠV ienna Ġmodern ist Ġ( 189 0 - 191 8 ) Ġwins Ġcritics ' Ġgr udging Ġrespect .\n",
      "\t Unique Tokens GPT2: {'ĠVienna', '90', '19', '18'}\n",
      "\t Unique Tokens LLAMA3: {'191', 'ĠV', 'ienna', '189', '0', '8'}\n",
      "Text 2: Critics are reluctant but ultimately they are forced to respect the Vienna Modernist. \n",
      "\t Tokenized GPT2:Cr itics Ġare Ġreluctant Ġbut Ġultimately Ġthey Ġare Ġforced Ġto Ġrespect Ġthe ĠVienna ĠModern ist . Ġ\n",
      "\t Tokenized LLAMA3:Cr itics Ġare Ġreluctant Ġbut Ġultimately Ġthey Ġare Ġforced Ġto Ġrespect Ġthe ĠV ienna ĠModern ist . Ġ\n",
      "\t Unique Tokens GPT2: {'ĠVienna'}\n",
      "\t Unique Tokens LLAMA3: {'ienna', 'ĠV'}\n",
      "==entailment==\n",
      "Text 1: The 37 hectares (91 acres) of garden are set on lands above the Wag Wag River, which twists through a steep and narrow valley.\n",
      "\t Tokenized GPT2:The Ġ37 Ġhe ct ares Ġ( 91 Ġacres ) Ġof Ġgarden Ġare Ġset Ġon Ġlands Ġabove Ġthe ĠW ag ĠW ag ĠRiver , Ġwhich Ġtwists Ġthrough Ġa Ġsteep Ġand Ġnarrow Ġvalley .\n",
      "\t Tokenized LLAMA3:The Ġ 37 Ġhe ct ares Ġ( 91 Ġacres ) Ġof Ġgarden Ġare Ġset Ġon Ġlands Ġabove Ġthe ĠW ag ĠW ag ĠRiver , Ġwhich Ġtwists Ġthrough Ġa Ġsteep Ġand Ġnarrow Ġvalley .\n",
      "\t Unique Tokens GPT2: {'Ġ37'}\n",
      "\t Unique Tokens LLAMA3: {'37', 'Ġ'}\n",
      "Text 2: 37 hectares is equivalent to just over 90 acres, and is the size of the gardens above the Wag Wag River.\n",
      "\t Tokenized GPT2:37 Ġhe ct ares Ġis Ġequivalent Ġto Ġjust Ġover Ġ90 Ġacres , Ġand Ġis Ġthe Ġsize Ġof Ġthe Ġgardens Ġabove Ġthe ĠW ag ĠW ag ĠRiver .\n",
      "\t Tokenized LLAMA3:37 Ġhe ct ares Ġis Ġequivalent Ġto Ġjust Ġover Ġ 90 Ġacres , Ġand Ġis Ġthe Ġsize Ġof Ġthe Ġgardens Ġabove Ġthe ĠW ag ĠW ag ĠRiver .\n",
      "\t Unique Tokens GPT2: {'Ġ90'}\n",
      "\t Unique Tokens LLAMA3: {'90', 'Ġ'}\n",
      "==entailment==\n",
      "Text 1: Ocho Rios is Spanish for  eight rivers,  but this name is not descriptive of the area.\n",
      "\t Tokenized GPT2:O cho ĠR ios Ġis ĠSpanish Ġfor Ġ Ġeight Ġrivers , Ġ Ġbut Ġthis Ġname Ġis Ġnot Ġdescriptive Ġof Ġthe Ġarea .\n",
      "\t Tokenized LLAMA3:O cho ĠR ios Ġis ĠSpanish Ġfor Ġ Ġeight Ġrivers , Ġ Ġbut Ġthis Ġname Ġis Ġnot Ġdescriptive Ġof Ġthe Ġarea .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: \"Ocho Rios\" means eight rivers in Spain's national language.\n",
      "\t Tokenized GPT2:\" O cho ĠR ios \" Ġmeans Ġeight Ġrivers Ġin ĠSpain 's Ġnational Ġlanguage .\n",
      "\t Tokenized LLAMA3:\" O cho ĠR ios \" Ġmeans Ġeight Ġrivers Ġin ĠSpain 's Ġnational Ġlanguage .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: See you Aug. 12, or soon thereafter, we hope.\n",
      "\t Tokenized GPT2:See Ġyou ĠAug . Ġ12 , Ġor Ġsoon Ġthere after , Ġwe Ġhope .\n",
      "\t Tokenized LLAMA3:See Ġyou ĠAug . Ġ 12 , Ġor Ġsoon Ġthere after , Ġwe Ġhope .\n",
      "\t Unique Tokens GPT2: {'Ġ12'}\n",
      "\t Unique Tokens LLAMA3: {'Ġ', '12'}\n",
      "Text 2: The person was going to attend on August 12.\n",
      "\t Tokenized GPT2:The Ġperson Ġwas Ġgoing Ġto Ġattend Ġon ĠAugust Ġ12 .\n",
      "\t Tokenized LLAMA3:The Ġperson Ġwas Ġgoing Ġto Ġattend Ġon ĠAugust Ġ 12 .\n",
      "\t Unique Tokens GPT2: {'Ġ12'}\n",
      "\t Unique Tokens LLAMA3: {'Ġ', '12'}\n",
      "==contradiction==\n",
      "Text 1: The number of steps built down into the interior means that it is unsuitable for the infirm or those with heart problems.\n",
      "\t Tokenized GPT2:The Ġnumber Ġof Ġsteps Ġbuilt Ġdown Ġinto Ġthe Ġinterior Ġmeans Ġthat Ġit Ġis Ġun su itable Ġfor Ġthe Ġinf irm Ġor Ġthose Ġwith Ġheart Ġproblems .\n",
      "\t Tokenized LLAMA3:The Ġnumber Ġof Ġsteps Ġbuilt Ġdown Ġinto Ġthe Ġinterior Ġmeans Ġthat Ġit Ġis Ġun su itable Ġfor Ġthe Ġinf irm Ġor Ġthose Ġwith Ġheart Ġproblems .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The interior is well suited for those with cardiac issues.\n",
      "\t Tokenized GPT2:The Ġinterior Ġis Ġwell Ġsuited Ġfor Ġthose Ġwith Ġcardiac Ġissues .\n",
      "\t Tokenized LLAMA3:The Ġinterior Ġis Ġwell Ġsuited Ġfor Ġthose Ġwith Ġcardiac Ġissues .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: His off-the-cuff style seems amateurish next to Inglis' polished mini-essays.\n",
      "\t Tokenized GPT2:His Ġoff - the - c uff Ġstyle Ġseems Ġamateur ish Ġnext Ġto ĠIn gl is ' Ġpolished Ġmini - ess ays .\n",
      "\t Tokenized LLAMA3:His Ġoff -the -c uff Ġstyle Ġseems Ġamateur ish Ġnext Ġto ĠIn gl is ' Ġpolished Ġmini - ess ays .\n",
      "\t Unique Tokens GPT2: {'c', 'the'}\n",
      "\t Unique Tokens LLAMA3: {'-c', '-the'}\n",
      "Text 2: He didn't look like an amateur \n",
      "\t Tokenized GPT2:He Ġdidn 't Ġlook Ġlike Ġan Ġamateur Ġ\n",
      "\t Tokenized LLAMA3:He Ġdidn 't Ġlook Ġlike Ġan Ġamateur Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The emotional effect is undiminished, and the gory effects are usually horribly creative.\n",
      "\t Tokenized GPT2:The Ġemotional Ġeffect Ġis Ġund imin ished , Ġand Ġthe Ġg ory Ġeffects Ġare Ġusually Ġhorribly Ġcreative .\n",
      "\t Tokenized LLAMA3:The Ġemotional Ġeffect Ġis Ġund imin ished , Ġand Ġthe Ġg ory Ġeffects Ġare Ġusually Ġhorribly Ġcreative .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The emotional effect includes feelings of horror and dismay.\n",
      "\t Tokenized GPT2:The Ġemotional Ġeffect Ġincludes Ġfeelings Ġof Ġhorror Ġand Ġdis may .\n",
      "\t Tokenized LLAMA3:The Ġemotional Ġeffect Ġincludes Ġfeelings Ġof Ġhorror Ġand Ġdis may .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The islands' names refer to the different force winds hitting them, not their topography.\n",
      "\t Tokenized GPT2:The Ġislands ' Ġnames Ġrefer Ġto Ġthe Ġdifferent Ġforce Ġwinds Ġhitting Ġthem , Ġnot Ġtheir Ġtop ography .\n",
      "\t Tokenized LLAMA3:The Ġislands ' Ġnames Ġrefer Ġto Ġthe Ġdifferent Ġforce Ġwinds Ġhitting Ġthem , Ġnot Ġtheir Ġtop ography .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The name of the islands are based on their topography.\n",
      "\t Tokenized GPT2:The Ġname Ġof Ġthe Ġislands Ġare Ġbased Ġon Ġtheir Ġtop ography .\n",
      "\t Tokenized LLAMA3:The Ġname Ġof Ġthe Ġislands Ġare Ġbased Ġon Ġtheir Ġtop ography .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The students' reaction was swift and contentious, as if their feelings had been hurt.\n",
      "\t Tokenized GPT2:The Ġstudents ' Ġreaction Ġwas Ġswift Ġand Ġcontent ious , Ġas Ġif Ġtheir Ġfeelings Ġhad Ġbeen Ġhurt .\n",
      "\t Tokenized LLAMA3:The Ġstudents ' Ġreaction Ġwas Ġswift Ġand Ġcontent ious , Ġas Ġif Ġtheir Ġfeelings Ġhad Ġbeen Ġhurt .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The students reacted with horror.\n",
      "\t Tokenized GPT2:The Ġstudents Ġreacted Ġwith Ġhorror .\n",
      "\t Tokenized LLAMA3:The Ġstudents Ġreacted Ġwith Ġhorror .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: 'Would you like some tea?'\n",
      "\t Tokenized GPT2:' Would Ġyou Ġlike Ġsome Ġtea ?'\n",
      "\t Tokenized LLAMA3:' Would Ġyou Ġlike Ġsome Ġtea ?'\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: DO you want a cup of tea?\n",
      "\t Tokenized GPT2:DO Ġyou Ġwant Ġa Ġcup Ġof Ġtea ?\n",
      "\t Tokenized LLAMA3:DO Ġyou Ġwant Ġa Ġcup Ġof Ġtea ?\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: And you are wrong in condemning it. \n",
      "\t Tokenized GPT2:And Ġyou Ġare Ġwrong Ġin Ġcondem ning Ġit . Ġ\n",
      "\t Tokenized LLAMA3:And Ġyou Ġare Ġwrong Ġin Ġcondem ning Ġit . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: You shouldn't be speaking out against it.\n",
      "\t Tokenized GPT2:You Ġshouldn 't Ġbe Ġspeaking Ġout Ġagainst Ġit .\n",
      "\t Tokenized LLAMA3:You Ġshouldn 't Ġbe Ġspeaking Ġout Ġagainst Ġit .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Alonissos has been settled longer than any other Aegean island, estimated by archaeologists to date from 100,000 b.c. , and was valued by many leaders in classical Greek times.\n",
      "\t Tokenized GPT2:Al on iss os Ġhas Ġbeen Ġsettled Ġlonger Ġthan Ġany Ġother ĠA e ge an Ġisland , Ġestimated Ġby Ġarchae ologists Ġto Ġdate Ġfrom Ġ100 , 000 Ġb . c . Ġ, Ġand Ġwas Ġvalued Ġby Ġmany Ġleaders Ġin Ġclassical ĠGreek Ġtimes .\n",
      "\t Tokenized LLAMA3:Al on iss os Ġhas Ġbeen Ġsettled Ġlonger Ġthan Ġany Ġother ĠA e ge an Ġisland , Ġestimated Ġby Ġarchae ologists Ġto Ġdate Ġfrom Ġ 100 , 000 Ġb .c . Ġ, Ġand Ġwas Ġvalued Ġby Ġmany Ġleaders Ġin Ġclassical ĠGreek Ġtimes .\n",
      "\t Unique Tokens GPT2: {'Ġ100', 'c'}\n",
      "\t Unique Tokens LLAMA3: {'.c', '100', 'Ġ'}\n",
      "Text 2: In the classical Greek era, Alonissos was highly regarded.\n",
      "\t Tokenized GPT2:In Ġthe Ġclassical ĠGreek Ġera , ĠAl on iss os Ġwas Ġhighly Ġregarded .\n",
      "\t Tokenized LLAMA3:In Ġthe Ġclassical ĠGreek Ġera , ĠAl on iss os Ġwas Ġhighly Ġregarded .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: From the corner of his eye he saw Jamus look over the broken mare.\n",
      "\t Tokenized GPT2:From Ġthe Ġcorner Ġof Ġhis Ġeye Ġhe Ġsaw ĠJam us Ġlook Ġover Ġthe Ġbroken Ġm are .\n",
      "\t Tokenized LLAMA3:From Ġthe Ġcorner Ġof Ġhis Ġeye Ġhe Ġsaw ĠJam us Ġlook Ġover Ġthe Ġbroken Ġm are .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Jamus was blinded by the sandstorm.\n",
      "\t Tokenized GPT2:J am us Ġwas Ġblind ed Ġby Ġthe Ġsand storm .\n",
      "\t Tokenized LLAMA3:J am us Ġwas Ġblind ed Ġby Ġthe Ġsand storm .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Under Ferdinand and Isabella, Spain underwent a dramatic transformation.\n",
      "\t Tokenized GPT2:Under ĠFer din and Ġand ĠIs ab ella , ĠSpain Ġunderwent Ġa Ġdramatic Ġtransformation .\n",
      "\t Tokenized LLAMA3:Under ĠFer din and Ġand ĠIs ab ella , ĠSpain Ġunderwent Ġa Ġdramatic Ġtransformation .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Ferdinand started his transformation by emancipating the peasant class.\n",
      "\t Tokenized GPT2:F er din and Ġstarted Ġhis Ġtransformation Ġby Ġem anc ip ating Ġthe Ġpe asant Ġclass .\n",
      "\t Tokenized LLAMA3:F er din and Ġstarted Ġhis Ġtransformation Ġby Ġem anc ip ating Ġthe Ġpe asant Ġclass .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Through the opt-out approach, Texas attorneys contributed $1 million this year, doubling 2001 contributions.\n",
      "\t Tokenized GPT2:Through Ġthe Ġopt - out Ġapproach , ĠTexas Ġattorneys Ġcontributed Ġ$ 1 Ġmillion Ġthis Ġyear , Ġdoub ling Ġ2001 Ġcontributions .\n",
      "\t Tokenized LLAMA3:Through Ġthe Ġopt -out Ġapproach , ĠTexas Ġattorneys Ġcontributed Ġ$ 1 Ġmillion Ġthis Ġyear , Ġdoub ling Ġ 200 1 Ġcontributions .\n",
      "\t Unique Tokens GPT2: {'-', 'Ġ2001', 'out'}\n",
      "\t Unique Tokens LLAMA3: {'200', 'Ġ', '-out'}\n",
      "Text 2: The opt-out approach has never increased how much Texas attorneys can contribute in the last twenty years.\n",
      "\t Tokenized GPT2:The Ġopt - out Ġapproach Ġhas Ġnever Ġincreased Ġhow Ġmuch ĠTexas Ġattorneys Ġcan Ġcontribute Ġin Ġthe Ġlast Ġtwenty Ġyears .\n",
      "\t Tokenized LLAMA3:The Ġopt -out Ġapproach Ġhas Ġnever Ġincreased Ġhow Ġmuch ĠTexas Ġattorneys Ġcan Ġcontribute Ġin Ġthe Ġlast Ġtwenty Ġyears .\n",
      "\t Unique Tokens GPT2: {'-', 'out'}\n",
      "\t Unique Tokens LLAMA3: {'-out'}\n",
      "==entailment==\n",
      "Text 1: Though the two cities remained unlinked by rail, this was about to change quickly.\n",
      "\t Tokenized GPT2:Though Ġthe Ġtwo Ġcities Ġremained Ġun linked Ġby Ġrail , Ġthis Ġwas Ġabout Ġto Ġchange Ġquickly .\n",
      "\t Tokenized LLAMA3:Though Ġthe Ġtwo Ġcities Ġremained Ġun link ed Ġby Ġrail , Ġthis Ġwas Ġabout Ġto Ġchange Ġquickly .\n",
      "\t Unique Tokens GPT2: {'linked'}\n",
      "\t Unique Tokens LLAMA3: {'ed', 'link'}\n",
      "Text 2: The two cities did not have a railway between them.\n",
      "\t Tokenized GPT2:The Ġtwo Ġcities Ġdid Ġnot Ġhave Ġa Ġrailway Ġbetween Ġthem .\n",
      "\t Tokenized LLAMA3:The Ġtwo Ġcities Ġdid Ġnot Ġhave Ġa Ġrailway Ġbetween Ġthem .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The Fray's reputation as a home for hostile, rude, and mean-spirited exchanges suffered a severe beating at the hands of the Reading thread, which was so civilized that participants suggested taking insulin shots afterward.\n",
      "\t Tokenized GPT2:The ĠF ray 's Ġreputation Ġas Ġa Ġhome Ġfor Ġhostile , Ġrude , Ġand Ġmean - sp ir ited Ġexchanges Ġsuffered Ġa Ġsevere Ġbeating Ġat Ġthe Ġhands Ġof Ġthe ĠReading Ġthread , Ġwhich Ġwas Ġso Ġcivil ized Ġthat Ġparticipants Ġsuggested Ġtaking Ġinsulin Ġshots Ġafterward .\n",
      "\t Tokenized LLAMA3:The ĠF ray 's Ġreputation Ġas Ġa Ġhome Ġfor Ġhostile , Ġrude , Ġand Ġmean -s pir ited Ġexchanges Ġsuffered Ġa Ġsevere Ġbeating Ġat Ġthe Ġhands Ġof Ġthe ĠReading Ġthread , Ġwhich Ġwas Ġso Ġcivil ized Ġthat Ġparticipants Ġsuggested Ġtaking Ġinsulin Ġshots Ġafterward .\n",
      "\t Unique Tokens GPT2: {'-', 'sp', 'ir'}\n",
      "\t Unique Tokens LLAMA3: {'pir', '-s'}\n",
      "Text 2: The Fray is almost always a hostile, rude, and mean place.\n",
      "\t Tokenized GPT2:The ĠF ray Ġis Ġalmost Ġalways Ġa Ġhostile , Ġrude , Ġand Ġmean Ġplace .\n",
      "\t Tokenized LLAMA3:The ĠF ray Ġis Ġalmost Ġalways Ġa Ġhostile , Ġrude , Ġand Ġmean Ġplace .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: This site provides information links, tools, and resources developed for the benefit of the audit profession, including audit programs, best practices, and research services.\n",
      "\t Tokenized GPT2:This Ġsite Ġprovides Ġinformation Ġlinks , Ġtools , Ġand Ġresources Ġdeveloped Ġfor Ġthe Ġbenefit Ġof Ġthe Ġaudit Ġprofession , Ġincluding Ġaudit Ġprograms , Ġbest Ġpractices , Ġand Ġresearch Ġservices .\n",
      "\t Tokenized LLAMA3:This Ġsite Ġprovides Ġinformation Ġlinks , Ġtools , Ġand Ġresources Ġdeveloped Ġfor Ġthe Ġbenefit Ġof Ġthe Ġaudit Ġprofession , Ġincluding Ġaudit Ġprograms , Ġbest Ġpractices , Ġand Ġresearch Ġservices .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: This site is a special portal for people who wish to make anonymous complaints about auditors.\n",
      "\t Tokenized GPT2:This Ġsite Ġis Ġa Ġspecial Ġportal Ġfor Ġpeople Ġwho Ġwish Ġto Ġmake Ġanonymous Ġcomplaints Ġabout Ġaud itors .\n",
      "\t Tokenized LLAMA3:This Ġsite Ġis Ġa Ġspecial Ġportal Ġfor Ġpeople Ġwho Ġwish Ġto Ġmake Ġanonymous Ġcomplaints Ġabout Ġaud itors .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: (j) Promotional items a member receives as a consequence of using travel or transportation services procured by the United States or accepted pursuant to 31\n",
      "\t Tokenized GPT2:( j ) ĠProm otional Ġitems Ġa Ġmember Ġreceives Ġas Ġa Ġconsequence Ġof Ġusing Ġtravel Ġor Ġtransportation Ġservices Ġproc ured Ġby Ġthe ĠUnited ĠStates Ġor Ġaccepted Ġpursu ant Ġto Ġ31\n",
      "\t Tokenized LLAMA3:(j ) ĠProm ot ional Ġitems Ġa Ġmember Ġreceives Ġas Ġa Ġconsequence Ġof Ġusing Ġtravel Ġor Ġtransportation Ġservices Ġproc ured Ġby Ġthe ĠUnited ĠStates Ġor Ġaccepted Ġpursu ant Ġto Ġ 31\n",
      "\t Unique Tokens GPT2: {'(', 'otional', 'Ġ31', 'j'}\n",
      "\t Unique Tokens LLAMA3: {'(j', 'Ġ', 'ot', 'ional', '31'}\n",
      "Text 2: A non-member can receive promotional items if they are from Spain and do not travel.\n",
      "\t Tokenized GPT2:A Ġnon - member Ġcan Ġreceive Ġpromot ional Ġitems Ġif Ġthey Ġare Ġfrom ĠSpain Ġand Ġdo Ġnot Ġtravel .\n",
      "\t Tokenized LLAMA3:A Ġnon -m ember Ġcan Ġreceive Ġpromot ional Ġitems Ġif Ġthey Ġare Ġfrom ĠSpain Ġand Ġdo Ġnot Ġtravel .\n",
      "\t Unique Tokens GPT2: {'-', 'member'}\n",
      "\t Unique Tokens LLAMA3: {'ember', '-m'}\n",
      "==contradiction==\n",
      "Text 1: For more sweeping panoramas, you can hike for less than an hour to either summit Petit-Bourg (716 m/2,349 ft) or Pigeon (770 m/2,526 ft).\n",
      "\t Tokenized GPT2:For Ġmore Ġsweeping Ġpan or amas , Ġyou Ġcan Ġhike Ġfor Ġless Ġthan Ġan Ġhour Ġto Ġeither Ġsummit ĠPet it - B our g Ġ( 7 16 Ġm / 2 , 349 Ġft ) Ġor ĠP ige on Ġ( 7 70 Ġm / 2 , 5 26 Ġft ).\n",
      "\t Tokenized LLAMA3:For Ġmore Ġsweeping Ġpan or amas , Ġyou Ġcan Ġhike Ġfor Ġless Ġthan Ġan Ġhour Ġto Ġeither Ġsummit ĠPet it -B our g Ġ( 7 16 Ġm / 2 , 349 Ġft ) Ġor ĠP ige on Ġ( 770 Ġm / 2 , 526 Ġft ).\n",
      "\t Unique Tokens GPT2: {'26', 'B', '70', '-', '5'}\n",
      "\t Unique Tokens LLAMA3: {'-B', '770', '526'}\n",
      "Text 2: For more sweeping panoramas, you can go swimming in the canyon.\n",
      "\t Tokenized GPT2:For Ġmore Ġsweeping Ġpan or amas , Ġyou Ġcan Ġgo Ġswimming Ġin Ġthe Ġcan y on .\n",
      "\t Tokenized LLAMA3:For Ġmore Ġsweeping Ġpan or amas , Ġyou Ġcan Ġgo Ġswimming Ġin Ġthe Ġcan y on .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Thus, the imbalance in the volume of mail exchanged magnifies the effect of the relatively higher rates in these countries.\n",
      "\t Tokenized GPT2:Thus , Ġthe Ġim balance Ġin Ġthe Ġvolume Ġof Ġmail Ġexchanged Ġmagn ifies Ġthe Ġeffect Ġof Ġthe Ġrelatively Ġhigher Ġrates Ġin Ġthese Ġcountries .\n",
      "\t Tokenized LLAMA3:Thus , Ġthe Ġim balance Ġin Ġthe Ġvolume Ġof Ġmail Ġexchanged Ġmagn ifies Ġthe Ġeffect Ġof Ġthe Ġrelatively Ġhigher Ġrates Ġin Ġthese Ġcountries .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: There is more mail coming in than going out.\n",
      "\t Tokenized GPT2:There Ġis Ġmore Ġmail Ġcoming Ġin Ġthan Ġgoing Ġout .\n",
      "\t Tokenized LLAMA3:There Ġis Ġmore Ġmail Ġcoming Ġin Ġthan Ġgoing Ġout .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: We shouldn't have been here as soon as this even, if it hadn't been for the fact that there was a smart doctor on the spot, who gave us the tip through the Coroner. \n",
      "\t Tokenized GPT2:We Ġshouldn 't Ġhave Ġbeen Ġhere Ġas Ġsoon Ġas Ġthis Ġeven , Ġif Ġit Ġhadn 't Ġbeen Ġfor Ġthe Ġfact Ġthat Ġthere Ġwas Ġa Ġsmart Ġdoctor Ġon Ġthe Ġspot , Ġwho Ġgave Ġus Ġthe Ġtip Ġthrough Ġthe ĠCor oner . Ġ\n",
      "\t Tokenized LLAMA3:We Ġshouldn 't Ġhave Ġbeen Ġhere Ġas Ġsoon Ġas Ġthis Ġeven , Ġif Ġit Ġhadn 't Ġbeen Ġfor Ġthe Ġfact Ġthat Ġthere Ġwas Ġa Ġsmart Ġdoctor Ġon Ġthe Ġspot , Ġwho Ġgave Ġus Ġthe Ġtip Ġthrough Ġthe ĠCor oner . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The doctor and the Coroner decided not to give us the tip.\n",
      "\t Tokenized GPT2:The Ġdoctor Ġand Ġthe ĠCor oner Ġdecided Ġnot Ġto Ġgive Ġus Ġthe Ġtip .\n",
      "\t Tokenized LLAMA3:The Ġdoctor Ġand Ġthe ĠCor oner Ġdecided Ġnot Ġto Ġgive Ġus Ġthe Ġtip .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Instead of indulging in the usual teary nostalgia about baseball (that means you, Ken Burns), Will considered it as a craft, explaining exactly why a manager calls a hit-and-run now and not on the next pitch, how a pitcher sets up his fastball, why a shortstop moves in a step for one kind of double \n",
      "\t Tokenized GPT2:Instead Ġof Ġindul ging Ġin Ġthe Ġusual Ġte ary Ġnostalg ia Ġabout Ġbaseball Ġ( that Ġmeans Ġyou , ĠKen ĠBurn s ), ĠWill Ġconsidered Ġit Ġas Ġa Ġcraft , Ġexplaining Ġexactly Ġwhy Ġa Ġmanager Ġcalls Ġa Ġhit - and - run Ġnow Ġand Ġnot Ġon Ġthe Ġnext Ġpitch , Ġhow Ġa Ġpitcher Ġsets Ġup Ġhis Ġfast ball , Ġwhy Ġa Ġshort stop Ġmoves Ġin Ġa Ġstep Ġfor Ġone Ġkind Ġof Ġdouble Ġ\n",
      "\t Tokenized LLAMA3:Instead Ġof Ġindul ging Ġin Ġthe Ġusual Ġte ary Ġnostalg ia Ġabout Ġbaseball Ġ( that Ġmeans Ġyou , ĠKen ĠBurn s ), ĠWill Ġconsidered Ġit Ġas Ġa Ġcraft , Ġexplaining Ġexactly Ġwhy Ġa Ġmanager Ġcalls Ġa Ġhit -and -run Ġnow Ġand Ġnot Ġon Ġthe Ġnext Ġpitch , Ġhow Ġa Ġpitcher Ġsets Ġup Ġhis Ġfast ball , Ġwhy Ġa Ġshort stop Ġmoves Ġin Ġa Ġstep Ġfor Ġone Ġkind Ġof Ġdouble Ġ\n",
      "\t Unique Tokens GPT2: {'-', 'and', 'run'}\n",
      "\t Unique Tokens LLAMA3: {'-run', '-and'}\n",
      "Text 2: Will is such an expert in explaining the details of baseball. \n",
      "\t Tokenized GPT2:Will Ġis Ġsuch Ġan Ġexpert Ġin Ġexplaining Ġthe Ġdetails Ġof Ġbaseball . Ġ\n",
      "\t Tokenized LLAMA3:Will Ġis Ġsuch Ġan Ġexpert Ġin Ġexplaining Ġthe Ġdetails Ġof Ġbaseball . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Second, Clinton hasn't used the bully pulpit to speak out against drug use nearly as often as his two predecessors did.\n",
      "\t Tokenized GPT2:Second , ĠClinton Ġhasn 't Ġused Ġthe Ġbully Ġpul pit Ġto Ġspeak Ġout Ġagainst Ġdrug Ġuse Ġnearly Ġas Ġoften Ġas Ġhis Ġtwo Ġpredecess ors Ġdid .\n",
      "\t Tokenized LLAMA3:Second , ĠClinton Ġhasn 't Ġused Ġthe Ġbully Ġpul pit Ġto Ġspeak Ġout Ġagainst Ġdrug Ġuse Ġnearly Ġas Ġoften Ġas Ġhis Ġtwo Ġpredecess ors Ġdid .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Hillary Clinton used the bully pulpit to speak out against drug use.\n",
      "\t Tokenized GPT2:H illary ĠClinton Ġused Ġthe Ġbully Ġpul pit Ġto Ġspeak Ġout Ġagainst Ġdrug Ġuse .\n",
      "\t Tokenized LLAMA3:H illary ĠClinton Ġused Ġthe Ġbully Ġpul pit Ġto Ġspeak Ġout Ġagainst Ġdrug Ġuse .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: A group of guys went out for a drink after work, and sitting at the bar was a real  a 6 foot blonde with a fabulous face and figure to match.\n",
      "\t Tokenized GPT2:A Ġgroup Ġof Ġguys Ġwent Ġout Ġfor Ġa Ġdrink Ġafter Ġwork , Ġand Ġsitting Ġat Ġthe Ġbar Ġwas Ġa Ġreal Ġ Ġa Ġ6 Ġfoot Ġblonde Ġwith Ġa Ġfabulous Ġface Ġand Ġfigure Ġto Ġmatch .\n",
      "\t Tokenized LLAMA3:A Ġgroup Ġof Ġguys Ġwent Ġout Ġfor Ġa Ġdrink Ġafter Ġwork , Ġand Ġsitting Ġat Ġthe Ġbar Ġwas Ġa Ġreal Ġ Ġa Ġ 6 Ġfoot Ġblonde Ġwith Ġa Ġfabulous Ġface Ġand Ġfigure Ġto Ġmatch .\n",
      "\t Unique Tokens GPT2: {'Ġ6'}\n",
      "\t Unique Tokens LLAMA3: {'6'}\n",
      "Text 2: A stunning six foot blonde woman sat at the bar with the men after work. \n",
      "\t Tokenized GPT2:A Ġstunning Ġsix Ġfoot Ġblonde Ġwoman Ġsat Ġat Ġthe Ġbar Ġwith Ġthe Ġmen Ġafter Ġwork . Ġ\n",
      "\t Tokenized LLAMA3:A Ġstunning Ġsix Ġfoot Ġblonde Ġwoman Ġsat Ġat Ġthe Ġbar Ġwith Ġthe Ġmen Ġafter Ġwork . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The data would be presented as required supplementary stewardship information accompanying the consolidated financial statements of the Federal Government but not in individual reports of its component units.\n",
      "\t Tokenized GPT2:The Ġdata Ġwould Ġbe Ġpresented Ġas Ġrequired Ġsupplement ary Ġstew ard ship Ġinformation Ġaccompanying Ġthe Ġconsolid ated Ġfinancial Ġstatements Ġof Ġthe ĠFederal ĠGovernment Ġbut Ġnot Ġin Ġindividual Ġreports Ġof Ġits Ġcomponent Ġunits .\n",
      "\t Tokenized LLAMA3:The Ġdata Ġwould Ġbe Ġpresented Ġas Ġrequired Ġsupplement ary Ġstew ard ship Ġinformation Ġaccompanying Ġthe Ġconsolid ated Ġfinancial Ġstatements Ġof Ġthe ĠFederal ĠGovernment Ġbut Ġnot Ġin Ġindividual Ġreports Ġof Ġits Ġcomponent Ġunits .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The data would be included in individual reports concerning the constituent units of the federal government.\n",
      "\t Tokenized GPT2:The Ġdata Ġwould Ġbe Ġincluded Ġin Ġindividual Ġreports Ġconcerning Ġthe Ġconstitu ent Ġunits Ġof Ġthe Ġfederal Ġgovernment .\n",
      "\t Tokenized LLAMA3:The Ġdata Ġwould Ġbe Ġincluded Ġin Ġindividual Ġreports Ġconcerning Ġthe Ġconstitu ent Ġunits Ġof Ġthe Ġfederal Ġgovernment .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: There are many homes built into the hillsides; some have been converted into art galleries and shops selling collectibles.\n",
      "\t Tokenized GPT2:There Ġare Ġmany Ġhomes Ġbuilt Ġinto Ġthe Ġhills ides ; Ġsome Ġhave Ġbeen Ġconverted Ġinto Ġart Ġgall eries Ġand Ġshops Ġselling Ġcollect ibles .\n",
      "\t Tokenized LLAMA3:There Ġare Ġmany Ġhomes Ġbuilt Ġinto Ġthe Ġhills ides ; Ġsome Ġhave Ġbeen Ġconverted Ġinto Ġart Ġgall eries Ġand Ġshops Ġselling Ġcollect ibles .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The remaining homes that have not been converted are still home to many locals.\n",
      "\t Tokenized GPT2:The Ġremaining Ġhomes Ġthat Ġhave Ġnot Ġbeen Ġconverted Ġare Ġstill Ġhome Ġto Ġmany Ġlocals .\n",
      "\t Tokenized LLAMA3:The Ġremaining Ġhomes Ġthat Ġhave Ġnot Ġbeen Ġconverted Ġare Ġstill Ġhome Ġto Ġmany Ġlocals .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: i think the rate of processing is just about uh reached the rate of housing anyway so keep the keep the normal as it is can't upset the system very much\n",
      "\t Tokenized GPT2:i Ġthink Ġthe Ġrate Ġof Ġprocessing Ġis Ġjust Ġabout Ġuh Ġreached Ġthe Ġrate Ġof Ġhousing Ġanyway Ġso Ġkeep Ġthe Ġkeep Ġthe Ġnormal Ġas Ġit Ġis Ġcan 't Ġupset Ġthe Ġsystem Ġvery Ġmuch\n",
      "\t Tokenized LLAMA3:i Ġthink Ġthe Ġrate Ġof Ġprocessing Ġis Ġjust Ġabout Ġuh Ġreached Ġthe Ġrate Ġof Ġhousing Ġanyway Ġso Ġkeep Ġthe Ġkeep Ġthe Ġnormal Ġas Ġit Ġis Ġcan 't Ġupset Ġthe Ġsystem Ġvery Ġmuch\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The rate of processing is way higher than the rate of housing to upset the system.\n",
      "\t Tokenized GPT2:The Ġrate Ġof Ġprocessing Ġis Ġway Ġhigher Ġthan Ġthe Ġrate Ġof Ġhousing Ġto Ġupset Ġthe Ġsystem .\n",
      "\t Tokenized LLAMA3:The Ġrate Ġof Ġprocessing Ġis Ġway Ġhigher Ġthan Ġthe Ġrate Ġof Ġhousing Ġto Ġupset Ġthe Ġsystem .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Another alternative is that our heroes were pursuing the noble goal of academics everywhere--tenure.\n",
      "\t Tokenized GPT2:Another Ġalternative Ġis Ġthat Ġour Ġheroes Ġwere Ġpursuing Ġthe Ġnoble Ġgoal Ġof Ġacad emics Ġeverywhere -- ten ure .\n",
      "\t Tokenized LLAMA3:Another Ġalternative Ġis Ġthat Ġour Ġheroes Ġwere Ġpursuing Ġthe Ġnoble Ġgoal Ġof Ġacad emics Ġeverywhere -- ten ure .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Academics are a good goal to strive for. \n",
      "\t Tokenized GPT2:Ac ad emics Ġare Ġa Ġgood Ġgoal Ġto Ġstrive Ġfor . Ġ\n",
      "\t Tokenized LLAMA3:Ac ad emics Ġare Ġa Ġgood Ġgoal Ġto Ġstrive Ġfor . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: appropriate agency representatives, help resolve\n",
      "\t Tokenized GPT2:appropriate Ġagency Ġrepresentatives , Ġhelp Ġresolve\n",
      "\t Tokenized LLAMA3:appropriate Ġagency Ġrepresentatives , Ġhelp Ġresolve\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: the right agency employees, help fix\n",
      "\t Tokenized GPT2:the Ġright Ġagency Ġemployees , Ġhelp Ġfix\n",
      "\t Tokenized LLAMA3:the Ġright Ġagency Ġemployees , Ġhelp Ġfix\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: You wonder whether he could win a general election coming out of the right lane of the Democratic Party.\n",
      "\t Tokenized GPT2:You Ġwonder Ġwhether Ġhe Ġcould Ġwin Ġa Ġgeneral Ġelection Ġcoming Ġout Ġof Ġthe Ġright Ġlane Ġof Ġthe ĠDemocratic ĠParty .\n",
      "\t Tokenized LLAMA3:You Ġwonder Ġwhether Ġhe Ġcould Ġwin Ġa Ġgeneral Ġelection Ġcoming Ġout Ġof Ġthe Ġright Ġlane Ġof Ġthe ĠDemocratic ĠParty .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He might run in a general election for governor while he is a conservative Democrat.\n",
      "\t Tokenized GPT2:He Ġmight Ġrun Ġin Ġa Ġgeneral Ġelection Ġfor Ġgovernor Ġwhile Ġhe Ġis Ġa Ġconservative ĠDemocrat .\n",
      "\t Tokenized LLAMA3:He Ġmight Ġrun Ġin Ġa Ġgeneral Ġelection Ġfor Ġgovernor Ġwhile Ġhe Ġis Ġa Ġconservative ĠDemocrat .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: apparently apparently the appraisers likes it because our taxes sure is high  isn't it it really is\n",
      "\t Tokenized GPT2:app arently Ġapparently Ġthe Ġapp ra isers Ġlikes Ġit Ġbecause Ġour Ġtaxes Ġsure Ġis Ġhigh Ġ Ġisn 't Ġit Ġit Ġreally Ġis\n",
      "\t Tokenized LLAMA3:app arently Ġapparently Ġthe Ġapp ra isers Ġlikes Ġit Ġbecause Ġour Ġtaxes Ġsure Ġis Ġhigh Ġ Ġisn 't Ġit Ġit Ġreally Ġis\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The appraiser did not like it one bit.\n",
      "\t Tokenized GPT2:The Ġapp ra iser Ġdid Ġnot Ġlike Ġit Ġone Ġbit .\n",
      "\t Tokenized LLAMA3:The Ġapp ra iser Ġdid Ġnot Ġlike Ġit Ġone Ġbit .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Perhaps North Africans and eastern Europeans peopled the Ligurian coast, while the Adriatic and south may have been settled by people from the Balkans and Asia Minor.\n",
      "\t Tokenized GPT2:Perhaps ĠNorth ĠAfr icans Ġand Ġeastern ĠEurope ans Ġpe op led Ġthe ĠL ig ur ian Ġcoast , Ġwhile Ġthe ĠAd ri atic Ġand Ġsouth Ġmay Ġhave Ġbeen Ġsettled Ġby Ġpeople Ġfrom Ġthe ĠB alk ans Ġand ĠAsia ĠMin or .\n",
      "\t Tokenized LLAMA3:Perhaps ĠNorth ĠAfr icans Ġand Ġeastern ĠEurope ans Ġpe op led Ġthe ĠL ig ur ian Ġcoast , Ġwhile Ġthe ĠAd ri atic Ġand Ġsouth Ġmay Ġhave Ġbeen Ġsettled Ġby Ġpeople Ġfrom Ġthe ĠB alk ans Ġand ĠAsia ĠMin or .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The people had no complaints after settling their new lands.\n",
      "\t Tokenized GPT2:The Ġpeople Ġhad Ġno Ġcomplaints Ġafter Ġsettling Ġtheir Ġnew Ġlands .\n",
      "\t Tokenized LLAMA3:The Ġpeople Ġhad Ġno Ġcomplaints Ġafter Ġsettling Ġtheir Ġnew Ġlands .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Waterloo.\n",
      "\t Tokenized GPT2:Water l oo .\n",
      "\t Tokenized LLAMA3:Water l oo .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The defeat of Napoleon.\n",
      "\t Tokenized GPT2:The Ġdefeat Ġof ĠNapoleon .\n",
      "\t Tokenized LLAMA3:The Ġdefeat Ġof ĠNap oleon .\n",
      "\t Unique Tokens GPT2: {'ĠNapoleon'}\n",
      "\t Unique Tokens LLAMA3: {'oleon', 'ĠNap'}\n",
      "==entailment==\n",
      "Text 1: i was trying to think about some of my favorite people that i liked in music and they're none of them are recent right\n",
      "\t Tokenized GPT2:i Ġwas Ġtrying Ġto Ġthink Ġabout Ġsome Ġof Ġmy Ġfavorite Ġpeople Ġthat Ġi Ġliked Ġin Ġmusic Ġand Ġthey 're Ġnone Ġof Ġthem Ġare Ġrecent Ġright\n",
      "\t Tokenized LLAMA3:i Ġwas Ġtrying Ġto Ġthink Ġabout Ġsome Ġof Ġmy Ġfavorite Ġpeople Ġthat Ġi Ġliked Ġin Ġmusic Ġand Ġthey 're Ġnone Ġof Ġthem Ġare Ġrecent Ġright\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I enjoy older music. \n",
      "\t Tokenized GPT2:I Ġenjoy Ġolder Ġmusic . Ġ\n",
      "\t Tokenized LLAMA3:I Ġenjoy Ġolder Ġmusic . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The streets are crammed with vendors selling shrine offerings of sweets, curds, and coconut, as well as garlands and holy images.\n",
      "\t Tokenized GPT2:The Ġstreets Ġare Ġcr ammed Ġwith Ġvendors Ġselling Ġshr ine Ġofferings Ġof Ġswe ets , Ġcur ds , Ġand Ġcoconut , Ġas Ġwell Ġas Ġgar lands Ġand Ġholy Ġimages .\n",
      "\t Tokenized LLAMA3:The Ġstreets Ġare Ġcr ammed Ġwith Ġvendors Ġselling Ġshr ine Ġofferings Ġof Ġswe ets , Ġcur ds , Ġand Ġcoconut , Ġas Ġwell Ġas Ġgar lands Ġand Ġholy Ġimages .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Vendors have lined the streets with torches and fires.\n",
      "\t Tokenized GPT2:V end ors Ġhave Ġlined Ġthe Ġstreets Ġwith Ġtor ches Ġand Ġfires .\n",
      "\t Tokenized LLAMA3:V end ors Ġhave Ġlined Ġthe Ġstreets Ġwith Ġtor ches Ġand Ġfires .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: He was waiting for the Scotland Yard men. \n",
      "\t Tokenized GPT2:He Ġwas Ġwaiting Ġfor Ġthe ĠScotland ĠY ard Ġmen . Ġ\n",
      "\t Tokenized LLAMA3:He Ġwas Ġwaiting Ġfor Ġthe ĠScotland ĠY ard Ġmen . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The Scotland Yard men were coming.\n",
      "\t Tokenized GPT2:The ĠScotland ĠY ard Ġmen Ġwere Ġcoming .\n",
      "\t Tokenized LLAMA3:The ĠScotland ĠY ard Ġmen Ġwere Ġcoming .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: yeah okay yeah those games are fun to watch you you you watch those games\n",
      "\t Tokenized GPT2:yeah Ġokay Ġyeah Ġthose Ġgames Ġare Ġfun Ġto Ġwatch Ġyou Ġyou Ġyou Ġwatch Ġthose Ġgames\n",
      "\t Tokenized LLAMA3:yeah Ġokay Ġyeah Ġthose Ġgames Ġare Ġfun Ġto Ġwatch Ġyou Ġyou Ġyou Ġwatch Ġthose Ġgames\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Those games are a lot of fun.\n",
      "\t Tokenized GPT2:Those Ġgames Ġare Ġa Ġlot Ġof Ġfun .\n",
      "\t Tokenized LLAMA3:Those Ġgames Ġare Ġa Ġlot Ġof Ġfun .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Where lies the real Japan?\n",
      "\t Tokenized GPT2:Where Ġlies Ġthe Ġreal ĠJapan ?\n",
      "\t Tokenized LLAMA3:Where Ġlies Ġthe Ġreal ĠJapan ?\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The real Japan can be found.\n",
      "\t Tokenized GPT2:The Ġreal ĠJapan Ġcan Ġbe Ġfound .\n",
      "\t Tokenized LLAMA3:The Ġreal ĠJapan Ġcan Ġbe Ġfound .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The notable thing for me about the Left Behind series--beside the fact that few in the secular media have noticed that millions of Americans are busy reading books warning about the imminence of one-world government, mass death, and the return of the Messiah, is that all the Jewish characters are Ch\n",
      "\t Tokenized GPT2:The Ġnotable Ġthing Ġfor Ġme Ġabout Ġthe ĠLeft ĠBe hind Ġseries -- bes ide Ġthe Ġfact Ġthat Ġfew Ġin Ġthe Ġsecular Ġmedia Ġhave Ġnoticed Ġthat Ġmillions Ġof ĠAmericans Ġare Ġbusy Ġreading Ġbooks Ġwarning Ġabout Ġthe Ġim min ence Ġof Ġone - world Ġgovernment , Ġmass Ġdeath , Ġand Ġthe Ġreturn Ġof Ġthe ĠMess iah , Ġis Ġthat Ġall Ġthe ĠJewish Ġcharacters Ġare ĠCh\n",
      "\t Tokenized LLAMA3:The Ġnotable Ġthing Ġfor Ġme Ġabout Ġthe ĠLeft ĠBe hind Ġseries -- bes ide Ġthe Ġfact Ġthat Ġfew Ġin Ġthe Ġsecular Ġmedia Ġhave Ġnoticed Ġthat Ġmillions Ġof ĠAmericans Ġare Ġbusy Ġreading Ġbooks Ġwarning Ġabout Ġthe Ġim min ence Ġof Ġone -world Ġgovernment , Ġmass Ġdeath , Ġand Ġthe Ġreturn Ġof Ġthe ĠMess iah , Ġis Ġthat Ġall Ġthe ĠJewish Ġcharacters Ġare ĠCh\n",
      "\t Unique Tokens GPT2: {'world', '-'}\n",
      "\t Unique Tokens LLAMA3: {'-world'}\n",
      "Text 2: The Left Behind series is about people converting to Christianity.\n",
      "\t Tokenized GPT2:The ĠLeft ĠBe hind Ġseries Ġis Ġabout Ġpeople Ġconverting Ġto ĠChristianity .\n",
      "\t Tokenized LLAMA3:The ĠLeft ĠBe hind Ġseries Ġis Ġabout Ġpeople Ġconverting Ġto ĠChristianity .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The event is the definition of a crowd pleaser, replete with appearances by the Rockettes, the Mormon Tabernacle Choir, and Santa Claus (the act isn't entirely without bite; there's also a very funny moment involving a heart attack).\n",
      "\t Tokenized GPT2:The Ġevent Ġis Ġthe Ġdefinition Ġof Ġa Ġcrowd Ġple aser , Ġre plete Ġwith Ġappearances Ġby Ġthe ĠR ocket tes , Ġthe ĠMorm on ĠTab ern acle ĠCh oir , Ġand ĠSanta ĠCl aus Ġ( the Ġact Ġisn 't Ġentirely Ġwithout Ġbite ; Ġthere 's Ġalso Ġa Ġvery Ġfunny Ġmoment Ġinvolving Ġa Ġheart Ġattack ).\n",
      "\t Tokenized LLAMA3:The Ġevent Ġis Ġthe Ġdefinition Ġof Ġa Ġcrowd Ġple aser , Ġre plete Ġwith Ġappearances Ġby Ġthe ĠR ocket tes , Ġthe ĠMorm on ĠTab ern acle ĠCh oir , Ġand ĠSanta ĠCl aus Ġ( the Ġact Ġisn 't Ġentirely Ġwithout Ġbite ; Ġthere 's Ġalso Ġa Ġvery Ġfunny Ġmoment Ġinvolving Ġa Ġheart Ġattack ).\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Expectations are low for the event on the back of Santa Claus's absence due to health concerns.\n",
      "\t Tokenized GPT2:Ex pect ations Ġare Ġlow Ġfor Ġthe Ġevent Ġon Ġthe Ġback Ġof ĠSanta ĠCl aus 's Ġabsence Ġdue Ġto Ġhealth Ġconcerns .\n",
      "\t Tokenized LLAMA3:Ex pect ations Ġare Ġlow Ġfor Ġthe Ġevent Ġon Ġthe Ġback Ġof ĠSanta ĠCl aus 's Ġabsence Ġdue Ġto Ġhealth Ġconcerns .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Then Shuman claims that Linux provides no graphical user interface.\n",
      "\t Tokenized GPT2:Then ĠSh uman Ġclaims Ġthat ĠLinux Ġprovides Ġno Ġgraphical Ġuser Ġinterface .\n",
      "\t Tokenized LLAMA3:Then ĠSh uman Ġclaims Ġthat ĠLinux Ġprovides Ġno Ġgraphical Ġuser Ġinterface .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: They made accusations about the platform.\n",
      "\t Tokenized GPT2:They Ġmade Ġaccusations Ġabout Ġthe Ġplatform .\n",
      "\t Tokenized LLAMA3:They Ġmade Ġaccusations Ġabout Ġthe Ġplatform .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Founded in 1979, AFFIRM's members include information resource management professionals within the federal, academic, and industry sectors.\n",
      "\t Tokenized GPT2:Found ed Ġin Ġ1979 , ĠA FF IR M 's Ġmembers Ġinclude Ġinformation Ġresource Ġmanagement Ġprofessionals Ġwithin Ġthe Ġfederal , Ġacademic , Ġand Ġindustry Ġsectors .\n",
      "\t Tokenized LLAMA3:F ounded Ġin Ġ 197 9 , ĠA FF IR M 's Ġmembers Ġinclude Ġinformation Ġresource Ġmanagement Ġprofessionals Ġwithin Ġthe Ġfederal , Ġacademic , Ġand Ġindustry Ġsectors .\n",
      "\t Unique Tokens GPT2: {'Ġ1979', 'ed', 'Found'}\n",
      "\t Unique Tokens LLAMA3: {'Ġ', '197', 'ounded', '9', 'F'}\n",
      "Text 2: AFFIRM was founded in the early 2000s.\n",
      "\t Tokenized GPT2:A FF IR M Ġwas Ġfounded Ġin Ġthe Ġearly Ġ2000 s .\n",
      "\t Tokenized LLAMA3:A FF IR M Ġwas Ġfounded Ġin Ġthe Ġearly Ġ 200 0 s .\n",
      "\t Unique Tokens GPT2: {'Ġ2000'}\n",
      "\t Unique Tokens LLAMA3: {'0', 'Ġ', '200'}\n",
      "==contradiction==\n",
      "Text 1: You will learn later that the person who usually poured out Mrs. Inglethorp's medicine was always extremely careful not to shake the bottle, but to leave the sediment at the bottom of it undisturbed. \n",
      "\t Tokenized GPT2:You Ġwill Ġlearn Ġlater Ġthat Ġthe Ġperson Ġwho Ġusually Ġpoured Ġout ĠMrs . ĠIn gle th orp 's Ġmedicine Ġwas Ġalways Ġextremely Ġcareful Ġnot Ġto Ġshake Ġthe Ġbottle , Ġbut Ġto Ġleave Ġthe Ġsed iment Ġat Ġthe Ġbottom Ġof Ġit Ġund ist ur bed . Ġ\n",
      "\t Tokenized LLAMA3:You Ġwill Ġlearn Ġlater Ġthat Ġthe Ġperson Ġwho Ġusually Ġpoured Ġout ĠMrs . ĠIn gle th orp 's Ġmedicine Ġwas Ġalways Ġextremely Ġcareful Ġnot Ġto Ġshake Ġthe Ġbottle , Ġbut Ġto Ġleave Ġthe Ġsed iment Ġat Ġthe Ġbottom Ġof Ġit Ġund ist ur bed . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The person pouring Mrs. Inglethorp's medicine was always very careful to shake the bottle. \n",
      "\t Tokenized GPT2:The Ġperson Ġpouring ĠMrs . ĠIn gle th orp 's Ġmedicine Ġwas Ġalways Ġvery Ġcareful Ġto Ġshake Ġthe Ġbottle . Ġ\n",
      "\t Tokenized LLAMA3:The Ġperson Ġpouring ĠMrs . ĠIn gle th orp 's Ġmedicine Ġwas Ġalways Ġvery Ġcareful Ġto Ġshake Ġthe Ġbottle . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Major journeys from one part of the country to another, say, from Milan to Rome or down to Naples, is most enjoyed by train buffs and travelers with plenty of time, patience, and curiosity.\n",
      "\t Tokenized GPT2:Major Ġjour neys Ġfrom Ġone Ġpart Ġof Ġthe Ġcountry Ġto Ġanother , Ġsay , Ġfrom ĠMil an Ġto ĠRome Ġor Ġdown Ġto ĠNap les , Ġis Ġmost Ġenjoyed Ġby Ġtrain Ġbuff s Ġand Ġtravelers Ġwith Ġplenty Ġof Ġtime , Ġpatience , Ġand Ġcuriosity .\n",
      "\t Tokenized LLAMA3:Major Ġjour neys Ġfrom Ġone Ġpart Ġof Ġthe Ġcountry Ġto Ġanother , Ġsay , Ġfrom ĠMil an Ġto ĠRome Ġor Ġdown Ġto ĠNap les , Ġis Ġmost Ġenjoyed Ġby Ġtrain Ġbuff s Ġand Ġtravel ers Ġwith Ġplenty Ġof Ġtime , Ġpatience , Ġand Ġcuriosity .\n",
      "\t Unique Tokens GPT2: {'Ġtravelers'}\n",
      "\t Unique Tokens LLAMA3: {'Ġtravel', 'ers'}\n",
      "Text 2: Train travel in Italy is generally quick, making it a practical option for impatient travelers.\n",
      "\t Tokenized GPT2:Tr ain Ġtravel Ġin ĠItaly Ġis Ġgenerally Ġquick , Ġmaking Ġit Ġa Ġpractical Ġoption Ġfor Ġimpatient Ġtravelers .\n",
      "\t Tokenized LLAMA3:Tr ain Ġtravel Ġin ĠItaly Ġis Ġgenerally Ġquick , Ġmaking Ġit Ġa Ġpractical Ġoption Ġfor Ġimpatient Ġtravel ers .\n",
      "\t Unique Tokens GPT2: {'Ġtravelers'}\n",
      "\t Unique Tokens LLAMA3: {'ers'}\n",
      "==entailment==\n",
      "Text 1: Lucy screamed, I've got to know.\n",
      "\t Tokenized GPT2:Lu cy Ġscreamed , ĠI 've Ġgot Ġto Ġknow .\n",
      "\t Tokenized LLAMA3:Lu cy Ġscreamed , ĠI 've Ġgot Ġto Ġknow .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Lucy wanted to know.\n",
      "\t Tokenized GPT2:Lu cy Ġwanted Ġto Ġknow .\n",
      "\t Tokenized LLAMA3:Lu cy Ġwanted Ġto Ġknow .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Are you sure we should take him down there?' Greuze asked Natalia.\n",
      "\t Tokenized GPT2:Are Ġyou Ġsure Ġwe Ġshould Ġtake Ġhim Ġdown Ġthere ?' ĠGre u ze Ġasked ĠNatal ia .\n",
      "\t Tokenized LLAMA3:Are Ġyou Ġsure Ġwe Ġshould Ġtake Ġhim Ġdown Ġthere ?' ĠGre u ze Ġasked ĠNatal ia .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Natalia was certain it was alright to bring him down there, and told Greuze that no matter what, he must end up down there. \n",
      "\t Tokenized GPT2:N atal ia Ġwas Ġcertain Ġit Ġwas Ġalright Ġto Ġbring Ġhim Ġdown Ġthere , Ġand Ġtold ĠGre u ze Ġthat Ġno Ġmatter Ġwhat , Ġhe Ġmust Ġend Ġup Ġdown Ġthere . Ġ\n",
      "\t Tokenized LLAMA3:N atal ia Ġwas Ġcertain Ġit Ġwas Ġalright Ġto Ġbring Ġhim Ġdown Ġthere , Ġand Ġtold ĠGre u ze Ġthat Ġno Ġmatter Ġwhat , Ġhe Ġmust Ġend Ġup Ġdown Ġthere . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: yeah it's true it is in in fact i have a friend of mine that moved to North Carolina she's um an emergency room nurse she does the operating room\n",
      "\t Tokenized GPT2:yeah Ġit 's Ġtrue Ġit Ġis Ġin Ġin Ġfact Ġi Ġhave Ġa Ġfriend Ġof Ġmine Ġthat Ġmoved Ġto ĠNorth ĠCarolina Ġshe 's Ġum Ġan Ġemergency Ġroom Ġnurse Ġshe Ġdoes Ġthe Ġoperating Ġroom\n",
      "\t Tokenized LLAMA3:yeah Ġit 's Ġtrue Ġit Ġis Ġin Ġin Ġfact Ġi Ġhave Ġa Ġfriend Ġof Ġmine Ġthat Ġmoved Ġto ĠNorth ĠCarolina Ġshe 's Ġum Ġan Ġemergency Ġroom Ġnurse Ġshe Ġdoes Ġthe Ġoperating Ġroom\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: My friend moved to NC to take a job as a nurse in a trauma unit at a hospital emergency room.\n",
      "\t Tokenized GPT2:My Ġfriend Ġmoved Ġto ĠNC Ġto Ġtake Ġa Ġjob Ġas Ġa Ġnurse Ġin Ġa Ġtrauma Ġunit Ġat Ġa Ġhospital Ġemergency Ġroom .\n",
      "\t Tokenized LLAMA3:My Ġfriend Ġmoved Ġto ĠNC Ġto Ġtake Ġa Ġjob Ġas Ġa Ġnurse Ġin Ġa Ġtrauma Ġunit Ġat Ġa Ġhospital Ġemergency Ġroom .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: or just get out and walk uh or even jog a little although i don't do that regularly but Washington's a great place to do that\n",
      "\t Tokenized GPT2:or Ġjust Ġget Ġout Ġand Ġwalk Ġuh Ġor Ġeven Ġjog Ġa Ġlittle Ġalthough Ġi Ġdon 't Ġdo Ġthat Ġregularly Ġbut ĠWashington 's Ġa Ġgreat Ġplace Ġto Ġdo Ġthat\n",
      "\t Tokenized LLAMA3:or Ġjust Ġget Ġout Ġand Ġwalk Ġuh Ġor Ġeven Ġjog Ġa Ġlittle Ġalthough Ġi Ġdon 't Ġdo Ġthat Ġregularly Ġbut ĠWashington 's Ġa Ġgreat Ġplace Ġto Ġdo Ġthat\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: \"I regularly go for a walk or a jog at Washington's.\"\n",
      "\t Tokenized GPT2:\" I Ġregularly Ġgo Ġfor Ġa Ġwalk Ġor Ġa Ġjog Ġat ĠWashington 's .\"\n",
      "\t Tokenized LLAMA3:\"I Ġregularly Ġgo Ġfor Ġa Ġwalk Ġor Ġa Ġjog Ġat ĠWashington 's .\"\n",
      "\t Unique Tokens GPT2: {'I', '\"'}\n",
      "\t Unique Tokens LLAMA3: {'\"I'}\n",
      "==entailment==\n",
      "Text 1: well that's pretty typical though uh i don't uh i don't guess it's going to be any much different uh than than it has been in the past so i expect uh July and August we'll see our  or uh share of hundred degree days\n",
      "\t Tokenized GPT2:well Ġthat 's Ġpretty Ġtypical Ġthough Ġuh Ġi Ġdon 't Ġuh Ġi Ġdon 't Ġguess Ġit 's Ġgoing Ġto Ġbe Ġany Ġmuch Ġdifferent Ġuh Ġthan Ġthan Ġit Ġhas Ġbeen Ġin Ġthe Ġpast Ġso Ġi Ġexpect Ġuh ĠJuly Ġand ĠAugust Ġwe 'll Ġsee Ġour Ġ Ġor Ġuh Ġshare Ġof Ġhundred Ġdegree Ġdays\n",
      "\t Tokenized LLAMA3:well Ġthat 's Ġpretty Ġtypical Ġthough Ġuh Ġi Ġdon 't Ġuh Ġi Ġdon 't Ġguess Ġit 's Ġgoing Ġto Ġbe Ġany Ġmuch Ġdifferent Ġuh Ġthan Ġthan Ġit Ġhas Ġbeen Ġin Ġthe Ġpast Ġso Ġi Ġexpect Ġuh ĠJuly Ġand ĠAugust Ġwe 'll Ġsee Ġour Ġ Ġor Ġuh Ġshare Ġof Ġhundred Ġdegree Ġdays\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It's pretty normal to have a few hundred degree days out here.\n",
      "\t Tokenized GPT2:It 's Ġpretty Ġnormal Ġto Ġhave Ġa Ġfew Ġhundred Ġdegree Ġdays Ġout Ġhere .\n",
      "\t Tokenized LLAMA3:It 's Ġpretty Ġnormal Ġto Ġhave Ġa Ġfew Ġhundred Ġdegree Ġdays Ġout Ġhere .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: That couldn't happen in a sane world, either.\n",
      "\t Tokenized GPT2:That Ġcouldn 't Ġhappen Ġin Ġa Ġsane Ġworld , Ġeither .\n",
      "\t Tokenized LLAMA3:That Ġcouldn 't Ġhappen Ġin Ġa Ġsane Ġworld , Ġeither .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: That could not happen in a world that wasn't insane, either.\n",
      "\t Tokenized GPT2:That Ġcould Ġnot Ġhappen Ġin Ġa Ġworld Ġthat Ġwasn 't Ġinsane , Ġeither .\n",
      "\t Tokenized LLAMA3:That Ġcould Ġnot Ġhappen Ġin Ġa Ġworld Ġthat Ġwasn 't Ġinsane , Ġeither .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: And far, far away- lying still on the tracks- was the back of the train.\n",
      "\t Tokenized GPT2:And Ġfar , Ġfar Ġaway - Ġlying Ġstill Ġon Ġthe Ġtracks - Ġwas Ġthe Ġback Ġof Ġthe Ġtrain .\n",
      "\t Tokenized LLAMA3:And Ġfar , Ġfar Ġaway - Ġlying Ġstill Ġon Ġthe Ġtracks - Ġwas Ġthe Ġback Ġof Ġthe Ġtrain .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The train was speeding along the track. \n",
      "\t Tokenized GPT2:The Ġtrain Ġwas Ġspeeding Ġalong Ġthe Ġtrack . Ġ\n",
      "\t Tokenized LLAMA3:The Ġtrain Ġwas Ġspeeding Ġalong Ġthe Ġtrack . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Explanation building is the inverse  starting with the observations, the evaluator develops a picture of what is happening and why.\n",
      "\t Tokenized GPT2:Ex planation Ġbuilding Ġis Ġthe Ġinverse Ġ Ġstarting Ġwith Ġthe Ġobservations , Ġthe Ġevalu ator Ġdevelops Ġa Ġpicture Ġof Ġwhat Ġis Ġhappening Ġand Ġwhy .\n",
      "\t Tokenized LLAMA3:Ex plan ation Ġbuilding Ġis Ġthe Ġinverse Ġ Ġstarting Ġwith Ġthe Ġobservations , Ġthe Ġevalu ator Ġdevelops Ġa Ġpicture Ġof Ġwhat Ġis Ġhappening Ġand Ġwhy .\n",
      "\t Unique Tokens GPT2: {'planation'}\n",
      "\t Unique Tokens LLAMA3: {'ation', 'plan'}\n",
      "Text 2: Explanation building is going to be the standard approach in the future.\n",
      "\t Tokenized GPT2:Ex planation Ġbuilding Ġis Ġgoing Ġto Ġbe Ġthe Ġstandard Ġapproach Ġin Ġthe Ġfuture .\n",
      "\t Tokenized LLAMA3:Ex plan ation Ġbuilding Ġis Ġgoing Ġto Ġbe Ġthe Ġstandard Ġapproach Ġin Ġthe Ġfuture .\n",
      "\t Unique Tokens GPT2: {'planation'}\n",
      "\t Unique Tokens LLAMA3: {'ation', 'plan'}\n",
      "==neutral==\n",
      "Text 1: Why bother to sacrifice your lives for dirt farmers and slavers?\n",
      "\t Tokenized GPT2:Why Ġbother Ġto Ġsacrifice Ġyour Ġlives Ġfor Ġdirt Ġfarmers Ġand Ġsl a vers ?\n",
      "\t Tokenized LLAMA3:Why Ġbother Ġto Ġsacrifice Ġyour Ġlives Ġfor Ġdirt Ġfarmers Ġand Ġsla vers ?\n",
      "\t Unique Tokens GPT2: {'a', 'Ġsl'}\n",
      "\t Unique Tokens LLAMA3: {'Ġsla'}\n",
      "Text 2: People sacrifice their lives for farmers and slaves.\n",
      "\t Tokenized GPT2:People Ġsacrifice Ġtheir Ġlives Ġfor Ġfarmers Ġand Ġslaves .\n",
      "\t Tokenized LLAMA3:People Ġsacrifice Ġtheir Ġlives Ġfor Ġfarmers Ġand Ġslaves .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: 11 These departures permit them take advantage of the lower cost of living as well as to be reunited with their spouses and children.\n",
      "\t Tokenized GPT2:11 ĠThese Ġdepart ures Ġpermit Ġthem Ġtake Ġadvantage Ġof Ġthe Ġlower Ġcost Ġof Ġliving Ġas Ġwell Ġas Ġto Ġbe Ġreun ited Ġwith Ġtheir Ġsp ouses Ġand Ġchildren .\n",
      "\t Tokenized LLAMA3:11 ĠThese Ġdepart ures Ġpermit Ġthem Ġtake Ġadvantage Ġof Ġthe Ġlower Ġcost Ġof Ġliving Ġas Ġwell Ġas Ġto Ġbe Ġreun ited Ġwith Ġtheir Ġsp ouses Ġand Ġchildren .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The departures help them take advantage of the high cost of living in other areas.\n",
      "\t Tokenized GPT2:The Ġdepart ures Ġhelp Ġthem Ġtake Ġadvantage Ġof Ġthe Ġhigh Ġcost Ġof Ġliving Ġin Ġother Ġareas .\n",
      "\t Tokenized LLAMA3:The Ġdepart ures Ġhelp Ġthem Ġtake Ġadvantage Ġof Ġthe Ġhigh Ġcost Ġof Ġliving Ġin Ġother Ġareas .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: 36 AC usage nationally for mercury control from power plants should be roughly proportional to the total MWe of coal-fired facilities that are equipped with the technology (this assumes an average capacity factor of 85 percent and other assumptions of Tables 4-4 and 4-5).\n",
      "\t Tokenized GPT2:36 ĠAC Ġusage Ġnation ally Ġfor Ġmer cury Ġcontrol Ġfrom Ġpower Ġplants Ġshould Ġbe Ġroughly Ġproportional Ġto Ġthe Ġtotal ĠM We Ġof Ġcoal - f ired Ġfacilities Ġthat Ġare Ġequipped Ġwith Ġthe Ġtechnology Ġ( this Ġassumes Ġan Ġaverage Ġcapacity Ġfactor Ġof Ġ85 Ġpercent Ġand Ġother Ġassumptions Ġof ĠT ables Ġ4 - 4 Ġand Ġ4 - 5 ).\n",
      "\t Tokenized LLAMA3:36 ĠAC Ġusage Ġnation ally Ġfor Ġmer cury Ġcontrol Ġfrom Ġpower Ġplants Ġshould Ġbe Ġroughly Ġproportional Ġto Ġthe Ġtotal ĠM We Ġof Ġcoal -f ired Ġfacilities Ġthat Ġare Ġequipped Ġwith Ġthe Ġtechnology Ġ( this Ġassumes Ġan Ġaverage Ġcapacity Ġfactor Ġof Ġ 85 Ġpercent Ġand Ġother Ġassumptions Ġof ĠT ables Ġ 4 - 4 Ġand Ġ 4 - 5 ).\n",
      "\t Unique Tokens GPT2: {'f', 'Ġ85', 'Ġ4'}\n",
      "\t Unique Tokens LLAMA3: {'-f', 'Ġ', '85'}\n",
      "Text 2: Power plants' mercury control AC usage is higher than total MWe from coal facilities.\n",
      "\t Tokenized GPT2:Power Ġplants ' Ġmer cury Ġcontrol ĠAC Ġusage Ġis Ġhigher Ġthan Ġtotal ĠM We Ġfrom Ġcoal Ġfacilities .\n",
      "\t Tokenized LLAMA3:Power Ġplants ' Ġmer cury Ġcontrol ĠAC Ġusage Ġis Ġhigher Ġthan Ġtotal ĠM We Ġfrom Ġcoal Ġfacilities .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Long ago--or away, or whatever--there was a world called Thar?? and another called Erath.\n",
      "\t Tokenized GPT2:Long Ġago -- or Ġaway , Ġor Ġwhatever -- there Ġwas Ġa Ġworld Ġcalled ĠTh ar ?? Ġand Ġanother Ġcalled ĠEr ath .\n",
      "\t Tokenized LLAMA3:Long Ġago -- or Ġaway , Ġor Ġwhatever -- there Ġwas Ġa Ġworld Ġcalled ĠTh ar ?? Ġand Ġanother Ġcalled ĠEr ath .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Erath is the only world that has ever existed.\n",
      "\t Tokenized GPT2:Er ath Ġis Ġthe Ġonly Ġworld Ġthat Ġhas Ġever Ġexisted .\n",
      "\t Tokenized LLAMA3:Er ath Ġis Ġthe Ġonly Ġworld Ġthat Ġhas Ġever Ġexisted .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Don't expect to be swinging much after midnight, even in towns.\n",
      "\t Tokenized GPT2:Don 't Ġexpect Ġto Ġbe Ġswinging Ġmuch Ġafter Ġmidnight , Ġeven Ġin Ġtowns .\n",
      "\t Tokenized LLAMA3:Don 't Ġexpect Ġto Ġbe Ġswinging Ġmuch Ġafter Ġmidnight , Ġeven Ġin Ġtowns .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Things stay open all night because it's a place to party.\n",
      "\t Tokenized GPT2:Things Ġstay Ġopen Ġall Ġnight Ġbecause Ġit 's Ġa Ġplace Ġto Ġparty .\n",
      "\t Tokenized LLAMA3:Things Ġstay Ġopen Ġall Ġnight Ġbecause Ġit 's Ġa Ġplace Ġto Ġparty .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: For ideological free-marketeers (like myself), theories like Smith and Wright's can be intellectually jarring.\n",
      "\t Tokenized GPT2:For Ġide ological Ġfree - market e ers Ġ( like Ġmyself ), Ġtheories Ġlike ĠSmith Ġand ĠWright 's Ġcan Ġbe Ġintellect ually Ġj arring .\n",
      "\t Tokenized LLAMA3:For Ġide ological Ġfree -mark ete ers Ġ( like Ġmyself ), Ġtheories Ġlike ĠSmith Ġand ĠWright 's Ġcan Ġbe Ġintellect ually Ġj arring .\n",
      "\t Unique Tokens GPT2: {'market', 'e', '-'}\n",
      "\t Unique Tokens LLAMA3: {'ete', '-mark'}\n",
      "Text 2: I can appreciate their position even if it does contradict my opinions. \n",
      "\t Tokenized GPT2:I Ġcan Ġappreciate Ġtheir Ġposition Ġeven Ġif Ġit Ġdoes Ġcontradict Ġmy Ġopinions . Ġ\n",
      "\t Tokenized LLAMA3:I Ġcan Ġappreciate Ġtheir Ġposition Ġeven Ġif Ġit Ġdoes Ġcontradict Ġmy Ġopinions . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: We are also advocating enhanced reporting in connection with key federal performance and projection information.\n",
      "\t Tokenized GPT2:We Ġare Ġalso Ġadvoc ating Ġenhanced Ġreporting Ġin Ġconnection Ġwith Ġkey Ġfederal Ġperformance Ġand Ġprojection Ġinformation .\n",
      "\t Tokenized LLAMA3:We Ġare Ġalso Ġadvoc ating Ġenhanced Ġreporting Ġin Ġconnection Ġwith Ġkey Ġfederal Ġperformance Ġand Ġprojection Ġinformation .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: We think reporting should be enhanced as it has a vital connection with projection statistics.\n",
      "\t Tokenized GPT2:We Ġthink Ġreporting Ġshould Ġbe Ġenhanced Ġas Ġit Ġhas Ġa Ġvital Ġconnection Ġwith Ġprojection Ġstatistics .\n",
      "\t Tokenized LLAMA3:We Ġthink Ġreporting Ġshould Ġbe Ġenhanced Ġas Ġit Ġhas Ġa Ġvital Ġconnection Ġwith Ġprojection Ġstatistics .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: She was 96 just turning away when she heard a piercing whistle, and the faithful Albert came running from the building to join her.\n",
      "\t Tokenized GPT2:She Ġwas Ġ96 Ġjust Ġturning Ġaway Ġwhen Ġshe Ġheard Ġa Ġpiercing Ġwhistle , Ġand Ġthe Ġfaithful ĠAlbert Ġcame Ġrunning Ġfrom Ġthe Ġbuilding Ġto Ġjoin Ġher .\n",
      "\t Tokenized LLAMA3:She Ġwas Ġ 96 Ġjust Ġturning Ġaway Ġwhen Ġshe Ġheard Ġa Ġpiercing Ġwhistle , Ġand Ġthe Ġfaithful ĠAlbert Ġcame Ġrunning Ġfrom Ġthe Ġbuilding Ġto Ġjoin Ġher .\n",
      "\t Unique Tokens GPT2: {'Ġ96'}\n",
      "\t Unique Tokens LLAMA3: {'96', 'Ġ'}\n",
      "Text 2: She was an old lady who was out for a walk when she heard a noise. \n",
      "\t Tokenized GPT2:She Ġwas Ġan Ġold Ġlady Ġwho Ġwas Ġout Ġfor Ġa Ġwalk Ġwhen Ġshe Ġheard Ġa Ġnoise . Ġ\n",
      "\t Tokenized LLAMA3:She Ġwas Ġan Ġold Ġlady Ġwho Ġwas Ġout Ġfor Ġa Ġwalk Ġwhen Ġshe Ġheard Ġa Ġnoise . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: oh yeah all all mine are uh purebreds so i keep them in\n",
      "\t Tokenized GPT2:oh Ġyeah Ġall Ġall Ġmine Ġare Ġuh Ġpure b red s Ġso Ġi Ġkeep Ġthem Ġin\n",
      "\t Tokenized LLAMA3:oh Ġyeah Ġall Ġall Ġmine Ġare Ġuh Ġpure b red s Ġso Ġi Ġkeep Ġthem Ġin\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: mine are all mixed breeds\n",
      "\t Tokenized GPT2:mine Ġare Ġall Ġmixed Ġbre eds\n",
      "\t Tokenized LLAMA3:mine Ġare Ġall Ġmixed Ġbre eds\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: aChange in personal saving depends on how much of the $4,000 IRA contribution represents new saving.\n",
      "\t Tokenized GPT2:a Change Ġin Ġpersonal Ġsaving Ġdepends Ġon Ġhow Ġmuch Ġof Ġthe Ġ$ 4 , 000 ĠI RA Ġcontribution Ġrepresents Ġnew Ġsaving .\n",
      "\t Tokenized LLAMA3:a Change Ġin Ġpersonal Ġsaving Ġdepends Ġon Ġhow Ġmuch Ġof Ġthe Ġ$ 4 , 000 ĠI RA Ġcontribution Ġrepresents Ġnew Ġsaving .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Personal savings need to be set by the IRA\n",
      "\t Tokenized GPT2:Person al Ġsavings Ġneed Ġto Ġbe Ġset Ġby Ġthe ĠI RA\n",
      "\t Tokenized LLAMA3:Person al Ġsavings Ġneed Ġto Ġbe Ġset Ġby Ġthe ĠI RA\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: The National Football League semifinals are set.\n",
      "\t Tokenized GPT2:The ĠNational ĠFootball ĠLeague Ġsem if inals Ġare Ġset .\n",
      "\t Tokenized LLAMA3:The ĠNational ĠFootball ĠLeague Ġsem if inals Ġare Ġset .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The dates for the semifinals have been determined.\n",
      "\t Tokenized GPT2:The Ġdates Ġfor Ġthe Ġsem if inals Ġhave Ġbeen Ġdetermined .\n",
      "\t Tokenized LLAMA3:The Ġdates Ġfor Ġthe Ġsem if inals Ġhave Ġbeen Ġdetermined .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Crosethe Rue de Rivoli to the Palais-Royal, built for Car?­di?­nal Richelieu as his Paris residence in 1639, and originally named Palais-Cardinal.\n",
      "\t Tokenized GPT2:C ro set he ĠR ue Ġde ĠR iv oli Ġto Ġthe ĠPal ais - R oyal , Ġbuilt Ġfor ĠCar ? ÂŃ di ? ÂŃ nal ĠRic hel ieu Ġas Ġhis ĠParis Ġresidence Ġin Ġ16 39 , Ġand Ġoriginally Ġnamed ĠPal ais - Card inal .\n",
      "\t Tokenized LLAMA3:C ro set he ĠR ue Ġde ĠR iv oli Ġto Ġthe ĠPal ais -R oyal , Ġbuilt Ġfor ĠCar ? ÂŃ di ? ÂŃ nal ĠRic hel ieu Ġas Ġhis ĠParis Ġresidence Ġin Ġ 163 9 , Ġand Ġoriginally Ġnamed ĠPal ais -C ard inal .\n",
      "\t Unique Tokens GPT2: {'R', 'Card', '39', 'Ġ16', '-'}\n",
      "\t Unique Tokens LLAMA3: {'ard', 'Ġ', '-R', '163', '9', '-C'}\n",
      "Text 2: The Crosethe Rue De Rivoli was built for Cardinal Richelieu to live in.\n",
      "\t Tokenized GPT2:The ĠCro set he ĠR ue ĠDe ĠR iv oli Ġwas Ġbuilt Ġfor ĠCard inal ĠRic hel ieu Ġto Ġlive Ġin .\n",
      "\t Tokenized LLAMA3:The ĠCro set he ĠR ue ĠDe ĠR iv oli Ġwas Ġbuilt Ġfor ĠCard inal ĠRic hel ieu Ġto Ġlive Ġin .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: and uh you know it's like they they consider that but it would be the same way here you know it's like if if you had to do it you know you have a big sign i'm sorry i don't get paid you know\n",
      "\t Tokenized GPT2:and Ġuh Ġyou Ġknow Ġit 's Ġlike Ġthey Ġthey Ġconsider Ġthat Ġbut Ġit Ġwould Ġbe Ġthe Ġsame Ġway Ġhere Ġyou Ġknow Ġit 's Ġlike Ġif Ġif Ġyou Ġhad Ġto Ġdo Ġit Ġyou Ġknow Ġyou Ġhave Ġa Ġbig Ġsign Ġi 'm Ġsorry Ġi Ġdon 't Ġget Ġpaid Ġyou Ġknow\n",
      "\t Tokenized LLAMA3:and Ġuh Ġyou Ġknow Ġit 's Ġlike Ġthey Ġthey Ġconsider Ġthat Ġbut Ġit Ġwould Ġbe Ġthe Ġsame Ġway Ġhere Ġyou Ġknow Ġit 's Ġlike Ġif Ġif Ġyou Ġhad Ġto Ġdo Ġit Ġyou Ġknow Ġyou Ġhave Ġa Ġbig Ġsign Ġi 'm Ġsorry Ġi Ġdon 't Ġget Ġpaid Ġyou Ġknow\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I am paid well, even if it's here and the same.\n",
      "\t Tokenized GPT2:I Ġam Ġpaid Ġwell , Ġeven Ġif Ġit 's Ġhere Ġand Ġthe Ġsame .\n",
      "\t Tokenized LLAMA3:I Ġam Ġpaid Ġwell , Ġeven Ġif Ġit 's Ġhere Ġand Ġthe Ġsame .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Part of the reason for the difference in pieces per possible delivery may be due to the fact that five percent of possible residential deliveries are businesses, and it is thought, but not known, that a lesser percentage of possible deliveries on rural routes are businesses.\n",
      "\t Tokenized GPT2:Part Ġof Ġthe Ġreason Ġfor Ġthe Ġdifference Ġin Ġpieces Ġper Ġpossible Ġdelivery Ġmay Ġbe Ġdue Ġto Ġthe Ġfact Ġthat Ġfive Ġpercent Ġof Ġpossible Ġresidential Ġdeliver ies Ġare Ġbusinesses , Ġand Ġit Ġis Ġthought , Ġbut Ġnot Ġknown , Ġthat Ġa Ġlesser Ġpercentage Ġof Ġpossible Ġdeliver ies Ġon Ġrural Ġroutes Ġare Ġbusinesses .\n",
      "\t Tokenized LLAMA3:Part Ġof Ġthe Ġreason Ġfor Ġthe Ġdifference Ġin Ġpieces Ġper Ġpossible Ġdelivery Ġmay Ġbe Ġdue Ġto Ġthe Ġfact Ġthat Ġfive Ġpercent Ġof Ġpossible Ġresidential Ġdeliver ies Ġare Ġbusinesses , Ġand Ġit Ġis Ġthought , Ġbut Ġnot Ġknown , Ġthat Ġa Ġlesser Ġpercentage Ġof Ġpossible Ġdeliver ies Ġon Ġrural Ġroutes Ġare Ġbusinesses .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: We all know that the reason for a lesser percentage of possible deliveries on rural routes being businesses, is because of the fact that people prefer living in cities rather than rural areas.\n",
      "\t Tokenized GPT2:We Ġall Ġknow Ġthat Ġthe Ġreason Ġfor Ġa Ġlesser Ġpercentage Ġof Ġpossible Ġdeliver ies Ġon Ġrural Ġroutes Ġbeing Ġbusinesses , Ġis Ġbecause Ġof Ġthe Ġfact Ġthat Ġpeople Ġprefer Ġliving Ġin Ġcities Ġrather Ġthan Ġrural Ġareas .\n",
      "\t Tokenized LLAMA3:We Ġall Ġknow Ġthat Ġthe Ġreason Ġfor Ġa Ġlesser Ġpercentage Ġof Ġpossible Ġdeliver ies Ġon Ġrural Ġroutes Ġbeing Ġbusinesses , Ġis Ġbecause Ġof Ġthe Ġfact Ġthat Ġpeople Ġprefer Ġliving Ġin Ġcities Ġrather Ġthan Ġrural Ġareas .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: At the least, he was hired in an attempt to influence administration China policy.\n",
      "\t Tokenized GPT2:At Ġthe Ġleast , Ġhe Ġwas Ġhired Ġin Ġan Ġattempt Ġto Ġinfluence Ġadministration ĠChina Ġpolicy .\n",
      "\t Tokenized LLAMA3:At Ġthe Ġleast , Ġhe Ġwas Ġhired Ġin Ġan Ġattempt Ġto Ġinfluence Ġadministration ĠChina Ġpolicy .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He was hired to influence the British economy.\n",
      "\t Tokenized GPT2:He Ġwas Ġhired Ġto Ġinfluence Ġthe ĠBritish Ġeconomy .\n",
      "\t Tokenized LLAMA3:He Ġwas Ġhired Ġto Ġinfluence Ġthe ĠBritish Ġeconomy .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: At Gatehouse, in Kent.\n",
      "\t Tokenized GPT2:At ĠGate house , Ġin ĠKent .\n",
      "\t Tokenized LLAMA3:At ĠGate house , Ġin ĠKent .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It's in a tent out by the Hundred Acre Woods.\n",
      "\t Tokenized GPT2:It 's Ġin Ġa Ġtent Ġout Ġby Ġthe ĠH undred ĠA cre ĠWoods .\n",
      "\t Tokenized LLAMA3:It 's Ġin Ġa Ġtent Ġout Ġby Ġthe ĠH undred ĠA cre ĠWoods .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: you want to punch the button and go\n",
      "\t Tokenized GPT2:you Ġwant Ġto Ġpunch Ġthe Ġbutton Ġand Ġgo\n",
      "\t Tokenized LLAMA3:you Ġwant Ġto Ġpunch Ġthe Ġbutton Ġand Ġgo\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: You should start going before punching the button.\n",
      "\t Tokenized GPT2:You Ġshould Ġstart Ġgoing Ġbefore Ġpunching Ġthe Ġbutton .\n",
      "\t Tokenized LLAMA3:You Ġshould Ġstart Ġgoing Ġbefore Ġpunch ing Ġthe Ġbutton .\n",
      "\t Unique Tokens GPT2: {'Ġpunching'}\n",
      "\t Unique Tokens LLAMA3: {'Ġpunch', 'ing'}\n",
      "==neutral==\n",
      "Text 1: The island's burgeoning economic significance propelled population growth, and by the middle of the 15th century Madeira was home to 800 families.\n",
      "\t Tokenized GPT2:The Ġisland 's Ġbur geon ing Ġeconomic Ġsignificance Ġpro pelled Ġpopulation Ġgrowth , Ġand Ġby Ġthe Ġmiddle Ġof Ġthe Ġ15 th Ġcentury ĠMade ira Ġwas Ġhome Ġto Ġ800 Ġfamilies .\n",
      "\t Tokenized LLAMA3:The Ġisland 's Ġbur geon ing Ġeconomic Ġsignificance Ġpro pelled Ġpopulation Ġgrowth , Ġand Ġby Ġthe Ġmiddle Ġof Ġthe Ġ 15 th Ġcentury ĠMade ira Ġwas Ġhome Ġto Ġ 800 Ġfamilies .\n",
      "\t Unique Tokens GPT2: {'Ġ800', 'Ġ15'}\n",
      "\t Unique Tokens LLAMA3: {'800', '15', 'Ġ'}\n",
      "Text 2: The population of Madeira was devastated by illness in 1475.\n",
      "\t Tokenized GPT2:The Ġpopulation Ġof ĠMade ira Ġwas Ġdevastated Ġby Ġillness Ġin Ġ14 75 .\n",
      "\t Tokenized LLAMA3:The Ġpopulation Ġof ĠMade ira Ġwas Ġdevastated Ġby Ġillness Ġin Ġ 147 5 .\n",
      "\t Unique Tokens GPT2: {'Ġ14', '75'}\n",
      "\t Unique Tokens LLAMA3: {'5', '147', 'Ġ'}\n",
      "==entailment==\n",
      "Text 1: A fine Crusader arch leads down a dimly-lit broad stairway to the dark subterranean Church of the Assumption, a Greek Orthodox church.\n",
      "\t Tokenized GPT2:A Ġfine ĠCr us ader Ġarch Ġleads Ġdown Ġa Ġdim ly - lit Ġbroad Ġstair way Ġto Ġthe Ġdark Ġsub ter r anean ĠChurch Ġof Ġthe ĠAss um ption , Ġa ĠGreek ĠOrth odox Ġchurch .\n",
      "\t Tokenized LLAMA3:A Ġfine ĠCr us ader Ġarch Ġleads Ġdown Ġa Ġdim ly -l it Ġbroad Ġstair way Ġto Ġthe Ġdark Ġsub ter r anean ĠChurch Ġof Ġthe ĠAss um ption , Ġa ĠGreek ĠOrth odox Ġchurch .\n",
      "\t Unique Tokens GPT2: {'-', 'lit'}\n",
      "\t Unique Tokens LLAMA3: {'it', '-l'}\n",
      "Text 2: The Church of the Assumption is located underground through a Crusader arch.\n",
      "\t Tokenized GPT2:The ĠChurch Ġof Ġthe ĠAss um ption Ġis Ġlocated Ġunderground Ġthrough Ġa ĠCr us ader Ġarch .\n",
      "\t Tokenized LLAMA3:The ĠChurch Ġof Ġthe ĠAss um ption Ġis Ġlocated Ġunderground Ġthrough Ġa ĠCr us ader Ġarch .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: um-hum they keep you entertained they sure do we have a uh my wife's uh mother is uh oh about  seventy seven i guess she really gets a thrill when we go over to see her and bring the dog i think she's more happy to see the dog than she is us\n",
      "\t Tokenized GPT2:um - hum Ġthey Ġkeep Ġyou Ġentertained Ġthey Ġsure Ġdo Ġwe Ġhave Ġa Ġuh Ġmy Ġwife 's Ġuh Ġmother Ġis Ġuh Ġoh Ġabout Ġ Ġsevent y Ġseven Ġi Ġguess Ġshe Ġreally Ġgets Ġa Ġthrill Ġwhen Ġwe Ġgo Ġover Ġto Ġsee Ġher Ġand Ġbring Ġthe Ġdog Ġi Ġthink Ġshe 's Ġmore Ġhappy Ġto Ġsee Ġthe Ġdog Ġthan Ġshe Ġis Ġus\n",
      "\t Tokenized LLAMA3:um -h um Ġthey Ġkeep Ġyou Ġentertained Ġthey Ġsure Ġdo Ġwe Ġhave Ġa Ġuh Ġmy Ġwife 's Ġuh Ġmother Ġis Ġuh Ġoh Ġabout Ġ Ġsevent y Ġseven Ġi Ġguess Ġshe Ġreally Ġgets Ġa Ġthrill Ġwhen Ġwe Ġgo Ġover Ġto Ġsee Ġher Ġand Ġbring Ġthe Ġdog Ġi Ġthink Ġshe 's Ġmore Ġhappy Ġto Ġsee Ġthe Ġdog Ġthan Ġshe Ġis Ġus\n",
      "\t Unique Tokens GPT2: {'-', 'hum'}\n",
      "\t Unique Tokens LLAMA3: {'-h'}\n",
      "Text 2: The dog cheers up my wife's mother. \n",
      "\t Tokenized GPT2:The Ġdog Ġcheers Ġup Ġmy Ġwife 's Ġmother . Ġ\n",
      "\t Tokenized LLAMA3:The Ġdog Ġcheers Ġup Ġmy Ġwife 's Ġmother . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Pat Buchanan followed immediately behind, handing out smallpox-infected blankets and bottles of whiskey.\n",
      "\t Tokenized GPT2:Pat ĠB uch anan Ġfollowed Ġimmediately Ġbehind , Ġhanding Ġout Ġsmall p ox - infect ed Ġblankets Ġand Ġbottles Ġof Ġwhiskey .\n",
      "\t Tokenized LLAMA3:Pat ĠB uch anan Ġfollowed Ġimmediately Ġbehind , Ġhanding Ġout Ġsmall p ox -in fected Ġblankets Ġand Ġbottles Ġof Ġwhiskey .\n",
      "\t Unique Tokens GPT2: {'-', 'ed', 'infect'}\n",
      "\t Unique Tokens LLAMA3: {'fected', '-in'}\n",
      "Text 2: Pat Buchanan led the group.\n",
      "\t Tokenized GPT2:Pat ĠB uch anan Ġled Ġthe Ġgroup .\n",
      "\t Tokenized LLAMA3:Pat ĠB uch anan Ġled Ġthe Ġgroup .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: For their part, family-planning organizations and the Clinton administration seem equally adamant.\n",
      "\t Tokenized GPT2:For Ġtheir Ġpart , Ġfamily - plan ning Ġorganizations Ġand Ġthe ĠClinton Ġadministration Ġseem Ġequally Ġadam ant .\n",
      "\t Tokenized LLAMA3:For Ġtheir Ġpart , Ġfamily - plan ning Ġorganizations Ġand Ġthe ĠClinton Ġadministration Ġseem Ġequally Ġadam ant .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: family-planning organizations agree with the Clinton administration about certain things.\n",
      "\t Tokenized GPT2:family - plan ning Ġorganizations Ġagree Ġwith Ġthe ĠClinton Ġadministration Ġabout Ġcertain Ġthings .\n",
      "\t Tokenized LLAMA3:family - plan ning Ġorganizations Ġagree Ġwith Ġthe ĠClinton Ġadministration Ġabout Ġcertain Ġthings .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: The Weekly Standard argues that America should back Lee with words now and, if necessary, military force later, but the Washington Post reports that the U.S. envoys will pressure him to back down.\n",
      "\t Tokenized GPT2:The ĠWeekly ĠStandard Ġargues Ġthat ĠAmerica Ġshould Ġback ĠLee Ġwith Ġwords Ġnow Ġand , Ġif Ġnecessary , Ġmilitary Ġforce Ġlater , Ġbut Ġthe ĠWashington ĠPost Ġreports Ġthat Ġthe ĠU . S . Ġenv oys Ġwill Ġpressure Ġhim Ġto Ġback Ġdown .\n",
      "\t Tokenized LLAMA3:The ĠWeekly ĠStandard Ġargues Ġthat ĠAmerica Ġshould Ġback ĠLee Ġwith Ġwords Ġnow Ġand , Ġif Ġnecessary , Ġmilitary Ġforce Ġlater , Ġbut Ġthe ĠWashington ĠPost Ġreports Ġthat Ġthe ĠU .S . Ġenv oys Ġwill Ġpressure Ġhim Ġto Ġback Ġdown .\n",
      "\t Unique Tokens GPT2: {'S'}\n",
      "\t Unique Tokens LLAMA3: {'.S'}\n",
      "Text 2: The Weekly Standard and Washington Post have opposing views on how the U.S. will approach Lee.\n",
      "\t Tokenized GPT2:The ĠWeekly ĠStandard Ġand ĠWashington ĠPost Ġhave Ġopposing Ġviews Ġon Ġhow Ġthe ĠU . S . Ġwill Ġapproach ĠLee .\n",
      "\t Tokenized LLAMA3:The ĠWeekly ĠStandard Ġand ĠWashington ĠPost Ġhave Ġopposing Ġviews Ġon Ġhow Ġthe ĠU .S . Ġwill Ġapproach ĠLee .\n",
      "\t Unique Tokens GPT2: {'S'}\n",
      "\t Unique Tokens LLAMA3: {'.S'}\n",
      "==neutral==\n",
      "Text 1: So, as he and Tipper walked out, my friend and I were right behind them, and I took the opportunity to say hello and reintroduce myself--as a journalist, I might add--and we chatted about the movie for a few minutes.\n",
      "\t Tokenized GPT2:So , Ġas Ġhe Ġand ĠT ipper Ġwalked Ġout , Ġmy Ġfriend Ġand ĠI Ġwere Ġright Ġbehind Ġthem , Ġand ĠI Ġtook Ġthe Ġopportunity Ġto Ġsay Ġhello Ġand Ġre introdu ce Ġmyself -- as Ġa Ġjournalist , ĠI Ġmight Ġadd -- and Ġwe Ġchat ted Ġabout Ġthe Ġmovie Ġfor Ġa Ġfew Ġminutes .\n",
      "\t Tokenized LLAMA3:So , Ġas Ġhe Ġand ĠT ipper Ġwalked Ġout , Ġmy Ġfriend Ġand ĠI Ġwere Ġright Ġbehind Ġthem , Ġand ĠI Ġtook Ġthe Ġopportunity Ġto Ġsay Ġhello Ġand Ġre int rodu ce Ġmyself -- as Ġa Ġjournalist , ĠI Ġmight Ġadd -- and Ġwe Ġchat ted Ġabout Ġthe Ġmovie Ġfor Ġa Ġfew Ġminutes .\n",
      "\t Unique Tokens GPT2: {'introdu'}\n",
      "\t Unique Tokens LLAMA3: {'int', 'rodu'}\n",
      "Text 2: I saw Al and Tipper together at the wedding.\n",
      "\t Tokenized GPT2:I Ġsaw ĠAl Ġand ĠT ipper Ġtogether Ġat Ġthe Ġwedding .\n",
      "\t Tokenized LLAMA3:I Ġsaw ĠAl Ġand ĠT ipper Ġtogether Ġat Ġthe Ġwedding .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Also, why Princess Di was like President  The public cared more about her empathy than about her actions.\n",
      "\t Tokenized GPT2:Also , Ġwhy ĠPrincess ĠDi Ġwas Ġlike ĠPresident Ġ ĠThe Ġpublic Ġcared Ġmore Ġabout Ġher Ġempathy Ġthan Ġabout Ġher Ġactions .\n",
      "\t Tokenized LLAMA3:Also , Ġwhy ĠPrincess ĠDi Ġwas Ġlike ĠPresident Ġ ĠThe Ġpublic Ġcared Ġmore Ġabout Ġher Ġempathy Ġthan Ġabout Ġher Ġactions .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: They scared more about empathy than actions.\n",
      "\t Tokenized GPT2:They Ġscared Ġmore Ġabout Ġempathy Ġthan Ġactions .\n",
      "\t Tokenized LLAMA3:They Ġscared Ġmore Ġabout Ġempathy Ġthan Ġactions .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: 'Wait here,' I was ordered.\n",
      "\t Tokenized GPT2:' Wait Ġhere ,' ĠI Ġwas Ġordered .\n",
      "\t Tokenized LLAMA3:' Wait Ġhere ,' ĠI Ġwas Ġordered .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He told me to come with him.\n",
      "\t Tokenized GPT2:He Ġtold Ġme Ġto Ġcome Ġwith Ġhim .\n",
      "\t Tokenized LLAMA3:He Ġtold Ġme Ġto Ġcome Ġwith Ġhim .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: In the short term, U.S. consumers will benefit from cheap imports (as will U.S. multinationals that use parts made in East Asian factories).\n",
      "\t Tokenized GPT2:In Ġthe Ġshort Ġterm , ĠU . S . Ġconsumers Ġwill Ġbenefit Ġfrom Ġcheap Ġimports Ġ( as Ġwill ĠU . S . Ġmult ination als Ġthat Ġuse Ġparts Ġmade Ġin ĠEast ĠAsian Ġfactories ).\n",
      "\t Tokenized LLAMA3:In Ġthe Ġshort Ġterm , ĠU .S . Ġconsumers Ġwill Ġbenefit Ġfrom Ġcheap Ġimports Ġ( as Ġwill ĠU .S . Ġmult ination als Ġthat Ġuse Ġparts Ġmade Ġin ĠEast ĠAsian Ġfactories ).\n",
      "\t Unique Tokens GPT2: {'S'}\n",
      "\t Unique Tokens LLAMA3: {'.S'}\n",
      "Text 2: U.S. consumers will put money in the pockets of East Asia over time.\n",
      "\t Tokenized GPT2:U . S . Ġconsumers Ġwill Ġput Ġmoney Ġin Ġthe Ġpockets Ġof ĠEast ĠAsia Ġover Ġtime .\n",
      "\t Tokenized LLAMA3:U .S . Ġconsumers Ġwill Ġput Ġmoney Ġin Ġthe Ġpockets Ġof ĠEast ĠAsia Ġover Ġtime .\n",
      "\t Unique Tokens GPT2: {'S'}\n",
      "\t Unique Tokens LLAMA3: {'.S'}\n",
      "==entailment==\n",
      "Text 1: Ca'daan felt his skin get hot and unable to come up with any suitable response, moved on.\n",
      "\t Tokenized GPT2:Ca 'd aan Ġfelt Ġhis Ġskin Ġget Ġhot Ġand Ġunable Ġto Ġcome Ġup Ġwith Ġany Ġsuitable Ġresponse , Ġmoved Ġon .\n",
      "\t Tokenized LLAMA3:Ca 'd aan Ġfelt Ġhis Ġskin Ġget Ġhot Ġand Ġunable Ġto Ġcome Ġup Ġwith Ġany Ġsuitable Ġresponse , Ġmoved Ġon .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Ca'daan felt the heat on his skin.\n",
      "\t Tokenized GPT2:Ca 'd aan Ġfelt Ġthe Ġheat Ġon Ġhis Ġskin .\n",
      "\t Tokenized LLAMA3:Ca 'd aan Ġfelt Ġthe Ġheat Ġon Ġhis Ġskin .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: See the idea?\" 35 \"Then you think\" Tuppence paused to grasp the supposition fully \"that it WAS as Jane Finn that they wanted me to go to Paris?\" Mr. Carter smiled more wearily than ever.\n",
      "\t Tokenized GPT2:See Ġthe Ġidea ?\" Ġ35 Ġ\" Then Ġyou Ġthink \" ĠT upp ence Ġpaused Ġto Ġgrasp Ġthe Ġsupp osition Ġfully Ġ\" that Ġit ĠWAS Ġas ĠJane ĠFinn Ġthat Ġthey Ġwanted Ġme Ġto Ġgo Ġto ĠParis ?\" ĠMr . ĠCarter Ġsmiled Ġmore Ġwe arily Ġthan Ġever .\n",
      "\t Tokenized LLAMA3:See Ġthe Ġidea ?\" Ġ 35 Ġ\" Then Ġyou Ġthink \" ĠT upp ence Ġpaused Ġto Ġgrasp Ġthe Ġsupp osition Ġfully Ġ\" that Ġit ĠWAS Ġas ĠJane ĠFinn Ġthat Ġthey Ġwanted Ġme Ġto Ġgo Ġto ĠParis ?\" ĠMr . ĠCarter Ġsmiled Ġmore Ġwe arily Ġthan Ġever .\n",
      "\t Unique Tokens GPT2: {'Ġ35'}\n",
      "\t Unique Tokens LLAMA3: {'35', 'Ġ'}\n",
      "Text 2: Mr. Carter had no energy left to continue the conversation.\n",
      "\t Tokenized GPT2:Mr . ĠCarter Ġhad Ġno Ġenergy Ġleft Ġto Ġcontinue Ġthe Ġconversation .\n",
      "\t Tokenized LLAMA3:Mr . ĠCarter Ġhad Ġno Ġenergy Ġleft Ġto Ġcontinue Ġthe Ġconversation .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Try a selection at the Whisky Heritage Centre (they have over 100 for you to sample), where you can then buy a bottle or two of your personal favorite in the shop or in stores around the city.\n",
      "\t Tokenized GPT2:Try Ġa Ġselection Ġat Ġthe ĠW his ky ĠHer itage ĠCentre Ġ( they Ġhave Ġover Ġ100 Ġfor Ġyou Ġto Ġsample ), Ġwhere Ġyou Ġcan Ġthen Ġbuy Ġa Ġbottle Ġor Ġtwo Ġof Ġyour Ġpersonal Ġfavorite Ġin Ġthe Ġshop Ġor Ġin Ġstores Ġaround Ġthe Ġcity .\n",
      "\t Tokenized LLAMA3:Try Ġa Ġselection Ġat Ġthe ĠW his ky ĠHer itage ĠCentre Ġ( they Ġhave Ġover Ġ 100 Ġfor Ġyou Ġto Ġsample ), Ġwhere Ġyou Ġcan Ġthen Ġbuy Ġa Ġbottle Ġor Ġtwo Ġof Ġyour Ġpersonal Ġfavorite Ġin Ġthe Ġshop Ġor Ġin Ġstores Ġaround Ġthe Ġcity .\n",
      "\t Unique Tokens GPT2: {'Ġ100'}\n",
      "\t Unique Tokens LLAMA3: {'100', 'Ġ'}\n",
      "Text 2: Whisky Heritage Centre was established in the 1800s and has been a destination ever since.\n",
      "\t Tokenized GPT2:W his ky ĠHer itage ĠCentre Ġwas Ġestablished Ġin Ġthe Ġ1800 s Ġand Ġhas Ġbeen Ġa Ġdestination Ġever Ġsince .\n",
      "\t Tokenized LLAMA3:W his ky ĠHer itage ĠCentre Ġwas Ġestablished Ġin Ġthe Ġ 180 0 s Ġand Ġhas Ġbeen Ġa Ġdestination Ġever Ġsince .\n",
      "\t Unique Tokens GPT2: {'Ġ1800'}\n",
      "\t Unique Tokens LLAMA3: {'180', '0', 'Ġ'}\n",
      "==entailment==\n",
      "Text 1: As shown in Exhibits A-1 and A-2 in Appendix A, in the first phase of technology implementation, an engineering review and assessment of the combustion unit is conducted to determine the preferred compliance alternative.\n",
      "\t Tokenized GPT2:As Ġshown Ġin ĠEx hib its ĠA - 1 Ġand ĠA - 2 Ġin ĠAppend ix ĠA , Ġin Ġthe Ġfirst Ġphase Ġof Ġtechnology Ġimplementation , Ġan Ġengineering Ġreview Ġand Ġassessment Ġof Ġthe Ġcomb ustion Ġunit Ġis Ġconducted Ġto Ġdetermine Ġthe Ġpreferred Ġcompliance Ġalternative .\n",
      "\t Tokenized LLAMA3:As Ġshown Ġin ĠEx hib its ĠA - 1 Ġand ĠA - 2 Ġin ĠAppend ix ĠA , Ġin Ġthe Ġfirst Ġphase Ġof Ġtechnology Ġimplementation , Ġan Ġengineering Ġreview Ġand Ġassessment Ġof Ġthe Ġcomb ust ion Ġunit Ġis Ġconducted Ġto Ġdetermine Ġthe Ġpreferred Ġcompliance Ġalternative .\n",
      "\t Unique Tokens GPT2: {'ustion'}\n",
      "\t Unique Tokens LLAMA3: {'ion', 'ust'}\n",
      "Text 2: The exhibits within the appendix shoe the initial phase of the technological implementations.\n",
      "\t Tokenized GPT2:The Ġexhibits Ġwithin Ġthe Ġappend ix Ġshoe Ġthe Ġinitial Ġphase Ġof Ġthe Ġtechnological Ġimplementations .\n",
      "\t Tokenized LLAMA3:The Ġexhibits Ġwithin Ġthe Ġappend ix Ġshoe Ġthe Ġinitial Ġphase Ġof Ġthe Ġtechnological Ġimplementations .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Republican consultants agree that conservative candidates in the South, Southwest, Midwest, and Rocky Mountains will beg for Reed's talents and connections.\n",
      "\t Tokenized GPT2:Republic an Ġconsult ants Ġagree Ġthat Ġconservative Ġcandidates Ġin Ġthe ĠSouth , ĠSouth west , ĠMid west , Ġand ĠRock y ĠMount ains Ġwill Ġbeg Ġfor ĠReed 's Ġtalents Ġand Ġconnections .\n",
      "\t Tokenized LLAMA3:Republic an Ġconsult ants Ġagree Ġthat Ġconservative Ġcandidates Ġin Ġthe ĠSouth , ĠSouth west , ĠMid west , Ġand ĠRock y ĠMount ains Ġwill Ġbeg Ġfor ĠReed 's Ġtalents Ġand Ġconnections .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Reed's talents will be begged for by people across the country, according to Republicans.\n",
      "\t Tokenized GPT2:Re ed 's Ġtalents Ġwill Ġbe Ġbegged Ġfor Ġby Ġpeople Ġacross Ġthe Ġcountry , Ġaccording Ġto ĠRepublicans .\n",
      "\t Tokenized LLAMA3:Re ed 's Ġtalents Ġwill Ġbe Ġbegged Ġfor Ġby Ġpeople Ġacross Ġthe Ġcountry , Ġaccording Ġto ĠRepublicans .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The Throne Room is one of a series of apartments built during the reign of Charles II, though it was originally designed as a guard room that screened entrants to the private chambers beyond.\n",
      "\t Tokenized GPT2:The ĠThr one ĠRoom Ġis Ġone Ġof Ġa Ġseries Ġof Ġapartments Ġbuilt Ġduring Ġthe Ġreign Ġof ĠCharles ĠII , Ġthough Ġit Ġwas Ġoriginally Ġdesigned Ġas Ġa Ġguard Ġroom Ġthat Ġscreen ed Ġentr ants Ġto Ġthe Ġprivate Ġchambers Ġbeyond .\n",
      "\t Tokenized LLAMA3:The ĠThr one ĠRoom Ġis Ġone Ġof Ġa Ġseries Ġof Ġapartments Ġbuilt Ġduring Ġthe Ġreign Ġof ĠCharles ĠII , Ġthough Ġit Ġwas Ġoriginally Ġdesigned Ġas Ġa Ġguard Ġroom Ġthat Ġscreen ed Ġentr ants Ġto Ġthe Ġprivate Ġchambers Ġbeyond .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The Throne Room is an apartment built during the reign of George III.\n",
      "\t Tokenized GPT2:The ĠThr one ĠRoom Ġis Ġan Ġapartment Ġbuilt Ġduring Ġthe Ġreign Ġof ĠGeorge ĠIII .\n",
      "\t Tokenized LLAMA3:The ĠThr one ĠRoom Ġis Ġan Ġapartment Ġbuilt Ġduring Ġthe Ġreign Ġof ĠGeorge ĠIII .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The emotional effect is undiminished, and the gory effects are usually horribly creative.\n",
      "\t Tokenized GPT2:The Ġemotional Ġeffect Ġis Ġund imin ished , Ġand Ġthe Ġg ory Ġeffects Ġare Ġusually Ġhorribly Ġcreative .\n",
      "\t Tokenized LLAMA3:The Ġemotional Ġeffect Ġis Ġund imin ished , Ġand Ġthe Ġg ory Ġeffects Ġare Ġusually Ġhorribly Ġcreative .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The emotional impact is greatly lessened and the way that gore is used is unoriginal.\n",
      "\t Tokenized GPT2:The Ġemotional Ġimpact Ġis Ġgreatly Ġless ened Ġand Ġthe Ġway Ġthat Ġg ore Ġis Ġused Ġis Ġun original .\n",
      "\t Tokenized LLAMA3:The Ġemotional Ġimpact Ġis Ġgreatly Ġless ened Ġand Ġthe Ġway Ġthat Ġg ore Ġis Ġused Ġis Ġun original .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: In the same issue, a document entitled Analysis Regarding The Food And Drug Administration's Jurisdiction Over Nicotine-Containing Cigarettes And Smokeless Tobacco Products was published and comments were requested.\n",
      "\t Tokenized GPT2:In Ġthe Ġsame Ġissue , Ġa Ġdocument Ġentitled ĠAnalysis ĠReg arding ĠThe ĠFood ĠAnd ĠDrug ĠAdministration 's ĠJ ur isd iction ĠOver ĠNic otine - Cont aining ĠC ig aret tes ĠAnd ĠSm oke less ĠTob acco ĠProducts Ġwas Ġpublished Ġand Ġcomments Ġwere Ġrequested .\n",
      "\t Tokenized LLAMA3:In Ġthe Ġsame Ġissue , Ġa Ġdocument Ġentitled ĠAnalysis ĠReg arding ĠThe ĠFood ĠAnd ĠDrug ĠAdministration 's ĠJ ur isd iction ĠOver ĠNic otine - Cont aining ĠC ig aret tes ĠAnd ĠSm oke less ĠTob acco ĠProducts Ġwas Ġpublished Ġand Ġcomments Ġwere Ġrequested .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: A document was published about the FDA's jurisdiction over cigarettes.\n",
      "\t Tokenized GPT2:A Ġdocument Ġwas Ġpublished Ġabout Ġthe ĠFDA 's Ġjurisdiction Ġover Ġcigarettes .\n",
      "\t Tokenized LLAMA3:A Ġdocument Ġwas Ġpublished Ġabout Ġthe ĠFDA 's Ġjurisdiction Ġover Ġcigarettes .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The White House denies this.\n",
      "\t Tokenized GPT2:The ĠWhite ĠHouse Ġden ies Ġthis .\n",
      "\t Tokenized LLAMA3:The ĠWhite ĠHouse Ġden ies Ġthis .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The White House, off the record, knows it to be true.\n",
      "\t Tokenized GPT2:The ĠWhite ĠHouse , Ġoff Ġthe Ġrecord , Ġknows Ġit Ġto Ġbe Ġtrue .\n",
      "\t Tokenized LLAMA3:The ĠWhite ĠHouse , Ġoff Ġthe Ġrecord , Ġknows Ġit Ġto Ġbe Ġtrue .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: I am not aware of any studies comparing the number of words an average person could expect to hear spoken in a typical day 500 years ago vs. the number that can be heard now, but the increase surely is vast.\n",
      "\t Tokenized GPT2:I Ġam Ġnot Ġaware Ġof Ġany Ġstudies Ġcomparing Ġthe Ġnumber Ġof Ġwords Ġan Ġaverage Ġperson Ġcould Ġexpect Ġto Ġhear Ġspoken Ġin Ġa Ġtypical Ġday Ġ500 Ġyears Ġago Ġvs . Ġthe Ġnumber Ġthat Ġcan Ġbe Ġheard Ġnow , Ġbut Ġthe Ġincrease Ġsurely Ġis Ġvast .\n",
      "\t Tokenized LLAMA3:I Ġam Ġnot Ġaware Ġof Ġany Ġstudies Ġcomparing Ġthe Ġnumber Ġof Ġwords Ġan Ġaverage Ġperson Ġcould Ġexpect Ġto Ġhear Ġspoken Ġin Ġa Ġtypical Ġday Ġ 500 Ġyears Ġago Ġvs . Ġthe Ġnumber Ġthat Ġcan Ġbe Ġheard Ġnow , Ġbut Ġthe Ġincrease Ġsurely Ġis Ġvast .\n",
      "\t Unique Tokens GPT2: {'Ġ500'}\n",
      "\t Unique Tokens LLAMA3: {'500', 'Ġ'}\n",
      "Text 2: According to the research I've seen, the average person hundreds of years ago heard many more words over the course of the day compared to a modern human being.\n",
      "\t Tokenized GPT2:According Ġto Ġthe Ġresearch ĠI 've Ġseen , Ġthe Ġaverage Ġperson Ġhundreds Ġof Ġyears Ġago Ġheard Ġmany Ġmore Ġwords Ġover Ġthe Ġcourse Ġof Ġthe Ġday Ġcompared Ġto Ġa Ġmodern Ġhuman Ġbeing .\n",
      "\t Tokenized LLAMA3:According Ġto Ġthe Ġresearch ĠI 've Ġseen , Ġthe Ġaverage Ġperson Ġhundreds Ġof Ġyears Ġago Ġheard Ġmany Ġmore Ġwords Ġover Ġthe Ġcourse Ġof Ġthe Ġday Ġcompared Ġto Ġa Ġmodern Ġhuman Ġbeing .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: do you really romance\n",
      "\t Tokenized GPT2:do Ġyou Ġreally Ġromance\n",
      "\t Tokenized LLAMA3:do Ġyou Ġreally Ġromance\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Do you really love him?\n",
      "\t Tokenized GPT2:Do Ġyou Ġreally Ġlove Ġhim ?\n",
      "\t Tokenized LLAMA3:Do Ġyou Ġreally Ġlove Ġhim ?\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: I leap!\" And, in very truth, run and leap he did, gambolling wildly down the stretch of lawn outside the long window. \n",
      "\t Tokenized GPT2:I Ġleap !\" ĠAnd , Ġin Ġvery Ġtruth , Ġrun Ġand Ġleap Ġhe Ġdid , Ġg amb oll ing Ġwildly Ġdown Ġthe Ġstretch Ġof Ġlawn Ġoutside Ġthe Ġlong Ġwindow . Ġ\n",
      "\t Tokenized LLAMA3:I Ġleap !\" ĠAnd , Ġin Ġvery Ġtruth , Ġrun Ġand Ġleap Ġhe Ġdid , Ġg amb oll ing Ġwildly Ġdown Ġthe Ġstretch Ġof Ġlawn Ġoutside Ġthe Ġlong Ġwindow . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The man yelled that he would sit down, and sit down he did.\n",
      "\t Tokenized GPT2:The Ġman Ġyelled Ġthat Ġhe Ġwould Ġsit Ġdown , Ġand Ġsit Ġdown Ġhe Ġdid .\n",
      "\t Tokenized LLAMA3:The Ġman Ġyelled Ġthat Ġhe Ġwould Ġsit Ġdown , Ġand Ġsit Ġdown Ġhe Ġdid .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: football and baseball and\n",
      "\t Tokenized GPT2:foot ball Ġand Ġbaseball Ġand\n",
      "\t Tokenized LLAMA3:foot ball Ġand Ġbaseball Ġand\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Both football and baseball.\n",
      "\t Tokenized GPT2:Both Ġfootball Ġand Ġbaseball .\n",
      "\t Tokenized LLAMA3:Both Ġfootball Ġand Ġbaseball .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Catch up on the Indian avant-garde and the bohemian people of Caletta at the Academy of Fine Arts on the southeast corner of the Maidan.\n",
      "\t Tokenized GPT2:C atch Ġup Ġon Ġthe ĠIndian Ġav ant - gard e Ġand Ġthe Ġbo hem ian Ġpeople Ġof ĠCal etta Ġat Ġthe ĠAcademy Ġof ĠFine ĠArts Ġon Ġthe Ġs outheast Ġcorner Ġof Ġthe ĠM aid an .\n",
      "\t Tokenized LLAMA3:C atch Ġup Ġon Ġthe ĠIndian Ġav ant -g ard e Ġand Ġthe Ġbo hem ian Ġpeople Ġof ĠCal etta Ġat Ġthe ĠAcademy Ġof ĠFine ĠArts Ġon Ġthe Ġs outheast Ġcorner Ġof Ġthe ĠM aid an .\n",
      "\t Unique Tokens GPT2: {'-', 'gard'}\n",
      "\t Unique Tokens LLAMA3: {'ard', '-g'}\n",
      "Text 2: In South-Eastern Maidan you will find the Academy of Fine Arts.\n",
      "\t Tokenized GPT2:In ĠSouth - E astern ĠM aid an Ġyou Ġwill Ġfind Ġthe ĠAcademy Ġof ĠFine ĠArts .\n",
      "\t Tokenized LLAMA3:In ĠSouth -E astern ĠM aid an Ġyou Ġwill Ġfind Ġthe ĠAcademy Ġof ĠFine ĠArts .\n",
      "\t Unique Tokens GPT2: {'-', 'E'}\n",
      "\t Unique Tokens LLAMA3: {'-E'}\n",
      "==entailment==\n",
      "Text 1: if it had rained any more in the last two weeks instead of planting Saint Augustine grass in the front yard i think i would have plowed everything under and had a rice field\n",
      "\t Tokenized GPT2:if Ġit Ġhad Ġr ained Ġany Ġmore Ġin Ġthe Ġlast Ġtwo Ġweeks Ġinstead Ġof Ġplanting ĠSaint ĠAugust ine Ġgrass Ġin Ġthe Ġfront Ġyard Ġi Ġthink Ġi Ġwould Ġhave Ġpl owed Ġeverything Ġunder Ġand Ġhad Ġa Ġrice Ġfield\n",
      "\t Tokenized LLAMA3:if Ġit Ġhad Ġr ained Ġany Ġmore Ġin Ġthe Ġlast Ġtwo Ġweeks Ġinstead Ġof Ġplanting ĠSaint ĠAugust ine Ġgrass Ġin Ġthe Ġfront Ġyard Ġi Ġthink Ġi Ġwould Ġhave Ġpl owed Ġeverything Ġunder Ġand Ġhad Ġa Ġrice Ġfield\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It has rained enough to flood everything here and make rice pattys.\n",
      "\t Tokenized GPT2:It Ġhas Ġr ained Ġenough Ġto Ġflood Ġeverything Ġhere Ġand Ġmake Ġrice Ġpat ty s .\n",
      "\t Tokenized LLAMA3:It Ġhas Ġr ained Ġenough Ġto Ġflood Ġeverything Ġhere Ġand Ġmake Ġrice Ġpat ty s .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Kicked out of the house when she was only 16 (she was called Suzie in those days), Roy went to Delhi and then to architecture school, supporting herself by selling empty milk bottles (some say beer bottles).\n",
      "\t Tokenized GPT2:K icked Ġout Ġof Ġthe Ġhouse Ġwhen Ġshe Ġwas Ġonly Ġ16 Ġ( she Ġwas Ġcalled ĠSuz ie Ġin Ġthose Ġdays ), ĠRoy Ġwent Ġto ĠDelhi Ġand Ġthen Ġto Ġarchitecture Ġschool , Ġsupporting Ġherself Ġby Ġselling Ġempty Ġmilk Ġbottles Ġ( some Ġsay Ġbeer Ġbottles ).\n",
      "\t Tokenized LLAMA3:K icked Ġout Ġof Ġthe Ġhouse Ġwhen Ġshe Ġwas Ġonly Ġ 16 Ġ( she Ġwas Ġcalled ĠSuz ie Ġin Ġthose Ġdays ), ĠRoy Ġwent Ġto ĠDelhi Ġand Ġthen Ġto Ġarchitecture Ġschool , Ġsupporting Ġherself Ġby Ġselling Ġempty Ġmilk Ġbottles Ġ( some Ġsay Ġbeer Ġbottles ).\n",
      "\t Unique Tokens GPT2: {'Ġ16'}\n",
      "\t Unique Tokens LLAMA3: {'16', 'Ġ'}\n",
      "Text 2: Roy had to sell bottles to make money.\n",
      "\t Tokenized GPT2:R oy Ġhad Ġto Ġsell Ġbottles Ġto Ġmake Ġmoney .\n",
      "\t Tokenized LLAMA3:R oy Ġhad Ġto Ġsell Ġbottles Ġto Ġmake Ġmoney .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Czarek was welcomed enthusiastically, even though the poultry brotherhood was paying a lot of sudden attention to the newcomers - a strong group of young and talented managers from an egzemo-exotic chicken farm in Fodder Band nearby Podunkowice.\n",
      "\t Tokenized GPT2:C z are k Ġwas Ġwelcomed Ġenthusi astically , Ġeven Ġthough Ġthe Ġpou lt ry Ġbrother hood Ġwas Ġpaying Ġa Ġlot Ġof Ġsudden Ġattention Ġto Ġthe Ġnewcom ers Ġ- Ġa Ġstrong Ġgroup Ġof Ġyoung Ġand Ġtalented Ġmanagers Ġfrom Ġan Ġe gz emo - ex otic Ġchicken Ġfarm Ġin ĠF od der ĠBand Ġnearby ĠPod unk ow ice .\n",
      "\t Tokenized LLAMA3:C z are k Ġwas Ġwelcomed Ġenthusi astically , Ġeven Ġthough Ġthe Ġpou lt ry Ġbrother hood Ġwas Ġpaying Ġa Ġlot Ġof Ġsudden Ġattention Ġto Ġthe Ġnewcom ers Ġ- Ġa Ġstrong Ġgroup Ġof Ġyoung Ġand Ġtalented Ġmanagers Ġfrom Ġan Ġeg z emo -ex otic Ġchicken Ġfarm Ġin ĠF od der ĠBand Ġnearby ĠPod unk ow ice .\n",
      "\t Unique Tokens GPT2: {'ex', '-', 'gz', 'Ġe'}\n",
      "\t Unique Tokens LLAMA3: {'Ġeg', '-ex'}\n",
      "Text 2: Czarek was welcomed into the group by the farmers.\n",
      "\t Tokenized GPT2:C z are k Ġwas Ġwelcomed Ġinto Ġthe Ġgroup Ġby Ġthe Ġfarmers .\n",
      "\t Tokenized LLAMA3:C z are k Ġwas Ġwelcomed Ġinto Ġthe Ġgroup Ġby Ġthe Ġfarmers .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The media focused on Liggett's admissions of the obvious--that cigarettes are addictive and cause cancer and heart disease--and its agreement to pay the states a quarter of its (relatively small) pretax profits for the next 25 years.\n",
      "\t Tokenized GPT2:The Ġmedia Ġfocused Ġon ĠL ig get t 's Ġad missions Ġof Ġthe Ġobvious -- that Ġcigarettes Ġare Ġaddictive Ġand Ġcause Ġcancer Ġand Ġheart Ġdisease -- and Ġits Ġagreement Ġto Ġpay Ġthe Ġstates Ġa Ġquarter Ġof Ġits Ġ( rel atively Ġsmall ) Ġpret ax Ġprofits Ġfor Ġthe Ġnext Ġ25 Ġyears .\n",
      "\t Tokenized LLAMA3:The Ġmedia Ġfocused Ġon ĠL ig get t 's Ġad missions Ġof Ġthe Ġobvious -- that Ġcigarettes Ġare Ġaddict ive Ġand Ġcause Ġcancer Ġand Ġheart Ġdisease -- and Ġits Ġagreement Ġto Ġpay Ġthe Ġstates Ġa Ġquarter Ġof Ġits Ġ( rel atively Ġsmall ) Ġpret ax Ġprofits Ġfor Ġthe Ġnext Ġ 25 Ġyears .\n",
      "\t Unique Tokens GPT2: {'Ġ25', 'Ġaddictive'}\n",
      "\t Unique Tokens LLAMA3: {'ive', '25', 'Ġ', 'Ġaddict'}\n",
      "Text 2: The media reported on Lingett's insistence that cigarettes don't cause cancer.\n",
      "\t Tokenized GPT2:The Ġmedia Ġreported Ġon ĠL ing ett 's Ġins istence Ġthat Ġcigarettes Ġdon 't Ġcause Ġcancer .\n",
      "\t Tokenized LLAMA3:The Ġmedia Ġreported Ġon ĠL ing ett 's Ġins istence Ġthat Ġcigarettes Ġdon 't Ġcause Ġcancer .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: More works can be seen in the museum attached to the cathedral (admission is around 100 pe?­setas).\n",
      "\t Tokenized GPT2:More Ġworks Ġcan Ġbe Ġseen Ġin Ġthe Ġmuseum Ġattached Ġto Ġthe Ġcat hedral Ġ( ad mission Ġis Ġaround Ġ100 Ġpe ? ÂŃ set as ).\n",
      "\t Tokenized LLAMA3:More Ġworks Ġcan Ġbe Ġseen Ġin Ġthe Ġmuseum Ġattached Ġto Ġthe Ġcat hedral Ġ( ad mission Ġis Ġaround Ġ 100 Ġpe ? ÂŃ set as ).\n",
      "\t Unique Tokens GPT2: {'Ġ100'}\n",
      "\t Unique Tokens LLAMA3: {'100', 'Ġ'}\n",
      "Text 2: The museum attached to the cathedral has art in it.\n",
      "\t Tokenized GPT2:The Ġmuseum Ġattached Ġto Ġthe Ġcat hedral Ġhas Ġart Ġin Ġit .\n",
      "\t Tokenized LLAMA3:The Ġmuseum Ġattached Ġto Ġthe Ġcat hedral Ġhas Ġart Ġin Ġit .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: In Roman times a temple to Jupiter stood here, followed in the fourth century by the first Christian church, Saint-Etienne.\n",
      "\t Tokenized GPT2:In ĠRoman Ġtimes Ġa Ġtemple Ġto ĠJ upiter Ġstood Ġhere , Ġfollowed Ġin Ġthe Ġfourth Ġcentury Ġby Ġthe Ġfirst ĠChristian Ġchurch , ĠSaint - E t ien ne .\n",
      "\t Tokenized LLAMA3:In ĠRoman Ġtimes Ġa Ġtemple Ġto ĠJ upiter Ġstood Ġhere , Ġfollowed Ġin Ġthe Ġfourth Ġcentury Ġby Ġthe Ġfirst ĠChristian Ġchurch , ĠSaint -E t ien ne .\n",
      "\t Unique Tokens GPT2: {'-', 'E'}\n",
      "\t Unique Tokens LLAMA3: {'-E'}\n",
      "Text 2: Saint-Etienne burned to the ground during the Roman times.\n",
      "\t Tokenized GPT2:S aint - E t ien ne Ġburned Ġto Ġthe Ġground Ġduring Ġthe ĠRoman Ġtimes .\n",
      "\t Tokenized LLAMA3:S aint -E t ien ne Ġburned Ġto Ġthe Ġground Ġduring Ġthe ĠRoman Ġtimes .\n",
      "\t Unique Tokens GPT2: {'-', 'E'}\n",
      "\t Unique Tokens LLAMA3: {'-E'}\n",
      "==neutral==\n",
      "Text 1: Diets for men in their prime\n",
      "\t Tokenized GPT2:Di ets Ġfor Ġmen Ġin Ġtheir Ġprime\n",
      "\t Tokenized LLAMA3:Di ets Ġfor Ġmen Ġin Ġtheir Ġprime\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: A diet for men made up completely of meat. \n",
      "\t Tokenized GPT2:A Ġdiet Ġfor Ġmen Ġmade Ġup Ġcompletely Ġof Ġmeat . Ġ\n",
      "\t Tokenized LLAMA3:A Ġdiet Ġfor Ġmen Ġmade Ġup Ġcompletely Ġof Ġmeat . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The Passaic office is refusing to join in that reconfiguration, which goes into effect Jan.\n",
      "\t Tokenized GPT2:The ĠPass a ic Ġoffice Ġis Ġrefusing Ġto Ġjoin Ġin Ġthat Ġre configuration , Ġwhich Ġgoes Ġinto Ġeffect ĠJan .\n",
      "\t Tokenized LLAMA3:The ĠPass a ic Ġoffice Ġis Ġrefusing Ġto Ġjoin Ġin Ġthat Ġre configuration , Ġwhich Ġgoes Ġinto Ġeffect ĠJan .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It will be reconfigured in March. \n",
      "\t Tokenized GPT2:It Ġwill Ġbe Ġre config ured Ġin ĠMarch . Ġ\n",
      "\t Tokenized LLAMA3:It Ġwill Ġbe Ġre config ured Ġin ĠMarch . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: After being diagnosed with cancer, Carrey's Kaufman decides to do a show at Carnegie Hall.\n",
      "\t Tokenized GPT2:After Ġbeing Ġdiagnosed Ġwith Ġcancer , ĠCar rey 's ĠK au f man Ġdecides Ġto Ġdo Ġa Ġshow Ġat ĠCar neg ie ĠHall .\n",
      "\t Tokenized LLAMA3:After Ġbeing Ġdiagnosed Ġwith Ġcancer , ĠCar rey 's ĠK au f man Ġdecides Ġto Ġdo Ġa Ġshow Ġat ĠCar neg ie ĠHall .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Carrey's Kaufman eventually recovers from the cancer he was diagnosed with.\n",
      "\t Tokenized GPT2:Car rey 's ĠK au f man Ġeventually Ġrec overs Ġfrom Ġthe Ġcancer Ġhe Ġwas Ġdiagnosed Ġwith .\n",
      "\t Tokenized LLAMA3:Car rey 's ĠK au f man Ġeventually Ġrec overs Ġfrom Ġthe Ġcancer Ġhe Ġwas Ġdiagnosed Ġwith .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: In May 1967, Gallup found that the number of people who said they intensely disliked RFK--who was also probably more intensely liked than any other practicing politician--was twice as high as the number who intensely disliked Johnson, the architect of the increasingly unpopular war in Vietnam.\n",
      "\t Tokenized GPT2:In ĠMay Ġ19 67 , ĠGall up Ġfound Ġthat Ġthe Ġnumber Ġof Ġpeople Ġwho Ġsaid Ġthey Ġintense ly Ġdisli ked ĠRF K -- who Ġwas Ġalso Ġprobably Ġmore Ġintense ly Ġliked Ġthan Ġany Ġother Ġpracticing Ġpolitician -- was Ġtwice Ġas Ġhigh Ġas Ġthe Ġnumber Ġwho Ġintense ly Ġdisli ked ĠJohnson , Ġthe Ġarchitect Ġof Ġthe Ġincreasingly Ġun popular Ġwar Ġin ĠVietnam .\n",
      "\t Tokenized LLAMA3:In ĠMay Ġ 196 7 , ĠGall up Ġfound Ġthat Ġthe Ġnumber Ġof Ġpeople Ġwho Ġsaid Ġthey Ġintense ly Ġdisli ked ĠRF K -- who Ġwas Ġalso Ġprobably Ġmore Ġintense ly Ġliked Ġthan Ġany Ġother Ġpracticing Ġpolitician -- was Ġtwice Ġas Ġhigh Ġas Ġthe Ġnumber Ġwho Ġintense ly Ġdisli ked ĠJohnson , Ġthe Ġarchitect Ġof Ġthe Ġincreasingly Ġun popular Ġwar Ġin ĠVietnam .\n",
      "\t Unique Tokens GPT2: {'67', 'Ġ19'}\n",
      "\t Unique Tokens LLAMA3: {'196', '7', 'Ġ'}\n",
      "Text 2: In 1967 more people preferred Johnson than RFK.\n",
      "\t Tokenized GPT2:In Ġ19 67 Ġmore Ġpeople Ġpreferred ĠJohnson Ġthan ĠRF K .\n",
      "\t Tokenized LLAMA3:In Ġ 196 7 Ġmore Ġpeople Ġpreferred ĠJohnson Ġthan ĠRF K .\n",
      "\t Unique Tokens GPT2: {'67', 'Ġ19'}\n",
      "\t Unique Tokens LLAMA3: {'196', '7', 'Ġ'}\n",
      "==contradiction==\n",
      "Text 1: 17 An alternative to unaddressed mail would be to auction off the right to be a third bundle on specific days in specific post offices.\n",
      "\t Tokenized GPT2:17 ĠAn Ġalternative Ġto Ġun address ed Ġmail Ġwould Ġbe Ġto Ġauction Ġoff Ġthe Ġright Ġto Ġbe Ġa Ġthird Ġbundle Ġon Ġspecific Ġdays Ġin Ġspecific Ġpost Ġoffices .\n",
      "\t Tokenized LLAMA3:17 ĠAn Ġalternative Ġto Ġun add ressed Ġmail Ġwould Ġbe Ġto Ġauction Ġoff Ġthe Ġright Ġto Ġbe Ġa Ġthird Ġbundle Ġon Ġspecific Ġdays Ġin Ġspecific Ġpost Ġoffices .\n",
      "\t Unique Tokens GPT2: {'ed', 'address'}\n",
      "\t Unique Tokens LLAMA3: {'add', 'ressed'}\n",
      "Text 2: You could auction off the right to a fourth bundle instead of doing unaddressed mail.\n",
      "\t Tokenized GPT2:You Ġcould Ġauction Ġoff Ġthe Ġright Ġto Ġa Ġfourth Ġbundle Ġinstead Ġof Ġdoing Ġun address ed Ġmail .\n",
      "\t Tokenized LLAMA3:You Ġcould Ġauction Ġoff Ġthe Ġright Ġto Ġa Ġfourth Ġbundle Ġinstead Ġof Ġdoing Ġun add ressed Ġmail .\n",
      "\t Unique Tokens GPT2: {'ed', 'address'}\n",
      "\t Unique Tokens LLAMA3: {'add', 'ressed'}\n",
      "==contradiction==\n",
      "Text 1: We always knew it was an outside chance.\n",
      "\t Tokenized GPT2:We Ġalways Ġknew Ġit Ġwas Ġan Ġoutside Ġchance .\n",
      "\t Tokenized LLAMA3:We Ġalways Ġknew Ġit Ġwas Ġan Ġoutside Ġchance .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: We felt it was definitely going to happen.\n",
      "\t Tokenized GPT2:We Ġfelt Ġit Ġwas Ġdefinitely Ġgoing Ġto Ġhappen .\n",
      "\t Tokenized LLAMA3:We Ġfelt Ġit Ġwas Ġdefinitely Ġgoing Ġto Ġhappen .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: She had thrown away her cloak and tied her hair back into a topknot to keep it out of the way.\n",
      "\t Tokenized GPT2:She Ġhad Ġthrown Ġaway Ġher Ġcloak Ġand Ġtied Ġher Ġhair Ġback Ġinto Ġa Ġtop k not Ġto Ġkeep Ġit Ġout Ġof Ġthe Ġway .\n",
      "\t Tokenized LLAMA3:She Ġhad Ġthrown Ġaway Ġher Ġcloak Ġand Ġtied Ġher Ġhair Ġback Ġinto Ġa Ġtop k not Ġto Ġkeep Ġit Ġout Ġof Ġthe Ġway .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: She put her hair up.\n",
      "\t Tokenized GPT2:She Ġput Ġher Ġhair Ġup .\n",
      "\t Tokenized LLAMA3:She Ġput Ġher Ġhair Ġup .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1:  Other villages are much less developed, and therein lies the essence of many delights.\n",
      "\t Tokenized GPT2:ĠOther Ġvillages Ġare Ġmuch Ġless Ġdeveloped , Ġand Ġthere in Ġlies Ġthe Ġessence Ġof Ġmany Ġdelight s .\n",
      "\t Tokenized LLAMA3:ĠOther Ġvillages Ġare Ġmuch Ġless Ġdeveloped , Ġand Ġthere in Ġlies Ġthe Ġessence Ġof Ġmany Ġdelight s .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The other villages are greatly developed.\n",
      "\t Tokenized GPT2:The Ġother Ġvillages Ġare Ġgreatly Ġdeveloped .\n",
      "\t Tokenized LLAMA3:The Ġother Ġvillages Ġare Ġgreatly Ġdeveloped .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Given the limits on the WTO's jurisdiction, it was probably unreasonable of Kodak to expect a real victory.\n",
      "\t Tokenized GPT2:Given Ġthe Ġlimits Ġon Ġthe ĠW TO 's Ġjurisdiction , Ġit Ġwas Ġprobably Ġunreasonable Ġof ĠK od ak Ġto Ġexpect Ġa Ġreal Ġvictory .\n",
      "\t Tokenized LLAMA3:Given Ġthe Ġlimits Ġon Ġthe ĠW TO 's Ġjurisdiction , Ġit Ġwas Ġprobably Ġunreasonable Ġof ĠK od ak Ġto Ġexpect Ġa Ġreal Ġvictory .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Kodak was naive and is still just a baby of a company.\n",
      "\t Tokenized GPT2:K od ak Ġwas Ġnaive Ġand Ġis Ġstill Ġjust Ġa Ġbaby Ġof Ġa Ġcompany .\n",
      "\t Tokenized LLAMA3:K od ak Ġwas Ġnaive Ġand Ġis Ġstill Ġjust Ġa Ġbaby Ġof Ġa Ġcompany .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: He said the Web site will help bridge the digital divide that keeps the poor from using the Internet as a resource.\n",
      "\t Tokenized GPT2:He Ġsaid Ġthe ĠWeb Ġsite Ġwill Ġhelp Ġbridge Ġthe Ġdigital Ġdivide Ġthat Ġkeeps Ġthe Ġpoor Ġfrom Ġusing Ġthe ĠInternet Ġas Ġa Ġresource .\n",
      "\t Tokenized LLAMA3:He Ġsaid Ġthe ĠWeb Ġsite Ġwill Ġhelp Ġbridge Ġthe Ġdigital Ġdivide Ġthat Ġkeeps Ġthe Ġpoor Ġfrom Ġusing Ġthe ĠInternet Ġas Ġa Ġresource .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He was telling us that the website is designed to make it harder for the poor to get online. \n",
      "\t Tokenized GPT2:He Ġwas Ġtelling Ġus Ġthat Ġthe Ġwebsite Ġis Ġdesigned Ġto Ġmake Ġit Ġharder Ġfor Ġthe Ġpoor Ġto Ġget Ġonline . Ġ\n",
      "\t Tokenized LLAMA3:He Ġwas Ġtelling Ġus Ġthat Ġthe Ġwebsite Ġis Ġdesigned Ġto Ġmake Ġit Ġharder Ġfor Ġthe Ġpoor Ġto Ġget Ġonline . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: uh high humidity\n",
      "\t Tokenized GPT2:uh Ġhigh Ġhumidity\n",
      "\t Tokenized LLAMA3:uh Ġhigh Ġhumidity\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Air with increased water content.\n",
      "\t Tokenized GPT2:Air Ġwith Ġincreased Ġwater Ġcontent .\n",
      "\t Tokenized LLAMA3:Air Ġwith Ġincreased Ġwater Ġcontent .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: She has believed that the sleeping draught she administered was perfectly harmless, but there is no doubt that for one terrible moment she must have feared that Mrs. Inglethorp's death lay at her door. \n",
      "\t Tokenized GPT2:She Ġhas Ġbelieved Ġthat Ġthe Ġsleeping Ġdr aught Ġshe Ġadministered Ġwas Ġperfectly Ġharmless , Ġbut Ġthere Ġis Ġno Ġdoubt Ġthat Ġfor Ġone Ġterrible Ġmoment Ġshe Ġmust Ġhave Ġfeared Ġthat ĠMrs . ĠIn gle th orp 's Ġdeath Ġlay Ġat Ġher Ġdoor . Ġ\n",
      "\t Tokenized LLAMA3:She Ġhas Ġbelieved Ġthat Ġthe Ġsleeping Ġdr aught Ġshe Ġadministered Ġwas Ġperfectly Ġharmless , Ġbut Ġthere Ġis Ġno Ġdoubt Ġthat Ġfor Ġone Ġterrible Ġmoment Ġshe Ġmust Ġhave Ġfeared Ġthat ĠMrs . ĠIn gle th orp 's Ġdeath Ġlay Ġat Ġher Ġdoor . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: She had no doubt that Mrs. Inglethorp's death was not a concern.\n",
      "\t Tokenized GPT2:She Ġhad Ġno Ġdoubt Ġthat ĠMrs . ĠIn gle th orp 's Ġdeath Ġwas Ġnot Ġa Ġconcern .\n",
      "\t Tokenized LLAMA3:She Ġhad Ġno Ġdoubt Ġthat ĠMrs . ĠIn gle th orp 's Ġdeath Ġwas Ġnot Ġa Ġconcern .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: you know like CODA comes out of your out of your pay and the credit union comes out of your pay so we don't have to do anything there and the rest of it as far as my salary goes i just have it automatically deposited in into our bank\n",
      "\t Tokenized GPT2:you Ġknow Ġlike ĠC OD A Ġcomes Ġout Ġof Ġyour Ġout Ġof Ġyour Ġpay Ġand Ġthe Ġcredit Ġunion Ġcomes Ġout Ġof Ġyour Ġpay Ġso Ġwe Ġdon 't Ġhave Ġto Ġdo Ġanything Ġthere Ġand Ġthe Ġrest Ġof Ġit Ġas Ġfar Ġas Ġmy Ġsalary Ġgoes Ġi Ġjust Ġhave Ġit Ġautomatically Ġdeposited Ġin Ġinto Ġour Ġbank\n",
      "\t Tokenized LLAMA3:you Ġknow Ġlike ĠC OD A Ġcomes Ġout Ġof Ġyour Ġout Ġof Ġyour Ġpay Ġand Ġthe Ġcredit Ġunion Ġcomes Ġout Ġof Ġyour Ġpay Ġso Ġwe Ġdon 't Ġhave Ġto Ġdo Ġanything Ġthere Ġand Ġthe Ġrest Ġof Ġit Ġas Ġfar Ġas Ġmy Ġsalary Ġgoes Ġi Ġjust Ġhave Ġit Ġautomatically Ġdeposited Ġin Ġinto Ġour Ġbank\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I set things up so that my salary automatically deposits into our bank.\n",
      "\t Tokenized GPT2:I Ġset Ġthings Ġup Ġso Ġthat Ġmy Ġsalary Ġautomatically Ġdeposits Ġinto Ġour Ġbank .\n",
      "\t Tokenized LLAMA3:I Ġset Ġthings Ġup Ġso Ġthat Ġmy Ġsalary Ġautomatically Ġdeposits Ġinto Ġour Ġbank .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: But Fish is not an upbeat pragmatist.\n",
      "\t Tokenized GPT2:But ĠFish Ġis Ġnot Ġan Ġup beat Ġprag mat ist .\n",
      "\t Tokenized LLAMA3:But ĠFish Ġis Ġnot Ġan Ġup beat Ġpr ag mat ist .\n",
      "\t Unique Tokens GPT2: {'Ġprag'}\n",
      "\t Unique Tokens LLAMA3: {'ag', 'Ġpr'}\n",
      "Text 2: Fish is the nickname of a human. \n",
      "\t Tokenized GPT2:F ish Ġis Ġthe Ġnickname Ġof Ġa Ġhuman . Ġ\n",
      "\t Tokenized LLAMA3:F ish Ġis Ġthe Ġnickname Ġof Ġa Ġhuman . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: This marvelous Victorian-Gothic building is famous for the fanciful stone carvings around the base of its pillars (one pillar, reputedly depicting the club members, shows monkeys playing billiards).\n",
      "\t Tokenized GPT2:This Ġmarvel ous ĠVictorian - G oth ic Ġbuilding Ġis Ġfamous Ġfor Ġthe Ġfan c iful Ġstone Ġcar v ings Ġaround Ġthe Ġbase Ġof Ġits Ġpill ars Ġ( one Ġpill ar , Ġreput edly Ġdep icting Ġthe Ġclub Ġmembers , Ġshows Ġmon keys Ġplaying Ġbill i ards ).\n",
      "\t Tokenized LLAMA3:This Ġmarvel ous ĠVictor ian -G oth ic Ġbuilding Ġis Ġfamous Ġfor Ġthe Ġfan c iful Ġstone Ġcar v ings Ġaround Ġthe Ġbase Ġof Ġits Ġpill ars Ġ( one Ġpill ar , Ġreput ed ly Ġdep icting Ġthe Ġclub Ġmembers , Ġshows Ġmon keys Ġplaying Ġbill i ards ).\n",
      "\t Unique Tokens GPT2: {'ĠVictorian', 'G', 'edly', '-'}\n",
      "\t Unique Tokens LLAMA3: {'ĠVictor', 'ly', 'ed', '-G', 'ian'}\n",
      "Text 2: Club members of the marvelous and famous Victorian-Gothic building are likened to monkeys for being rich douchebags.\n",
      "\t Tokenized GPT2:Cl ub Ġmembers Ġof Ġthe Ġmarvel ous Ġand Ġfamous ĠVictorian - G oth ic Ġbuilding Ġare Ġli ken ed Ġto Ġmon keys Ġfor Ġbeing Ġrich Ġdouche b ags .\n",
      "\t Tokenized LLAMA3:Cl ub Ġmembers Ġof Ġthe Ġmarvel ous Ġand Ġfamous ĠVictor ian -G oth ic Ġbuilding Ġare Ġli ken ed Ġto Ġmon keys Ġfor Ġbeing Ġrich Ġdouche b ags .\n",
      "\t Unique Tokens GPT2: {'ĠVictorian', 'G', '-'}\n",
      "\t Unique Tokens LLAMA3: {'ĠVictor', 'ian', '-G'}\n",
      "==contradiction==\n",
      "Text 1: and the professors who go there and you're not going to see the professors you know you're going to see some TA you know uh\n",
      "\t Tokenized GPT2:and Ġthe Ġprofessors Ġwho Ġgo Ġthere Ġand Ġyou 're Ġnot Ġgoing Ġto Ġsee Ġthe Ġprofessors Ġyou Ġknow Ġyou 're Ġgoing Ġto Ġsee Ġsome ĠTA Ġyou Ġknow Ġuh\n",
      "\t Tokenized LLAMA3:and Ġthe Ġprofessors Ġwho Ġgo Ġthere Ġand Ġyou 're Ġnot Ġgoing Ġto Ġsee Ġthe Ġprofessors Ġyou Ġknow Ġyou 're Ġgoing Ġto Ġsee Ġsome ĠTA Ġyou Ġknow Ġuh\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: You don't really see the TAs.\n",
      "\t Tokenized GPT2:You Ġdon 't Ġreally Ġsee Ġthe ĠT As .\n",
      "\t Tokenized LLAMA3:You Ġdon 't Ġreally Ġsee Ġthe ĠT As .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: see too much crime on TV and they think it's way to go i don't know what do you think\n",
      "\t Tokenized GPT2:see Ġtoo Ġmuch Ġcrime Ġon ĠTV Ġand Ġthey Ġthink Ġit 's Ġway Ġto Ġgo Ġi Ġdon 't Ġknow Ġwhat Ġdo Ġyou Ġthink\n",
      "\t Tokenized LLAMA3:see Ġtoo Ġmuch Ġcrime Ġon ĠTV Ġand Ġthey Ġthink Ġit 's Ġway Ġto Ġgo Ġi Ġdon 't Ġknow Ġwhat Ġdo Ġyou Ġthink\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: TV has a lot of crime shown on it.\n",
      "\t Tokenized GPT2:TV Ġhas Ġa Ġlot Ġof Ġcrime Ġshown Ġon Ġit .\n",
      "\t Tokenized LLAMA3:TV Ġhas Ġa Ġlot Ġof Ġcrime Ġshown Ġon Ġit .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Once or twice, but they seem more show than battle, said Adrin.\n",
      "\t Tokenized GPT2:Once Ġor Ġtwice , Ġbut Ġthey Ġseem Ġmore Ġshow Ġthan Ġbattle , Ġsaid ĠAd rin .\n",
      "\t Tokenized LLAMA3:Once Ġor Ġtwice , Ġbut Ġthey Ġseem Ġmore Ġshow Ġthan Ġbattle , Ġsaid ĠAd rin .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Adrin said they were amazing warriors.\n",
      "\t Tokenized GPT2:Ad rin Ġsaid Ġthey Ġwere Ġamazing Ġwarriors .\n",
      "\t Tokenized LLAMA3:Ad rin Ġsaid Ġthey Ġwere Ġamazing Ġwarriors .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: yeah yeah i i went i went off to school wanting to either be a high school algebra teacher or high school French teacher because my two favorite people in the in high school were my algebra teacher and French teacher and uh and i was going to do that until the end of our sophomore year when we wante\n",
      "\t Tokenized GPT2:yeah Ġyeah Ġi Ġi Ġwent Ġi Ġwent Ġoff Ġto Ġschool Ġwanting Ġto Ġeither Ġbe Ġa Ġhigh Ġschool Ġalgebra Ġteacher Ġor Ġhigh Ġschool ĠFrench Ġteacher Ġbecause Ġmy Ġtwo Ġfavorite Ġpeople Ġin Ġthe Ġin Ġhigh Ġschool Ġwere Ġmy Ġalgebra Ġteacher Ġand ĠFrench Ġteacher Ġand Ġuh Ġand Ġi Ġwas Ġgoing Ġto Ġdo Ġthat Ġuntil Ġthe Ġend Ġof Ġour Ġsophomore Ġyear Ġwhen Ġwe Ġwant e\n",
      "\t Tokenized LLAMA3:yeah Ġyeah Ġi Ġi Ġwent Ġi Ġwent Ġoff Ġto Ġschool Ġwanting Ġto Ġeither Ġbe Ġa Ġhigh Ġschool Ġalgebra Ġteacher Ġor Ġhigh Ġschool ĠFrench Ġteacher Ġbecause Ġmy Ġtwo Ġfavorite Ġpeople Ġin Ġthe Ġin Ġhigh Ġschool Ġwere Ġmy Ġalgebra Ġteacher Ġand ĠFrench Ġteacher Ġand Ġuh Ġand Ġi Ġwas Ġgoing Ġto Ġdo Ġthat Ġuntil Ġthe Ġend Ġof Ġour Ġsophomore Ġyear Ġwhen Ġwe Ġwant e\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I was going to major in algebra or french but I ended falling in love with chemistry. \n",
      "\t Tokenized GPT2:I Ġwas Ġgoing Ġto Ġmajor Ġin Ġalgebra Ġor Ġfrench Ġbut ĠI Ġended Ġfalling Ġin Ġlove Ġwith Ġchemistry . Ġ\n",
      "\t Tokenized LLAMA3:I Ġwas Ġgoing Ġto Ġmajor Ġin Ġalgebra Ġor Ġfrench Ġbut ĠI Ġended Ġfalling Ġin Ġlove Ġwith Ġchemistry . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: But in fact Haveman and Wolfe's statistical analysis is designed to rule out this and similar alternative theories, leaving us to conclude that the moves themselves are harmful.\n",
      "\t Tokenized GPT2:But Ġin Ġfact ĠHave man Ġand ĠWol fe 's Ġstatistical Ġanalysis Ġis Ġdesigned Ġto Ġrule Ġout Ġthis Ġand Ġsimilar Ġalternative Ġtheories , Ġleaving Ġus Ġto Ġconclude Ġthat Ġthe Ġmoves Ġthemselves Ġare Ġharmful .\n",
      "\t Tokenized LLAMA3:But Ġin Ġfact ĠHave man Ġand ĠWol fe 's Ġstatistical Ġanalysis Ġis Ġdesigned Ġto Ġrule Ġout Ġthis Ġand Ġsimilar Ġalternative Ġtheories , Ġleaving Ġus Ġto Ġconclude Ġthat Ġthe Ġmoves Ġthemselves Ġare Ġharmful .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: We think these moves are benevolent.\n",
      "\t Tokenized GPT2:We Ġthink Ġthese Ġmoves Ġare Ġbene vol ent .\n",
      "\t Tokenized LLAMA3:We Ġthink Ġthese Ġmoves Ġare Ġbene vol ent .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: It is worth a visit, if only to see the theater itself.\n",
      "\t Tokenized GPT2:It Ġis Ġworth Ġa Ġvisit , Ġif Ġonly Ġto Ġsee Ġthe Ġtheater Ġitself .\n",
      "\t Tokenized LLAMA3:It Ġis Ġworth Ġa Ġvisit , Ġif Ġonly Ġto Ġsee Ġthe Ġtheater Ġitself .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The place is definitely worth visiting, especially for its theater.\n",
      "\t Tokenized GPT2:The Ġplace Ġis Ġdefinitely Ġworth Ġvisiting , Ġespecially Ġfor Ġits Ġtheater .\n",
      "\t Tokenized LLAMA3:The Ġplace Ġis Ġdefinitely Ġworth Ġvisiting , Ġespecially Ġfor Ġits Ġtheater .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The Standard , published a few days before Deng's death, covers similar territory.\n",
      "\t Tokenized GPT2:The ĠStandard Ġ, Ġpublished Ġa Ġfew Ġdays Ġbefore ĠD eng 's Ġdeath , Ġcovers Ġsimilar Ġterritory .\n",
      "\t Tokenized LLAMA3:The ĠStandard Ġ, Ġpublished Ġa Ġfew Ġdays Ġbefore ĠD eng 's Ġdeath , Ġcovers Ġsimilar Ġterritory .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The Washington Post covers similar territory.\n",
      "\t Tokenized GPT2:The ĠWashington ĠPost Ġcovers Ġsimilar Ġterritory .\n",
      "\t Tokenized LLAMA3:The ĠWashington ĠPost Ġcovers Ġsimilar Ġterritory .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The stuff was strong, but somewhat brittle.\n",
      "\t Tokenized GPT2:The Ġstuff Ġwas Ġstrong , Ġbut Ġsomewhat Ġbr ittle .\n",
      "\t Tokenized LLAMA3:The Ġstuff Ġwas Ġstrong , Ġbut Ġsomewhat Ġbr ittle .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The plaster was strong, yet brittle.\n",
      "\t Tokenized GPT2:The Ġpl aster Ġwas Ġstrong , Ġyet Ġbr ittle .\n",
      "\t Tokenized LLAMA3:The Ġpl aster Ġwas Ġstrong , Ġyet Ġbr ittle .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Don't mean the police, but the people that are right in it. \n",
      "\t Tokenized GPT2:Don 't Ġmean Ġthe Ġpolice , Ġbut Ġthe Ġpeople Ġthat Ġare Ġright Ġin Ġit . Ġ\n",
      "\t Tokenized LLAMA3:Don 't Ġmean Ġthe Ġpolice , Ġbut Ġthe Ġpeople Ġthat Ġare Ġright Ġin Ġit . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The people were right. \n",
      "\t Tokenized GPT2:The Ġpeople Ġwere Ġright . Ġ\n",
      "\t Tokenized LLAMA3:The Ġpeople Ġwere Ġright . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Last year, Arafat cracked down on Hamas after a string of bombings in Tel Aviv and Jerusalem, arresting more than 1,200 suspected terrorists, destroying Hamas safe houses, and confiscating its weapons caches.\n",
      "\t Tokenized GPT2:Last Ġyear , ĠA ra fat Ġcracked Ġdown Ġon ĠHam as Ġafter Ġa Ġstring Ġof Ġbomb ings Ġin ĠTel ĠAv iv Ġand ĠJerusalem , Ġarrest ing Ġmore Ġthan Ġ1 , 200 Ġsuspected Ġterrorists , Ġdestroying ĠHam as Ġsafe Ġhouses , Ġand Ġconf isc ating Ġits Ġweapons Ġc aches .\n",
      "\t Tokenized LLAMA3:Last Ġyear , ĠA raf at Ġcracked Ġdown Ġon ĠHam as Ġafter Ġa Ġstring Ġof Ġbomb ings Ġin ĠTel ĠAv iv Ġand ĠJerusalem , Ġarrest ing Ġmore Ġthan Ġ 1 , 200 Ġsuspected Ġterrorists , Ġdestroying ĠHam as Ġsafe Ġhouses , Ġand Ġconf isc ating Ġits Ġweapons Ġc aches .\n",
      "\t Unique Tokens GPT2: {'ra', 'Ġ1', 'fat'}\n",
      "\t Unique Tokens LLAMA3: {'1', 'at', 'raf', 'Ġ'}\n",
      "Text 2: Arafat stood back and watched Hamas attack Tel Aviv and Jerusalem.\n",
      "\t Tokenized GPT2:A ra fat Ġstood Ġback Ġand Ġwatched ĠHam as Ġattack ĠTel ĠAv iv Ġand ĠJerusalem .\n",
      "\t Tokenized LLAMA3:A raf at Ġstood Ġback Ġand Ġwatched ĠHam as Ġattack ĠTel ĠAv iv Ġand ĠJerusalem .\n",
      "\t Unique Tokens GPT2: {'ra', 'fat'}\n",
      "\t Unique Tokens LLAMA3: {'at', 'raf'}\n",
      "==contradiction==\n",
      "Text 1: Allow time in Thirasia to explore Santorini's smaller sibling islands.\n",
      "\t Tokenized GPT2:Allow Ġtime Ġin ĠTh ir asia Ġto Ġexplore ĠSant or ini 's Ġsmaller Ġsibling Ġislands .\n",
      "\t Tokenized LLAMA3:Allow Ġtime Ġin ĠTh ir asia Ġto Ġexplore ĠSant or ini 's Ġsmaller Ġsibling Ġislands .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Allow time in Thirasia to visit the famous Guy Fieri restaurant.\n",
      "\t Tokenized GPT2:Allow Ġtime Ġin ĠTh ir asia Ġto Ġvisit Ġthe Ġfamous ĠGuy ĠF ier i Ġrestaurant .\n",
      "\t Tokenized LLAMA3:Allow Ġtime Ġin ĠTh ir asia Ġto Ġvisit Ġthe Ġfamous ĠGuy ĠF ier i Ġrestaurant .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: we wouldn't be expected to cast a ballet on the subject\n",
      "\t Tokenized GPT2:we Ġwouldn 't Ġbe Ġexpected Ġto Ġcast Ġa Ġbal let Ġon Ġthe Ġsubject\n",
      "\t Tokenized LLAMA3:we Ġwouldn 't Ġbe Ġexpected Ġto Ġcast Ġa Ġbal let Ġon Ġthe Ġsubject\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: There is no expectation for us to vote on the matter.\n",
      "\t Tokenized GPT2:There Ġis Ġno Ġexpectation Ġfor Ġus Ġto Ġvote Ġon Ġthe Ġmatter .\n",
      "\t Tokenized LLAMA3:There Ġis Ġno Ġexpectation Ġfor Ġus Ġto Ġvote Ġon Ġthe Ġmatter .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: oh that's not really important the the other stuff is just you know window dressing because we we've never ordered anything fact the the van that we've got we bought uh from an estate it was an estate trade uh it was almost brand new the the gentlemen who owned it had died\n",
      "\t Tokenized GPT2:oh Ġthat 's Ġnot Ġreally Ġimportant Ġthe Ġthe Ġother Ġstuff Ġis Ġjust Ġyou Ġknow Ġwindow Ġdressing Ġbecause Ġwe Ġwe 've Ġnever Ġordered Ġanything Ġfact Ġthe Ġthe Ġvan Ġthat Ġwe 've Ġgot Ġwe Ġbought Ġuh Ġfrom Ġan Ġestate Ġit Ġwas Ġan Ġestate Ġtrade Ġuh Ġit Ġwas Ġalmost Ġbrand Ġnew Ġthe Ġthe Ġgentlemen Ġwho Ġowned Ġit Ġhad Ġdied\n",
      "\t Tokenized LLAMA3:oh Ġthat 's Ġnot Ġreally Ġimportant Ġthe Ġthe Ġother Ġstuff Ġis Ġjust Ġyou Ġknow Ġwindow Ġdressing Ġbecause Ġwe Ġwe 've Ġnever Ġordered Ġanything Ġfact Ġthe Ġthe Ġvan Ġthat Ġwe 've Ġgot Ġwe Ġbought Ġuh Ġfrom Ġan Ġestate Ġit Ġwas Ġan Ġestate Ġtrade Ġuh Ġit Ġwas Ġalmost Ġbrand Ġnew Ġthe Ġthe Ġgentlemen Ġwho Ġowned Ġit Ġhad Ġdied\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: We ordered our van and bought it from a used car dealer.\n",
      "\t Tokenized GPT2:We Ġordered Ġour Ġvan Ġand Ġbought Ġit Ġfrom Ġa Ġused Ġcar Ġdealer .\n",
      "\t Tokenized LLAMA3:We Ġordered Ġour Ġvan Ġand Ġbought Ġit Ġfrom Ġa Ġused Ġcar Ġdealer .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Do you trust me, Uncle?Gauve hesitated.\n",
      "\t Tokenized GPT2:Do Ġyou Ġtrust Ġme , ĠUncle ? G au ve Ġhesitated .\n",
      "\t Tokenized LLAMA3:Do Ġyou Ġtrust Ġme , ĠUncle ? G au ve Ġhesitated .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Gauve's uncle has trust issues.\n",
      "\t Tokenized GPT2:G au ve 's Ġuncle Ġhas Ġtrust Ġissues .\n",
      "\t Tokenized LLAMA3:G au ve 's Ġuncle Ġhas Ġtrust Ġissues .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Critics praise Goodman's finely honed descriptive abilities and instinctive grasp of familial dynamics, the ways in which dreams and emotional habits are handed down ...\n",
      "\t Tokenized GPT2:Cr itics Ġpraise ĠGood man 's Ġfine ly Ġhon ed Ġdescriptive Ġabilities Ġand Ġinstinct ive Ġgrasp Ġof Ġfam il ial Ġdynamics , Ġthe Ġways Ġin Ġwhich Ġdreams Ġand Ġemotional Ġhabits Ġare Ġhanded Ġdown Ġ...\n",
      "\t Tokenized LLAMA3:Cr itics Ġpraise ĠGood man 's Ġfine ly Ġhon ed Ġdescriptive Ġabilities Ġand Ġinstinct ive Ġgrasp Ġof Ġfam il ial Ġdynamics , Ġthe Ġways Ġin Ġwhich Ġdreams Ġand Ġemotional Ġhabits Ġare Ġhanded Ġdown Ġ...\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Emotional habits and dreams are always derived from candy and popcorn.\n",
      "\t Tokenized GPT2:Em otional Ġhabits Ġand Ġdreams Ġare Ġalways Ġderived Ġfrom Ġcandy Ġand Ġpopcorn .\n",
      "\t Tokenized LLAMA3:Em ot ional Ġhabits Ġand Ġdreams Ġare Ġalways Ġderived Ġfrom Ġcandy Ġand Ġpop corn .\n",
      "\t Unique Tokens GPT2: {'otional', 'Ġpopcorn'}\n",
      "\t Unique Tokens LLAMA3: {'Ġpop', 'ional', 'corn', 'ot'}\n",
      "==neutral==\n",
      "Text 1: The most comfortable courses are in the cooler hill stations, notably Cameron Highlands and Fraser's Hill.\n",
      "\t Tokenized GPT2:The Ġmost Ġcomfortable Ġcourses Ġare Ġin Ġthe Ġcooler Ġhill Ġstations , Ġnotably ĠCameron ĠHigh lands Ġand ĠFr aser 's ĠHill .\n",
      "\t Tokenized LLAMA3:The Ġmost Ġcomfortable Ġcourses Ġare Ġin Ġthe Ġcooler Ġhill Ġstations , Ġnotably ĠCameron ĠHigh lands Ġand ĠFr aser 's ĠHill .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It's best to golf in the cooler hill stations.\n",
      "\t Tokenized GPT2:It 's Ġbest Ġto Ġgolf Ġin Ġthe Ġcooler Ġhill Ġstations .\n",
      "\t Tokenized LLAMA3:It 's Ġbest Ġto Ġgolf Ġin Ġthe Ġcooler Ġhill Ġstations .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Sometimes it flattens entire neighbourhoods to make life easier for them.\n",
      "\t Tokenized GPT2:Sometimes Ġit Ġfl att ens Ġentire Ġneighbourhood s Ġto Ġmake Ġlife Ġeasier Ġfor Ġthem .\n",
      "\t Tokenized LLAMA3:Sometimes Ġit Ġfl att ens Ġentire Ġneighbourhood s Ġto Ġmake Ġlife Ġeasier Ġfor Ġthem .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Entire neighborhoods have been flattened just to make life easier for them.\n",
      "\t Tokenized GPT2:Ent ire Ġneighborhoods Ġhave Ġbeen Ġflatten ed Ġjust Ġto Ġmake Ġlife Ġeasier Ġfor Ġthem .\n",
      "\t Tokenized LLAMA3:Ent ire Ġneighborhoods Ġhave Ġbeen Ġflatten ed Ġjust Ġto Ġmake Ġlife Ġeasier Ġfor Ġthem .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Each working group met several times to develop recommendations for changes to the legal services delivery system.\n",
      "\t Tokenized GPT2:Each Ġworking Ġgroup Ġmet Ġseveral Ġtimes Ġto Ġdevelop Ġrecommendations Ġfor Ġchanges Ġto Ġthe Ġlegal Ġservices Ġdelivery Ġsystem .\n",
      "\t Tokenized LLAMA3:Each Ġworking Ġgroup Ġmet Ġseveral Ġtimes Ġto Ġdevelop Ġrecommendations Ġfor Ġchanges Ġto Ġthe Ġlegal Ġservices Ġdelivery Ġsystem .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The groups disagreed on the appropriate action to take, but they finally found a solution. \n",
      "\t Tokenized GPT2:The Ġgroups Ġdisag reed Ġon Ġthe Ġappropriate Ġaction Ġto Ġtake , Ġbut Ġthey Ġfinally Ġfound Ġa Ġsolution . Ġ\n",
      "\t Tokenized LLAMA3:The Ġgroups Ġdisag reed Ġon Ġthe Ġappropriate Ġaction Ġto Ġtake , Ġbut Ġthey Ġfinally Ġfound Ġa Ġsolution . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: We are assured of success?\"\n",
      "\t Tokenized GPT2:We Ġare Ġassured Ġof Ġsuccess ?\"\n",
      "\t Tokenized LLAMA3:We Ġare Ġassured Ġof Ġsuccess ?\"\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: \"Are we definitely going to be successful and stay alive?\" \n",
      "\t Tokenized GPT2:\" Are Ġwe Ġdefinitely Ġgoing Ġto Ġbe Ġsuccessful Ġand Ġstay Ġalive ?\" Ġ\n",
      "\t Tokenized LLAMA3:\"Are Ġwe Ġdefinitely Ġgoing Ġto Ġbe Ġsuccessful Ġand Ġstay Ġalive ?\" Ġ\n",
      "\t Unique Tokens GPT2: {'\"', 'Are'}\n",
      "\t Unique Tokens LLAMA3: {'\"Are'}\n",
      "==neutral==\n",
      "Text 1: At the western end of Cowgate (where it meets Holyrood Road), you will see one of the few remaining sections of Edinburgh's old city wall (Flodden Wall), built following the Lang Siege of the 1570s.\n",
      "\t Tokenized GPT2:At Ġthe Ġwestern Ġend Ġof ĠCow gate Ġ( where Ġit Ġmeets ĠHoly ro od ĠRoad ), Ġyou Ġwill Ġsee Ġone Ġof Ġthe Ġfew Ġremaining Ġsections Ġof ĠEdinburgh 's Ġold Ġcity Ġwall Ġ( Fl od den ĠWall ), Ġbuilt Ġfollowing Ġthe ĠLang ĠSie ge Ġof Ġthe Ġ15 70 s .\n",
      "\t Tokenized LLAMA3:At Ġthe Ġwestern Ġend Ġof ĠCow gate Ġ( where Ġit Ġmeets ĠHoly ro od ĠRoad ), Ġyou Ġwill Ġsee Ġone Ġof Ġthe Ġfew Ġremaining Ġsections Ġof ĠEdinburgh 's Ġold Ġcity Ġwall Ġ( Fl od den ĠWall ), Ġbuilt Ġfollowing Ġthe ĠLang ĠSie ge Ġof Ġthe Ġ 157 0 s .\n",
      "\t Unique Tokens GPT2: {'Ġ15', '70'}\n",
      "\t Unique Tokens LLAMA3: {'157', '0', 'Ġ'}\n",
      "Text 2: Flodden Wall was built by the townspeople to protect against further invasion.\n",
      "\t Tokenized GPT2:Fl od den ĠWall Ġwas Ġbuilt Ġby Ġthe Ġtown spe ople Ġto Ġprotect Ġagainst Ġfurther Ġinvasion .\n",
      "\t Tokenized LLAMA3:Fl od den ĠWall Ġwas Ġbuilt Ġby Ġthe Ġtown spe ople Ġto Ġprotect Ġagainst Ġfurther Ġinvasion .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: they'll they'll say yeah why didn't you buy why didn't you try something more mainline\n",
      "\t Tokenized GPT2:they 'll Ġthey 'll Ġsay Ġyeah Ġwhy Ġdidn 't Ġyou Ġbuy Ġwhy Ġdidn 't Ġyou Ġtry Ġsomething Ġmore Ġmain line\n",
      "\t Tokenized LLAMA3:they 'll Ġthey 'll Ġsay Ġyeah Ġwhy Ġdidn 't Ġyou Ġbuy Ġwhy Ġdidn 't Ġyou Ġtry Ġsomething Ġmore Ġmain line\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: They will scoff at you for doing something so mainline.\n",
      "\t Tokenized GPT2:They Ġwill Ġscoff Ġat Ġyou Ġfor Ġdoing Ġsomething Ġso Ġmain line .\n",
      "\t Tokenized LLAMA3:They Ġwill Ġscoff Ġat Ġyou Ġfor Ġdoing Ġsomething Ġso Ġmain line .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Ah, yes, actually, two weeks ago we had a very similar situation, the captain alertly added and quickly changed the subject, 'What's important now is that you get ready for about 2 minutes in the state of weightlessness, and not some Slovakian satellite from two weeks ago.\n",
      "\t Tokenized GPT2:Ah , Ġyes , Ġactually , Ġtwo Ġweeks Ġago Ġwe Ġhad Ġa Ġvery Ġsimilar Ġsituation , Ġthe Ġcaptain Ġalert ly Ġadded Ġand Ġquickly Ġchanged Ġthe Ġsubject , Ġ' What 's Ġimportant Ġnow Ġis Ġthat Ġyou Ġget Ġready Ġfor Ġabout Ġ2 Ġminutes Ġin Ġthe Ġstate Ġof Ġweight lessness , Ġand Ġnot Ġsome ĠSl ov ak ian Ġsatellite Ġfrom Ġtwo Ġweeks Ġago .\n",
      "\t Tokenized LLAMA3:Ah , Ġyes , Ġactually , Ġtwo Ġweeks Ġago Ġwe Ġhad Ġa Ġvery Ġsimilar Ġsituation , Ġthe Ġcaptain Ġalert ly Ġadded Ġand Ġquickly Ġchanged Ġthe Ġsubject , Ġ' What 's Ġimportant Ġnow Ġis Ġthat Ġyou Ġget Ġready Ġfor Ġabout Ġ 2 Ġminutes Ġin Ġthe Ġstate Ġof Ġweight lessness , Ġand Ġnot Ġsome ĠSl ov ak ian Ġsatellite Ġfrom Ġtwo Ġweeks Ġago .\n",
      "\t Unique Tokens GPT2: {'Ġ2'}\n",
      "\t Unique Tokens LLAMA3: {'2', 'Ġ'}\n",
      "Text 2: The captain of the ship didn't want to bring up the destruction of the Slovakian satellite that happened two weeks prior. \n",
      "\t Tokenized GPT2:The Ġcaptain Ġof Ġthe Ġship Ġdidn 't Ġwant Ġto Ġbring Ġup Ġthe Ġdestruction Ġof Ġthe ĠSl ov ak ian Ġsatellite Ġthat Ġhappened Ġtwo Ġweeks Ġprior . Ġ\n",
      "\t Tokenized LLAMA3:The Ġcaptain Ġof Ġthe Ġship Ġdidn 't Ġwant Ġto Ġbring Ġup Ġthe Ġdestruction Ġof Ġthe ĠSl ov ak ian Ġsatellite Ġthat Ġhappened Ġtwo Ġweeks Ġprior . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: He touched it and felt his skin swelling and growing hot.\n",
      "\t Tokenized GPT2:He Ġtouched Ġit Ġand Ġfelt Ġhis Ġskin Ġswelling Ġand Ġgrowing Ġhot .\n",
      "\t Tokenized LLAMA3:He Ġtouched Ġit Ġand Ġfelt Ġhis Ġskin Ġswelling Ġand Ġgrowing Ġhot .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He touched a hot rod.\n",
      "\t Tokenized GPT2:He Ġtouched Ġa Ġhot Ġrod .\n",
      "\t Tokenized LLAMA3:He Ġtouched Ġa Ġhot Ġrod .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: According to a 1995 Financial Executives Research Foundation report,5 transaction processing and other routine accounting activities, such as accounts payable, payroll, and external reporting, consume about 69 percent of costs within finance.\n",
      "\t Tokenized GPT2:According Ġto Ġa Ġ1995 ĠFinancial ĠExecut ives ĠResearch ĠFoundation Ġreport , 5 Ġtransaction Ġprocessing Ġand Ġother Ġroutine Ġaccounting Ġactivities , Ġsuch Ġas Ġaccounts Ġpay able , Ġpayroll , Ġand Ġexternal Ġreporting , Ġconsume Ġabout Ġ69 Ġpercent Ġof Ġcosts Ġwithin Ġfinance .\n",
      "\t Tokenized LLAMA3:According Ġto Ġa Ġ 199 5 ĠFinancial ĠExecut ives ĠResearch ĠFoundation Ġreport , 5 Ġtransaction Ġprocessing Ġand Ġother Ġroutine Ġaccounting Ġactivities , Ġsuch Ġas Ġaccounts Ġpay able , Ġpay roll , Ġand Ġexternal Ġreporting , Ġconsume Ġabout Ġ 69 Ġpercent Ġof Ġcosts Ġwithin Ġfinance .\n",
      "\t Unique Tokens GPT2: {'Ġ1995', 'Ġ69', 'Ġpayroll'}\n",
      "\t Unique Tokens LLAMA3: {'roll', '199', 'Ġ', '69'}\n",
      "Text 2: Almost 70% of costs within finance are for routine accounting activities. \n",
      "\t Tokenized GPT2:Almost Ġ70 % Ġof Ġcosts Ġwithin Ġfinance Ġare Ġfor Ġroutine Ġaccounting Ġactivities . Ġ\n",
      "\t Tokenized LLAMA3:Almost Ġ 70 % Ġof Ġcosts Ġwithin Ġfinance Ġare Ġfor Ġroutine Ġaccounting Ġactivities . Ġ\n",
      "\t Unique Tokens GPT2: {'Ġ70'}\n",
      "\t Unique Tokens LLAMA3: {'70'}\n",
      "==entailment==\n",
      "Text 1: (The employee was later rehired, and Bob denies the charge.)\n",
      "\t Tokenized GPT2:( The Ġemployee Ġwas Ġlater Ġre h ired , Ġand ĠBob Ġden ies Ġthe Ġcharge .)\n",
      "\t Tokenized LLAMA3:( The Ġemployee Ġwas Ġlater Ġre h ired , Ġand ĠBob Ġden ies Ġthe Ġcharge .)\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The employee got their job back.\n",
      "\t Tokenized GPT2:The Ġemployee Ġgot Ġtheir Ġjob Ġback .\n",
      "\t Tokenized LLAMA3:The Ġemployee Ġgot Ġtheir Ġjob Ġback .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Despite their 17th-century origins, these gardens avoid the rigid geometry of the Tuileries and Ver?­sailles.\n",
      "\t Tokenized GPT2:Despite Ġtheir Ġ17 th - century Ġorigins , Ġthese Ġgardens Ġavoid Ġthe Ġrigid Ġgeometry Ġof Ġthe ĠTu iler ies Ġand ĠVer ? ÂŃ sa illes .\n",
      "\t Tokenized LLAMA3:Despite Ġtheir Ġ 17 th -century Ġorigins , Ġthese Ġgardens Ġavoid Ġthe Ġrigid Ġgeometry Ġof Ġthe ĠTu iler ies Ġand ĠVer ? ÂŃ sa illes .\n",
      "\t Unique Tokens GPT2: {'-', 'century', 'Ġ17'}\n",
      "\t Unique Tokens LLAMA3: {'-century', 'Ġ', '17'}\n",
      "Text 2: These gardens were around well before the 17th-century.\n",
      "\t Tokenized GPT2:These Ġgardens Ġwere Ġaround Ġwell Ġbefore Ġthe Ġ17 th - century .\n",
      "\t Tokenized LLAMA3:These Ġgardens Ġwere Ġaround Ġwell Ġbefore Ġthe Ġ 17 th -century .\n",
      "\t Unique Tokens GPT2: {'-', 'century', 'Ġ17'}\n",
      "\t Unique Tokens LLAMA3: {'-century', 'Ġ', '17'}\n",
      "==neutral==\n",
      "Text 1: (The Ramseys buried their daughter in Atlanta, then vacationed in Sea Island, Ga.) This absence, some speculate, gave the Ramseys time to work out a story to explain their innocence.\n",
      "\t Tokenized GPT2:( The ĠRam se ys Ġburied Ġtheir Ġdaughter Ġin ĠAtlanta , Ġthen Ġvacation ed Ġin ĠSea ĠIsland , ĠGa .) ĠThis Ġabsence , Ġsome Ġspec ulate , Ġgave Ġthe ĠRam se ys Ġtime Ġto Ġwork Ġout Ġa Ġstory Ġto Ġexplain Ġtheir Ġinnocence .\n",
      "\t Tokenized LLAMA3:( The ĠRam se ys Ġburied Ġtheir Ġdaughter Ġin ĠAtlanta , Ġthen Ġvacation ed Ġin ĠSea ĠIsland , ĠGa .) ĠThis Ġabsence , Ġsome Ġspec ulate , Ġgave Ġthe ĠRam se ys Ġtime Ġto Ġwork Ġout Ġa Ġstory Ġto Ġexplain Ġtheir Ġinnocence .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The Ramseys went on vacation to relieve themselves of killing their daughter.\n",
      "\t Tokenized GPT2:The ĠRam se ys Ġwent Ġon Ġvacation Ġto Ġrelieve Ġthemselves Ġof Ġkilling Ġtheir Ġdaughter .\n",
      "\t Tokenized LLAMA3:The ĠRam se ys Ġwent Ġon Ġvacation Ġto Ġrel ieve Ġthemselves Ġof Ġkilling Ġtheir Ġdaughter .\n",
      "\t Unique Tokens GPT2: {'Ġrelieve'}\n",
      "\t Unique Tokens LLAMA3: {'ieve', 'Ġrel'}\n",
      "==entailment==\n",
      "Text 1: The students' reaction was swift and contentious, as if their feelings had been hurt.\n",
      "\t Tokenized GPT2:The Ġstudents ' Ġreaction Ġwas Ġswift Ġand Ġcontent ious , Ġas Ġif Ġtheir Ġfeelings Ġhad Ġbeen Ġhurt .\n",
      "\t Tokenized LLAMA3:The Ġstudents ' Ġreaction Ġwas Ġswift Ġand Ġcontent ious , Ġas Ġif Ġtheir Ġfeelings Ġhad Ġbeen Ġhurt .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The students responded strongly.\n",
      "\t Tokenized GPT2:The Ġstudents Ġresponded Ġstrongly .\n",
      "\t Tokenized LLAMA3:The Ġstudents Ġresponded Ġstrongly .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Changes in technology and its application to electronic commerce and expanding Internet applications will change the specific control activities that may be employed and how they are implemented, but the basic requirements of control will not have changed.\n",
      "\t Tokenized GPT2:Changes Ġin Ġtechnology Ġand Ġits Ġapplication Ġto Ġelectronic Ġcommerce Ġand Ġexpanding ĠInternet Ġapplications Ġwill Ġchange Ġthe Ġspecific Ġcontrol Ġactivities Ġthat Ġmay Ġbe Ġemployed Ġand Ġhow Ġthey Ġare Ġimplemented , Ġbut Ġthe Ġbasic Ġrequirements Ġof Ġcontrol Ġwill Ġnot Ġhave Ġchanged .\n",
      "\t Tokenized LLAMA3:Changes Ġin Ġtechnology Ġand Ġits Ġapplication Ġto Ġelectronic Ġcommerce Ġand Ġexpanding ĠInternet Ġapplications Ġwill Ġchange Ġthe Ġspecific Ġcontrol Ġactivities Ġthat Ġmay Ġbe Ġemployed Ġand Ġhow Ġthey Ġare Ġimplemented , Ġbut Ġthe Ġbasic Ġrequirements Ġof Ġcontrol Ġwill Ġnot Ġhave Ġchanged .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Technology will make it so we have less control of activities. \n",
      "\t Tokenized GPT2:Techn ology Ġwill Ġmake Ġit Ġso Ġwe Ġhave Ġless Ġcontrol Ġof Ġactivities . Ġ\n",
      "\t Tokenized LLAMA3:Te chnology Ġwill Ġmake Ġit Ġso Ġwe Ġhave Ġless Ġcontrol Ġof Ġactivities . Ġ\n",
      "\t Unique Tokens GPT2: {'Techn', 'ology'}\n",
      "\t Unique Tokens LLAMA3: {'Te', 'chnology'}\n",
      "==contradiction==\n",
      "Text 1: The idea that Clinton's approval represents something new and immoral in the country is historically shortsighted.\n",
      "\t Tokenized GPT2:The Ġidea Ġthat ĠClinton 's Ġapproval Ġrepresents Ġsomething Ġnew Ġand Ġimm oral Ġin Ġthe Ġcountry Ġis Ġhistorically Ġshort sight ed .\n",
      "\t Tokenized LLAMA3:The Ġidea Ġthat ĠClinton 's Ġapproval Ġrepresents Ġsomething Ġnew Ġand Ġimm oral Ġin Ġthe Ġcountry Ġis Ġhistorically Ġshorts ighted .\n",
      "\t Unique Tokens GPT2: {'sight', 'ed', 'Ġshort'}\n",
      "\t Unique Tokens LLAMA3: {'Ġshorts', 'ighted'}\n",
      "Text 2: It's accurate to conclude that Clinton's approvals signify the start of a new form of immorality in the country.\n",
      "\t Tokenized GPT2:It 's Ġaccurate Ġto Ġconclude Ġthat ĠClinton 's Ġapproval s Ġsign ify Ġthe Ġstart Ġof Ġa Ġnew Ġform Ġof Ġimm or ality Ġin Ġthe Ġcountry .\n",
      "\t Tokenized LLAMA3:It 's Ġaccurate Ġto Ġconclude Ġthat ĠClinton 's Ġapproval s Ġsign ify Ġthe Ġstart Ġof Ġa Ġnew Ġform Ġof Ġimm or ality Ġin Ġthe Ġcountry .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Catch up on the Indian avant-garde and the bohemian people of Caletta at the Academy of Fine Arts on the southeast corner of the Maidan.\n",
      "\t Tokenized GPT2:C atch Ġup Ġon Ġthe ĠIndian Ġav ant - gard e Ġand Ġthe Ġbo hem ian Ġpeople Ġof ĠCal etta Ġat Ġthe ĠAcademy Ġof ĠFine ĠArts Ġon Ġthe Ġs outheast Ġcorner Ġof Ġthe ĠM aid an .\n",
      "\t Tokenized LLAMA3:C atch Ġup Ġon Ġthe ĠIndian Ġav ant -g ard e Ġand Ġthe Ġbo hem ian Ġpeople Ġof ĠCal etta Ġat Ġthe ĠAcademy Ġof ĠFine ĠArts Ġon Ġthe Ġs outheast Ġcorner Ġof Ġthe ĠM aid an .\n",
      "\t Unique Tokens GPT2: {'-', 'gard'}\n",
      "\t Unique Tokens LLAMA3: {'ard', '-g'}\n",
      "Text 2: The Academy of Fine Arts is located in Northern Maidan.\n",
      "\t Tokenized GPT2:The ĠAcademy Ġof ĠFine ĠArts Ġis Ġlocated Ġin ĠNorthern ĠM aid an .\n",
      "\t Tokenized LLAMA3:The ĠAcademy Ġof ĠFine ĠArts Ġis Ġlocated Ġin ĠNorthern ĠM aid an .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Wagonheim said the program not only will benefit the needy, but also will help improve the public image of lawyers.\n",
      "\t Tokenized GPT2:W agon heim Ġsaid Ġthe Ġprogram Ġnot Ġonly Ġwill Ġbenefit Ġthe Ġneed y , Ġbut Ġalso Ġwill Ġhelp Ġimprove Ġthe Ġpublic Ġimage Ġof Ġlawyers .\n",
      "\t Tokenized LLAMA3:W agon heim Ġsaid Ġthe Ġprogram Ġnot Ġonly Ġwill Ġbenefit Ġthe Ġneed y , Ġbut Ġalso Ġwill Ġhelp Ġimprove Ġthe Ġpublic Ġimage Ġof Ġlawyers .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The program isn't going to improve lawyers' public image.\n",
      "\t Tokenized GPT2:The Ġprogram Ġisn 't Ġgoing Ġto Ġimprove Ġlawyers ' Ġpublic Ġimage .\n",
      "\t Tokenized LLAMA3:The Ġprogram Ġisn 't Ġgoing Ġto Ġimprove Ġlawyers ' Ġpublic Ġimage .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: you know even even into major things just to keep our car longer because i don't think we get the money that we put into them out of them in two years or three years and of course i was never in a position where i could trade my car off every two years\n",
      "\t Tokenized GPT2:you Ġknow Ġeven Ġeven Ġinto Ġmajor Ġthings Ġjust Ġto Ġkeep Ġour Ġcar Ġlonger Ġbecause Ġi Ġdon 't Ġthink Ġwe Ġget Ġthe Ġmoney Ġthat Ġwe Ġput Ġinto Ġthem Ġout Ġof Ġthem Ġin Ġtwo Ġyears Ġor Ġthree Ġyears Ġand Ġof Ġcourse Ġi Ġwas Ġnever Ġin Ġa Ġposition Ġwhere Ġi Ġcould Ġtrade Ġmy Ġcar Ġoff Ġevery Ġtwo Ġyears\n",
      "\t Tokenized LLAMA3:you Ġknow Ġeven Ġeven Ġinto Ġmajor Ġthings Ġjust Ġto Ġkeep Ġour Ġcar Ġlonger Ġbecause Ġi Ġdon 't Ġthink Ġwe Ġget Ġthe Ġmoney Ġthat Ġwe Ġput Ġinto Ġthem Ġout Ġof Ġthem Ġin Ġtwo Ġyears Ġor Ġthree Ġyears Ġand Ġof Ġcourse Ġi Ġwas Ġnever Ġin Ġa Ġposition Ġwhere Ġi Ġcould Ġtrade Ġmy Ġcar Ġoff Ġevery Ġtwo Ġyears\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I find it to be more cost effective to keep and maintain an older car.\n",
      "\t Tokenized GPT2:I Ġfind Ġit Ġto Ġbe Ġmore Ġcost Ġeffective Ġto Ġkeep Ġand Ġmaintain Ġan Ġolder Ġcar .\n",
      "\t Tokenized LLAMA3:I Ġfind Ġit Ġto Ġbe Ġmore Ġcost Ġeffective Ġto Ġkeep Ġand Ġmaintain Ġan Ġolder Ġcar .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: there and they uh they in fact they had this was in uh the late twenties and they in fact used some of the equipment that had been left over and uh he turned them down it it's interesting that that most people don't realize how small the canal is have you ever been there\n",
      "\t Tokenized GPT2:there Ġand Ġthey Ġuh Ġthey Ġin Ġfact Ġthey Ġhad Ġthis Ġwas Ġin Ġuh Ġthe Ġlate Ġtwent ies Ġand Ġthey Ġin Ġfact Ġused Ġsome Ġof Ġthe Ġequipment Ġthat Ġhad Ġbeen Ġleft Ġover Ġand Ġuh Ġhe Ġturned Ġthem Ġdown Ġit Ġit 's Ġinteresting Ġthat Ġthat Ġmost Ġpeople Ġdon 't Ġrealize Ġhow Ġsmall Ġthe Ġcanal Ġis Ġhave Ġyou Ġever Ġbeen Ġthere\n",
      "\t Tokenized LLAMA3:there Ġand Ġthey Ġuh Ġthey Ġin Ġfact Ġthey Ġhad Ġthis Ġwas Ġin Ġuh Ġthe Ġlate Ġtwent ies Ġand Ġthey Ġin Ġfact Ġused Ġsome Ġof Ġthe Ġequipment Ġthat Ġhad Ġbeen Ġleft Ġover Ġand Ġuh Ġhe Ġturned Ġthem Ġdown Ġit Ġit 's Ġinteresting Ġthat Ġthat Ġmost Ġpeople Ġdon 't Ġrealize Ġhow Ġsmall Ġthe Ġcanal Ġis Ġhave Ġyou Ġever Ġbeen Ġthere\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Most people think the canal is tiny \n",
      "\t Tokenized GPT2:Most Ġpeople Ġthink Ġthe Ġcanal Ġis Ġtiny Ġ\n",
      "\t Tokenized LLAMA3:Most Ġpeople Ġthink Ġthe Ġcanal Ġis Ġtiny Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Cirque du Soleil's The latest from the acclaimed international troupe, O dazzles in an aquatic environment that utilizes 1.5 million gallons (6.8 million liters) of water.\n",
      "\t Tokenized GPT2:C ir que Ġdu ĠSo le il 's ĠThe Ġlatest Ġfrom Ġthe Ġac claimed Ġinternational Ġtr ou pe , ĠO Ġd azz les Ġin Ġan Ġaqu atic Ġenvironment Ġthat Ġutil izes Ġ1 . 5 Ġmillion Ġgall ons Ġ( 6 . 8 Ġmillion Ġlit ers ) Ġof Ġwater .\n",
      "\t Tokenized LLAMA3:C ir que Ġdu ĠSo le il 's ĠThe Ġlatest Ġfrom Ġthe Ġac claimed Ġinternational Ġtr ou pe , ĠO Ġd azz les Ġin Ġan Ġaqu atic Ġenvironment Ġthat Ġutil izes Ġ 1 . 5 Ġmillion Ġgall ons Ġ( 6 . 8 Ġmillion Ġlit ers ) Ġof Ġwater .\n",
      "\t Unique Tokens GPT2: {'Ġ1'}\n",
      "\t Unique Tokens LLAMA3: {'1', 'Ġ'}\n",
      "Text 2: Cirque du Soleil is an international troupe that performs a lot in Vegas.\n",
      "\t Tokenized GPT2:C ir que Ġdu ĠSo le il Ġis Ġan Ġinternational Ġtr ou pe Ġthat Ġperforms Ġa Ġlot Ġin ĠVegas .\n",
      "\t Tokenized LLAMA3:C ir que Ġdu ĠSo le il Ġis Ġan Ġinternational Ġtr ou pe Ġthat Ġperforms Ġa Ġlot Ġin ĠVegas .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Ah, triple pig! \n",
      "\t Tokenized GPT2:Ah , Ġtriple Ġpig ! Ġ\n",
      "\t Tokenized LLAMA3:Ah , Ġtriple Ġpig ! Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The pig tripled.\n",
      "\t Tokenized GPT2:The Ġpig Ġtri pled .\n",
      "\t Tokenized LLAMA3:The Ġpig Ġtri pled .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Managing better requires that agencies have, and rely upon, sound financial and program information.\n",
      "\t Tokenized GPT2:Man aging Ġbetter Ġrequires Ġthat Ġagencies Ġhave , Ġand Ġrely Ġupon , Ġsound Ġfinancial Ġand Ġprogram Ġinformation .\n",
      "\t Tokenized LLAMA3:Man aging Ġbetter Ġrequires Ġthat Ġagencies Ġhave , Ġand Ġrely Ġupon , Ġsound Ġfinancial Ġand Ġprogram Ġinformation .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Agencies that rely on information based on unsound financial information will have management problems.\n",
      "\t Tokenized GPT2:Ag encies Ġthat Ġrely Ġon Ġinformation Ġbased Ġon Ġuns ound Ġfinancial Ġinformation Ġwill Ġhave Ġmanagement Ġproblems .\n",
      "\t Tokenized LLAMA3:Ag encies Ġthat Ġrely Ġon Ġinformation Ġbased Ġon Ġuns ound Ġfinancial Ġinformation Ġwill Ġhave Ġmanagement Ġproblems .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: yeah i went to i went to uh Rice and we had the marching owl band which is quite a it's not known for its musical abilities more so its um comedy abilities\n",
      "\t Tokenized GPT2:yeah Ġi Ġwent Ġto Ġi Ġwent Ġto Ġuh ĠRice Ġand Ġwe Ġhad Ġthe Ġmarch ing Ġow l Ġband Ġwhich Ġis Ġquite Ġa Ġit 's Ġnot Ġknown Ġfor Ġits Ġmusical Ġabilities Ġmore Ġso Ġits Ġum Ġcomedy Ġabilities\n",
      "\t Tokenized LLAMA3:yeah Ġi Ġwent Ġto Ġi Ġwent Ġto Ġuh ĠRice Ġand Ġwe Ġhad Ġthe Ġmarch ing Ġow l Ġband Ġwhich Ġis Ġquite Ġa Ġit 's Ġnot Ġknown Ġfor Ġits Ġmusical Ġabilities Ġmore Ġso Ġits Ġum Ġcomedy Ġabilities\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The owl band was well known for its music abilities.\n",
      "\t Tokenized GPT2:The Ġow l Ġband Ġwas Ġwell Ġknown Ġfor Ġits Ġmusic Ġabilities .\n",
      "\t Tokenized LLAMA3:The Ġow l Ġband Ġwas Ġwell Ġknown Ġfor Ġits Ġmusic Ġabilities .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Have her show it,\" said Thorn.\n",
      "\t Tokenized GPT2:Have Ġher Ġshow Ġit ,\" Ġsaid ĠTh orn .\n",
      "\t Tokenized LLAMA3:Have Ġher Ġshow Ġit ,\" Ġsaid ĠTh orn .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Thorn told her to hide it.\n",
      "\t Tokenized GPT2:Th orn Ġtold Ġher Ġto Ġhide Ġit .\n",
      "\t Tokenized LLAMA3:Th orn Ġtold Ġher Ġto Ġhide Ġit .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: They are levied through the power of the Government to compel payment, and the person or entity that pays these fees does not receive anything of value from the Government in exchange.\n",
      "\t Tokenized GPT2:They Ġare Ġle v ied Ġthrough Ġthe Ġpower Ġof Ġthe ĠGovernment Ġto Ġcom pel Ġpayment , Ġand Ġthe Ġperson Ġor Ġentity Ġthat Ġpays Ġthese Ġfees Ġdoes Ġnot Ġreceive Ġanything Ġof Ġvalue Ġfrom Ġthe ĠGovernment Ġin Ġexchange .\n",
      "\t Tokenized LLAMA3:They Ġare Ġle v ied Ġthrough Ġthe Ġpower Ġof Ġthe ĠGovernment Ġto Ġcom pel Ġpayment , Ġand Ġthe Ġperson Ġor Ġentity Ġthat Ġpays Ġthese Ġfees Ġdoes Ġnot Ġreceive Ġanything Ġof Ġvalue Ġfrom Ġthe ĠGovernment Ġin Ġexchange .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: They are levied through the power of the Government to compel payment.\n",
      "\t Tokenized GPT2:They Ġare Ġle v ied Ġthrough Ġthe Ġpower Ġof Ġthe ĠGovernment Ġto Ġcom pel Ġpayment .\n",
      "\t Tokenized LLAMA3:They Ġare Ġle v ied Ġthrough Ġthe Ġpower Ġof Ġthe ĠGovernment Ġto Ġcom pel Ġpayment .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: yeah yeah yeah well because that's the way they they might seem outwardly but boy there's a lots going on in there\n",
      "\t Tokenized GPT2:yeah Ġyeah Ġyeah Ġwell Ġbecause Ġthat 's Ġthe Ġway Ġthey Ġthey Ġmight Ġseem Ġoutward ly Ġbut Ġboy Ġthere 's Ġa Ġlots Ġgoing Ġon Ġin Ġthere\n",
      "\t Tokenized LLAMA3:yeah Ġyeah Ġyeah Ġwell Ġbecause Ġthat 's Ġthe Ġway Ġthey Ġthey Ġmight Ġseem Ġoutward ly Ġbut Ġboy Ġthere 's Ġa Ġlots Ġgoing Ġon Ġin Ġthere\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: There is a lot of bad in the inside.\n",
      "\t Tokenized GPT2:There Ġis Ġa Ġlot Ġof Ġbad Ġin Ġthe Ġinside .\n",
      "\t Tokenized LLAMA3:There Ġis Ġa Ġlot Ġof Ġbad Ġin Ġthe Ġinside .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: not only that but they don't pay the money either\n",
      "\t Tokenized GPT2:not Ġonly Ġthat Ġbut Ġthey Ġdon 't Ġpay Ġthe Ġmoney Ġeither\n",
      "\t Tokenized LLAMA3:not Ġonly Ġthat Ġbut Ġthey Ġdon 't Ġpay Ġthe Ġmoney Ġeither\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: They've paid out their life savings.\n",
      "\t Tokenized GPT2:They 've Ġpaid Ġout Ġtheir Ġlife Ġsavings .\n",
      "\t Tokenized LLAMA3:They 've Ġpaid Ġout Ġtheir Ġlife Ġsavings .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: I'm sure I won't get stuck to it,' Julia remarked about the suitcase she was carrying.\n",
      "\t Tokenized GPT2:I 'm Ġsure ĠI Ġwon 't Ġget Ġstuck Ġto Ġit ,' ĠJulia Ġremarked Ġabout Ġthe Ġsuit case Ġshe Ġwas Ġcarrying .\n",
      "\t Tokenized LLAMA3:I 'm Ġsure ĠI Ġwon 't Ġget Ġstuck Ġto Ġit ,' ĠJulia Ġremarked Ġabout Ġthe Ġsuit case Ġshe Ġwas Ġcarrying .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Julia said that she was sure she would get stuck to the suitcase. \n",
      "\t Tokenized GPT2:Jul ia Ġsaid Ġthat Ġshe Ġwas Ġsure Ġshe Ġwould Ġget Ġstuck Ġto Ġthe Ġsuit case . Ġ\n",
      "\t Tokenized LLAMA3:Jul ia Ġsaid Ġthat Ġshe Ġwas Ġsure Ġshe Ġwould Ġget Ġstuck Ġto Ġthe Ġsuit case . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Each room was outfitted with a leather sofa and three fold-out beds for students exhausted after a full day of hard work.\n",
      "\t Tokenized GPT2:Each Ġroom Ġwas Ġout f itted Ġwith Ġa Ġleather Ġsofa Ġand Ġthree Ġfold - out Ġbeds Ġfor Ġstudents Ġexhausted Ġafter Ġa Ġfull Ġday Ġof Ġhard Ġwork .\n",
      "\t Tokenized LLAMA3:Each Ġroom Ġwas Ġout f itted Ġwith Ġa Ġleather Ġsofa Ġand Ġthree Ġfold -out Ġbeds Ġfor Ġstudents Ġexhausted Ġafter Ġa Ġfull Ġday Ġof Ġhard Ġwork .\n",
      "\t Unique Tokens GPT2: {'-', 'out'}\n",
      "\t Unique Tokens LLAMA3: {'-out'}\n",
      "Text 2: Every student slept in these housings, no matter what.\n",
      "\t Tokenized GPT2:Every Ġstudent Ġslept Ġin Ġthese Ġhous ings , Ġno Ġmatter Ġwhat .\n",
      "\t Tokenized LLAMA3:Every Ġstudent Ġslept Ġin Ġthese Ġhous ings , Ġno Ġmatter Ġwhat .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: She wears either revealing clothes or professional clothes (or perhaps both).\n",
      "\t Tokenized GPT2:She Ġwears Ġeither Ġrevealing Ġclothes Ġor Ġprofessional Ġclothes Ġ( or Ġperhaps Ġboth ).\n",
      "\t Tokenized LLAMA3:She Ġwears Ġeither Ġrevealing Ġclothes Ġor Ġprofessional Ġclothes Ġ( or Ġperhaps Ġboth ).\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: She only wears short skirts.\n",
      "\t Tokenized GPT2:She Ġonly Ġwears Ġshort Ġsk irts .\n",
      "\t Tokenized LLAMA3:She Ġonly Ġwears Ġshort Ġsk irts .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Lalley also is enthused about other bar efforts on behalf of the poor, most notably the Legal Assistance Center will operate out of the new courthouse.\n",
      "\t Tokenized GPT2:L al ley Ġalso Ġis Ġent h used Ġabout Ġother Ġbar Ġefforts Ġon Ġbehalf Ġof Ġthe Ġpoor , Ġmost Ġnotably Ġthe ĠLe gal ĠAss istance ĠCenter Ġwill Ġoperate Ġout Ġof Ġthe Ġnew Ġcour th ouse .\n",
      "\t Tokenized LLAMA3:L al ley Ġalso Ġis Ġent h used Ġabout Ġother Ġbar Ġefforts Ġon Ġbehalf Ġof Ġthe Ġpoor , Ġmost Ġnotably Ġthe ĠLe gal ĠAss istance ĠCenter Ġwill Ġoperate Ġout Ġof Ġthe Ġnew Ġcour th ouse .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The Legal Assistance Center will keep offering its services from its current location.\n",
      "\t Tokenized GPT2:The ĠLe gal ĠAss istance ĠCenter Ġwill Ġkeep Ġoffering Ġits Ġservices Ġfrom Ġits Ġcurrent Ġlocation .\n",
      "\t Tokenized LLAMA3:The ĠLe gal ĠAss istance ĠCenter Ġwill Ġkeep Ġoffering Ġits Ġservices Ġfrom Ġits Ġcurrent Ġlocation .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The center had become a hodgepodge of unconnected programs--a day-care center, a library, a nonviolence training school.\n",
      "\t Tokenized GPT2:The Ġcenter Ġhad Ġbecome Ġa Ġh odge p odge Ġof Ġun connected Ġprograms -- a Ġday - care Ġcenter , Ġa Ġlibrary , Ġa Ġnon viol ence Ġtraining Ġschool .\n",
      "\t Tokenized LLAMA3:The Ġcenter Ġhad Ġbecome Ġa Ġh odge p odge Ġof Ġun connected Ġprograms -- a Ġday -care Ġcenter , Ġa Ġlibrary , Ġa Ġnon viol ence Ġtraining Ġschool .\n",
      "\t Unique Tokens GPT2: {'-', 'care'}\n",
      "\t Unique Tokens LLAMA3: {'-care'}\n",
      "Text 2: The center was lacking a library.\n",
      "\t Tokenized GPT2:The Ġcenter Ġwas Ġlacking Ġa Ġlibrary .\n",
      "\t Tokenized LLAMA3:The Ġcenter Ġwas Ġlacking Ġa Ġlibrary .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Randy's Anecdotal Wrap-Up\n",
      "\t Tokenized GPT2:R andy 's ĠA ne cd otal ĠW rap - Up\n",
      "\t Tokenized LLAMA3:R andy 's ĠA ne cd otal ĠW rap - Up\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Randy's Conclusions\n",
      "\t Tokenized GPT2:R andy 's ĠCon clusions\n",
      "\t Tokenized LLAMA3:R andy 's ĠCon clusions\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Moreover, Las Vegas has recently started to show signs of maturity in its cultural status as well.\n",
      "\t Tokenized GPT2:Moreover , ĠLas ĠVegas Ġhas Ġrecently Ġstarted Ġto Ġshow Ġsigns Ġof Ġmaturity Ġin Ġits Ġcultural Ġstatus Ġas Ġwell .\n",
      "\t Tokenized LLAMA3:More over , ĠLas ĠVegas Ġhas Ġrecently Ġstarted Ġto Ġshow Ġsigns Ġof Ġmaturity Ġin Ġits Ġcultural Ġstatus Ġas Ġwell .\n",
      "\t Unique Tokens GPT2: {'Moreover'}\n",
      "\t Unique Tokens LLAMA3: {'over', 'More'}\n",
      "Text 2: The culture of Las Vegas has a lot of room for improvement.\n",
      "\t Tokenized GPT2:The Ġculture Ġof ĠLas ĠVegas Ġhas Ġa Ġlot Ġof Ġroom Ġfor Ġimprovement .\n",
      "\t Tokenized LLAMA3:The Ġculture Ġof ĠLas ĠVegas Ġhas Ġa Ġlot Ġof Ġroom Ġfor Ġimprovement .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Placido Domingo's appearance on the package, compellingly photographed in costume as the ancient King of Crete, (Anthony Tommasini, the New York Times ) is the main selling point for this new recording of one of Mozart's more obscure operas--a fact that does not make critics happy.\n",
      "\t Tokenized GPT2:Pl ac ido ĠDom ingo 's Ġappearance Ġon Ġthe Ġpackage , Ġcompelling ly Ġphotograp hed Ġin Ġcostume Ġas Ġthe Ġancient ĠKing Ġof ĠCre te , Ġ( An thony ĠTom mas ini , Ġthe ĠNew ĠYork ĠTimes Ġ) Ġis Ġthe Ġmain Ġselling Ġpoint Ġfor Ġthis Ġnew Ġrecording Ġof Ġone Ġof ĠM oz art 's Ġmore Ġobscure Ġoper as -- a Ġfact Ġthat Ġdoes Ġnot Ġmake Ġcritics Ġhappy .\n",
      "\t Tokenized LLAMA3:Pl ac ido ĠDom ingo 's Ġappearance Ġon Ġthe Ġpackage , Ġcompelling ly Ġphotograp hed Ġin Ġcostume Ġas Ġthe Ġancient ĠKing Ġof ĠCre te , Ġ( An thony ĠTom mas ini , Ġthe ĠNew ĠYork ĠTimes Ġ) Ġis Ġthe Ġmain Ġselling Ġpoint Ġfor Ġthis Ġnew Ġrecording Ġof Ġone Ġof ĠM oz art 's Ġmore Ġobscure Ġoper as -- a Ġfact Ġthat Ġdoes Ġnot Ġmake Ġcritics Ġhappy .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Placido Domingo is the reason that people are purchasing the new Mozard opera recordings.\n",
      "\t Tokenized GPT2:Pl ac ido ĠDom ingo Ġis Ġthe Ġreason Ġthat Ġpeople Ġare Ġpurchasing Ġthe Ġnew ĠM oz ard Ġopera Ġrecordings .\n",
      "\t Tokenized LLAMA3:Pl ac ido ĠDom ingo Ġis Ġthe Ġreason Ġthat Ġpeople Ġare Ġpurchasing Ġthe Ġnew ĠM oz ard Ġopera Ġrecordings .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: 'These are human lives.\n",
      "\t Tokenized GPT2:' These Ġare Ġhuman Ġlives .\n",
      "\t Tokenized LLAMA3:'T he se Ġare Ġhuman Ġlives .\n",
      "\t Unique Tokens GPT2: {'These', \"'\"}\n",
      "\t Unique Tokens LLAMA3: {\"'T\", 'se', 'he'}\n",
      "Text 2: These are animal lives \n",
      "\t Tokenized GPT2:These Ġare Ġanimal Ġlives Ġ\n",
      "\t Tokenized LLAMA3:These Ġare Ġanimal Ġlives Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: so do you have do you have the long i guess not not if there's see i was raised in New York but i guess up there you all don't have too long of a growing season do you\n",
      "\t Tokenized GPT2:so Ġdo Ġyou Ġhave Ġdo Ġyou Ġhave Ġthe Ġlong Ġi Ġguess Ġnot Ġnot Ġif Ġthere 's Ġsee Ġi Ġwas Ġraised Ġin ĠNew ĠYork Ġbut Ġi Ġguess Ġup Ġthere Ġyou Ġall Ġdon 't Ġhave Ġtoo Ġlong Ġof Ġa Ġgrowing Ġseason Ġdo Ġyou\n",
      "\t Tokenized LLAMA3:so Ġdo Ġyou Ġhave Ġdo Ġyou Ġhave Ġthe Ġlong Ġi Ġguess Ġnot Ġnot Ġif Ġthere 's Ġsee Ġi Ġwas Ġraised Ġin ĠNew ĠYork Ġbut Ġi Ġguess Ġup Ġthere Ġyou Ġall Ġdon 't Ġhave Ġtoo Ġlong Ġof Ġa Ġgrowing Ġseason Ġdo Ġyou\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I suppose you have a shorter growing season where you are.\n",
      "\t Tokenized GPT2:I Ġsuppose Ġyou Ġhave Ġa Ġshorter Ġgrowing Ġseason Ġwhere Ġyou Ġare .\n",
      "\t Tokenized LLAMA3:I Ġsuppose Ġyou Ġhave Ġa Ġshorter Ġgrowing Ġseason Ġwhere Ġyou Ġare .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: In the 1980s, and as late as 1994, a major Republican theme was a sort of taunting, nyah-nyah populism.\n",
      "\t Tokenized GPT2:In Ġthe Ġ1980 s , Ġand Ġas Ġlate Ġas Ġ1994 , Ġa Ġmajor ĠRepublican Ġtheme Ġwas Ġa Ġsort Ġof Ġta unting , Ġn y ah - ny ah Ġpopul ism .\n",
      "\t Tokenized LLAMA3:In Ġthe Ġ 198 0 s , Ġand Ġas Ġlate Ġas Ġ 199 4 , Ġa Ġmajor ĠRepublican Ġtheme Ġwas Ġa Ġsort Ġof Ġt aun ting , Ġn y ah - ny ah Ġpopul ism .\n",
      "\t Unique Tokens GPT2: {'Ġ1980', 'Ġta', 'unting', 'Ġ1994'}\n",
      "\t Unique Tokens LLAMA3: {'ting', '198', 'aun', 'Ġt', '199', 'Ġ', '0', '4'}\n",
      "Text 2: The Republicans changed their theme after 1995.\n",
      "\t Tokenized GPT2:The ĠRepublicans Ġchanged Ġtheir Ġtheme Ġafter Ġ1995 .\n",
      "\t Tokenized LLAMA3:The ĠRepublicans Ġchanged Ġtheir Ġtheme Ġafter Ġ 199 5 .\n",
      "\t Unique Tokens GPT2: {'Ġ1995'}\n",
      "\t Unique Tokens LLAMA3: {'5', 'Ġ', '199'}\n",
      "==neutral==\n",
      "Text 1: Julius nodded gravely.\n",
      "\t Tokenized GPT2:Jul ius Ġnodded Ġgrave ly .\n",
      "\t Tokenized LLAMA3:Jul ius Ġnodded Ġgrave ly .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Julius loves to ask questions. \n",
      "\t Tokenized GPT2:Jul ius Ġloves Ġto Ġask Ġquestions . Ġ\n",
      "\t Tokenized LLAMA3:Jul ius Ġloves Ġto Ġask Ġquestions . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: The Drawing Room was partially destroyed by fire in 1941, and its furnishings are faithful reproductions; the huge (repaired) Ming punch bowl is striking.\n",
      "\t Tokenized GPT2:The ĠDraw ing ĠRoom Ġwas Ġpartially Ġdestroyed Ġby Ġfire Ġin Ġ19 41 , Ġand Ġits Ġfurn ishing s Ġare Ġfaithful Ġreprodu ctions ; Ġthe Ġhuge Ġ( rep aired ) ĠMing Ġpunch Ġbowl Ġis Ġstriking .\n",
      "\t Tokenized LLAMA3:The ĠDraw ing ĠRoom Ġwas Ġpartially Ġdestroyed Ġby Ġfire Ġin Ġ 194 1 , Ġand Ġits Ġfurn ishing s Ġare Ġfaithful Ġreprodu ctions ; Ġthe Ġhuge Ġ( rep aired ) ĠM ing Ġpunch Ġbowl Ġis Ġstriking .\n",
      "\t Unique Tokens GPT2: {'41', 'ĠMing', 'Ġ19'}\n",
      "\t Unique Tokens LLAMA3: {'ĠM', '194', '1', 'Ġ'}\n",
      "Text 2: Furnishings in the Drawing room are all reproductions.\n",
      "\t Tokenized GPT2:F urn ishing s Ġin Ġthe ĠDraw ing Ġroom Ġare Ġall Ġreprodu ctions .\n",
      "\t Tokenized LLAMA3:F urn ishing s Ġin Ġthe ĠDraw ing Ġroom Ġare Ġall Ġreprodu ctions .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Sainte-Anne itself has a long, broad beach used not only by fishermen in vividly painted boats, but also by families with small children.\n",
      "\t Tokenized GPT2:S ain te - An ne Ġitself Ġhas Ġa Ġlong , Ġbroad Ġbeach Ġused Ġnot Ġonly Ġby Ġfisher men Ġin Ġvivid ly Ġpainted Ġboats , Ġbut Ġalso Ġby Ġfamilies Ġwith Ġsmall Ġchildren .\n",
      "\t Tokenized LLAMA3:S ain te - An ne Ġitself Ġhas Ġa Ġlong , Ġbroad Ġbeach Ġused Ġnot Ġonly Ġby Ġfisher men Ġin Ġvivid ly Ġpainted Ġboats , Ġbut Ġalso Ġby Ġfamilies Ġwith Ġsmall Ġchildren .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Only fishermen and their boats can be seen lining the beaches of Sainte-Anne.\n",
      "\t Tokenized GPT2:Only Ġfisher men Ġand Ġtheir Ġboats Ġcan Ġbe Ġseen Ġlining Ġthe Ġbeaches Ġof ĠS ain te - An ne .\n",
      "\t Tokenized LLAMA3:Only Ġfisher men Ġand Ġtheir Ġboats Ġcan Ġbe Ġseen Ġlining Ġthe Ġbe aches Ġof ĠS ain te - An ne .\n",
      "\t Unique Tokens GPT2: {'Ġbeaches'}\n",
      "\t Unique Tokens LLAMA3: {'aches'}\n",
      "==contradiction==\n",
      "Text 1: probably yeah i would imagine the judge could throw it out\n",
      "\t Tokenized GPT2:probably Ġyeah Ġi Ġwould Ġimagine Ġthe Ġjudge Ġcould Ġthrow Ġit Ġout\n",
      "\t Tokenized LLAMA3:probably Ġyeah Ġi Ġwould Ġimagine Ġthe Ġjudge Ġcould Ġthrow Ġit Ġout\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I cannot believe the judge threw the book at them so fast.\n",
      "\t Tokenized GPT2:I Ġcannot Ġbelieve Ġthe Ġjudge Ġthrew Ġthe Ġbook Ġat Ġthem Ġso Ġfast .\n",
      "\t Tokenized LLAMA3:I Ġcannot Ġbelieve Ġthe Ġjudge Ġthrew Ġthe Ġbook Ġat Ġthem Ġso Ġfast .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Tom is the winner of a year's supply of Turtle Wax, and he will receive his prize just as soon as the Shopping Avenger figures out how much Turtle Wax actually constitutes a year's supply.\n",
      "\t Tokenized GPT2:Tom Ġis Ġthe Ġwinner Ġof Ġa Ġyear 's Ġsupply Ġof ĠT urt le ĠW ax , Ġand Ġhe Ġwill Ġreceive Ġhis Ġprize Ġjust Ġas Ġsoon Ġas Ġthe ĠSh opping ĠAven ger Ġfigures Ġout Ġhow Ġmuch ĠT urt le ĠW ax Ġactually Ġconstitutes Ġa Ġyear 's Ġsupply .\n",
      "\t Tokenized LLAMA3:Tom Ġis Ġthe Ġwinner Ġof Ġa Ġyear 's Ġsupply Ġof ĠT urt le ĠW ax , Ġand Ġhe Ġwill Ġreceive Ġhis Ġprize Ġjust Ġas Ġsoon Ġas Ġthe ĠSh opping ĠAven ger Ġfigures Ġout Ġhow Ġmuch ĠT urt le ĠW ax Ġactually Ġconstitutes Ġa Ġyear 's Ġsupply .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Tom is the winner of this years contest.\n",
      "\t Tokenized GPT2:Tom Ġis Ġthe Ġwinner Ġof Ġthis Ġyears Ġcontest .\n",
      "\t Tokenized LLAMA3:Tom Ġis Ġthe Ġwinner Ġof Ġthis Ġyears Ġcontest .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: The author began with a set of hunches or hypotheses about what can go wrong in agency management, and what would be evidence supporting-or contradicting-these hypotheses.\n",
      "\t Tokenized GPT2:The Ġauthor Ġbegan Ġwith Ġa Ġset Ġof Ġhun ches Ġor Ġhypot heses Ġabout Ġwhat Ġcan Ġgo Ġwrong Ġin Ġagency Ġmanagement , Ġand Ġwhat Ġwould Ġbe Ġevidence Ġsupporting - or Ġcontrad icting - these Ġhypot heses .\n",
      "\t Tokenized LLAMA3:The Ġauthor Ġbegan Ġwith Ġa Ġset Ġof Ġhun ches Ġor Ġhypot heses Ġabout Ġwhat Ġcan Ġgo Ġwrong Ġin Ġagency Ġmanagement , Ġand Ġwhat Ġwould Ġbe Ġevidence Ġsupporting -or Ġcontrad icting -the se Ġhypot heses .\n",
      "\t Unique Tokens GPT2: {'-', 'these', 'or'}\n",
      "\t Unique Tokens LLAMA3: {'se', '-the', '-or'}\n",
      "Text 2: The author had several theories about the ways in which agency management can go awry.\n",
      "\t Tokenized GPT2:The Ġauthor Ġhad Ġseveral Ġtheories Ġabout Ġthe Ġways Ġin Ġwhich Ġagency Ġmanagement Ġcan Ġgo Ġaw ry .\n",
      "\t Tokenized LLAMA3:The Ġauthor Ġhad Ġseveral Ġtheories Ġabout Ġthe Ġways Ġin Ġwhich Ġagency Ġmanagement Ġcan Ġgo Ġaw ry .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: After their savage battles, the warriors recuperated through meditation in the peace of a Zen monastery rock garden.\n",
      "\t Tokenized GPT2:After Ġtheir Ġsavage Ġbattles , Ġthe Ġwarriors Ġrec u per ated Ġthrough Ġmeditation Ġin Ġthe Ġpeace Ġof Ġa ĠZen Ġmon aster y Ġrock Ġgarden .\n",
      "\t Tokenized LLAMA3:After Ġtheir Ġsavage Ġbattles , Ġthe Ġwarriors Ġrec uper ated Ġthrough Ġmeditation Ġin Ġthe Ġpeace Ġof Ġa ĠZen Ġmon aster y Ġrock Ġgarden .\n",
      "\t Unique Tokens GPT2: {'per', 'u'}\n",
      "\t Unique Tokens LLAMA3: {'uper'}\n",
      "Text 2: The warriors recuperated through mediation learned from monks.\n",
      "\t Tokenized GPT2:The Ġwarriors Ġrec u per ated Ġthrough Ġmed iation Ġlearned Ġfrom Ġmon ks .\n",
      "\t Tokenized LLAMA3:The Ġwarriors Ġrec uper ated Ġthrough Ġmed iation Ġlearned Ġfrom Ġmon ks .\n",
      "\t Unique Tokens GPT2: {'per', 'u'}\n",
      "\t Unique Tokens LLAMA3: {'uper'}\n",
      "==neutral==\n",
      "Text 1:  Dinghies are available for hire from the marinas at Tel Aviv, Jaffa, Akko, Netanya, and Nahariya.\n",
      "\t Tokenized GPT2:ĠD ing h ies Ġare Ġavailable Ġfor Ġhire Ġfrom Ġthe Ġmar inas Ġat ĠTel ĠAv iv , ĠJ aff a , ĠAk ko , ĠNet anya , Ġand ĠN ah ari ya .\n",
      "\t Tokenized LLAMA3:ĠD ing h ies Ġare Ġavailable Ġfor Ġhire Ġfrom Ġthe Ġmar inas Ġat ĠTel ĠAv iv , ĠJ aff a , ĠAk ko , ĠNet anya , Ġand ĠN ah ari ya .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Excellent quality dinghies are rent-able for the marinas at Tel Aviv, Jaffa, Akko, Akko, Netanya, and Nahariya.\n",
      "\t Tokenized GPT2:Excellent Ġquality Ġd ing h ies Ġare Ġrent - able Ġfor Ġthe Ġmar inas Ġat ĠTel ĠAv iv , ĠJ aff a , ĠAk ko , ĠAk ko , ĠNet anya , Ġand ĠN ah ari ya .\n",
      "\t Tokenized LLAMA3:Excellent Ġquality Ġd ing h ies Ġare Ġrent - able Ġfor Ġthe Ġmar inas Ġat ĠTel ĠAv iv , ĠJ aff a , ĠAk ko , ĠAk ko , ĠNet anya , Ġand ĠN ah ari ya .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: no not it not no it's a it's not something\n",
      "\t Tokenized GPT2:no Ġnot Ġit Ġnot Ġno Ġit 's Ġa Ġit 's Ġnot Ġsomething\n",
      "\t Tokenized LLAMA3:no Ġnot Ġit Ġnot Ġno Ġit 's Ġa Ġit 's Ġnot Ġsomething\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It is something\n",
      "\t Tokenized GPT2:It Ġis Ġsomething\n",
      "\t Tokenized LLAMA3:It Ġis Ġsomething\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: i think yeah and it's a just a nice escape and you know it's something to laugh at and enjoy\n",
      "\t Tokenized GPT2:i Ġthink Ġyeah Ġand Ġit 's Ġa Ġjust Ġa Ġnice Ġescape Ġand Ġyou Ġknow Ġit 's Ġsomething Ġto Ġlaugh Ġat Ġand Ġenjoy\n",
      "\t Tokenized LLAMA3:i Ġthink Ġyeah Ġand Ġit 's Ġa Ġjust Ġa Ġnice Ġescape Ġand Ġyou Ġknow Ġit 's Ġsomething Ġto Ġlaugh Ġat Ġand Ġenjoy\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It's an escape that is short-lived.\n",
      "\t Tokenized GPT2:It 's Ġan Ġescape Ġthat Ġis Ġshort - lived .\n",
      "\t Tokenized LLAMA3:It 's Ġan Ġescape Ġthat Ġis Ġshort -l ived .\n",
      "\t Unique Tokens GPT2: {'-', 'lived'}\n",
      "\t Unique Tokens LLAMA3: {'ived', '-l'}\n",
      "==entailment==\n",
      "Text 1: some of the professors i think imitate Big Bird\n",
      "\t Tokenized GPT2:some Ġof Ġthe Ġprofessors Ġi Ġthink Ġim itate ĠBig ĠBird\n",
      "\t Tokenized LLAMA3:some Ġof Ġthe Ġprofessors Ġi Ġthink Ġim itate ĠBig ĠBird\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: There are some professors that re an awful lot like Big Bird.\n",
      "\t Tokenized GPT2:There Ġare Ġsome Ġprofessors Ġthat Ġre Ġan Ġawful Ġlot Ġlike ĠBig ĠBird .\n",
      "\t Tokenized LLAMA3:There Ġare Ġsome Ġprofessors Ġthat Ġre Ġan Ġawful Ġlot Ġlike ĠBig ĠBird .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: He jumped up, planting one hand on the charging horse, and came at the brute with the axe.\n",
      "\t Tokenized GPT2:He Ġjumped Ġup , Ġplanting Ġone Ġhand Ġon Ġthe Ġcharging Ġhorse , Ġand Ġcame Ġat Ġthe Ġbrute Ġwith Ġthe Ġaxe .\n",
      "\t Tokenized LLAMA3:He Ġjumped Ġup , Ġplanting Ġone Ġhand Ġon Ġthe Ġcharging Ġhorse , Ġand Ġcame Ġat Ġthe Ġbr ute Ġwith Ġthe Ġaxe .\n",
      "\t Unique Tokens GPT2: {'Ġbrute'}\n",
      "\t Unique Tokens LLAMA3: {'Ġbr', 'ute'}\n",
      "Text 2: He swung at the brute with his sword.\n",
      "\t Tokenized GPT2:He Ġswung Ġat Ġthe Ġbrute Ġwith Ġhis Ġsword .\n",
      "\t Tokenized LLAMA3:He Ġswung Ġat Ġthe Ġbr ute Ġwith Ġhis Ġsword .\n",
      "\t Unique Tokens GPT2: {'Ġbrute'}\n",
      "\t Unique Tokens LLAMA3: {'Ġbr', 'ute'}\n",
      "==neutral==\n",
      "Text 1: The Saver-Spender Theory of Fiscal Policy, Working Paper 7571.\n",
      "\t Tokenized GPT2:The ĠS aver - Spe nder ĠTheory Ġof ĠF iscal ĠPolicy , ĠWorking ĠPaper Ġ7 57 1 .\n",
      "\t Tokenized LLAMA3:The ĠS aver -S pe nder ĠTheory Ġof ĠF iscal ĠPolicy , ĠWorking ĠPaper Ġ 75 7 1 .\n",
      "\t Unique Tokens GPT2: {'-', 'Ġ7', 'Spe', '57'}\n",
      "\t Unique Tokens LLAMA3: {'pe', 'Ġ', '-S', '7', '75'}\n",
      "Text 2: The paper was peer-reviewed.\n",
      "\t Tokenized GPT2:The Ġpaper Ġwas Ġpeer - review ed .\n",
      "\t Tokenized LLAMA3:The Ġpaper Ġwas Ġpeer -re view ed .\n",
      "\t Unique Tokens GPT2: {'-', 'review'}\n",
      "\t Unique Tokens LLAMA3: {'view', '-re'}\n",
      "==contradiction==\n",
      "Text 1: The park was established in 1935 and was given Corbett's name after India became independent.\n",
      "\t Tokenized GPT2:The Ġpark Ġwas Ġestablished Ġin Ġ19 35 Ġand Ġwas Ġgiven ĠCor bet t 's Ġname Ġafter ĠIndia Ġbecame Ġindependent .\n",
      "\t Tokenized LLAMA3:The Ġpark Ġwas Ġestablished Ġin Ġ 193 5 Ġand Ġwas Ġgiven ĠCor bet t 's Ġname Ġafter ĠIndia Ġbecame Ġindependent .\n",
      "\t Unique Tokens GPT2: {'35', 'Ġ19'}\n",
      "\t Unique Tokens LLAMA3: {'5', '193', 'Ġ'}\n",
      "Text 2: The name of the park has always been Corbett.\n",
      "\t Tokenized GPT2:The Ġname Ġof Ġthe Ġpark Ġhas Ġalways Ġbeen ĠCor bet t .\n",
      "\t Tokenized LLAMA3:The Ġname Ġof Ġthe Ġpark Ġhas Ġalways Ġbeen ĠCor bet t .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: It is worth a visit, if only to see the theater itself.\n",
      "\t Tokenized GPT2:It Ġis Ġworth Ġa Ġvisit , Ġif Ġonly Ġto Ġsee Ġthe Ġtheater Ġitself .\n",
      "\t Tokenized LLAMA3:It Ġis Ġworth Ġa Ġvisit , Ġif Ġonly Ġto Ġsee Ġthe Ġtheater Ġitself .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The theater is on your left when you first walk inside.\n",
      "\t Tokenized GPT2:The Ġtheater Ġis Ġon Ġyour Ġleft Ġwhen Ġyou Ġfirst Ġwalk Ġinside .\n",
      "\t Tokenized LLAMA3:The Ġtheater Ġis Ġon Ġyour Ġleft Ġwhen Ġyou Ġfirst Ġwalk Ġinside .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: 2.5 Financial audits are performed under the American Institute of Certified Public Accountants' (AICPA) generally accepted auditing standards for field work and reporting, as well as the related AICPA Statements on Auditing Standards (SASs) which interpret the standards and provide guidance on cond\n",
      "\t Tokenized GPT2:2 . 5 ĠFinancial Ġaud its Ġare Ġperformed Ġunder Ġthe ĠAmerican ĠInstitute Ġof ĠCert ified ĠPublic ĠAccount ants ' Ġ( A IC PA ) Ġgenerally Ġaccepted Ġaud iting Ġstandards Ġfor Ġfield Ġwork Ġand Ġreporting , Ġas Ġwell Ġas Ġthe Ġrelated ĠA IC PA ĠState ments Ġon ĠAud iting ĠStand ards Ġ( S AS s ) Ġwhich Ġinterpret Ġthe Ġstandards Ġand Ġprovide Ġguidance Ġon Ġcond\n",
      "\t Tokenized LLAMA3:2 . 5 ĠFinancial Ġaud its Ġare Ġperformed Ġunder Ġthe ĠAmerican ĠInstitute Ġof ĠCert ified ĠPublic ĠAccount ants ' Ġ( A IC PA ) Ġgenerally Ġaccepted Ġaud iting Ġstandards Ġfor Ġfield Ġwork Ġand Ġreporting , Ġas Ġwell Ġas Ġthe Ġrelated ĠA IC PA ĠState ments Ġon ĠAud iting ĠStand ards Ġ( S AS s ) Ġwhich Ġinterpret Ġthe Ġstandards Ġand Ġprovide Ġguidance Ġon Ġcond\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The AICPA makes financial audits with generally accepted standards.\n",
      "\t Tokenized GPT2:The ĠA IC PA Ġmakes Ġfinancial Ġaud its Ġwith Ġgenerally Ġaccepted Ġstandards .\n",
      "\t Tokenized LLAMA3:The ĠA IC PA Ġmakes Ġfinancial Ġaud its Ġwith Ġgenerally Ġaccepted Ġstandards .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Scutari is traditionally associated with the name of Florence Nightingale.\n",
      "\t Tokenized GPT2:Sc ut ari Ġis Ġtraditionally Ġassociated Ġwith Ġthe Ġname Ġof ĠFlore nce ĠNight ing ale .\n",
      "\t Tokenized LLAMA3:Sc ut ari Ġis Ġtraditionally Ġassociated Ġwith Ġthe Ġname Ġof ĠFlore nce ĠNight ing ale .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Scutari was linked with Florence Nightingale posthumously.\n",
      "\t Tokenized GPT2:Sc ut ari Ġwas Ġlinked Ġwith ĠFlore nce ĠNight ing ale Ġpost hum ously .\n",
      "\t Tokenized LLAMA3:Sc ut ari Ġwas Ġlinked Ġwith ĠFlore nce ĠNight ing ale Ġpost hum ously .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: right right they left a woman and a child or the cat the sheep yeah\n",
      "\t Tokenized GPT2:right Ġright Ġthey Ġleft Ġa Ġwoman Ġand Ġa Ġchild Ġor Ġthe Ġcat Ġthe Ġsheep Ġyeah\n",
      "\t Tokenized LLAMA3:right Ġright Ġthey Ġleft Ġa Ġwoman Ġand Ġa Ġchild Ġor Ġthe Ġcat Ġthe Ġsheep Ġyeah\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: They let a woman and child remain, or it might have been the cat or sheep.\n",
      "\t Tokenized GPT2:They Ġlet Ġa Ġwoman Ġand Ġchild Ġremain , Ġor Ġit Ġmight Ġhave Ġbeen Ġthe Ġcat Ġor Ġsheep .\n",
      "\t Tokenized LLAMA3:They Ġlet Ġa Ġwoman Ġand Ġchild Ġremain , Ġor Ġit Ġmight Ġhave Ġbeen Ġthe Ġcat Ġor Ġsheep .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: In kampung workshops you can watch fantastic birds and butterflies being made of paper (and increasingly, nowadays, of plastic, too) drawn over strong, flexible bamboo frames.\n",
      "\t Tokenized GPT2:In Ġk amp ung Ġworkshops Ġyou Ġcan Ġwatch Ġfantastic Ġbirds Ġand Ġbutter flies Ġbeing Ġmade Ġof Ġpaper Ġ( and Ġincreasingly , Ġnowadays , Ġof Ġplastic , Ġtoo ) Ġdrawn Ġover Ġstrong , Ġflexible Ġb amb oo Ġframes .\n",
      "\t Tokenized LLAMA3:In Ġk amp ung Ġworkshops Ġyou Ġcan Ġwatch Ġfantastic Ġbirds Ġand Ġbutter flies Ġbeing Ġmade Ġof Ġpaper Ġ( and Ġincreasingly , Ġnowadays , Ġof Ġplastic , Ġtoo ) Ġdrawn Ġover Ġstrong , Ġflexible Ġb amb oo Ġframes .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Birds and butterflies can only been seen in the kampung workshops.\n",
      "\t Tokenized GPT2:B ird s Ġand Ġbutter flies Ġcan Ġonly Ġbeen Ġseen Ġin Ġthe Ġk amp ung Ġworkshops .\n",
      "\t Tokenized LLAMA3:B ird s Ġand Ġbutter flies Ġcan Ġonly Ġbeen Ġseen Ġin Ġthe Ġk amp ung Ġworkshops .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: I understand,\" continued the Coroner deliberately, \"that you were sitting reading on the bench just outside the long window of the boudoir. \n",
      "\t Tokenized GPT2:I Ġunderstand ,\" Ġcontinued Ġthe ĠCor oner Ġdeliberately , Ġ\" that Ġyou Ġwere Ġsitting Ġreading Ġon Ġthe Ġbench Ġjust Ġoutside Ġthe Ġlong Ġwindow Ġof Ġthe Ġb oud oir . Ġ\n",
      "\t Tokenized LLAMA3:I Ġunderstand ,\" Ġcontinued Ġthe ĠCor oner Ġdeliberately , Ġ\" that Ġyou Ġwere Ġsitting Ġreading Ġon Ġthe Ġbench Ġjust Ġoutside Ġthe Ġlong Ġwindow Ġof Ġthe Ġb oud oir . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: \"I understand that you were reading inside the boudoir.\", continued the Coroner.\n",
      "\t Tokenized GPT2:\" I Ġunderstand Ġthat Ġyou Ġwere Ġreading Ġinside Ġthe Ġb oud oir .\", Ġcontinued Ġthe ĠCor oner .\n",
      "\t Tokenized LLAMA3:\"I Ġunderstand Ġthat Ġyou Ġwere Ġreading Ġinside Ġthe Ġb oud oir .\", Ġcontinued Ġthe ĠCor oner .\n",
      "\t Unique Tokens GPT2: {'I', '\"'}\n",
      "\t Unique Tokens LLAMA3: {'\"I'}\n",
      "==entailment==\n",
      "Text 1: He pulled his cloak tighter and wished for a moment that he had not shaved his head.\n",
      "\t Tokenized GPT2:He Ġpulled Ġhis Ġcloak Ġtighter Ġand Ġwished Ġfor Ġa Ġmoment Ġthat Ġhe Ġhad Ġnot Ġsh aved Ġhis Ġhead .\n",
      "\t Tokenized LLAMA3:He Ġpulled Ġhis Ġcloak Ġtighter Ġand Ġwished Ġfor Ġa Ġmoment Ġthat Ġhe Ġhad Ġnot Ġsh aved Ġhis Ġhead .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The man  wrapped himself tightly in his cloak and was distressed about not having hair. \n",
      "\t Tokenized GPT2:The Ġman Ġ Ġwrapped Ġhimself Ġtightly Ġin Ġhis Ġcloak Ġand Ġwas Ġdist ressed Ġabout Ġnot Ġhaving Ġhair . Ġ\n",
      "\t Tokenized LLAMA3:The Ġman Ġ Ġwrapped Ġhimself Ġtightly Ġin Ġhis Ġcloak Ġand Ġwas Ġdist ressed Ġabout Ġnot Ġhaving Ġhair . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: There was no longer any  when you wanted some unbridled adult fun, Las Vegas was the place to be.\n",
      "\t Tokenized GPT2:There Ġwas Ġno Ġlonger Ġany Ġ Ġwhen Ġyou Ġwanted Ġsome Ġun brid led Ġadult Ġfun , ĠLas ĠVegas Ġwas Ġthe Ġplace Ġto Ġbe .\n",
      "\t Tokenized LLAMA3:There Ġwas Ġno Ġlonger Ġany Ġ Ġwhen Ġyou Ġwanted Ġsome Ġun brid led Ġadult Ġfun , ĠLas ĠVegas Ġwas Ġthe Ġplace Ġto Ġbe .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Las Vegas was the destination for pure fun for adults.\n",
      "\t Tokenized GPT2:L as ĠVegas Ġwas Ġthe Ġdestination Ġfor Ġpure Ġfun Ġfor Ġadults .\n",
      "\t Tokenized LLAMA3:L as ĠVegas Ġwas Ġthe Ġdestination Ġfor Ġpure Ġfun Ġfor Ġadults .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Jamaican music ska and, especially, reggae has since the 1970s been exported and enjoyed around the world.\n",
      "\t Tokenized GPT2:J ama ican Ġmusic Ġsk a Ġand , Ġespecially , Ġreg g ae Ġhas Ġsince Ġthe Ġ1970 s Ġbeen Ġexported Ġand Ġenjoyed Ġaround Ġthe Ġworld .\n",
      "\t Tokenized LLAMA3:J ama ican Ġmusic Ġsk a Ġand , Ġespecially , Ġreg g ae Ġhas Ġsince Ġthe Ġ 197 0 s Ġbeen Ġexported Ġand Ġenjoyed Ġaround Ġthe Ġworld .\n",
      "\t Unique Tokens GPT2: {'Ġ1970'}\n",
      "\t Unique Tokens LLAMA3: {'0', '197', 'Ġ'}\n",
      "Text 2: Reggae is one of the American music style.\n",
      "\t Tokenized GPT2:Re gg ae Ġis Ġone Ġof Ġthe ĠAmerican Ġmusic Ġstyle .\n",
      "\t Tokenized LLAMA3:Re gg ae Ġis Ġone Ġof Ġthe ĠAmerican Ġmusic Ġstyle .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The following are examples of how teams were used in the agency initiatives we reviewed.\n",
      "\t Tokenized GPT2:The Ġfollowing Ġare Ġexamples Ġof Ġhow Ġteams Ġwere Ġused Ġin Ġthe Ġagency Ġinitiatives Ġwe Ġreviewed .\n",
      "\t Tokenized LLAMA3:The Ġfollowing Ġare Ġexamples Ġof Ġhow Ġteams Ġwere Ġused Ġin Ġthe Ġagency Ġinitiatives Ġwe Ġreviewed .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: We reviewed how sales teams were used in the initiatives.\n",
      "\t Tokenized GPT2:We Ġreviewed Ġhow Ġsales Ġteams Ġwere Ġused Ġin Ġthe Ġinitiatives .\n",
      "\t Tokenized LLAMA3:We Ġreviewed Ġhow Ġsales Ġteams Ġwere Ġused Ġin Ġthe Ġinitiatives .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: oh really it wouldn't matter if we plant them when it was starting to get warmer\n",
      "\t Tokenized GPT2:oh Ġreally Ġit Ġwouldn 't Ġmatter Ġif Ġwe Ġplant Ġthem Ġwhen Ġit Ġwas Ġstarting Ġto Ġget Ġwarmer\n",
      "\t Tokenized LLAMA3:oh Ġreally Ġit Ġwouldn 't Ġmatter Ġif Ġwe Ġplant Ġthem Ġwhen Ġit Ġwas Ġstarting Ġto Ġget Ġwarmer\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The plants are strictly seasonal, only grown during the winter.\n",
      "\t Tokenized GPT2:The Ġplants Ġare Ġstrictly Ġseasonal , Ġonly Ġgrown Ġduring Ġthe Ġwinter .\n",
      "\t Tokenized LLAMA3:The Ġplants Ġare Ġstrictly Ġseasonal , Ġonly Ġgrown Ġduring Ġthe Ġwinter .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: When a GAGAS attestation engagement is the basis for an auditor's subsequent report under the AICPA standards, it would be advantageous to users of the subsequent report for the auditor's report to include the information on compliance with laws and regulations and internal control that is required \n",
      "\t Tokenized GPT2:When Ġa ĠG AG AS Ġatt est ation Ġengagement Ġis Ġthe Ġbasis Ġfor Ġan Ġaud itor 's Ġsubsequent Ġreport Ġunder Ġthe ĠA IC PA Ġstandards , Ġit Ġwould Ġbe Ġadvantage ous Ġto Ġusers Ġof Ġthe Ġsubsequent Ġreport Ġfor Ġthe Ġaud itor 's Ġreport Ġto Ġinclude Ġthe Ġinformation Ġon Ġcompliance Ġwith Ġlaws Ġand Ġregulations Ġand Ġinternal Ġcontrol Ġthat Ġis Ġrequired Ġ\n",
      "\t Tokenized LLAMA3:When Ġa ĠG AG AS Ġatt est ation Ġengagement Ġis Ġthe Ġbasis Ġfor Ġan Ġaud itor 's Ġsubsequent Ġreport Ġunder Ġthe ĠA IC PA Ġstandards , Ġit Ġwould Ġbe Ġadvantage ous Ġto Ġusers Ġof Ġthe Ġsubsequent Ġreport Ġfor Ġthe Ġaud itor 's Ġreport Ġto Ġinclude Ġthe Ġinformation Ġon Ġcompliance Ġwith Ġlaws Ġand Ġregulations Ġand Ġinternal Ġcontrol Ġthat Ġis Ġrequired Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The report is required by GAGAS but not AICPA.\n",
      "\t Tokenized GPT2:The Ġreport Ġis Ġrequired Ġby ĠG AG AS Ġbut Ġnot ĠA IC PA .\n",
      "\t Tokenized LLAMA3:The Ġreport Ġis Ġrequired Ġby ĠG AG AS Ġbut Ġnot ĠA IC PA .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1:  said San'doro.\n",
      "\t Tokenized GPT2:Ġsaid ĠSan 'd oro .\n",
      "\t Tokenized LLAMA3:Ġsaid ĠSan 'd oro .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: San'doro whispered. \n",
      "\t Tokenized GPT2:San 'd oro Ġwhispered . Ġ\n",
      "\t Tokenized LLAMA3:San 'd oro Ġwhispered . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Slate could have put someone with a reasonable grasp of elementary finance and a balanced viewpoint in charge of writing a tax piece.\n",
      "\t Tokenized GPT2:Sl ate Ġcould Ġhave Ġput Ġsomeone Ġwith Ġa Ġreasonable Ġgrasp Ġof Ġelementary Ġfinance Ġand Ġa Ġbalanced Ġviewpoint Ġin Ġcharge Ġof Ġwriting Ġa Ġtax Ġpiece .\n",
      "\t Tokenized LLAMA3:Sl ate Ġcould Ġhave Ġput Ġsomeone Ġwith Ġa Ġreasonable Ġgrasp Ġof Ġelementary Ġfinance Ġand Ġa Ġbalanced Ġviewpoint Ġin Ġcharge Ġof Ġwriting Ġa Ġtax Ġpiece .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Slate has incompetent people write their tax pieces.\n",
      "\t Tokenized GPT2:Sl ate Ġhas Ġincompetent Ġpeople Ġwrite Ġtheir Ġtax Ġpieces .\n",
      "\t Tokenized LLAMA3:Sl ate Ġhas Ġincompet ent Ġpeople Ġwrite Ġtheir Ġtax Ġpieces .\n",
      "\t Unique Tokens GPT2: {'Ġincompetent'}\n",
      "\t Unique Tokens LLAMA3: {'Ġincompet', 'ent'}\n",
      "==contradiction==\n",
      "Text 1: Act Accounting the Great Management Reform Act\n",
      "\t Tokenized GPT2:Act ĠAccount ing Ġthe ĠGreat ĠManagement ĠRe form ĠAct\n",
      "\t Tokenized LLAMA3:Act ĠAccount ing Ġthe ĠGreat ĠManagement ĠRe form ĠAct\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Accounting bad management \n",
      "\t Tokenized GPT2:Account ing Ġbad Ġmanagement Ġ\n",
      "\t Tokenized LLAMA3:Account ing Ġbad Ġmanagement Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: medical and surgical expense coverage.\n",
      "\t Tokenized GPT2:med ical Ġand Ġsurgical Ġexpense Ġcoverage .\n",
      "\t Tokenized LLAMA3:med ical Ġand Ġsurgical Ġexpense Ġcoverage .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Medical and surgical expenses were fully covered\n",
      "\t Tokenized GPT2:Medical Ġand Ġsurgical Ġexpenses Ġwere Ġfully Ġcovered\n",
      "\t Tokenized LLAMA3:Med ical Ġand Ġsurgical Ġexpenses Ġwere Ġfully Ġcovered\n",
      "\t Unique Tokens GPT2: {'Medical'}\n",
      "\t Unique Tokens LLAMA3: {'ical', 'Med'}\n",
      "==entailment==\n",
      "Text 1: Maybe in that sense, the behavior of the Pippens and Iversons of the world is defensible.\n",
      "\t Tokenized GPT2:Maybe Ġin Ġthat Ġsense , Ġthe Ġbehavior Ġof Ġthe ĠP ipp ens Ġand ĠI vers ons Ġof Ġthe Ġworld Ġis Ġdef ens ible .\n",
      "\t Tokenized LLAMA3:Maybe Ġin Ġthat Ġsense , Ġthe Ġbehavior Ġof Ġthe ĠP ipp ens Ġand ĠI vers ons Ġof Ġthe Ġworld Ġis Ġdef ens ible .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: In one sense their antics are justifiable.\n",
      "\t Tokenized GPT2:In Ġone Ġsense Ġtheir Ġant ics Ġare Ġjust ifiable .\n",
      "\t Tokenized LLAMA3:In Ġone Ġsense Ġtheir Ġant ics Ġare Ġjust ifiable .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: and uh really they're about it they've got a guy named Herb Williams that that i guess sort of was supposed to take the place of uh Tarpley but he uh he just doesn't have the offensive skills\n",
      "\t Tokenized GPT2:and Ġuh Ġreally Ġthey 're Ġabout Ġit Ġthey 've Ġgot Ġa Ġguy Ġnamed ĠHer b ĠWilliams Ġthat Ġthat Ġi Ġguess Ġsort Ġof Ġwas Ġsupposed Ġto Ġtake Ġthe Ġplace Ġof Ġuh ĠTar ple y Ġbut Ġhe Ġuh Ġhe Ġjust Ġdoesn 't Ġhave Ġthe Ġoffensive Ġskills\n",
      "\t Tokenized LLAMA3:and Ġuh Ġreally Ġthey 're Ġabout Ġit Ġthey 've Ġgot Ġa Ġguy Ġnamed ĠHer b ĠWilliams Ġthat Ġthat Ġi Ġguess Ġsort Ġof Ġwas Ġsupposed Ġto Ġtake Ġthe Ġplace Ġof Ġuh ĠTar ple y Ġbut Ġhe Ġuh Ġhe Ġjust Ġdoesn 't Ġhave Ġthe Ġoffensive Ġskills\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Herb Williams and Tarpley are on par in terms of skills.\n",
      "\t Tokenized GPT2:Her b ĠWilliams Ġand ĠTar ple y Ġare Ġon Ġpar Ġin Ġterms Ġof Ġskills .\n",
      "\t Tokenized LLAMA3:Her b ĠWilliams Ġand ĠTar ple y Ġare Ġon Ġpar Ġin Ġterms Ġof Ġskills .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: because we don't always read the newspaper sometimes it just sits around for a while and then we just chuck it\n",
      "\t Tokenized GPT2:because Ġwe Ġdon 't Ġalways Ġread Ġthe Ġnewspaper Ġsometimes Ġit Ġjust Ġsits Ġaround Ġfor Ġa Ġwhile Ġand Ġthen Ġwe Ġjust Ġchuck Ġit\n",
      "\t Tokenized LLAMA3:because Ġwe Ġdon 't Ġalways Ġread Ġthe Ġnewspaper Ġsometimes Ġit Ġjust Ġsits Ġaround Ġfor Ġa Ġwhile Ġand Ġthen Ġwe Ġjust Ġchuck Ġit\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: We save all of our old newspapers whether we read them or not.\n",
      "\t Tokenized GPT2:We Ġsave Ġall Ġof Ġour Ġold Ġnewspapers Ġwhether Ġwe Ġread Ġthem Ġor Ġnot .\n",
      "\t Tokenized LLAMA3:We Ġsave Ġall Ġof Ġour Ġold Ġnewspapers Ġwhether Ġwe Ġread Ġthem Ġor Ġnot .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1:  He found himself thinking in circles of worry and pulled himself back to his problem.\n",
      "\t Tokenized GPT2:ĠHe Ġfound Ġhimself Ġthinking Ġin Ġcircles Ġof Ġworry Ġand Ġpulled Ġhimself Ġback Ġto Ġhis Ġproblem .\n",
      "\t Tokenized LLAMA3:ĠHe Ġfound Ġhimself Ġthinking Ġin Ġcircles Ġof Ġworry Ġand Ġpulled Ġhimself Ġback Ġto Ġhis Ġproblem .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He got lost in loops of worry, but snapped himself back to his problem.\n",
      "\t Tokenized GPT2:He Ġgot Ġlost Ġin Ġloops Ġof Ġworry , Ġbut Ġsnapped Ġhimself Ġback Ġto Ġhis Ġproblem .\n",
      "\t Tokenized LLAMA3:He Ġgot Ġlost Ġin Ġloops Ġof Ġworry , Ġbut Ġsnapped Ġhimself Ġback Ġto Ġhis Ġproblem .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Meanwhile, critics on the left argue that because the United States failed to intervene in Rwanda, its intervention in Kosovo is morally suspect and probably racist.\n",
      "\t Tokenized GPT2:Meanwhile , Ġcritics Ġon Ġthe Ġleft Ġargue Ġthat Ġbecause Ġthe ĠUnited ĠStates Ġfailed Ġto Ġinterven e Ġin ĠR w anda , Ġits Ġintervention Ġin ĠKos ovo Ġis Ġmorally Ġsuspect Ġand Ġprobably Ġracist .\n",
      "\t Tokenized LLAMA3:Meanwhile , Ġcritics Ġon Ġthe Ġleft Ġargue Ġthat Ġbecause Ġthe ĠUnited ĠStates Ġfailed Ġto Ġinterven e Ġin ĠR w anda , Ġits Ġintervention Ġin ĠK os ovo Ġis Ġmorally Ġsuspect Ġand Ġprobably Ġracist .\n",
      "\t Unique Tokens GPT2: {'ĠKos'}\n",
      "\t Unique Tokens LLAMA3: {'ĠK', 'os'}\n",
      "Text 2: The US did not intervene in the Rwandan conflict.\n",
      "\t Tokenized GPT2:The ĠUS Ġdid Ġnot Ġinterven e Ġin Ġthe ĠR w and an Ġconflict .\n",
      "\t Tokenized LLAMA3:The ĠUS Ġdid Ġnot Ġinterven e Ġin Ġthe ĠR w and an Ġconflict .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The last 12 years of his life are a blank.\n",
      "\t Tokenized GPT2:The Ġlast Ġ12 Ġyears Ġof Ġhis Ġlife Ġare Ġa Ġblank .\n",
      "\t Tokenized LLAMA3:The Ġlast Ġ 12 Ġyears Ġof Ġhis Ġlife Ġare Ġa Ġblank .\n",
      "\t Unique Tokens GPT2: {'Ġ12'}\n",
      "\t Unique Tokens LLAMA3: {'Ġ', '12'}\n",
      "Text 2: He recalls every moment of the last 12 years in excruciating detail\n",
      "\t Tokenized GPT2:He Ġrecall s Ġevery Ġmoment Ġof Ġthe Ġlast Ġ12 Ġyears Ġin Ġexc ru ci ating Ġdetail\n",
      "\t Tokenized LLAMA3:He Ġrecall s Ġevery Ġmoment Ġof Ġthe Ġlast Ġ 12 Ġyears Ġin Ġexc ru ci ating Ġdetail\n",
      "\t Unique Tokens GPT2: {'Ġ12'}\n",
      "\t Unique Tokens LLAMA3: {'Ġ', '12'}\n",
      "==neutral==\n",
      "Text 1: Two, most other productive operations are easier to study and understand, since few firms have 40,000 locations and a large proportion of their workforce working outdoors.\n",
      "\t Tokenized GPT2:Two , Ġmost Ġother Ġproductive Ġoperations Ġare Ġeasier Ġto Ġstudy Ġand Ġunderstand , Ġsince Ġfew Ġfirms Ġhave Ġ40 , 000 Ġlocations Ġand Ġa Ġlarge Ġproportion Ġof Ġtheir Ġworkforce Ġworking Ġoutdoors .\n",
      "\t Tokenized LLAMA3:Two , Ġmost Ġother Ġproductive Ġoperations Ġare Ġeasier Ġto Ġstudy Ġand Ġunderstand , Ġsince Ġfew Ġfirms Ġhave Ġ 40 , 000 Ġlocations Ġand Ġa Ġlarge Ġproportion Ġof Ġtheir Ġworkforce Ġworking Ġoutdoors .\n",
      "\t Unique Tokens GPT2: {'Ġ40'}\n",
      "\t Unique Tokens LLAMA3: {'40', 'Ġ'}\n",
      "Text 2: The productivity of the operations is directly related to the workforce that's based outdoors.\n",
      "\t Tokenized GPT2:The Ġproductivity Ġof Ġthe Ġoperations Ġis Ġdirectly Ġrelated Ġto Ġthe Ġworkforce Ġthat 's Ġbased Ġoutdoors .\n",
      "\t Tokenized LLAMA3:The Ġproductivity Ġof Ġthe Ġoperations Ġis Ġdirectly Ġrelated Ġto Ġthe Ġworkforce Ġthat 's Ġbased Ġoutdoors .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: right and that was back in nineteen fifty nine\n",
      "\t Tokenized GPT2:right Ġand Ġthat Ġwas Ġback Ġin Ġninet een Ġfifty Ġnine\n",
      "\t Tokenized LLAMA3:right Ġand Ġthat Ġwas Ġback Ġin Ġninet een Ġfifty Ġnine\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It was in the month of August.\n",
      "\t Tokenized GPT2:It Ġwas Ġin Ġthe Ġmonth Ġof ĠAugust .\n",
      "\t Tokenized LLAMA3:It Ġwas Ġin Ġthe Ġmonth Ġof ĠAugust .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Good sir, Jon began.\n",
      "\t Tokenized GPT2:Good Ġsir , ĠJon Ġbegan .\n",
      "\t Tokenized LLAMA3:Good Ġsir , ĠJon Ġbegan .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Jon addressed the king.\n",
      "\t Tokenized GPT2:Jon Ġaddressed Ġthe Ġking .\n",
      "\t Tokenized LLAMA3:Jon Ġaddressed Ġthe Ġking .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: From here it's all through the charming hillside village of Saint-Claude, with its upper-income homes, and on toward the summit or as far as the gendarmes are allowing traffic to proceed that day.\n",
      "\t Tokenized GPT2:From Ġhere Ġit 's Ġall Ġthrough Ġthe Ġcharming Ġhill side Ġvillage Ġof ĠSaint - Cl a ude , Ġwith Ġits Ġupper - income Ġhomes , Ġand Ġon Ġtoward Ġthe Ġsummit Ġor Ġas Ġfar Ġas Ġthe Ġg end arm es Ġare Ġallowing Ġtraffic Ġto Ġproceed Ġthat Ġday .\n",
      "\t Tokenized LLAMA3:From Ġhere Ġit 's Ġall Ġthrough Ġthe Ġcharming Ġhill side Ġvillage Ġof ĠSaint - Cl a ude , Ġwith Ġits Ġupper -income Ġhomes , Ġand Ġon Ġtoward Ġthe Ġsummit Ġor Ġas Ġfar Ġas Ġthe Ġg end arm es Ġare Ġallowing Ġtraffic Ġto Ġproceed Ġthat Ġday .\n",
      "\t Unique Tokens GPT2: {'income'}\n",
      "\t Unique Tokens LLAMA3: {'-income'}\n",
      "Text 2: The upper-income homes are very decadent and majestic.\n",
      "\t Tokenized GPT2:The Ġupper - income Ġhomes Ġare Ġvery Ġdec ad ent Ġand Ġmaj estic .\n",
      "\t Tokenized LLAMA3:The Ġupper -income Ġhomes Ġare Ġvery Ġdec ad ent Ġand Ġmaj estic .\n",
      "\t Unique Tokens GPT2: {'-', 'income'}\n",
      "\t Unique Tokens LLAMA3: {'-income'}\n",
      "==contradiction==\n",
      "Text 1:  \"You're not going to marry him, do you hear?\" he said dictatorially.\n",
      "\t Tokenized GPT2:Ġ\" You 're Ġnot Ġgoing Ġto Ġmarry Ġhim , Ġdo Ġyou Ġhear ?\" Ġhe Ġsaid Ġdict ator ially .\n",
      "\t Tokenized LLAMA3:Ġ\" You 're Ġnot Ġgoing Ġto Ġmarry Ġhim , Ġdo Ġyou Ġhear ?\" Ġhe Ġsaid Ġdict ator ially .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: \"I approve of your marriage to this man.\"\n",
      "\t Tokenized GPT2:\" I Ġapprove Ġof Ġyour Ġmarriage Ġto Ġthis Ġman .\"\n",
      "\t Tokenized LLAMA3:\"I Ġapprove Ġof Ġyour Ġmarriage Ġto Ġthis Ġman .\"\n",
      "\t Unique Tokens GPT2: {'I', '\"'}\n",
      "\t Unique Tokens LLAMA3: {'\"I'}\n",
      "==contradiction==\n",
      "Text 1: well his knees were bothering him yeah\n",
      "\t Tokenized GPT2:well Ġhis Ġknees Ġwere Ġbothering Ġhim Ġyeah\n",
      "\t Tokenized LLAMA3:well Ġhis Ġknees Ġwere Ġbothering Ġhim Ġyeah\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He was in tip-top condition.\n",
      "\t Tokenized GPT2:He Ġwas Ġin Ġtip - top Ġcondition .\n",
      "\t Tokenized LLAMA3:He Ġwas Ġin Ġtip -top Ġcondition .\n",
      "\t Unique Tokens GPT2: {'-', 'top'}\n",
      "\t Unique Tokens LLAMA3: {'-top'}\n",
      "==contradiction==\n",
      "Text 1: Exhibitions are often held in the splendid entrance hall.\n",
      "\t Tokenized GPT2:Ex hib itions Ġare Ġoften Ġheld Ġin Ġthe Ġsplendid Ġentrance Ġhall .\n",
      "\t Tokenized LLAMA3:Ex hib itions Ġare Ġoften Ġheld Ġin Ġthe Ġsple ndid Ġentrance Ġhall .\n",
      "\t Unique Tokens GPT2: {'Ġsplendid'}\n",
      "\t Unique Tokens LLAMA3: {'Ġsple', 'ndid'}\n",
      "Text 2: The entrance hall is kept clear of any exhibitions.\n",
      "\t Tokenized GPT2:The Ġentrance Ġhall Ġis Ġkept Ġclear Ġof Ġany Ġexhib itions .\n",
      "\t Tokenized LLAMA3:The Ġentrance Ġhall Ġis Ġkept Ġclear Ġof Ġany Ġexhib itions .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Mallorca prospered.\n",
      "\t Tokenized GPT2:M all or ca Ġpro spe red .\n",
      "\t Tokenized LLAMA3:M all or ca Ġpros pered .\n",
      "\t Unique Tokens GPT2: {'red', 'Ġpro', 'spe'}\n",
      "\t Unique Tokens LLAMA3: {'pered', 'Ġpros'}\n",
      "Text 2: Mallorca did extremely well.\n",
      "\t Tokenized GPT2:M all or ca Ġdid Ġextremely Ġwell .\n",
      "\t Tokenized LLAMA3:M all or ca Ġdid Ġextremely Ġwell .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: You've got the keys still, haven't you, Poirot? I asked, as we reached the door of the locked room. \n",
      "\t Tokenized GPT2:You 've Ġgot Ġthe Ġkeys Ġstill , Ġhaven 't Ġyou , ĠP oir ot ? ĠI Ġasked , Ġas Ġwe Ġreached Ġthe Ġdoor Ġof Ġthe Ġlocked Ġroom . Ġ\n",
      "\t Tokenized LLAMA3:You 've Ġgot Ġthe Ġkeys Ġstill , Ġhaven 't Ġyou , ĠP oir ot ? ĠI Ġasked , Ġas Ġwe Ġreached Ġthe Ġdoor Ġof Ġthe Ġlocked Ġroom . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I had the keys in my pocket.\n",
      "\t Tokenized GPT2:I Ġhad Ġthe Ġkeys Ġin Ġmy Ġpocket .\n",
      "\t Tokenized LLAMA3:I Ġhad Ġthe Ġkeys Ġin Ġmy Ġpocket .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: he's a college graduate type guy he's been in all he's an entrepreneur and he gives very practical financial advice about cars very you know not not nothing college level basic stuff his name is Bruce Williams he's on national radio uh i don't know what it would be down there you might want to whate\n",
      "\t Tokenized GPT2:he 's Ġa Ġcollege Ġgraduate Ġtype Ġguy Ġhe 's Ġbeen Ġin Ġall Ġhe 's Ġan Ġentrepreneur Ġand Ġhe Ġgives Ġvery Ġpractical Ġfinancial Ġadvice Ġabout Ġcars Ġvery Ġyou Ġknow Ġnot Ġnot Ġnothing Ġcollege Ġlevel Ġbasic Ġstuff Ġhis Ġname Ġis ĠBruce ĠWilliams Ġhe 's Ġon Ġnational Ġradio Ġuh Ġi Ġdon 't Ġknow Ġwhat Ġit Ġwould Ġbe Ġdown Ġthere Ġyou Ġmight Ġwant Ġto Ġwh ate\n",
      "\t Tokenized LLAMA3:he 's Ġa Ġcollege Ġgraduate Ġtype Ġguy Ġhe 's Ġbeen Ġin Ġall Ġhe 's Ġan Ġentrepreneur Ġand Ġhe Ġgives Ġvery Ġpractical Ġfinancial Ġadvice Ġabout Ġcars Ġvery Ġyou Ġknow Ġnot Ġnot Ġnothing Ġcollege Ġlevel Ġbasic Ġstuff Ġhis Ġname Ġis ĠBruce ĠWilliams Ġhe 's Ġon Ġnational Ġradio Ġuh Ġi Ġdon 't Ġknow Ġwhat Ġit Ġwould Ġbe Ġdown Ġthere Ġyou Ġmight Ġwant Ġto Ġwh ate\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: the entrepreneur gives people financial advice about real estate, but he doesn't know anything about cars\n",
      "\t Tokenized GPT2:the Ġentrepreneur Ġgives Ġpeople Ġfinancial Ġadvice Ġabout Ġreal Ġestate , Ġbut Ġhe Ġdoesn 't Ġknow Ġanything Ġabout Ġcars\n",
      "\t Tokenized LLAMA3:the Ġentrepreneur Ġgives Ġpeople Ġfinancial Ġadvice Ġabout Ġreal Ġestate , Ġbut Ġhe Ġdoesn 't Ġknow Ġanything Ġabout Ġcars\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: so they don't deal much in cash anymore either\n",
      "\t Tokenized GPT2:so Ġthey Ġdon 't Ġdeal Ġmuch Ġin Ġcash Ġanymore Ġeither\n",
      "\t Tokenized LLAMA3:so Ġthey Ġdon 't Ġdeal Ġmuch Ġin Ġcash Ġanymore Ġeither\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: They still heavily use cash for their transactions.\n",
      "\t Tokenized GPT2:They Ġstill Ġheavily Ġuse Ġcash Ġfor Ġtheir Ġtransactions .\n",
      "\t Tokenized LLAMA3:They Ġstill Ġheavily Ġuse Ġcash Ġfor Ġtheir Ġtransactions .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: He sat up, trying to free himself.\n",
      "\t Tokenized GPT2:He Ġsat Ġup , Ġtrying Ġto Ġfree Ġhimself .\n",
      "\t Tokenized LLAMA3:He Ġsat Ġup , Ġtrying Ġto Ġfree Ġhimself .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He was trying to break free while sitting up.\n",
      "\t Tokenized GPT2:He Ġwas Ġtrying Ġto Ġbreak Ġfree Ġwhile Ġsitting Ġup .\n",
      "\t Tokenized LLAMA3:He Ġwas Ġtrying Ġto Ġbreak Ġfree Ġwhile Ġsitting Ġup .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: It is not a surprise, either, that Al Pacino chews the scenery in Devil's Advocate . And the idea that if the devil showed up on Earth he'd be running a New York corporate-law firm is also, to say the least, pre-chewed.\n",
      "\t Tokenized GPT2:It Ġis Ġnot Ġa Ġsurprise , Ġeither , Ġthat ĠAl ĠPac ino Ġche ws Ġthe Ġscenery Ġin ĠDevil 's ĠAdv ocate Ġ. ĠAnd Ġthe Ġidea Ġthat Ġif Ġthe Ġdevil Ġshowed Ġup Ġon ĠEarth Ġhe 'd Ġbe Ġrunning Ġa ĠNew ĠYork Ġcorporate - law Ġfirm Ġis Ġalso , Ġto Ġsay Ġthe Ġleast , Ġpre - che wed .\n",
      "\t Tokenized LLAMA3:It Ġis Ġnot Ġa Ġsurprise , Ġeither , Ġthat ĠAl ĠPac ino Ġche ws Ġthe Ġscenery Ġin ĠDevil 's ĠAdv ocate Ġ. ĠAnd Ġthe Ġidea Ġthat Ġif Ġthe Ġdevil Ġshowed Ġup Ġon ĠEarth Ġhe 'd Ġbe Ġrunning Ġa ĠNew ĠYork Ġcorporate -law Ġfirm Ġis Ġalso , Ġto Ġsay Ġthe Ġleast , Ġpre -c he wed .\n",
      "\t Unique Tokens GPT2: {'-', 'che', 'law'}\n",
      "\t Unique Tokens LLAMA3: {'-c', 'he', '-law'}\n",
      "Text 2: Nobody expects that the devil would take the form of a lawyer.\n",
      "\t Tokenized GPT2:Nobody Ġexpects Ġthat Ġthe Ġdevil Ġwould Ġtake Ġthe Ġform Ġof Ġa Ġlawyer .\n",
      "\t Tokenized LLAMA3:Nobody Ġexpects Ġthat Ġthe Ġdevil Ġwould Ġtake Ġthe Ġform Ġof Ġa Ġlawyer .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: His plan was to drive straight up to the house.\n",
      "\t Tokenized GPT2:His Ġplan Ġwas Ġto Ġdrive Ġstraight Ġup Ġto Ġthe Ġhouse .\n",
      "\t Tokenized LLAMA3:His Ġplan Ġwas Ġto Ġdrive Ġstraight Ġup Ġto Ġthe Ġhouse .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Driving directly to the house was no longer an option with the roadblock.\n",
      "\t Tokenized GPT2:D riving Ġdirectly Ġto Ġthe Ġhouse Ġwas Ġno Ġlonger Ġan Ġoption Ġwith Ġthe Ġroad block .\n",
      "\t Tokenized LLAMA3:D riving Ġdirectly Ġto Ġthe Ġhouse Ġwas Ġno Ġlonger Ġan Ġoption Ġwith Ġthe Ġroad block .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: In 1995 and again in 1998, the Legal Services Corporation recognized that legal services programs were going to have to change the method and manner in which they conducted their business if they were going to remain viable and responsive to the needs of low income persons.\n",
      "\t Tokenized GPT2:In Ġ1995 Ġand Ġagain Ġin Ġ1998 , Ġthe ĠLe gal ĠServices ĠCorporation Ġrecognized Ġthat Ġlegal Ġservices Ġprograms Ġwere Ġgoing Ġto Ġhave Ġto Ġchange Ġthe Ġmethod Ġand Ġmanner Ġin Ġwhich Ġthey Ġconducted Ġtheir Ġbusiness Ġif Ġthey Ġwere Ġgoing Ġto Ġremain Ġviable Ġand Ġresponsive Ġto Ġthe Ġneeds Ġof Ġlow Ġincome Ġpersons .\n",
      "\t Tokenized LLAMA3:In Ġ 199 5 Ġand Ġagain Ġin Ġ 199 8 , Ġthe ĠLe gal ĠServices ĠCorporation Ġrecognized Ġthat Ġlegal Ġservices Ġprograms Ġwere Ġgoing Ġto Ġhave Ġto Ġchange Ġthe Ġmethod Ġand Ġmanner Ġin Ġwhich Ġthey Ġconducted Ġtheir Ġbusiness Ġif Ġthey Ġwere Ġgoing Ġto Ġremain Ġviable Ġand Ġresponsive Ġto Ġthe Ġneeds Ġof Ġlow Ġincome Ġpersons .\n",
      "\t Unique Tokens GPT2: {'Ġ1995', 'Ġ1998'}\n",
      "\t Unique Tokens LLAMA3: {'199', '5', 'Ġ', '8'}\n",
      "Text 2: Low income persons have very difficult needs to meet.\n",
      "\t Tokenized GPT2:Low Ġincome Ġpersons Ġhave Ġvery Ġdifficult Ġneeds Ġto Ġmeet .\n",
      "\t Tokenized LLAMA3:Low Ġincome Ġpersons Ġhave Ġvery Ġdifficult Ġneeds Ġto Ġmeet .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: He's too cautious.\n",
      "\t Tokenized GPT2:He 's Ġtoo Ġcautious .\n",
      "\t Tokenized LLAMA3:He 's Ġtoo Ġcautious .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He's not brave enough.\n",
      "\t Tokenized GPT2:He 's Ġnot Ġbrave Ġenough .\n",
      "\t Tokenized LLAMA3:He 's Ġnot Ġbrave Ġenough .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The long-sought, the mysterious, the elusive Jane Finn! \n",
      "\t Tokenized GPT2:The Ġlong - s ought , Ġthe Ġmysterious , Ġthe Ġel usive ĠJane ĠFinn ! Ġ\n",
      "\t Tokenized LLAMA3:The Ġlong -s ought , Ġthe Ġmysterious , Ġthe Ġel usive ĠJane ĠFinn ! Ġ\n",
      "\t Unique Tokens GPT2: {'s', '-'}\n",
      "\t Unique Tokens LLAMA3: {'-s'}\n",
      "Text 2: Jane Finn is as beautiful as she is mysterious.\n",
      "\t Tokenized GPT2:Jane ĠFinn Ġis Ġas Ġbeautiful Ġas Ġshe Ġis Ġmysterious .\n",
      "\t Tokenized LLAMA3:J ane ĠFinn Ġis Ġas Ġbeautiful Ġas Ġshe Ġis Ġmysterious .\n",
      "\t Unique Tokens GPT2: {'Jane'}\n",
      "\t Unique Tokens LLAMA3: {'ane', 'J'}\n",
      "==contradiction==\n",
      "Text 1: Ocho Rios is Spanish for  eight rivers,  but this name is not descriptive of the area.\n",
      "\t Tokenized GPT2:O cho ĠR ios Ġis ĠSpanish Ġfor Ġ Ġeight Ġrivers , Ġ Ġbut Ġthis Ġname Ġis Ġnot Ġdescriptive Ġof Ġthe Ġarea .\n",
      "\t Tokenized LLAMA3:O cho ĠR ios Ġis ĠSpanish Ġfor Ġ Ġeight Ġrivers , Ġ Ġbut Ġthis Ġname Ġis Ġnot Ġdescriptive Ġof Ġthe Ġarea .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Ocho Rios means pink penguins in Spanish, and this name describes the area.\n",
      "\t Tokenized GPT2:O cho ĠR ios Ġmeans Ġpink Ġpeng u ins Ġin ĠSpanish , Ġand Ġthis Ġname Ġdescribes Ġthe Ġarea .\n",
      "\t Tokenized LLAMA3:O cho ĠR ios Ġmeans Ġpink Ġpeng u ins Ġin ĠSpanish , Ġand Ġthis Ġname Ġdescribes Ġthe Ġarea .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: There is.\n",
      "\t Tokenized GPT2:There Ġis .\n",
      "\t Tokenized LLAMA3:There Ġis .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: There sure is.\n",
      "\t Tokenized GPT2:There Ġsure Ġis .\n",
      "\t Tokenized LLAMA3:There Ġsure Ġis .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Once or twice, but they seem more show than battle, said Adrin.\n",
      "\t Tokenized GPT2:Once Ġor Ġtwice , Ġbut Ġthey Ġseem Ġmore Ġshow Ġthan Ġbattle , Ġsaid ĠAd rin .\n",
      "\t Tokenized LLAMA3:Once Ġor Ġtwice , Ġbut Ġthey Ġseem Ġmore Ġshow Ġthan Ġbattle , Ġsaid ĠAd rin .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Adrin said they weren't serious about battling.\n",
      "\t Tokenized GPT2:Ad rin Ġsaid Ġthey Ġweren 't Ġserious Ġabout Ġbattling .\n",
      "\t Tokenized LLAMA3:Ad rin Ġsaid Ġthey Ġweren 't Ġserious Ġabout Ġbattling .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1:  13-year-olds.\n",
      "\t Tokenized GPT2:Ġ13 - year - olds .\n",
      "\t Tokenized LLAMA3:Ġ 13 -year -old s .\n",
      "\t Unique Tokens GPT2: {'olds', 'year', 'Ġ13', '-'}\n",
      "\t Unique Tokens LLAMA3: {'-year', 'Ġ', 's', '-old', '13'}\n",
      "Text 2: Young teenagers \n",
      "\t Tokenized GPT2:Young Ġteenagers Ġ\n",
      "\t Tokenized LLAMA3:Young Ġteenagers Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Component modularization and prefabrication off-site can reduce the amount of time cranes are needed on a site, as well as provide opportunities to reduce project schedules and construction costs and to concentrate jobs locally at the prefabrication facility.\n",
      "\t Tokenized GPT2:Component Ġmodular ization Ġand Ġpref ab ric ation Ġoff - site Ġcan Ġreduce Ġthe Ġamount Ġof Ġtime Ġcr anes Ġare Ġneeded Ġon Ġa Ġsite , Ġas Ġwell Ġas Ġprovide Ġopportunities Ġto Ġreduce Ġproject Ġschedules Ġand Ġconstruction Ġcosts Ġand Ġto Ġconcentrate Ġjobs Ġlocally Ġat Ġthe Ġpref ab ric ation Ġfacility .\n",
      "\t Tokenized LLAMA3:Component Ġmodular ization Ġand Ġpref ab ric ation Ġoff -site Ġcan Ġreduce Ġthe Ġamount Ġof Ġtime Ġcr anes Ġare Ġneeded Ġon Ġa Ġsite , Ġas Ġwell Ġas Ġprovide Ġopportunities Ġto Ġreduce Ġproject Ġschedules Ġand Ġconstruction Ġcosts Ġand Ġto Ġconcentrate Ġjobs Ġlocally Ġat Ġthe Ġpref ab ric ation Ġfacility .\n",
      "\t Unique Tokens GPT2: {'-', 'site'}\n",
      "\t Unique Tokens LLAMA3: {'-site'}\n",
      "Text 2: When work is done off-site, such as prefabrication, it increases the time cranes are need on a site and raises construction costs.\n",
      "\t Tokenized GPT2:When Ġwork Ġis Ġdone Ġoff - site , Ġsuch Ġas Ġpref ab ric ation , Ġit Ġincreases Ġthe Ġtime Ġcr anes Ġare Ġneed Ġon Ġa Ġsite Ġand Ġraises Ġconstruction Ġcosts .\n",
      "\t Tokenized LLAMA3:When Ġwork Ġis Ġdone Ġoff -site , Ġsuch Ġas Ġpref ab ric ation , Ġit Ġincreases Ġthe Ġtime Ġcr anes Ġare Ġneed Ġon Ġa Ġsite Ġand Ġraises Ġconstruction Ġcosts .\n",
      "\t Unique Tokens GPT2: {'-', 'site'}\n",
      "\t Unique Tokens LLAMA3: {'-site'}\n",
      "==entailment==\n",
      "Text 1: Students of human misery can savor its underlying sadness and futility.\n",
      "\t Tokenized GPT2:Stud ents Ġof Ġhuman Ġmisery Ġcan Ġs avor Ġits Ġunderlying Ġsadness Ġand Ġfut ility .\n",
      "\t Tokenized LLAMA3:Stud ents Ġof Ġhuman Ġmisery Ġcan Ġs avor Ġits Ġunderlying Ġsadness Ġand Ġfut ility .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Students of human misery will be delighted to see how sad it truly is.\n",
      "\t Tokenized GPT2:Stud ents Ġof Ġhuman Ġmisery Ġwill Ġbe Ġdelighted Ġto Ġsee Ġhow Ġsad Ġit Ġtruly Ġis .\n",
      "\t Tokenized LLAMA3:Stud ents Ġof Ġhuman Ġmisery Ġwill Ġbe Ġdelighted Ġto Ġsee Ġhow Ġsad Ġit Ġtruly Ġis .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: How long, Thaler and Siegel ask, will it take most investors to get wise to the fact that the equity premium is just too damned high?\n",
      "\t Tokenized GPT2:How Ġlong , ĠTh aler Ġand ĠSie gel Ġask , Ġwill Ġit Ġtake Ġmost Ġinvestors Ġto Ġget Ġwise Ġto Ġthe Ġfact Ġthat Ġthe Ġequity Ġpremium Ġis Ġjust Ġtoo Ġdamned Ġhigh ?\n",
      "\t Tokenized LLAMA3:How Ġlong , ĠTh aler Ġand ĠSie gel Ġask , Ġwill Ġit Ġtake Ġmost Ġinvestors Ġto Ġget Ġwise Ġto Ġthe Ġfact Ġthat Ġthe Ġequity Ġpremium Ġis Ġjust Ġtoo Ġdamned Ġhigh ?\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Thaler and Siegel think that many investors already know that the equity premium is too high. \n",
      "\t Tokenized GPT2:Th aler Ġand ĠSie gel Ġthink Ġthat Ġmany Ġinvestors Ġalready Ġknow Ġthat Ġthe Ġequity Ġpremium Ġis Ġtoo Ġhigh . Ġ\n",
      "\t Tokenized LLAMA3:Th aler Ġand ĠSie gel Ġthink Ġthat Ġmany Ġinvestors Ġalready Ġknow Ġthat Ġthe Ġequity Ġpremium Ġis Ġtoo Ġhigh . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: I was pulled into the bar.\n",
      "\t Tokenized GPT2:I Ġwas Ġpulled Ġinto Ġthe Ġbar .\n",
      "\t Tokenized LLAMA3:I Ġwas Ġpulled Ġinto Ġthe Ġbar .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I managed to escape their grasp and ran just outside the bar.\n",
      "\t Tokenized GPT2:I Ġmanaged Ġto Ġescape Ġtheir Ġgrasp Ġand Ġran Ġjust Ġoutside Ġthe Ġbar .\n",
      "\t Tokenized LLAMA3:I Ġmanaged Ġto Ġescape Ġtheir Ġgrasp Ġand Ġran Ġjust Ġoutside Ġthe Ġbar .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: STANDARD COSTING - A costing method that attaches costs to cost objects based on reasonable estimates or cost studies and by means of budgeted rates rather than according to actual costs incurred.\n",
      "\t Tokenized GPT2:ST AND ARD ĠC OST ING Ġ- ĠA Ġcost ing Ġmethod Ġthat Ġatt aches Ġcosts Ġto Ġcost Ġobjects Ġbased Ġon Ġreasonable Ġestimates Ġor Ġcost Ġstudies Ġand Ġby Ġmeans Ġof Ġbudget ed Ġrates Ġrather Ġthan Ġaccording Ġto Ġactual Ġcosts Ġincur red .\n",
      "\t Tokenized LLAMA3:ST AND ARD ĠC OST ING Ġ- ĠA Ġcost ing Ġmethod Ġthat Ġatt aches Ġcosts Ġto Ġcost Ġobjects Ġbased Ġon Ġreasonable Ġestimates Ġor Ġcost Ġstudies Ġand Ġby Ġmeans Ġof Ġbudget ed Ġrates Ġrather Ġthan Ġaccording Ġto Ġactual Ġcosts Ġinc urred .\n",
      "\t Unique Tokens GPT2: {'red', 'Ġincur'}\n",
      "\t Unique Tokens LLAMA3: {'Ġinc', 'urred'}\n",
      "Text 2: This costing model is based on research and analytical activity.\n",
      "\t Tokenized GPT2:This Ġcost ing Ġmodel Ġis Ġbased Ġon Ġresearch Ġand Ġanalytical Ġactivity .\n",
      "\t Tokenized LLAMA3:This Ġcost ing Ġmodel Ġis Ġbased Ġon Ġresearch Ġand Ġanalytical Ġactivity .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: 'Can I get a drink?'\n",
      "\t Tokenized GPT2:' Can ĠI Ġget Ġa Ġdrink ?'\n",
      "\t Tokenized LLAMA3:' Can ĠI Ġget Ġa Ġdrink ?'\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Can you make me an espresso?\n",
      "\t Tokenized GPT2:Can Ġyou Ġmake Ġme Ġan Ġes pres so ?\n",
      "\t Tokenized LLAMA3:Can Ġyou Ġmake Ġme Ġan Ġes pres so ?\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The mansions have been downgraded to consulates since the capital was transferred to Ankara in 1923, and modern shops and restaurants have sprung up.\n",
      "\t Tokenized GPT2:The Ġmans ions Ġhave Ġbeen Ġdown grad ed Ġto Ġcons ulates Ġsince Ġthe Ġcapital Ġwas Ġtransferred Ġto ĠAn k ara Ġin Ġ19 23 , Ġand Ġmodern Ġshops Ġand Ġrestaurants Ġhave Ġspr ung Ġup .\n",
      "\t Tokenized LLAMA3:The Ġmans ions Ġhave Ġbeen Ġdown grad ed Ġto Ġcons ulates Ġsince Ġthe Ġcapital Ġwas Ġtransferred Ġto ĠAn k ara Ġin Ġ 192 3 , Ġand Ġmodern Ġshops Ġand Ġrestaurants Ġhave Ġspr ung Ġup .\n",
      "\t Unique Tokens GPT2: {'Ġ19', '23'}\n",
      "\t Unique Tokens LLAMA3: {'3', '192', 'Ġ'}\n",
      "Text 2: The capital had been located in the city of Ankara once before.\n",
      "\t Tokenized GPT2:The Ġcapital Ġhad Ġbeen Ġlocated Ġin Ġthe Ġcity Ġof ĠAn k ara Ġonce Ġbefore .\n",
      "\t Tokenized LLAMA3:The Ġcapital Ġhad Ġbeen Ġlocated Ġin Ġthe Ġcity Ġof ĠAn k ara Ġonce Ġbefore .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: 1 Now that each unit is fully staffed, the LSC Office of Program Performance and its state planning team contain over 260 years of experience in LSC-funded programs.\n",
      "\t Tokenized GPT2:1 ĠNow Ġthat Ġeach Ġunit Ġis Ġfully Ġstaff ed , Ġthe ĠL SC ĠOffice Ġof ĠProgram ĠPerformance Ġand Ġits Ġstate Ġplanning Ġteam Ġcontain Ġover Ġ260 Ġyears Ġof Ġexperience Ġin ĠL SC - fund ed Ġprograms .\n",
      "\t Tokenized LLAMA3:1 ĠNow Ġthat Ġeach Ġunit Ġis Ġfully Ġstaff ed , Ġthe ĠL SC ĠOffice Ġof ĠProgram ĠPerformance Ġand Ġits Ġstate Ġplanning Ġteam Ġcontain Ġover Ġ 260 Ġyears Ġof Ġexperience Ġin ĠL SC -f und ed Ġprograms .\n",
      "\t Unique Tokens GPT2: {'-', 'Ġ260', 'fund'}\n",
      "\t Unique Tokens LLAMA3: {'-f', '260', 'und', 'Ġ'}\n",
      "Text 2: The LSC has over 1260 years of experience with their staff.\n",
      "\t Tokenized GPT2:The ĠL SC Ġhas Ġover Ġ12 60 Ġyears Ġof Ġexperience Ġwith Ġtheir Ġstaff .\n",
      "\t Tokenized LLAMA3:The ĠL SC Ġhas Ġover Ġ 126 0 Ġyears Ġof Ġexperience Ġwith Ġtheir Ġstaff .\n",
      "\t Unique Tokens GPT2: {'Ġ12', '60'}\n",
      "\t Unique Tokens LLAMA3: {'126', '0', 'Ġ'}\n",
      "==entailment==\n",
      "Text 1: Who? asked Tommy.\n",
      "\t Tokenized GPT2:Who ? Ġasked ĠTommy .\n",
      "\t Tokenized LLAMA3:Who ? Ġasked ĠTommy .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Tommy inquired about the identity of the person.\n",
      "\t Tokenized GPT2:Tom my Ġinqu ired Ġabout Ġthe Ġidentity Ġof Ġthe Ġperson .\n",
      "\t Tokenized LLAMA3:Tom my Ġinqu ired Ġabout Ġthe Ġidentity Ġof Ġthe Ġperson .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: profit rather\n",
      "\t Tokenized GPT2:profit Ġrather\n",
      "\t Tokenized LLAMA3:pro fit Ġrather\n",
      "\t Unique Tokens GPT2: {'profit'}\n",
      "\t Unique Tokens LLAMA3: {'pro', 'fit'}\n",
      "Text 2: Our profit has not been good.\n",
      "\t Tokenized GPT2:Our Ġprofit Ġhas Ġnot Ġbeen Ġgood .\n",
      "\t Tokenized LLAMA3:Our Ġprofit Ġhas Ġnot Ġbeen Ġgood .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: He was of two minds, one reveled in the peace of this village.\n",
      "\t Tokenized GPT2:He Ġwas Ġof Ġtwo Ġminds , Ġone Ġrevel ed Ġin Ġthe Ġpeace Ġof Ġthis Ġvillage .\n",
      "\t Tokenized LLAMA3:He Ġwas Ġof Ġtwo Ġminds , Ġone Ġrevel ed Ġin Ġthe Ġpeace Ġof Ġthis Ġvillage .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The village was full of violence.\n",
      "\t Tokenized GPT2:The Ġvillage Ġwas Ġfull Ġof Ġviolence .\n",
      "\t Tokenized LLAMA3:The Ġvillage Ġwas Ġfull Ġof Ġviolence .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Too bad it chose to use McIntyre instead.\n",
      "\t Tokenized GPT2:Too Ġbad Ġit Ġchose Ġto Ġuse ĠMc Int y re Ġinstead .\n",
      "\t Tokenized LLAMA3:Too Ġbad Ġit Ġchose Ġto Ġuse ĠMc Int y re Ġinstead .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: McIntyre was picked to be used.\n",
      "\t Tokenized GPT2:Mc Int y re Ġwas Ġpicked Ġto Ġbe Ġused .\n",
      "\t Tokenized LLAMA3:Mc Int y re Ġwas Ġpicked Ġto Ġbe Ġused .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The arts also flourished in India during these early times.\n",
      "\t Tokenized GPT2:The Ġarts Ġalso Ġflour ished Ġin ĠIndia Ġduring Ġthese Ġearly Ġtimes .\n",
      "\t Tokenized LLAMA3:The Ġarts Ġalso Ġflour ished Ġin ĠIndia Ġduring Ġthese Ġearly Ġtimes .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The arts languished during those early days.\n",
      "\t Tokenized GPT2:The Ġarts Ġl angu ished Ġduring Ġthose Ġearly Ġdays .\n",
      "\t Tokenized LLAMA3:The Ġarts Ġl angu ished Ġduring Ġthose Ġearly Ġdays .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The Wall Street Journal Business Bulletin has a fact that dramatizes how profoundly well-off this country is--Americans throw out approximately 12 percent of the stuff they buy at the supermarket.\n",
      "\t Tokenized GPT2:The ĠWall ĠStreet ĠJournal ĠBusiness ĠBul let in Ġhas Ġa Ġfact Ġthat Ġdram at izes Ġhow Ġprofound ly Ġwell - off Ġthis Ġcountry Ġis -- Americans Ġthrow Ġout Ġapproximately Ġ12 Ġpercent Ġof Ġthe Ġstuff Ġthey Ġbuy Ġat Ġthe Ġsupermarket .\n",
      "\t Tokenized LLAMA3:The ĠWall ĠStreet ĠJournal ĠBusiness ĠBul let in Ġhas Ġa Ġfact Ġthat Ġdram at izes Ġhow Ġprofound ly Ġwell -off Ġthis Ġcountry Ġis -- Americ ans Ġthrow Ġout Ġapproximately Ġ 12 Ġpercent Ġof Ġthe Ġstuff Ġthey Ġbuy Ġat Ġthe Ġsuper market .\n",
      "\t Unique Tokens GPT2: {'Ġ12', 'Americans', '-', 'off', 'Ġsupermarket'}\n",
      "\t Unique Tokens LLAMA3: {'12', 'Ġ', 'ans', 'Ġsuper', 'Americ', 'market', '-off'}\n",
      "Text 2: Americans just throw away 12 percent of what they buy at foreign supermarkets.\n",
      "\t Tokenized GPT2:Americans Ġjust Ġthrow Ġaway Ġ12 Ġpercent Ġof Ġwhat Ġthey Ġbuy Ġat Ġforeign Ġsuper mark ets .\n",
      "\t Tokenized LLAMA3:Americ ans Ġjust Ġthrow Ġaway Ġ 12 Ġpercent Ġof Ġwhat Ġthey Ġbuy Ġat Ġforeign Ġsuper mark ets .\n",
      "\t Unique Tokens GPT2: {'Americans', 'Ġ12'}\n",
      "\t Unique Tokens LLAMA3: {'Ġ', 'ans', 'Americ', '12'}\n",
      "==entailment==\n",
      "Text 1: yeah yeah you probably get this probably pretty sticky after you get done then you've got to drain the water out of the watermelon because you know when you scrape it it makes the water\n",
      "\t Tokenized GPT2:yeah Ġyeah Ġyou Ġprobably Ġget Ġthis Ġprobably Ġpretty Ġsticky Ġafter Ġyou Ġget Ġdone Ġthen Ġyou 've Ġgot Ġto Ġdrain Ġthe Ġwater Ġout Ġof Ġthe Ġwater mel on Ġbecause Ġyou Ġknow Ġwhen Ġyou Ġsc rape Ġit Ġit Ġmakes Ġthe Ġwater\n",
      "\t Tokenized LLAMA3:yeah Ġyeah Ġyou Ġprobably Ġget Ġthis Ġprobably Ġpretty Ġsticky Ġafter Ġyou Ġget Ġdone Ġthen Ġyou 've Ġgot Ġto Ġdrain Ġthe Ġwater Ġout Ġof Ġthe Ġwater mel on Ġbecause Ġyou Ġknow Ġwhen Ġyou Ġsc rape Ġit Ġit Ġmakes Ġthe Ġwater\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: You need to drain the water our of the watermelon.\n",
      "\t Tokenized GPT2:You Ġneed Ġto Ġdrain Ġthe Ġwater Ġour Ġof Ġthe Ġwater mel on .\n",
      "\t Tokenized LLAMA3:You Ġneed Ġto Ġdrain Ġthe Ġwater Ġour Ġof Ġthe Ġwater mel on .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: But the world is not run for the edification of tourists.\n",
      "\t Tokenized GPT2:But Ġthe Ġworld Ġis Ġnot Ġrun Ġfor Ġthe Ġed ification Ġof Ġtourists .\n",
      "\t Tokenized LLAMA3:But Ġthe Ġworld Ġis Ġnot Ġrun Ġfor Ġthe Ġed ification Ġof Ġtourists .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The world does not try and morally subject to tourists.\n",
      "\t Tokenized GPT2:The Ġworld Ġdoes Ġnot Ġtry Ġand Ġmorally Ġsubject Ġto Ġtourists .\n",
      "\t Tokenized LLAMA3:The Ġworld Ġdoes Ġnot Ġtry Ġand Ġmorally Ġsubject Ġto Ġtourists .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: There were maybe three hundred people present.\n",
      "\t Tokenized GPT2:There Ġwere Ġmaybe Ġthree Ġhundred Ġpeople Ġpresent .\n",
      "\t Tokenized LLAMA3:There Ġwere Ġmaybe Ġthree Ġhundred Ġpeople Ġpresent .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: 300 people were there.\n",
      "\t Tokenized GPT2:300 Ġpeople Ġwere Ġthere .\n",
      "\t Tokenized LLAMA3:300 Ġpeople Ġwere Ġthere .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: yeah they uh they the voters voted one way and it and then uh some federal judge said no that was unconstitutional and they have had two or three votes and the city council is divided over what the district should be because they divide it one way and the minorities say we're losing representation  \n",
      "\t Tokenized GPT2:yeah Ġthey Ġuh Ġthey Ġthe Ġvoters Ġvoted Ġone Ġway Ġand Ġit Ġand Ġthen Ġuh Ġsome Ġfederal Ġjudge Ġsaid Ġno Ġthat Ġwas Ġun const itutional Ġand Ġthey Ġhave Ġhad Ġtwo Ġor Ġthree Ġvotes Ġand Ġthe Ġcity Ġcouncil Ġis Ġdivided Ġover Ġwhat Ġthe Ġdistrict Ġshould Ġbe Ġbecause Ġthey Ġdivide Ġit Ġone Ġway Ġand Ġthe Ġminorities Ġsay Ġwe 're Ġlosing Ġrepresentation ĠĠ\n",
      "\t Tokenized LLAMA3:yeah Ġthey Ġuh Ġthey Ġthe Ġvoters Ġvoted Ġone Ġway Ġand Ġit Ġand Ġthen Ġuh Ġsome Ġfederal Ġjudge Ġsaid Ġno Ġthat Ġwas Ġun const itutional Ġand Ġthey Ġhave Ġhad Ġtwo Ġor Ġthree Ġvotes Ġand Ġthe Ġcity Ġcouncil Ġis Ġdivided Ġover Ġwhat Ġthe Ġdistrict Ġshould Ġbe Ġbecause Ġthey Ġdivide Ġit Ġone Ġway Ġand Ġthe Ġminorities Ġsay Ġwe 're Ġlosing Ġrepresentation ĠĠ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: the federal judge overruled the vote of the people on the ground that it was unconstitutional\n",
      "\t Tokenized GPT2:the Ġfederal Ġjudge Ġoverr uled Ġthe Ġvote Ġof Ġthe Ġpeople Ġon Ġthe Ġground Ġthat Ġit Ġwas Ġun const itutional\n",
      "\t Tokenized LLAMA3:the Ġfederal Ġjudge Ġoverr uled Ġthe Ġvote Ġof Ġthe Ġpeople Ġon Ġthe Ġground Ġthat Ġit Ġwas Ġun const itutional\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: They keep romance and marriage apart  \" Tommy flushed.\n",
      "\t Tokenized GPT2:They Ġkeep Ġromance Ġand Ġmarriage Ġapart Ġ Ġ\" ĠTommy Ġflushed .\n",
      "\t Tokenized LLAMA3:They Ġkeep Ġromance Ġand Ġmarriage Ġapart Ġ Ġ\" ĠTommy Ġflushed .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Tommy said they don't mix romance and marriage and they have a good relationship.\n",
      "\t Tokenized GPT2:Tom my Ġsaid Ġthey Ġdon 't Ġmix Ġromance Ġand Ġmarriage Ġand Ġthey Ġhave Ġa Ġgood Ġrelationship .\n",
      "\t Tokenized LLAMA3:Tom my Ġsaid Ġthey Ġdon 't Ġmix Ġromance Ġand Ġmarriage Ġand Ġthey Ġhave Ġa Ġgood Ġrelationship .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Companies that were foreign had to accept Indian financial participation and management.\n",
      "\t Tokenized GPT2:Comp anies Ġthat Ġwere Ġforeign Ġhad Ġto Ġaccept ĠIndian Ġfinancial Ġparticipation Ġand Ġmanagement .\n",
      "\t Tokenized LLAMA3:Comp anies Ġthat Ġwere Ġforeign Ġhad Ġto Ġaccept ĠIndian Ġfinancial Ġparticipation Ġand Ġmanagement .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Foreign companies had to take Indian money in order to operate their businesses.\n",
      "\t Tokenized GPT2:Foreign Ġcompanies Ġhad Ġto Ġtake ĠIndian Ġmoney Ġin Ġorder Ġto Ġoperate Ġtheir Ġbusinesses .\n",
      "\t Tokenized LLAMA3:Fore ign Ġcompanies Ġhad Ġto Ġtake ĠIndian Ġmoney Ġin Ġorder Ġto Ġoperate Ġtheir Ġbusinesses .\n",
      "\t Unique Tokens GPT2: {'Foreign'}\n",
      "\t Unique Tokens LLAMA3: {'ign', 'Fore'}\n",
      "==neutral==\n",
      "Text 1: Castlerigg near Keswick is the best example.\n",
      "\t Tokenized GPT2:Cast ler ig g Ġnear ĠK es wick Ġis Ġthe Ġbest Ġexample .\n",
      "\t Tokenized LLAMA3:Cast ler ig g Ġnear ĠK es wick Ġis Ġthe Ġbest Ġexample .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: A good example would be Castlerigg near Keswick, in Scotland.\n",
      "\t Tokenized GPT2:A Ġgood Ġexample Ġwould Ġbe ĠCast ler ig g Ġnear ĠK es wick , Ġin ĠScotland .\n",
      "\t Tokenized LLAMA3:A Ġgood Ġexample Ġwould Ġbe ĠCast ler ig g Ġnear ĠK es wick , Ġin ĠScotland .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: 4) Not enough is known about how nontransportation costs vary with distance.\n",
      "\t Tokenized GPT2:4 ) ĠNot Ġenough Ġis Ġknown Ġabout Ġhow Ġnon transport ation Ġcosts Ġvary Ġwith Ġdistance .\n",
      "\t Tokenized LLAMA3:4 ) ĠNot Ġenough Ġis Ġknown Ġabout Ġhow Ġnon trans port ation Ġcosts Ġvary Ġwith Ġdistance .\n",
      "\t Unique Tokens GPT2: {'transport'}\n",
      "\t Unique Tokens LLAMA3: {'port', 'trans'}\n",
      "Text 2: There's more than enough known concerning the ways in which costs associated with nontransportation change according to the distance.\n",
      "\t Tokenized GPT2:There 's Ġmore Ġthan Ġenough Ġknown Ġconcerning Ġthe Ġways Ġin Ġwhich Ġcosts Ġassociated Ġwith Ġnon transport ation Ġchange Ġaccording Ġto Ġthe Ġdistance .\n",
      "\t Tokenized LLAMA3:There 's Ġmore Ġthan Ġenough Ġknown Ġconcerning Ġthe Ġways Ġin Ġwhich Ġcosts Ġassociated Ġwith Ġnon trans port ation Ġchange Ġaccording Ġto Ġthe Ġdistance .\n",
      "\t Unique Tokens GPT2: {'transport'}\n",
      "\t Unique Tokens LLAMA3: {'port', 'trans'}\n",
      "==neutral==\n",
      "Text 1: She admits to Dorcas, 'I don't know what to do; scandal between husband and wife is a dreadful thing.' At 4 o'clock she has been angry, but completely mistress of herself. \n",
      "\t Tokenized GPT2:She Ġadmits Ġto ĠDor cas , Ġ' I Ġdon 't Ġknow Ġwhat Ġto Ġdo ; Ġscandal Ġbetween Ġhusband Ġand Ġwife Ġis Ġa Ġdread ful Ġthing .' ĠAt Ġ4 Ġo ' clock Ġshe Ġhas Ġbeen Ġangry , Ġbut Ġcompletely Ġmist ress Ġof Ġherself . Ġ\n",
      "\t Tokenized LLAMA3:She Ġadmits Ġto ĠDor cas , Ġ' I Ġdon 't Ġknow Ġwhat Ġto Ġdo ; Ġscandal Ġbetween Ġhusband Ġand Ġwife Ġis Ġa Ġdread ful Ġthing .' ĠAt Ġ 4 Ġo 'clock Ġshe Ġhas Ġbeen Ġangry , Ġbut Ġcompletely Ġmist ress Ġof Ġherself . Ġ\n",
      "\t Unique Tokens GPT2: {'Ġ4', \"'\", 'clock'}\n",
      "\t Unique Tokens LLAMA3: {\"'clock\", '4'}\n",
      "Text 2: Dorcas agreed with her comments about scandals between husbands and wives.\n",
      "\t Tokenized GPT2:D or cas Ġagreed Ġwith Ġher Ġcomments Ġabout Ġsc and als Ġbetween Ġhus bands Ġand Ġwives .\n",
      "\t Tokenized LLAMA3:D or cas Ġagreed Ġwith Ġher Ġcomments Ġabout Ġsc and als Ġbetween Ġhus bands Ġand Ġwives .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: yeah i mean this this Escort even when the head gasket went i mean it would start first time every time\n",
      "\t Tokenized GPT2:yeah Ġi Ġmean Ġthis Ġthis ĠEsc ort Ġeven Ġwhen Ġthe Ġhead Ġg asket Ġwent Ġi Ġmean Ġit Ġwould Ġstart Ġfirst Ġtime Ġevery Ġtime\n",
      "\t Tokenized LLAMA3:yeah Ġi Ġmean Ġthis Ġthis ĠEsc ort Ġeven Ġwhen Ġthe Ġhead Ġg asket Ġwent Ġi Ġmean Ġit Ġwould Ġstart Ġfirst Ġtime Ġevery Ġtime\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Once the head gasket went out, the Escort stopped working.\n",
      "\t Tokenized GPT2:Once Ġthe Ġhead Ġg asket Ġwent Ġout , Ġthe ĠEsc ort Ġstopped Ġworking .\n",
      "\t Tokenized LLAMA3:Once Ġthe Ġhead Ġg asket Ġwent Ġout , Ġthe ĠEsc ort Ġstopped Ġworking .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Daniel nodded, fetching me a glass of beer.\n",
      "\t Tokenized GPT2:Daniel Ġnodded , Ġfetch ing Ġme Ġa Ġglass Ġof Ġbeer .\n",
      "\t Tokenized LLAMA3:Daniel Ġnodded , Ġfetch ing Ġme Ġa Ġglass Ġof Ġbeer .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Daniel got me a vodka and tonic. \n",
      "\t Tokenized GPT2:Daniel Ġgot Ġme Ġa Ġv odka Ġand Ġton ic . Ġ\n",
      "\t Tokenized LLAMA3:Daniel Ġgot Ġme Ġa Ġv od ka Ġand Ġton ic . Ġ\n",
      "\t Unique Tokens GPT2: {'odka'}\n",
      "\t Unique Tokens LLAMA3: {'ka', 'od'}\n",
      "==contradiction==\n",
      "Text 1: He reported masterfully on the '72 campaign and the Hell's Angels.\n",
      "\t Tokenized GPT2:He Ġreported Ġmaster fully Ġon Ġthe Ġ' 72 Ġcampaign Ġand Ġthe ĠHell 's ĠAngels .\n",
      "\t Tokenized LLAMA3:He Ġreported Ġmaster fully Ġon Ġthe Ġ' 72 Ġcampaign Ġand Ġthe ĠHell 's ĠAngels .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He did an extraordinarily bad job reporting on the Hell's Angels.\n",
      "\t Tokenized GPT2:He Ġdid Ġan Ġextra ordin arily Ġbad Ġjob Ġreporting Ġon Ġthe ĠHell 's ĠAngels .\n",
      "\t Tokenized LLAMA3:He Ġdid Ġan Ġextra ordin arily Ġbad Ġjob Ġreporting Ġon Ġthe ĠHell 's ĠAngels .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: There followed the Balkan Wars, in which Turkey lost western Thrace and Macedonia, then World War I, into which Turkey entered on Germany's side.\n",
      "\t Tokenized GPT2:There Ġfollowed Ġthe ĠB alk an ĠWars , Ġin Ġwhich ĠTurkey Ġlost Ġwestern ĠThr ace Ġand ĠM aced onia , Ġthen ĠWorld ĠWar ĠI , Ġinto Ġwhich ĠTurkey Ġentered Ġon ĠGermany 's Ġside .\n",
      "\t Tokenized LLAMA3:There Ġfollowed Ġthe ĠB alk an ĠWars , Ġin Ġwhich ĠTurkey Ġlost Ġwestern ĠThr ace Ġand ĠM aced onia , Ġthen ĠWorld ĠWar ĠI , Ġinto Ġwhich ĠTurkey Ġentered Ġon ĠGermany 's Ġside .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Turkey entered World War I in order to regain territory lost during the Balkan Wars.\n",
      "\t Tokenized GPT2:Tur key Ġentered ĠWorld ĠWar ĠI Ġin Ġorder Ġto Ġregain Ġterritory Ġlost Ġduring Ġthe ĠB alk an ĠWars .\n",
      "\t Tokenized LLAMA3:Tur key Ġentered ĠWorld ĠWar ĠI Ġin Ġorder Ġto Ġregain Ġterritory Ġlost Ġduring Ġthe ĠB alk an ĠWars .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: She had thrown away her cloak and tied her hair back into a topknot to keep it out of the way.\n",
      "\t Tokenized GPT2:She Ġhad Ġthrown Ġaway Ġher Ġcloak Ġand Ġtied Ġher Ġhair Ġback Ġinto Ġa Ġtop k not Ġto Ġkeep Ġit Ġout Ġof Ġthe Ġway .\n",
      "\t Tokenized LLAMA3:She Ġhad Ġthrown Ġaway Ġher Ġcloak Ġand Ġtied Ġher Ġhair Ġback Ġinto Ġa Ġtop k not Ġto Ġkeep Ġit Ġout Ġof Ġthe Ġway .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: She shaved her head.\n",
      "\t Tokenized GPT2:She Ġsh aved Ġher Ġhead .\n",
      "\t Tokenized LLAMA3:She Ġsh aved Ġher Ġhead .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Daniel sat buried by the lights, occasionally pressing things.\n",
      "\t Tokenized GPT2:Daniel Ġsat Ġburied Ġby Ġthe Ġlights , Ġoccasionally Ġpressing Ġthings .\n",
      "\t Tokenized LLAMA3:Daniel Ġsat Ġburied Ġby Ġthe Ġlights , Ġoccasionally Ġpressing Ġthings .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Daniel was covered by lights and he was not standing. \n",
      "\t Tokenized GPT2:Daniel Ġwas Ġcovered Ġby Ġlights Ġand Ġhe Ġwas Ġnot Ġstanding . Ġ\n",
      "\t Tokenized LLAMA3:Daniel Ġwas Ġcovered Ġby Ġlights Ġand Ġhe Ġwas Ġnot Ġstanding . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: You wonder what youre going to be when you grow up, lawyer Smith said. \n",
      "\t Tokenized GPT2:You Ġwonder Ġwhat Ġyoure Ġgoing Ġto Ġbe Ġwhen Ġyou Ġgrow Ġup , Ġlawyer ĠSmith Ġsaid . Ġ\n",
      "\t Tokenized LLAMA3:You Ġwonder Ġwhat Ġyoure Ġgoing Ġto Ġbe Ġwhen Ġyou Ġgrow Ġup , Ġlawyer ĠSmith Ġsaid . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The lawyer, Smith, pointed out that you wanted to know what you would be when you grew up.\n",
      "\t Tokenized GPT2:The Ġlawyer , ĠSmith , Ġpointed Ġout Ġthat Ġyou Ġwanted Ġto Ġknow Ġwhat Ġyou Ġwould Ġbe Ġwhen Ġyou Ġgrew Ġup .\n",
      "\t Tokenized LLAMA3:The Ġlawyer , ĠSmith , Ġpointed Ġout Ġthat Ġyou Ġwanted Ġto Ġknow Ġwhat Ġyou Ġwould Ġbe Ġwhen Ġyou Ġgrew Ġup .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Yes, you've done very well, young man.\n",
      "\t Tokenized GPT2:Yes , Ġyou 've Ġdone Ġvery Ġwell , Ġyoung Ġman .\n",
      "\t Tokenized LLAMA3:Yes , Ġyou 've Ġdone Ġvery Ġwell , Ġyoung Ġman .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Yes, you have a done a great job, young man.\n",
      "\t Tokenized GPT2:Yes , Ġyou Ġhave Ġa Ġdone Ġa Ġgreat Ġjob , Ġyoung Ġman .\n",
      "\t Tokenized LLAMA3:Yes , Ġyou Ġhave Ġa Ġdone Ġa Ġgreat Ġjob , Ġyoung Ġman .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Yet, despite the stock market boom of the 1990s, many households have accumulated little, if any, wealth (see figure 1.3), and half of American households did not own stocks as of 1998.\n",
      "\t Tokenized GPT2:Yet , Ġdespite Ġthe Ġstock Ġmarket Ġboom Ġof Ġthe Ġ1990 s , Ġmany Ġhouseholds Ġhave Ġaccumulated Ġlittle , Ġif Ġany , Ġwealth Ġ( see Ġfigure Ġ1 . 3 ), Ġand Ġhalf Ġof ĠAmerican Ġhouseholds Ġdid Ġnot Ġown Ġstocks Ġas Ġof Ġ1998 .\n",
      "\t Tokenized LLAMA3:Yet , Ġdespite Ġthe Ġstock Ġmarket Ġboom Ġof Ġthe Ġ 199 0 s , Ġmany Ġhouseholds Ġhave Ġaccumulated Ġlittle , Ġif Ġany , Ġwealth Ġ( see Ġfigure Ġ 1 . 3 ), Ġand Ġhalf Ġof ĠAmerican Ġhouseholds Ġdid Ġnot Ġown Ġstocks Ġas Ġof Ġ 199 8 .\n",
      "\t Unique Tokens GPT2: {'Ġ1990', 'Ġ1', 'Ġ1998'}\n",
      "\t Unique Tokens LLAMA3: {'199', 'Ġ', '0', '8', '1'}\n",
      "Text 2: The stock market boom of the 1990s led to explosive wealth accumulation in most households.\n",
      "\t Tokenized GPT2:The Ġstock Ġmarket Ġboom Ġof Ġthe Ġ1990 s Ġled Ġto Ġexplosive Ġwealth Ġaccumulation Ġin Ġmost Ġhouseholds .\n",
      "\t Tokenized LLAMA3:The Ġstock Ġmarket Ġboom Ġof Ġthe Ġ 199 0 s Ġled Ġto Ġexplosive Ġwealth Ġaccumulation Ġin Ġmost Ġhouseholds .\n",
      "\t Unique Tokens GPT2: {'Ġ1990'}\n",
      "\t Unique Tokens LLAMA3: {'199', '0', 'Ġ'}\n",
      "==contradiction==\n",
      "Text 1: and uh well if you if you got got him a power mower it'd probably take him a lot less time to do it but i enjoy doing it i feel good doing it uh i i feel a lot better doing it with a power mower with that with a  with a pull tractor on it so i don't have to push so hard\n",
      "\t Tokenized GPT2:and Ġuh Ġwell Ġif Ġyou Ġif Ġyou Ġgot Ġgot Ġhim Ġa Ġpower Ġm ower Ġit 'd Ġprobably Ġtake Ġhim Ġa Ġlot Ġless Ġtime Ġto Ġdo Ġit Ġbut Ġi Ġenjoy Ġdoing Ġit Ġi Ġfeel Ġgood Ġdoing Ġit Ġuh Ġi Ġi Ġfeel Ġa Ġlot Ġbetter Ġdoing Ġit Ġwith Ġa Ġpower Ġm ower Ġwith Ġthat Ġwith Ġa Ġ Ġwith Ġa Ġpull Ġtr actor Ġon Ġit Ġso Ġi Ġdon 't Ġhave Ġto Ġpush Ġso Ġhard\n",
      "\t Tokenized LLAMA3:and Ġuh Ġwell Ġif Ġyou Ġif Ġyou Ġgot Ġgot Ġhim Ġa Ġpower Ġm ower Ġit 'd Ġprobably Ġtake Ġhim Ġa Ġlot Ġless Ġtime Ġto Ġdo Ġit Ġbut Ġi Ġenjoy Ġdoing Ġit Ġi Ġfeel Ġgood Ġdoing Ġit Ġuh Ġi Ġi Ġfeel Ġa Ġlot Ġbetter Ġdoing Ġit Ġwith Ġa Ġpower Ġm ower Ġwith Ġthat Ġwith Ġa Ġ Ġwith Ġa Ġpull Ġtr actor Ġon Ġit Ġso Ġi Ġdon 't Ġhave Ġto Ġpush Ġso Ġhard\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Keeping the old push mower is the best idea, since it gets the job done as fast as a power mower.\n",
      "\t Tokenized GPT2:Ke eping Ġthe Ġold Ġpush Ġm ower Ġis Ġthe Ġbest Ġidea , Ġsince Ġit Ġgets Ġthe Ġjob Ġdone Ġas Ġfast Ġas Ġa Ġpower Ġm ower .\n",
      "\t Tokenized LLAMA3:Ke eping Ġthe Ġold Ġpush Ġm ower Ġis Ġthe Ġbest Ġidea , Ġsince Ġit Ġgets Ġthe Ġjob Ġdone Ġas Ġfast Ġas Ġa Ġpower Ġm ower .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: We know they will have to come from the south but that gives them a space as wide as the town in which to launch their attack.\n",
      "\t Tokenized GPT2:We Ġknow Ġthey Ġwill Ġhave Ġto Ġcome Ġfrom Ġthe Ġsouth Ġbut Ġthat Ġgives Ġthem Ġa Ġspace Ġas Ġwide Ġas Ġthe Ġtown Ġin Ġwhich Ġto Ġlaunch Ġtheir Ġattack .\n",
      "\t Tokenized LLAMA3:We Ġknow Ġthey Ġwill Ġhave Ġto Ġcome Ġfrom Ġthe Ġsouth Ġbut Ġthat Ġgives Ġthem Ġa Ġspace Ġas Ġwide Ġas Ġthe Ġtown Ġin Ġwhich Ġto Ġlaunch Ġtheir Ġattack .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The people will come from the south with lots of weapons.\n",
      "\t Tokenized GPT2:The Ġpeople Ġwill Ġcome Ġfrom Ġthe Ġsouth Ġwith Ġlots Ġof Ġweapons .\n",
      "\t Tokenized LLAMA3:The Ġpeople Ġwill Ġcome Ġfrom Ġthe Ġsouth Ġwith Ġlots Ġof Ġweapons .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: In 1995 and again in 1998, the Legal Services Corporation recognized that legal services programs were going to have to change the method and manner in which they conducted their business if they were going to remain viable and responsive to the needs of low income persons.\n",
      "\t Tokenized GPT2:In Ġ1995 Ġand Ġagain Ġin Ġ1998 , Ġthe ĠLe gal ĠServices ĠCorporation Ġrecognized Ġthat Ġlegal Ġservices Ġprograms Ġwere Ġgoing Ġto Ġhave Ġto Ġchange Ġthe Ġmethod Ġand Ġmanner Ġin Ġwhich Ġthey Ġconducted Ġtheir Ġbusiness Ġif Ġthey Ġwere Ġgoing Ġto Ġremain Ġviable Ġand Ġresponsive Ġto Ġthe Ġneeds Ġof Ġlow Ġincome Ġpersons .\n",
      "\t Tokenized LLAMA3:In Ġ 199 5 Ġand Ġagain Ġin Ġ 199 8 , Ġthe ĠLe gal ĠServices ĠCorporation Ġrecognized Ġthat Ġlegal Ġservices Ġprograms Ġwere Ġgoing Ġto Ġhave Ġto Ġchange Ġthe Ġmethod Ġand Ġmanner Ġin Ġwhich Ġthey Ġconducted Ġtheir Ġbusiness Ġif Ġthey Ġwere Ġgoing Ġto Ġremain Ġviable Ġand Ġresponsive Ġto Ġthe Ġneeds Ġof Ġlow Ġincome Ġpersons .\n",
      "\t Unique Tokens GPT2: {'Ġ1995', 'Ġ1998'}\n",
      "\t Unique Tokens LLAMA3: {'199', '5', 'Ġ', '8'}\n",
      "Text 2: The Legal Services Corporation realized things had to change more than once in the past.\n",
      "\t Tokenized GPT2:The ĠLe gal ĠServices ĠCorporation Ġrealized Ġthings Ġhad Ġto Ġchange Ġmore Ġthan Ġonce Ġin Ġthe Ġpast .\n",
      "\t Tokenized LLAMA3:The ĠLe gal ĠServices ĠCorporation Ġrealized Ġthings Ġhad Ġto Ġchange Ġmore Ġthan Ġonce Ġin Ġthe Ġpast .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: By seeding packs with a few high-value cards, the manufacturer is encouraging kids to buy Pokemon cards like lottery tickets.\n",
      "\t Tokenized GPT2:By Ġse eding Ġpacks Ġwith Ġa Ġfew Ġhigh - value Ġcards , Ġthe Ġmanufacturer Ġis Ġencouraging Ġkids Ġto Ġbuy ĠPokemon Ġcards Ġlike Ġlottery Ġtickets .\n",
      "\t Tokenized LLAMA3:By Ġse eding Ġpacks Ġwith Ġa Ġfew Ġhigh -value Ġcards , Ġthe Ġmanufacturer Ġis Ġencouraging Ġkids Ġto Ġbuy ĠPokemon Ġcards Ġlike Ġlottery Ġtickets .\n",
      "\t Unique Tokens GPT2: {'-', 'value'}\n",
      "\t Unique Tokens LLAMA3: {'-value'}\n",
      "Text 2: Each Pokemon card pack is filled with every rare card a kid could want.\n",
      "\t Tokenized GPT2:Each ĠPokemon Ġcard Ġpack Ġis Ġfilled Ġwith Ġevery Ġrare Ġcard Ġa Ġkid Ġcould Ġwant .\n",
      "\t Tokenized LLAMA3:Each ĠPokemon Ġcard Ġpack Ġis Ġfilled Ġwith Ġevery Ġrare Ġcard Ġa Ġkid Ġcould Ġwant .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Citing conservative critics of Brown vs.\n",
      "\t Tokenized GPT2:C iting Ġconservative Ġcritics Ġof ĠBrown Ġvs .\n",
      "\t Tokenized LLAMA3:C iting Ġconservative Ġcritics Ġof ĠBrown Ġvs .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Conservative critics wrote about the Brown case.  \n",
      "\t Tokenized GPT2:Cons erv ative Ġcritics Ġwrote Ġabout Ġthe ĠBrown Ġcase . ĠĠ\n",
      "\t Tokenized LLAMA3:Cons erv ative Ġcritics Ġwrote Ġabout Ġthe ĠBrown Ġcase . ĠĠ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: You are sure that you did not in any way disclose your identity?\" Tommy shook his head.\n",
      "\t Tokenized GPT2:You Ġare Ġsure Ġthat Ġyou Ġdid Ġnot Ġin Ġany Ġway Ġdisclose Ġyour Ġidentity ?\" ĠTommy Ġshook Ġhis Ġhead .\n",
      "\t Tokenized LLAMA3:You Ġare Ġsure Ġthat Ġyou Ġdid Ġnot Ġin Ġany Ġway Ġdisclose Ġyour Ġidentity ?\" ĠTommy Ġshook Ġhis Ġhead .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: You are sure that you did not in any way disclose that your last name is Smith? \n",
      "\t Tokenized GPT2:You Ġare Ġsure Ġthat Ġyou Ġdid Ġnot Ġin Ġany Ġway Ġdisclose Ġthat Ġyour Ġlast Ġname Ġis ĠSmith ? Ġ\n",
      "\t Tokenized LLAMA3:You Ġare Ġsure Ġthat Ġyou Ġdid Ġnot Ġin Ġany Ġway Ġdisclose Ġthat Ġyour Ġlast Ġname Ġis ĠSmith ? Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: In the first instance, IRS would have no record of time before the person could get through to an agent and of discouraged callers.\n",
      "\t Tokenized GPT2:In Ġthe Ġfirst Ġinstance , ĠIRS Ġwould Ġhave Ġno Ġrecord Ġof Ġtime Ġbefore Ġthe Ġperson Ġcould Ġget Ġthrough Ġto Ġan Ġagent Ġand Ġof Ġdiscouraged Ġcall ers .\n",
      "\t Tokenized LLAMA3:In Ġthe Ġfirst Ġinstance , ĠIRS Ġwould Ġhave Ġno Ġrecord Ġof Ġtime Ġbefore Ġthe Ġperson Ġcould Ġget Ġthrough Ġto Ġan Ġagent Ġand Ġof Ġdiscouraged Ġcall ers .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: There is no recording of the time for callers.\n",
      "\t Tokenized GPT2:There Ġis Ġno Ġrecording Ġof Ġthe Ġtime Ġfor Ġcall ers .\n",
      "\t Tokenized LLAMA3:There Ġis Ġno Ġrecording Ġof Ġthe Ġtime Ġfor Ġcall ers .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: A profile crowns Chris Rock The Funniest Man in America.\n",
      "\t Tokenized GPT2:A Ġprofile Ġcrown s ĠChris ĠRock ĠThe ĠFun n iest ĠMan Ġin ĠAmerica .\n",
      "\t Tokenized LLAMA3:A Ġprofile Ġcrown s ĠChris ĠRock ĠThe ĠFun n iest ĠMan Ġin ĠAmerica .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Chris Rock has been crowned The Funniest Man in America.\n",
      "\t Tokenized GPT2:Chris ĠRock Ġhas Ġbeen Ġcr owned ĠThe ĠFun n iest ĠMan Ġin ĠAmerica .\n",
      "\t Tokenized LLAMA3:Chris ĠRock Ġhas Ġbeen Ġcr owned ĠThe ĠFun n iest ĠMan Ġin ĠAmerica .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: She wears either revealing clothes or professional clothes (or perhaps both).\n",
      "\t Tokenized GPT2:She Ġwears Ġeither Ġrevealing Ġclothes Ġor Ġprofessional Ġclothes Ġ( or Ġperhaps Ġboth ).\n",
      "\t Tokenized LLAMA3:She Ġwears Ġeither Ġrevealing Ġclothes Ġor Ġprofessional Ġclothes Ġ( or Ġperhaps Ġboth ).\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Her clothes are either provocative or conservative.\n",
      "\t Tokenized GPT2:Her Ġclothes Ġare Ġeither Ġprov oc ative Ġor Ġconservative .\n",
      "\t Tokenized LLAMA3:Her Ġclothes Ġare Ġeither Ġprov oc ative Ġor Ġconservative .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: More than half of 800,000 native islanders are children, and the mother is traditionally responsible for bringing them up, handling the money, and making key domestic decisions.\n",
      "\t Tokenized GPT2:More Ġthan Ġhalf Ġof Ġ800 , 000 Ġnative Ġisland ers Ġare Ġchildren , Ġand Ġthe Ġmother Ġis Ġtraditionally Ġresponsible Ġfor Ġbringing Ġthem Ġup , Ġhandling Ġthe Ġmoney , Ġand Ġmaking Ġkey Ġdomestic Ġdecisions .\n",
      "\t Tokenized LLAMA3:More Ġthan Ġhalf Ġof Ġ 800 , 000 Ġnative Ġisland ers Ġare Ġchildren , Ġand Ġthe Ġmother Ġis Ġtraditionally Ġresponsible Ġfor Ġbringing Ġthem Ġup , Ġhandling Ġthe Ġmoney , Ġand Ġmaking Ġkey Ġdomestic Ġdecisions .\n",
      "\t Unique Tokens GPT2: {'Ġ800'}\n",
      "\t Unique Tokens LLAMA3: {'800', 'Ġ'}\n",
      "Text 2: The mother is responsible for the raising of the native islander children.\n",
      "\t Tokenized GPT2:The Ġmother Ġis Ġresponsible Ġfor Ġthe Ġraising Ġof Ġthe Ġnative Ġisland er Ġchildren .\n",
      "\t Tokenized LLAMA3:The Ġmother Ġis Ġresponsible Ġfor Ġthe Ġraising Ġof Ġthe Ġnative Ġisland er Ġchildren .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: right well the warmth that developed between them and again it i think was a picture of relationships\n",
      "\t Tokenized GPT2:right Ġwell Ġthe Ġwarmth Ġthat Ġdeveloped Ġbetween Ġthem Ġand Ġagain Ġit Ġi Ġthink Ġwas Ġa Ġpicture Ġof Ġrelationships\n",
      "\t Tokenized LLAMA3:right Ġwell Ġthe Ġwarmth Ġthat Ġdeveloped Ġbetween Ġthem Ġand Ġagain Ġit Ġi Ġthink Ġwas Ġa Ġpicture Ġof Ġrelationships\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: There was a mutual feeling in their relationship.\n",
      "\t Tokenized GPT2:There Ġwas Ġa Ġmutual Ġfeeling Ġin Ġtheir Ġrelationship .\n",
      "\t Tokenized LLAMA3:There Ġwas Ġa Ġmutual Ġfeeling Ġin Ġtheir Ġrelationship .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: He reverted to his former point of view.\n",
      "\t Tokenized GPT2:He Ġreverted Ġto Ġhis Ġformer Ġpoint Ġof Ġview .\n",
      "\t Tokenized LLAMA3:He Ġreverted Ġto Ġhis Ġformer Ġpoint Ġof Ġview .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He went back to his previous thoughts.\n",
      "\t Tokenized GPT2:He Ġwent Ġback Ġto Ġhis Ġprevious Ġthoughts .\n",
      "\t Tokenized LLAMA3:He Ġwent Ġback Ġto Ġhis Ġprevious Ġthoughts .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: For the next two centuries Aelia Capitolina enjoyed an innocuous history.\n",
      "\t Tokenized GPT2:For Ġthe Ġnext Ġtwo Ġcenturies ĠA elia ĠCapitol ina Ġenjoyed Ġan Ġinnoc uous Ġhistory .\n",
      "\t Tokenized LLAMA3:For Ġthe Ġnext Ġtwo Ġcenturies ĠA elia ĠCapitol ina Ġenjoyed Ġan Ġinnoc uous Ġhistory .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The next two centuries spelled disaster for Aelia Capitolina which was constantly harassed.\n",
      "\t Tokenized GPT2:The Ġnext Ġtwo Ġcenturies Ġspell ed Ġdisaster Ġfor ĠA elia ĠCapitol ina Ġwhich Ġwas Ġconstantly Ġharass ed .\n",
      "\t Tokenized LLAMA3:The Ġnext Ġtwo Ġcenturies Ġspell ed Ġdisaster Ġfor ĠA elia ĠCapitol ina Ġwhich Ġwas Ġconstantly Ġharass ed .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: In the meantime we must send for a doctor, but before we do so, is there anything in this room that might be of value to us?\" Hastily, the three searched.\n",
      "\t Tokenized GPT2:In Ġthe Ġmeantime Ġwe Ġmust Ġsend Ġfor Ġa Ġdoctor , Ġbut Ġbefore Ġwe Ġdo Ġso , Ġis Ġthere Ġanything Ġin Ġthis Ġroom Ġthat Ġmight Ġbe Ġof Ġvalue Ġto Ġus ?\" ĠH ast ily , Ġthe Ġthree Ġsearched .\n",
      "\t Tokenized LLAMA3:In Ġthe Ġmeantime Ġwe Ġmust Ġsend Ġfor Ġa Ġdoctor , Ġbut Ġbefore Ġwe Ġdo Ġso , Ġis Ġthere Ġanything Ġin Ġthis Ġroom Ġthat Ġmight Ġbe Ġof Ġvalue Ġto Ġus ?\" ĠH ast ily , Ġthe Ġthree Ġsearched .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The three searched for valuable items before sending for a doctor.\n",
      "\t Tokenized GPT2:The Ġthree Ġsearched Ġfor Ġvaluable Ġitems Ġbefore Ġsending Ġfor Ġa Ġdoctor .\n",
      "\t Tokenized LLAMA3:The Ġthree Ġsearched Ġfor Ġvaluable Ġitems Ġbefore Ġsending Ġfor Ġa Ġdoctor .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: saving that did not finance domestic investment would increase net foreign investment and improve the current account balance.\n",
      "\t Tokenized GPT2:s aving Ġthat Ġdid Ġnot Ġfinance Ġdomestic Ġinvestment Ġwould Ġincrease Ġnet Ġforeign Ġinvestment Ġand Ġimprove Ġthe Ġcurrent Ġaccount Ġbalance .\n",
      "\t Tokenized LLAMA3:s aving Ġthat Ġdid Ġnot Ġfinance Ġdomestic Ġinvestment Ġwould Ġincrease Ġnet Ġforeign Ġinvestment Ġand Ġimprove Ġthe Ġcurrent Ġaccount Ġbalance .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Saving could increase net foreign investment  substantially and quickly. \n",
      "\t Tokenized GPT2:S aving Ġcould Ġincrease Ġnet Ġforeign Ġinvestment Ġ Ġsubstantially Ġand Ġquickly . Ġ\n",
      "\t Tokenized LLAMA3:S aving Ġcould Ġincrease Ġnet Ġforeign Ġinvestment Ġ Ġsubstantially Ġand Ġquickly . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Tourist Information offices can be very helpful.\n",
      "\t Tokenized GPT2:T our ist ĠInformation Ġoffices Ġcan Ġbe Ġvery Ġhelpful .\n",
      "\t Tokenized LLAMA3:T our ist ĠInformation Ġoffices Ġcan Ġbe Ġvery Ġhelpful .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: One can often get help at Tourist Information offices.\n",
      "\t Tokenized GPT2:One Ġcan Ġoften Ġget Ġhelp Ġat ĠTour ist ĠInformation Ġoffices .\n",
      "\t Tokenized LLAMA3:One Ġcan Ġoften Ġget Ġhelp Ġat ĠTour ist ĠInformation Ġoffices .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: so i how do you feel that it should be applied\n",
      "\t Tokenized GPT2:so Ġi Ġhow Ġdo Ġyou Ġfeel Ġthat Ġit Ġshould Ġbe Ġapplied\n",
      "\t Tokenized LLAMA3:so Ġi Ġhow Ġdo Ġyou Ġfeel Ġthat Ġit Ġshould Ġbe Ġapplied\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: With application how do you think it should be done?\n",
      "\t Tokenized GPT2:With Ġapplication Ġhow Ġdo Ġyou Ġthink Ġit Ġshould Ġbe Ġdone ?\n",
      "\t Tokenized LLAMA3:With Ġapplication Ġhow Ġdo Ġyou Ġthink Ġit Ġshould Ġbe Ġdone ?\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Robust  came in third among words and phrases submitted (220 citations in the CR ), and unlike the previous two, it seems to be a genuinely new cliche; at any rate, Chatterbox hadn't previously been aware of its overuse.\n",
      "\t Tokenized GPT2:Rob ust Ġ Ġcame Ġin Ġthird Ġamong Ġwords Ġand Ġphrases Ġsubmitted Ġ( 220 Ġcit ations Ġin Ġthe ĠCR Ġ), Ġand Ġunlike Ġthe Ġprevious Ġtwo , Ġit Ġseems Ġto Ġbe Ġa Ġgenuinely Ġnew Ġcliche ; Ġat Ġany Ġrate , ĠChat ter box Ġhadn 't Ġpreviously Ġbeen Ġaware Ġof Ġits Ġover use .\n",
      "\t Tokenized LLAMA3:Rob ust Ġ Ġcame Ġin Ġthird Ġamong Ġwords Ġand Ġphrases Ġsubmitted Ġ( 220 Ġcit ations Ġin Ġthe ĠCR Ġ), Ġand Ġunlike Ġthe Ġprevious Ġtwo , Ġit Ġseems Ġto Ġbe Ġa Ġgenuinely Ġnew Ġcl iche ; Ġat Ġany Ġrate , ĠChat ter box Ġhadn 't Ġpreviously Ġbeen Ġaware Ġof Ġits Ġover use .\n",
      "\t Unique Tokens GPT2: {'Ġcliche'}\n",
      "\t Unique Tokens LLAMA3: {'iche', 'Ġcl'}\n",
      "Text 2: Robust came in last place among the submitted words and phrases.\n",
      "\t Tokenized GPT2:Rob ust Ġcame Ġin Ġlast Ġplace Ġamong Ġthe Ġsubmitted Ġwords Ġand Ġphrases .\n",
      "\t Tokenized LLAMA3:Rob ust Ġcame Ġin Ġlast Ġplace Ġamong Ġthe Ġsubmitted Ġwords Ġand Ġphrases .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: This time around, Lloyd believes he's the Messiah.\n",
      "\t Tokenized GPT2:This Ġtime Ġaround , ĠLl oyd Ġbelieves Ġhe 's Ġthe ĠMess iah .\n",
      "\t Tokenized LLAMA3:This Ġtime Ġaround , ĠLl oyd Ġbelieves Ġhe 's Ġthe ĠMess iah .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: This time, Lloyd believes he is a space ship travelling through space. \n",
      "\t Tokenized GPT2:This Ġtime , ĠLl oyd Ġbelieves Ġhe Ġis Ġa Ġspace Ġship Ġtravelling Ġthrough Ġspace . Ġ\n",
      "\t Tokenized LLAMA3:This Ġtime , ĠLl oyd Ġbelieves Ġhe Ġis Ġa Ġspace Ġship Ġtravelling Ġthrough Ġspace . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: it's the very same type of paint and everything\n",
      "\t Tokenized GPT2:it 's Ġthe Ġvery Ġsame Ġtype Ġof Ġpaint Ġand Ġeverything\n",
      "\t Tokenized LLAMA3:it 's Ġthe Ġvery Ġsame Ġtype Ġof Ġpaint Ġand Ġeverything\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It's the same paint formula, it's great!\n",
      "\t Tokenized GPT2:It 's Ġthe Ġsame Ġpaint Ġformula , Ġit 's Ġgreat !\n",
      "\t Tokenized LLAMA3:It 's Ġthe Ġsame Ġpaint Ġformula , Ġit 's Ġgreat !\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: They were so sure of themselves that they took it for granted he had made a mistake.\"\n",
      "\t Tokenized GPT2:They Ġwere Ġso Ġsure Ġof Ġthemselves Ġthat Ġthey Ġtook Ġit Ġfor Ġgranted Ġhe Ġhad Ġmade Ġa Ġmistake .\"\n",
      "\t Tokenized LLAMA3:They Ġwere Ġso Ġsure Ġof Ġthemselves Ġthat Ġthey Ġtook Ġit Ġfor Ġgranted Ġhe Ġhad Ġmade Ġa Ġmistake .\"\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: They were unconfident so they were hesitant to think he might have made a mistake.\n",
      "\t Tokenized GPT2:They Ġwere Ġun conf ident Ġso Ġthey Ġwere Ġhesitant Ġto Ġthink Ġhe Ġmight Ġhave Ġmade Ġa Ġmistake .\n",
      "\t Tokenized LLAMA3:They Ġwere Ġun conf ident Ġso Ġthey Ġwere Ġhesitant Ġto Ġthink Ġhe Ġmight Ġhave Ġmade Ġa Ġmistake .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: It's a great novelty, but very expensive.\n",
      "\t Tokenized GPT2:It 's Ġa Ġgreat Ġnovel ty , Ġbut Ġvery Ġexpensive .\n",
      "\t Tokenized LLAMA3:It 's Ġa Ġgreat Ġnovel ty , Ġbut Ġvery Ġexpensive .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The novelty comes at a large price.\n",
      "\t Tokenized GPT2:The Ġnovel ty Ġcomes Ġat Ġa Ġlarge Ġprice .\n",
      "\t Tokenized LLAMA3:The Ġnovel ty Ġcomes Ġat Ġa Ġlarge Ġprice .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Should we invite these young wealthies back to our comparatively humble, small home?\n",
      "\t Tokenized GPT2:Should Ġwe Ġinvite Ġthese Ġyoung Ġwealth ies Ġback Ġto Ġour Ġcompar atively Ġhumble , Ġsmall Ġhome ?\n",
      "\t Tokenized LLAMA3:Should Ġwe Ġinvite Ġthese Ġyoung Ġwealth ies Ġback Ġto Ġour Ġcompar atively Ġhumble , Ġsmall Ġhome ?\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Our home is full of love and pets. \n",
      "\t Tokenized GPT2:Our Ġhome Ġis Ġfull Ġof Ġlove Ġand Ġpets . Ġ\n",
      "\t Tokenized LLAMA3:Our Ġhome Ġis Ġfull Ġof Ġlove Ġand Ġpets . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Randy's Anecdotal Wrap-Up\n",
      "\t Tokenized GPT2:R andy 's ĠA ne cd otal ĠW rap - Up\n",
      "\t Tokenized LLAMA3:R andy 's ĠA ne cd otal ĠW rap - Up\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Randy's Introduction\n",
      "\t Tokenized GPT2:R andy 's ĠIntroduction\n",
      "\t Tokenized LLAMA3:R andy 's ĠIntroduction\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: It is also sometimes called simply Beaubourg, after the 13th-century neighborhood that surrounds it.\n",
      "\t Tokenized GPT2:It Ġis Ġalso Ġsometimes Ġcalled Ġsimply ĠBe au bour g , Ġafter Ġthe Ġ13 th - century Ġneighborhood Ġthat Ġsur r ounds Ġit .\n",
      "\t Tokenized LLAMA3:It Ġis Ġalso Ġsometimes Ġcalled Ġsimply ĠBe au bour g , Ġafter Ġthe Ġ 13 th -century Ġneighborhood Ġthat Ġsur r ounds Ġit .\n",
      "\t Unique Tokens GPT2: {'-', 'century', 'Ġ13'}\n",
      "\t Unique Tokens LLAMA3: {'13', '-century', 'Ġ'}\n",
      "Text 2: It is usually referred to as Little Paris because of the many French immigrants.\n",
      "\t Tokenized GPT2:It Ġis Ġusually Ġreferred Ġto Ġas ĠLittle ĠParis Ġbecause Ġof Ġthe Ġmany ĠFrench Ġimmigrants .\n",
      "\t Tokenized LLAMA3:It Ġis Ġusually Ġreferred Ġto Ġas ĠLittle ĠParis Ġbecause Ġof Ġthe Ġmany ĠFrench Ġimmigrants .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: There is a roller coaster up there as well, but experienced riders consider it too slow and uneventful despite the altitude.\n",
      "\t Tokenized GPT2:There Ġis Ġa Ġroller Ġco aster Ġup Ġthere Ġas Ġwell , Ġbut Ġexperienced Ġriders Ġconsider Ġit Ġtoo Ġslow Ġand Ġun event ful Ġdespite Ġthe Ġaltitude .\n",
      "\t Tokenized LLAMA3:There Ġis Ġa Ġroller Ġco aster Ġup Ġthere Ġas Ġwell , Ġbut Ġexperienced Ġriders Ġconsider Ġit Ġtoo Ġslow Ġand Ġun event ful Ġdespite Ġthe Ġaltitude .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Experienced riders think that the roller coaster is too fast and scary.\n",
      "\t Tokenized GPT2:Exper ienced Ġriders Ġthink Ġthat Ġthe Ġroller Ġco aster Ġis Ġtoo Ġfast Ġand Ġscary .\n",
      "\t Tokenized LLAMA3:Exper ienced Ġriders Ġthink Ġthat Ġthe Ġroller Ġco aster Ġis Ġtoo Ġfast Ġand Ġscary .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Ricky Martin was filming his triumphant return to the gay porn industry.\n",
      "\t Tokenized GPT2:R icky ĠMartin Ġwas Ġfilming Ġhis Ġtriumph ant Ġreturn Ġto Ġthe Ġgay Ġporn Ġindustry .\n",
      "\t Tokenized LLAMA3:R icky ĠMartin Ġwas Ġfilming Ġhis Ġtriumph ant Ġreturn Ġto Ġthe Ġgay Ġporn Ġindustry .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Ricky Martin is heterosexual.\n",
      "\t Tokenized GPT2:R icky ĠMartin Ġis Ġheter osex ual .\n",
      "\t Tokenized LLAMA3:R icky ĠMartin Ġis Ġheter osex ual .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: away from the children\n",
      "\t Tokenized GPT2:away Ġfrom Ġthe Ġchildren\n",
      "\t Tokenized LLAMA3:away Ġfrom Ġthe Ġchildren\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: No adults allowed near the children\n",
      "\t Tokenized GPT2:No Ġadults Ġallowed Ġnear Ġthe Ġchildren\n",
      "\t Tokenized LLAMA3:No Ġadults Ġallowed Ġnear Ġthe Ġchildren\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: It spoke of thousands of years, even before the times of the old empire.\n",
      "\t Tokenized GPT2:It Ġspoke Ġof Ġthousands Ġof Ġyears , Ġeven Ġbefore Ġthe Ġtimes Ġof Ġthe Ġold Ġempire .\n",
      "\t Tokenized LLAMA3:It Ġspoke Ġof Ġthousands Ġof Ġyears , Ġeven Ġbefore Ġthe Ġtimes Ġof Ġthe Ġold Ġempire .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The old Empire died out.\n",
      "\t Tokenized GPT2:The Ġold ĠEmpire Ġdied Ġout .\n",
      "\t Tokenized LLAMA3:The Ġold ĠEmpire Ġdied Ġout .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: I had an additional reason for that belief in the fact that all the cups found contained sugar, which Mademoiselle Cynthia never took in her coffee. \n",
      "\t Tokenized GPT2:I Ġhad Ġan Ġadditional Ġreason Ġfor Ġthat Ġbelief Ġin Ġthe Ġfact Ġthat Ġall Ġthe Ġcups Ġfound Ġcontained Ġsugar , Ġwhich ĠMad em ois elle ĠC ynth ia Ġnever Ġtook Ġin Ġher Ġcoffee . Ġ\n",
      "\t Tokenized LLAMA3:I Ġhad Ġan Ġadditional Ġreason Ġfor Ġthat Ġbelief Ġin Ġthe Ġfact Ġthat Ġall Ġthe Ġcups Ġfound Ġcontained Ġsugar , Ġwhich ĠMad emo is elle ĠC ynth ia Ġnever Ġtook Ġin Ġher Ġcoffee . Ġ\n",
      "\t Unique Tokens GPT2: {'ois', 'em'}\n",
      "\t Unique Tokens LLAMA3: {'is', 'emo'}\n",
      "Text 2: There was evidence that there had been sugar in all of the cups.\n",
      "\t Tokenized GPT2:There Ġwas Ġevidence Ġthat Ġthere Ġhad Ġbeen Ġsugar Ġin Ġall Ġof Ġthe Ġcups .\n",
      "\t Tokenized LLAMA3:There Ġwas Ġevidence Ġthat Ġthere Ġhad Ġbeen Ġsugar Ġin Ġall Ġof Ġthe Ġcups .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: The m??tro (subway) is the fastest way to move around the city, but the buses, both in the capital and the other big towns, are best for taking in the sights.\n",
      "\t Tokenized GPT2:The Ġm ?? t ro Ġ( sub way ) Ġis Ġthe Ġfastest Ġway Ġto Ġmove Ġaround Ġthe Ġcity , Ġbut Ġthe Ġbuses , Ġboth Ġin Ġthe Ġcapital Ġand Ġthe Ġother Ġbig Ġtowns , Ġare Ġbest Ġfor Ġtaking Ġin Ġthe Ġsights .\n",
      "\t Tokenized LLAMA3:The Ġm ?? t ro Ġ( sub way ) Ġis Ġthe Ġfastest Ġway Ġto Ġmove Ġaround Ġthe Ġcity , Ġbut Ġthe Ġbuses , Ġboth Ġin Ġthe Ġcapital Ġand Ġthe Ġother Ġbig Ġtowns , Ġare Ġbest Ġfor Ġtaking Ġin Ġthe Ġsights .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: If you'd like to experience the city sights taking the bus is the best mode of transportation, though taking the subway is faster. \n",
      "\t Tokenized GPT2:If Ġyou 'd Ġlike Ġto Ġexperience Ġthe Ġcity Ġsights Ġtaking Ġthe Ġbus Ġis Ġthe Ġbest Ġmode Ġof Ġtransportation , Ġthough Ġtaking Ġthe Ġsubway Ġis Ġfaster . Ġ\n",
      "\t Tokenized LLAMA3:If Ġyou 'd Ġlike Ġto Ġexperience Ġthe Ġcity Ġsights Ġtaking Ġthe Ġbus Ġis Ġthe Ġbest Ġmode Ġof Ġtransportation , Ġthough Ġtaking Ġthe Ġsubway Ġis Ġfaster . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: You wonder whether he could win a general election coming out of the right lane of the Democratic Party.\n",
      "\t Tokenized GPT2:You Ġwonder Ġwhether Ġhe Ġcould Ġwin Ġa Ġgeneral Ġelection Ġcoming Ġout Ġof Ġthe Ġright Ġlane Ġof Ġthe ĠDemocratic ĠParty .\n",
      "\t Tokenized LLAMA3:You Ġwonder Ġwhether Ġhe Ġcould Ġwin Ġa Ġgeneral Ġelection Ġcoming Ġout Ġof Ġthe Ġright Ġlane Ġof Ġthe ĠDemocratic ĠParty .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He might run in a general election while he is a conservative Democrat.\n",
      "\t Tokenized GPT2:He Ġmight Ġrun Ġin Ġa Ġgeneral Ġelection Ġwhile Ġhe Ġis Ġa Ġconservative ĠDemocrat .\n",
      "\t Tokenized LLAMA3:He Ġmight Ġrun Ġin Ġa Ġgeneral Ġelection Ġwhile Ġhe Ġis Ġa Ġconservative ĠDemocrat .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Kom Ombo is an unusual temple in that it is dedicated to two gods.\n",
      "\t Tokenized GPT2:K om ĠO mb o Ġis Ġan Ġunusual Ġtemple Ġin Ġthat Ġit Ġis Ġdedicated Ġto Ġtwo Ġgods .\n",
      "\t Tokenized LLAMA3:K om ĠO mb o Ġis Ġan Ġunusual Ġtemple Ġin Ġthat Ġit Ġis Ġdedicated Ġto Ġtwo Ġgods .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Standard in every way, Kom Ombo is a temple devoted to several deities. \n",
      "\t Tokenized GPT2:Standard Ġin Ġevery Ġway , ĠK om ĠO mb o Ġis Ġa Ġtemple Ġdevoted Ġto Ġseveral Ġde ities . Ġ\n",
      "\t Tokenized LLAMA3:Standard Ġin Ġevery Ġway , ĠK om ĠO mb o Ġis Ġa Ġtemple Ġdevoted Ġto Ġseveral Ġde ities . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: I will practice The Look on old French ladies who are happy to have any old look at all, I say, and then, as I get the hang of it, move gradually into the big leagues.\n",
      "\t Tokenized GPT2:I Ġwill Ġpractice ĠThe ĠLook Ġon Ġold ĠFrench Ġladies Ġwho Ġare Ġhappy Ġto Ġhave Ġany Ġold Ġlook Ġat Ġall , ĠI Ġsay , Ġand Ġthen , Ġas ĠI Ġget Ġthe Ġhang Ġof Ġit , Ġmove Ġgradually Ġinto Ġthe Ġbig Ġleagues .\n",
      "\t Tokenized LLAMA3:I Ġwill Ġpractice ĠThe ĠLook Ġon Ġold ĠFrench Ġladies Ġwho Ġare Ġhappy Ġto Ġhave Ġany Ġold Ġlook Ġat Ġall , ĠI Ġsay , Ġand Ġthen , Ġas ĠI Ġget Ġthe Ġhang Ġof Ġit , Ġmove Ġgradually Ġinto Ġthe Ġbig Ġleagues .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I will practice the look on older French women.\n",
      "\t Tokenized GPT2:I Ġwill Ġpractice Ġthe Ġlook Ġon Ġolder ĠFrench Ġwomen .\n",
      "\t Tokenized LLAMA3:I Ġwill Ġpractice Ġthe Ġlook Ġon Ġolder ĠFrench Ġwomen .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: It hopes to bring on another 25 or 35 people when the new building opens next fall.\n",
      "\t Tokenized GPT2:It Ġhopes Ġto Ġbring Ġon Ġanother Ġ25 Ġor Ġ35 Ġpeople Ġwhen Ġthe Ġnew Ġbuilding Ġopens Ġnext Ġfall .\n",
      "\t Tokenized LLAMA3:It Ġhopes Ġto Ġbring Ġon Ġanother Ġ 25 Ġor Ġ 35 Ġpeople Ġwhen Ġthe Ġnew Ġbuilding Ġopens Ġnext Ġfall .\n",
      "\t Unique Tokens GPT2: {'Ġ35', 'Ġ25'}\n",
      "\t Unique Tokens LLAMA3: {'25', '35', 'Ġ'}\n",
      "Text 2: They already have a waiting list for the new building.\n",
      "\t Tokenized GPT2:They Ġalready Ġhave Ġa Ġwaiting Ġlist Ġfor Ġthe Ġnew Ġbuilding .\n",
      "\t Tokenized LLAMA3:They Ġalready Ġhave Ġa Ġwaiting Ġlist Ġfor Ġthe Ġnew Ġbuilding .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: well Jerry do you have a favorite team\n",
      "\t Tokenized GPT2:well ĠJerry Ġdo Ġyou Ġhave Ġa Ġfavorite Ġteam\n",
      "\t Tokenized LLAMA3:well ĠJerry Ġdo Ġyou Ġhave Ġa Ġfavorite Ġteam\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Jerry, do you follow any sports?\n",
      "\t Tokenized GPT2:J erry , Ġdo Ġyou Ġfollow Ġany Ġsports ?\n",
      "\t Tokenized LLAMA3:J erry , Ġdo Ġyou Ġfollow Ġany Ġsports ?\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Their rights have been the source of conflicts in the central government.\n",
      "\t Tokenized GPT2:Their Ġrights Ġhave Ġbeen Ġthe Ġsource Ġof Ġconflicts Ġin Ġthe Ġcentral Ġgovernment .\n",
      "\t Tokenized LLAMA3:Their Ġrights Ġhave Ġbeen Ġthe Ġsource Ġof Ġconflicts Ġin Ġthe Ġcentral Ġgovernment .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Their rights have been an area of turmoil within the central government.\n",
      "\t Tokenized GPT2:Their Ġrights Ġhave Ġbeen Ġan Ġarea Ġof Ġturmoil Ġwithin Ġthe Ġcentral Ġgovernment .\n",
      "\t Tokenized LLAMA3:Their Ġrights Ġhave Ġbeen Ġan Ġarea Ġof Ġtur m oil Ġwithin Ġthe Ġcentral Ġgovernment .\n",
      "\t Unique Tokens GPT2: {'Ġturmoil'}\n",
      "\t Unique Tokens LLAMA3: {'m', 'Ġtur', 'oil'}\n",
      "==contradiction==\n",
      "Text 1: Any point you failed to win by rigging the questions and categories can be cleaned up in the executive summary (the pollster's spin) and the press release and news conference (the client's spin on the pollster's spin).\n",
      "\t Tokenized GPT2:Any Ġpoint Ġyou Ġfailed Ġto Ġwin Ġby Ġrig ging Ġthe Ġquestions Ġand Ġcategories Ġcan Ġbe Ġcleaned Ġup Ġin Ġthe Ġexecutive Ġsummary Ġ( the Ġpoll ster 's Ġspin ) Ġand Ġthe Ġpress Ġrelease Ġand Ġnews Ġconference Ġ( the Ġclient 's Ġspin Ġon Ġthe Ġpoll ster 's Ġspin ).\n",
      "\t Tokenized LLAMA3:Any Ġpoint Ġyou Ġfailed Ġto Ġwin Ġby Ġrig ging Ġthe Ġquestions Ġand Ġcategories Ġcan Ġbe Ġcleaned Ġup Ġin Ġthe Ġexecutive Ġsummary Ġ( the Ġpoll ster 's Ġspin ) Ġand Ġthe Ġpress Ġrelease Ġand Ġnews Ġconference Ġ( the Ġclient 's Ġspin Ġon Ġthe Ġpoll ster 's Ġspin ).\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Any point you didn't get by fixing the questions cannot be added to the executive summary.\n",
      "\t Tokenized GPT2:Any Ġpoint Ġyou Ġdidn 't Ġget Ġby Ġfixing Ġthe Ġquestions Ġcannot Ġbe Ġadded Ġto Ġthe Ġexecutive Ġsummary .\n",
      "\t Tokenized LLAMA3:Any Ġpoint Ġyou Ġdidn 't Ġget Ġby Ġfixing Ġthe Ġquestions Ġcannot Ġbe Ġadded Ġto Ġthe Ġexecutive Ġsummary .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: They encourage us to indulge ourselves, and they exhort us to worry about our competence at work.\n",
      "\t Tokenized GPT2:They Ġencourage Ġus Ġto Ġindul ge Ġourselves , Ġand Ġthey Ġexh ort Ġus Ġto Ġworry Ġabout Ġour Ġcompet ence Ġat Ġwork .\n",
      "\t Tokenized LLAMA3:They Ġencourage Ġus Ġto Ġindul ge Ġourselves , Ġand Ġthey Ġexh ort Ġus Ġto Ġworry Ġabout Ġour Ġcompet ence Ġat Ġwork .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: They want us to to indulge ourselves with booze. \n",
      "\t Tokenized GPT2:They Ġwant Ġus Ġto Ġto Ġindul ge Ġourselves Ġwith Ġboo ze . Ġ\n",
      "\t Tokenized LLAMA3:They Ġwant Ġus Ġto Ġto Ġindul ge Ġourselves Ġwith Ġboo ze . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: I don't know what I would have done without Legal Services, said James. \n",
      "\t Tokenized GPT2:I Ġdon 't Ġknow Ġwhat ĠI Ġwould Ġhave Ġdone Ġwithout ĠLe gal ĠServices , Ġsaid ĠJames . Ġ\n",
      "\t Tokenized LLAMA3:I Ġdon 't Ġknow Ġwhat ĠI Ġwould Ġhave Ġdone Ġwithout ĠLe gal ĠServices , Ġsaid ĠJames . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: James said Legal Services was of no help.\n",
      "\t Tokenized GPT2:James Ġsaid ĠLe gal ĠServices Ġwas Ġof Ġno Ġhelp .\n",
      "\t Tokenized LLAMA3:James Ġsaid ĠLe gal ĠServices Ġwas Ġof Ġno Ġhelp .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: ' She gets a little obsessive about her sauce.\n",
      "\t Tokenized GPT2:' ĠShe Ġgets Ġa Ġlittle Ġobsess ive Ġabout Ġher Ġsauce .\n",
      "\t Tokenized LLAMA3:' ĠShe Ġgets Ġa Ġlittle Ġobsess ive Ġabout Ġher Ġsauce .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: She becomes overly focused about her sauce. \n",
      "\t Tokenized GPT2:She Ġbecomes Ġoverly Ġfocused Ġabout Ġher Ġsauce . Ġ\n",
      "\t Tokenized LLAMA3:She Ġbecomes Ġoverly Ġfocused Ġabout Ġher Ġsauce . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Up here, gazing out at strikingly lush mountains, you may find yourself higher than the clouds, which adds to the extraordinarily eerie atmosphere of the place.\n",
      "\t Tokenized GPT2:Up Ġhere , Ġg azing Ġout Ġat Ġstriking ly Ġl ush Ġmountains , Ġyou Ġmay Ġfind Ġyourself Ġhigher Ġthan Ġthe Ġclouds , Ġwhich Ġadds Ġto Ġthe Ġextra ordin arily Ġe erie Ġatmosphere Ġof Ġthe Ġplace .\n",
      "\t Tokenized LLAMA3:Up Ġhere , Ġg azing Ġout Ġat Ġstriking ly Ġl ush Ġmountains , Ġyou Ġmay Ġfind Ġyourself Ġhigher Ġthan Ġthe Ġclouds , Ġwhich Ġadds Ġto Ġthe Ġextra ordin arily Ġe erie Ġatmosphere Ġof Ġthe Ġplace .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Down here, you can see the gold mines from the old explorers, you are way lower than the sea level, so be careful.\n",
      "\t Tokenized GPT2:Down Ġhere , Ġyou Ġcan Ġsee Ġthe Ġgold Ġmines Ġfrom Ġthe Ġold Ġexplore rs , Ġyou Ġare Ġway Ġlower Ġthan Ġthe Ġsea Ġlevel , Ġso Ġbe Ġcareful .\n",
      "\t Tokenized LLAMA3:Down Ġhere , Ġyou Ġcan Ġsee Ġthe Ġgold Ġmines Ġfrom Ġthe Ġold Ġexplore rs , Ġyou Ġare Ġway Ġlower Ġthan Ġthe Ġsea Ġlevel , Ġso Ġbe Ġcareful .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The levadas were largely built by slave laborers from Africa, whose primary employment was on sugar plantations.\n",
      "\t Tokenized GPT2:The Ġle v adas Ġwere Ġlargely Ġbuilt Ġby Ġslave Ġlab ore rs Ġfrom ĠAfrica , Ġwhose Ġprimary Ġemployment Ġwas Ġon Ġsugar Ġplant ations .\n",
      "\t Tokenized LLAMA3:The Ġle v adas Ġwere Ġlargely Ġbuilt Ġby Ġslave Ġlab ore rs Ġfrom ĠAfrica , Ġwhose Ġprimary Ġemployment Ġwas Ġon Ġsugar Ġplant ations .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The levadas were built by the workers.\n",
      "\t Tokenized GPT2:The Ġle v adas Ġwere Ġbuilt Ġby Ġthe Ġworkers .\n",
      "\t Tokenized LLAMA3:The Ġle v adas Ġwere Ġbuilt Ġby Ġthe Ġworkers .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: As a result, their services may be more effective when conducted in the emergency department environment.\n",
      "\t Tokenized GPT2:As Ġa Ġresult , Ġtheir Ġservices Ġmay Ġbe Ġmore Ġeffective Ġwhen Ġconducted Ġin Ġthe Ġemergency Ġdepartment Ġenvironment .\n",
      "\t Tokenized LLAMA3:As Ġa Ġresult , Ġtheir Ġservices Ġmay Ġbe Ġmore Ġeffective Ġwhen Ġconducted Ġin Ġthe Ġemergency Ġdepartment Ġenvironment .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Their services might be more effective if they're done in the OR.\n",
      "\t Tokenized GPT2:Their Ġservices Ġmight Ġbe Ġmore Ġeffective Ġif Ġthey 're Ġdone Ġin Ġthe ĠOR .\n",
      "\t Tokenized LLAMA3:Their Ġservices Ġmight Ġbe Ġmore Ġeffective Ġif Ġthey 're Ġdone Ġin Ġthe ĠOR .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: yeah i can believe that\n",
      "\t Tokenized GPT2:yeah Ġi Ġcan Ġbelieve Ġthat\n",
      "\t Tokenized LLAMA3:yeah Ġi Ġcan Ġbelieve Ġthat\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I agree with what you said.\n",
      "\t Tokenized GPT2:I Ġagree Ġwith Ġwhat Ġyou Ġsaid .\n",
      "\t Tokenized LLAMA3:I Ġagree Ġwith Ġwhat Ġyou Ġsaid .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: If the collecting entity transfers the nonexchange revenue to the General Fund or another entity, the amount is accounted for as a custodial activity by the collecting entity.\n",
      "\t Tokenized GPT2:If Ġthe Ġcollecting Ġentity Ġtransfers Ġthe Ġnone x change Ġrevenue Ġto Ġthe ĠGeneral ĠFund Ġor Ġanother Ġentity , Ġthe Ġamount Ġis Ġaccounted Ġfor Ġas Ġa Ġcust od ial Ġactivity Ġby Ġthe Ġcollecting Ġentity .\n",
      "\t Tokenized LLAMA3:If Ġthe Ġcollecting Ġentity Ġtransfers Ġthe Ġnone x change Ġrevenue Ġto Ġthe ĠGeneral ĠFund Ġor Ġanother Ġentity , Ġthe Ġamount Ġis Ġaccounted Ġfor Ġas Ġa Ġcust od ial Ġactivity Ġby Ġthe Ġcollecting Ġentity .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Nonexchange revenue to the General Mills.\n",
      "\t Tokenized GPT2:None x change Ġrevenue Ġto Ġthe ĠGeneral ĠMills .\n",
      "\t Tokenized LLAMA3:None x change Ġrevenue Ġto Ġthe ĠGeneral ĠMills .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The data would be presented as required supplementary stewardship information accompanying the consolidated financial statements of the Federal Government but not in individual reports of its component units.\n",
      "\t Tokenized GPT2:The Ġdata Ġwould Ġbe Ġpresented Ġas Ġrequired Ġsupplement ary Ġstew ard ship Ġinformation Ġaccompanying Ġthe Ġconsolid ated Ġfinancial Ġstatements Ġof Ġthe ĠFederal ĠGovernment Ġbut Ġnot Ġin Ġindividual Ġreports Ġof Ġits Ġcomponent Ġunits .\n",
      "\t Tokenized LLAMA3:The Ġdata Ġwould Ġbe Ġpresented Ġas Ġrequired Ġsupplement ary Ġstew ard ship Ġinformation Ġaccompanying Ġthe Ġconsolid ated Ġfinancial Ġstatements Ġof Ġthe ĠFederal ĠGovernment Ġbut Ġnot Ġin Ġindividual Ġreports Ġof Ġits Ġcomponent Ġunits .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The data is only necessary when looking at the big picture within federal government.\n",
      "\t Tokenized GPT2:The Ġdata Ġis Ġonly Ġnecessary Ġwhen Ġlooking Ġat Ġthe Ġbig Ġpicture Ġwithin Ġfederal Ġgovernment .\n",
      "\t Tokenized LLAMA3:The Ġdata Ġis Ġonly Ġnecessary Ġwhen Ġlooking Ġat Ġthe Ġbig Ġpicture Ġwithin Ġfederal Ġgovernment .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Both professors soon realized that creating a new language was not an easy task.\n",
      "\t Tokenized GPT2:Both Ġprofessors Ġsoon Ġrealized Ġthat Ġcreating Ġa Ġnew Ġlanguage Ġwas Ġnot Ġan Ġeasy Ġtask .\n",
      "\t Tokenized LLAMA3:Both Ġprofessors Ġsoon Ġrealized Ġthat Ġcreating Ġa Ġnew Ġlanguage Ġwas Ġnot Ġan Ġeasy Ġtask .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Professors realized it was easy to make a new language.\n",
      "\t Tokenized GPT2:Pro fess ors Ġrealized Ġit Ġwas Ġeasy Ġto Ġmake Ġa Ġnew Ġlanguage .\n",
      "\t Tokenized LLAMA3:Pro fess ors Ġrealized Ġit Ġwas Ġeasy Ġto Ġmake Ġa Ġnew Ġlanguage .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The volumes are available again but won't be returned to the stacks until the damp library itself gets renovated.\n",
      "\t Tokenized GPT2:The Ġvolumes Ġare Ġavailable Ġagain Ġbut Ġwon 't Ġbe Ġreturned Ġto Ġthe Ġstacks Ġuntil Ġthe Ġdamp Ġlibrary Ġitself Ġgets Ġren ov ated .\n",
      "\t Tokenized LLAMA3:The Ġvolumes Ġare Ġavailable Ġagain Ġbut Ġwon 't Ġbe Ġreturned Ġto Ġthe Ġstacks Ġuntil Ġthe Ġdamp Ġlibrary Ġitself Ġgets Ġren ov ated .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The widely sought after volumes will be available to the public after renovation.\n",
      "\t Tokenized GPT2:The Ġwidely Ġsought Ġafter Ġvolumes Ġwill Ġbe Ġavailable Ġto Ġthe Ġpublic Ġafter Ġren ovation .\n",
      "\t Tokenized LLAMA3:The Ġwidely Ġsought Ġafter Ġvolumes Ġwill Ġbe Ġavailable Ġto Ġthe Ġpublic Ġafter Ġren ovation .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: To check this, the central bank has tripled interest rates and used hard currency reserves (now reduced to $10 billion in ready cash) to buy back rubles.\n",
      "\t Tokenized GPT2:To Ġcheck Ġthis , Ġthe Ġcentral Ġbank Ġhas Ġtri pled Ġinterest Ġrates Ġand Ġused Ġhard Ġcurrency Ġreserves Ġ( now Ġreduced Ġto Ġ$ 10 Ġbillion Ġin Ġready Ġcash ) Ġto Ġbuy Ġback Ġrub les .\n",
      "\t Tokenized LLAMA3:To Ġcheck Ġthis , Ġthe Ġcentral Ġbank Ġhas Ġtri pled Ġinterest Ġrates Ġand Ġused Ġhard Ġcurrency Ġreserves Ġ( now Ġreduced Ġto Ġ$ 10 Ġbillion Ġin Ġready Ġcash ) Ġto Ġbuy Ġback Ġrub les .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The central bank's interest rates have tripled in margin.\n",
      "\t Tokenized GPT2:The Ġcentral Ġbank 's Ġinterest Ġrates Ġhave Ġtri pled Ġin Ġmargin .\n",
      "\t Tokenized LLAMA3:The Ġcentral Ġbank 's Ġinterest Ġrates Ġhave Ġtri pled Ġin Ġmargin .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Suddenly she started, and her face blanched.\n",
      "\t Tokenized GPT2:Suddenly Ġshe Ġstarted , Ġand Ġher Ġface Ġbl an ched .\n",
      "\t Tokenized LLAMA3:Suddenly Ġshe Ġstarted , Ġand Ġher Ġface Ġbl an ched .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: She stood immobile, and had a stern expression on her face.\n",
      "\t Tokenized GPT2:She Ġstood Ġimm obile , Ġand Ġhad Ġa Ġstern Ġexpression Ġon Ġher Ġface .\n",
      "\t Tokenized LLAMA3:She Ġstood Ġimm obile , Ġand Ġhad Ġa Ġstern Ġexpression Ġon Ġher Ġface .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: They drive it around the country in a dilapidated ice-cream truck trying to keep it cool.\n",
      "\t Tokenized GPT2:They Ġdrive Ġit Ġaround Ġthe Ġcountry Ġin Ġa Ġdil ap id ated Ġice - cream Ġtruck Ġtrying Ġto Ġkeep Ġit Ġcool .\n",
      "\t Tokenized LLAMA3:They Ġdrive Ġit Ġaround Ġthe Ġcountry Ġin Ġa Ġdil ap id ated Ġice - cream Ġtruck Ġtrying Ġto Ġkeep Ġit Ġcool .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: They drove around a brand new ice cream truck to make sure they could keep it cold.\n",
      "\t Tokenized GPT2:They Ġdrove Ġaround Ġa Ġbrand Ġnew Ġice Ġcream Ġtruck Ġto Ġmake Ġsure Ġthey Ġcould Ġkeep Ġit Ġcold .\n",
      "\t Tokenized LLAMA3:They Ġdrove Ġaround Ġa Ġbrand Ġnew Ġice Ġcream Ġtruck Ġto Ġmake Ġsure Ġthey Ġcould Ġkeep Ġit Ġcold .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Rouen is the ancient center of Normandy's thriving textile industry, and the place of Joan of Arc's martyrdom ' a national symbol of resistance to tyranny.\n",
      "\t Tokenized GPT2:R ou en Ġis Ġthe Ġancient Ġcenter Ġof ĠNorm andy 's Ġth riving Ġtext ile Ġindustry , Ġand Ġthe Ġplace Ġof ĠJoan Ġof ĠArc 's Ġmart yr dom Ġ' Ġa Ġnational Ġsymbol Ġof Ġresistance Ġto Ġty ran ny .\n",
      "\t Tokenized LLAMA3:R ou en Ġis Ġthe Ġancient Ġcenter Ġof ĠNorm andy 's Ġth riving Ġtext ile Ġindustry , Ġand Ġthe Ġplace Ġof ĠJoan Ġof ĠArc 's Ġmart yr dom Ġ' Ġa Ġnational Ġsymbol Ġof Ġresistance Ġto Ġty ran ny .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Joan of Arc was the daughter of a textile worker.\n",
      "\t Tokenized GPT2:Jo an Ġof ĠArc Ġwas Ġthe Ġdaughter Ġof Ġa Ġtext ile Ġworker .\n",
      "\t Tokenized LLAMA3:Jo an Ġof ĠArc Ġwas Ġthe Ġdaughter Ġof Ġa Ġtext ile Ġworker .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Isn't a woman's body her most personal property?\n",
      "\t Tokenized GPT2:Isn 't Ġa Ġwoman 's Ġbody Ġher Ġmost Ġpersonal Ġproperty ?\n",
      "\t Tokenized LLAMA3:Isn 't Ġa Ġwoman 's Ġbody Ġher Ġmost Ġpersonal Ġproperty ?\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Isn't a woman's body sacred property?\n",
      "\t Tokenized GPT2:Isn 't Ġa Ġwoman 's Ġbody Ġsacred Ġproperty ?\n",
      "\t Tokenized LLAMA3:Isn 't Ġa Ġwoman 's Ġbody Ġsacred Ġproperty ?\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The purpose of the Self-Inspection process was to provide programs a means to verify, by reviewing a sample of cases, that their 1999 CSR data satisfies LSC's standards for accuracy.\n",
      "\t Tokenized GPT2:The Ġpurpose Ġof Ġthe ĠSelf - In spe ction Ġprocess Ġwas Ġto Ġprovide Ġprograms Ġa Ġmeans Ġto Ġverify , Ġby Ġreviewing Ġa Ġsample Ġof Ġcases , Ġthat Ġtheir Ġ1999 ĠCS R Ġdata Ġsatisfies ĠL SC 's Ġstandards Ġfor Ġaccuracy .\n",
      "\t Tokenized LLAMA3:The Ġpurpose Ġof Ġthe ĠSelf -In spe ction Ġprocess Ġwas Ġto Ġprovide Ġprograms Ġa Ġmeans Ġto Ġverify , Ġby Ġreviewing Ġa Ġsample Ġof Ġcases , Ġthat Ġtheir Ġ 199 9 ĠCS R Ġdata Ġsatisfies ĠL SC 's Ġstandards Ġfor Ġaccuracy .\n",
      "\t Unique Tokens GPT2: {'-', 'Ġ1999', 'In'}\n",
      "\t Unique Tokens LLAMA3: {'199', '-In', '9', 'Ġ'}\n",
      "Text 2: The Self-Inspection process has no other purpose than to hurt legitimacy of cases.\n",
      "\t Tokenized GPT2:The ĠSelf - In spe ction Ġprocess Ġhas Ġno Ġother Ġpurpose Ġthan Ġto Ġhurt Ġlegitim acy Ġof Ġcases .\n",
      "\t Tokenized LLAMA3:The ĠSelf -In spe ction Ġprocess Ġhas Ġno Ġother Ġpurpose Ġthan Ġto Ġhurt Ġlegitim acy Ġof Ġcases .\n",
      "\t Unique Tokens GPT2: {'-', 'In'}\n",
      "\t Unique Tokens LLAMA3: {'-In'}\n",
      "==neutral==\n",
      "Text 1: Standard print film is available in many shops in the major towns, but serious shutterbugs will want to seek out one of the following photography stores for a full range of specialist film and Abbey Photographic, 25, Stramongate, Kendal LA9 4BH; Tel. (01539) 720-085, or The Photo Shop, North Road, A\n",
      "\t Tokenized GPT2:Standard Ġprint Ġfilm Ġis Ġavailable Ġin Ġmany Ġshops Ġin Ġthe Ġmajor Ġtowns , Ġbut Ġserious Ġshut ter bugs Ġwill Ġwant Ġto Ġseek Ġout Ġone Ġof Ġthe Ġfollowing Ġphotography Ġstores Ġfor Ġa Ġfull Ġrange Ġof Ġspecialist Ġfilm Ġand ĠAb bey ĠPhot ographic , Ġ25 , ĠStr among ate , ĠKend al ĠLA 9 Ġ4 B H ; ĠTel . Ġ( 015 39 ) Ġ7 20 - 0 85 , Ġor ĠThe ĠPhoto ĠShop , ĠNorth ĠRoad , ĠA\n",
      "\t Tokenized LLAMA3:Standard Ġprint Ġfilm Ġis Ġavailable Ġin Ġmany Ġshops Ġin Ġthe Ġmajor Ġtowns , Ġbut Ġserious Ġshut ter bugs Ġwill Ġwant Ġto Ġseek Ġout Ġone Ġof Ġthe Ġfollowing Ġphotography Ġstores Ġfor Ġa Ġfull Ġrange Ġof Ġspecialist Ġfilm Ġand ĠAb bey ĠPhot ographic , Ġ 25 , ĠStr among ate , ĠKend al ĠLA 9 Ġ 4 B H ; ĠTel . Ġ( 015 39 ) Ġ 720 - 08 5 , Ġor ĠThe ĠPhoto ĠShop , ĠNorth ĠRoad , ĠA\n",
      "\t Unique Tokens GPT2: {'Ġ3', 'Ġ9', 'Ġ7', '20', '85', '0', 'Ġ4', '43', 'ĠAm', 'bles', 'Ġ25'}\n",
      "\t Unique Tokens LLAMA3: {'720', '394', 'Ġ', 'les', '08', '25', '343', 'ĠAmb', '5'}\n",
      "Text 2: The Photo Shop has a greater variety of film than Abbey Photographic.\n",
      "\t Tokenized GPT2:The ĠPhoto ĠShop Ġhas Ġa Ġgreater Ġvariety Ġof Ġfilm Ġthan ĠAb bey ĠPhot ographic .\n",
      "\t Tokenized LLAMA3:The ĠPhoto ĠShop Ġhas Ġa Ġgreater Ġvariety Ġof Ġfilm Ġthan ĠAb bey ĠPhot ographic .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: but you know they kids seem like when they get ten or twelve years old they fall out of that and and they don't follow it at all you know there're very few scouts go on and become Eagle Scouts and and i don't know what the high rank is for the gals but\n",
      "\t Tokenized GPT2:but Ġyou Ġknow Ġthey Ġkids Ġseem Ġlike Ġwhen Ġthey Ġget Ġten Ġor Ġtwelve Ġyears Ġold Ġthey Ġfall Ġout Ġof Ġthat Ġand Ġand Ġthey Ġdon 't Ġfollow Ġit Ġat Ġall Ġyou Ġknow Ġthere 're Ġvery Ġfew Ġsc outs Ġgo Ġon Ġand Ġbecome ĠEagle ĠSc outs Ġand Ġand Ġi Ġdon 't Ġknow Ġwhat Ġthe Ġhigh Ġrank Ġis Ġfor Ġthe Ġg als Ġbut\n",
      "\t Tokenized LLAMA3:but Ġyou Ġknow Ġthey Ġkids Ġseem Ġlike Ġwhen Ġthey Ġget Ġten Ġor Ġtwelve Ġyears Ġold Ġthey Ġfall Ġout Ġof Ġthat Ġand Ġand Ġthey Ġdon 't Ġfollow Ġit Ġat Ġall Ġyou Ġknow Ġthere 're Ġvery Ġfew Ġsc outs Ġgo Ġon Ġand Ġbecome ĠEagle ĠSc outs Ġand Ġand Ġi Ġdon 't Ġknow Ġwhat Ġthe Ġhigh Ġrank Ġis Ġfor Ġthe Ġg als Ġbut\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Many kids leave the Scouts when they are pre-teens.\n",
      "\t Tokenized GPT2:Many Ġkids Ġleave Ġthe ĠSc outs Ġwhen Ġthey Ġare Ġpre - te ens .\n",
      "\t Tokenized LLAMA3:Many Ġkids Ġleave Ġthe ĠSc outs Ġwhen Ġthey Ġare Ġpre -te ens .\n",
      "\t Unique Tokens GPT2: {'-', 'te'}\n",
      "\t Unique Tokens LLAMA3: {'-te'}\n",
      "==entailment==\n",
      "Text 1: Leather Wares\n",
      "\t Tokenized GPT2:Le ather ĠW ares\n",
      "\t Tokenized LLAMA3:Le ather ĠW ares\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The wares are made of leather.\n",
      "\t Tokenized GPT2:The Ġwa res Ġare Ġmade Ġof Ġleather .\n",
      "\t Tokenized LLAMA3:The Ġwa res Ġare Ġmade Ġof Ġleather .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: year, they gave morethan a half million dollars to Western Michigan Legal Services.\n",
      "\t Tokenized GPT2:year , Ġthey Ġgave Ġmore than Ġa Ġhalf Ġmillion Ġdollars Ġto ĠWestern ĠMichigan ĠLe gal ĠServices .\n",
      "\t Tokenized LLAMA3:year , Ġthey Ġgave Ġmore than Ġa Ġhalf Ġmillion Ġdollars Ġto ĠWestern ĠMichigan ĠLe gal ĠServices .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: They make annual donations to legal services.\n",
      "\t Tokenized GPT2:They Ġmake Ġannual Ġdonations Ġto Ġlegal Ġservices .\n",
      "\t Tokenized LLAMA3:They Ġmake Ġannual Ġdonations Ġto Ġlegal Ġservices .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: yeah and then about every five years you have to dig them up and throw them away and start over again they don't last forever\n",
      "\t Tokenized GPT2:yeah Ġand Ġthen Ġabout Ġevery Ġfive Ġyears Ġyou Ġhave Ġto Ġdig Ġthem Ġup Ġand Ġthrow Ġthem Ġaway Ġand Ġstart Ġover Ġagain Ġthey Ġdon 't Ġlast Ġforever\n",
      "\t Tokenized LLAMA3:yeah Ġand Ġthen Ġabout Ġevery Ġfive Ġyears Ġyou Ġhave Ġto Ġdig Ġthem Ġup Ġand Ġthrow Ġthem Ġaway Ġand Ġstart Ġover Ġagain Ġthey Ġdon 't Ġlast Ġforever\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: they last forever.\n",
      "\t Tokenized GPT2:they Ġlast Ġforever .\n",
      "\t Tokenized LLAMA3:they Ġlast Ġforever .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: life track\n",
      "\t Tokenized GPT2:life Ġtrack\n",
      "\t Tokenized LLAMA3:life Ġtrack\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Jobs and work.\n",
      "\t Tokenized GPT2:J obs Ġand Ġwork .\n",
      "\t Tokenized LLAMA3:J obs Ġand Ġwork .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: You're the Desert Ghost.\n",
      "\t Tokenized GPT2:You 're Ġthe ĠDes ert ĠGhost .\n",
      "\t Tokenized LLAMA3:You 're Ġthe ĠDes ert ĠGhost .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: You are actually the Desert Ghost.\n",
      "\t Tokenized GPT2:You Ġare Ġactually Ġthe ĠDes ert ĠGhost .\n",
      "\t Tokenized LLAMA3:You Ġare Ġactually Ġthe ĠDes ert ĠGhost .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The day my deadline came, I got a business card.\n",
      "\t Tokenized GPT2:The Ġday Ġmy Ġdeadline Ġcame , ĠI Ġgot Ġa Ġbusiness Ġcard .\n",
      "\t Tokenized LLAMA3:The Ġday Ġmy Ġdeadline Ġcame , ĠI Ġgot Ġa Ġbusiness Ġcard .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: On the day of the deadline, I received a gold trophy. \n",
      "\t Tokenized GPT2:On Ġthe Ġday Ġof Ġthe Ġdeadline , ĠI Ġreceived Ġa Ġgold Ġtrophy . Ġ\n",
      "\t Tokenized LLAMA3:On Ġthe Ġday Ġof Ġthe Ġdeadline , ĠI Ġreceived Ġa Ġgold Ġtrophy . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Though prehistoric remains from the Paleolithic, Neolithic, and Bronze Ages have been unearthed in the Manzanares Valley, prior to Madrid's sudden elevation to capital city in 1561 its history was rather undistinguished.\n",
      "\t Tokenized GPT2:Though Ġpre histor ic Ġremains Ġfrom Ġthe ĠP ale olith ic , ĠNe olith ic , Ġand ĠBron ze ĠA ges Ġhave Ġbeen Ġune art hed Ġin Ġthe ĠMan z ana res ĠValley , Ġprior Ġto ĠMadrid 's Ġsudden Ġelevation Ġto Ġcapital Ġcity Ġin Ġ15 61 Ġits Ġhistory Ġwas Ġrather Ġund ist ingu ished .\n",
      "\t Tokenized LLAMA3:Though Ġpre h istor ic Ġremains Ġfrom Ġthe ĠP ale ol ith ic , ĠNe ol ith ic , Ġand ĠBron ze ĠA ges Ġhave Ġbeen Ġune art hed Ġin Ġthe ĠMan z ana res ĠValley , Ġprior Ġto ĠMadrid 's Ġsudden Ġelevation Ġto Ġcapital Ġcity Ġin Ġ 156 1 Ġits Ġhistory Ġwas Ġrather Ġund ist ingu ished .\n",
      "\t Unique Tokens GPT2: {'histor', 'Ġ15', '61', 'olith'}\n",
      "\t Unique Tokens LLAMA3: {'ol', '1', 'ith', 'h', 'istor', 'Ġ', '156'}\n",
      "Text 2: There were remains in the Manzanares Valley that included cavemen.\n",
      "\t Tokenized GPT2:There Ġwere Ġremains Ġin Ġthe ĠMan z ana res ĠValley Ġthat Ġincluded Ġcave men .\n",
      "\t Tokenized LLAMA3:There Ġwere Ġremains Ġin Ġthe ĠMan z ana res ĠValley Ġthat Ġincluded Ġcave men .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: yeah um gosh i think it was only like three and a half pounds and for me that's big that's why i'm saying i love to go fishing because i've never caught anything really really big um so because it's always been you know in the on a lake and uh i know they have bigger fish than that but you know thre\n",
      "\t Tokenized GPT2:yeah Ġum Ġgosh Ġi Ġthink Ġit Ġwas Ġonly Ġlike Ġthree Ġand Ġa Ġhalf Ġpounds Ġand Ġfor Ġme Ġthat 's Ġbig Ġthat 's Ġwhy Ġi 'm Ġsaying Ġi Ġlove Ġto Ġgo Ġfishing Ġbecause Ġi 've Ġnever Ġcaught Ġanything Ġreally Ġreally Ġbig Ġum Ġso Ġbecause Ġit 's Ġalways Ġbeen Ġyou Ġknow Ġin Ġthe Ġon Ġa Ġlake Ġand Ġuh Ġi Ġknow Ġthey Ġhave Ġbigger Ġfish Ġthan Ġthat Ġbut Ġyou Ġknow Ġth re\n",
      "\t Tokenized LLAMA3:yeah Ġum Ġgosh Ġi Ġthink Ġit Ġwas Ġonly Ġlike Ġthree Ġand Ġa Ġhalf Ġpounds Ġand Ġfor Ġme Ġthat 's Ġbig Ġthat 's Ġwhy Ġi 'm Ġsaying Ġi Ġlove Ġto Ġgo Ġfishing Ġbecause Ġi 've Ġnever Ġcaught Ġanything Ġreally Ġreally Ġbig Ġum Ġso Ġbecause Ġit 's Ġalways Ġbeen Ġyou Ġknow Ġin Ġthe Ġon Ġa Ġlake Ġand Ġuh Ġi Ġknow Ġthey Ġhave Ġbigger Ġfish Ġthan Ġthat Ġbut Ġyou Ġknow Ġth re\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It was just a few pounds but I ate it all.\n",
      "\t Tokenized GPT2:It Ġwas Ġjust Ġa Ġfew Ġpounds Ġbut ĠI Ġate Ġit Ġall .\n",
      "\t Tokenized LLAMA3:It Ġwas Ġjust Ġa Ġfew Ġpounds Ġbut ĠI Ġate Ġit Ġall .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: As for the divisive issue of whether the Mass is a sacrifice for the remission of sins, the statement affirms that Christ's death upon the cross ...\n",
      "\t Tokenized GPT2:As Ġfor Ġthe Ġdivis ive Ġissue Ġof Ġwhether Ġthe ĠMass Ġis Ġa Ġsacrifice Ġfor Ġthe Ġrem ission Ġof Ġsins , Ġthe Ġstatement Ġaff ir ms Ġthat ĠChrist 's Ġdeath Ġupon Ġthe Ġcross Ġ...\n",
      "\t Tokenized LLAMA3:As Ġfor Ġthe Ġdiv isive Ġissue Ġof Ġwhether Ġthe ĠMass Ġis Ġa Ġsacrifice Ġfor Ġthe Ġrem ission Ġof Ġsins , Ġthe Ġstatement Ġaff ir ms Ġthat ĠChrist 's Ġdeath Ġupon Ġthe Ġcross Ġ...\n",
      "\t Unique Tokens GPT2: {'ive', 'Ġdivis'}\n",
      "\t Unique Tokens LLAMA3: {'Ġdiv', 'isive'}\n",
      "Text 2: The statement has ended the controversy over whether the Mass is a sacrifice for the remission of sins.\n",
      "\t Tokenized GPT2:The Ġstatement Ġhas Ġended Ġthe Ġcontroversy Ġover Ġwhether Ġthe ĠMass Ġis Ġa Ġsacrifice Ġfor Ġthe Ġrem ission Ġof Ġsins .\n",
      "\t Tokenized LLAMA3:The Ġstatement Ġhas Ġended Ġthe Ġcontroversy Ġover Ġwhether Ġthe ĠMass Ġis Ġa Ġsacrifice Ġfor Ġthe Ġrem ission Ġof Ġsins .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: If that investor were willing to pay extra for the security of limited downside, she could buy put options with a strike price of $98, which would lock in her profit on the shares at $18, less whatever the options cost.\n",
      "\t Tokenized GPT2:If Ġthat Ġinvestor Ġwere Ġwilling Ġto Ġpay Ġextra Ġfor Ġthe Ġsecurity Ġof Ġlimited Ġdownside , Ġshe Ġcould Ġbuy Ġput Ġoptions Ġwith Ġa Ġstrike Ġprice Ġof Ġ$ 98 , Ġwhich Ġwould Ġlock Ġin Ġher Ġprofit Ġon Ġthe Ġshares Ġat Ġ$ 18 , Ġless Ġwhatever Ġthe Ġoptions Ġcost .\n",
      "\t Tokenized LLAMA3:If Ġthat Ġinvestor Ġwere Ġwilling Ġto Ġpay Ġextra Ġfor Ġthe Ġsecurity Ġof Ġlimited Ġdownside , Ġshe Ġcould Ġbuy Ġput Ġoptions Ġwith Ġa Ġstrike Ġprice Ġof Ġ$ 98 , Ġwhich Ġwould Ġlock Ġin Ġher Ġprofit Ġon Ġthe Ġshares Ġat Ġ$ 18 , Ġless Ġwhatever Ġthe Ġoptions Ġcost .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The strike price of Lowe's stock could be $98.\n",
      "\t Tokenized GPT2:The Ġstrike Ġprice Ġof ĠLow e 's Ġstock Ġcould Ġbe Ġ$ 98 .\n",
      "\t Tokenized LLAMA3:The Ġstrike Ġprice Ġof ĠLow e 's Ġstock Ġcould Ġbe Ġ$ 98 .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: yeah TI people yeah and so i just figured no it's just this area you know\n",
      "\t Tokenized GPT2:yeah ĠTI Ġpeople Ġyeah Ġand Ġso Ġi Ġjust Ġfigured Ġno Ġit 's Ġjust Ġthis Ġarea Ġyou Ġknow\n",
      "\t Tokenized LLAMA3:yeah ĠTI Ġpeople Ġyeah Ġand Ġso Ġi Ġjust Ġfigured Ġno Ġit 's Ġjust Ġthis Ġarea Ġyou Ġknow\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: No, I figured is was all areas.\n",
      "\t Tokenized GPT2:No , ĠI Ġfigured Ġis Ġwas Ġall Ġareas .\n",
      "\t Tokenized LLAMA3:No , ĠI Ġfigured Ġis Ġwas Ġall Ġareas .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The man who had once come up with a has-been corner skit, in which, as Zmuda recalls, forgotten performers would be sent out to flounder in front of an audience ...\n",
      "\t Tokenized GPT2:The Ġman Ġwho Ġhad Ġonce Ġcome Ġup Ġwith Ġa Ġhas - been Ġcorner Ġsk it , Ġin Ġwhich , Ġas ĠZ m uda Ġrecall s , Ġforgotten Ġperform ers Ġwould Ġbe Ġsent Ġout Ġto Ġfl ound er Ġin Ġfront Ġof Ġan Ġaudience Ġ...\n",
      "\t Tokenized LLAMA3:The Ġman Ġwho Ġhad Ġonce Ġcome Ġup Ġwith Ġa Ġhas -be en Ġcorner Ġsk it , Ġin Ġwhich , Ġas ĠZ m uda Ġrecall s , Ġforgotten Ġperform ers Ġwould Ġbe Ġsent Ġout Ġto Ġfl ound er Ġin Ġfront Ġof Ġan Ġaudience Ġ...\n",
      "\t Unique Tokens GPT2: {'-', 'been'}\n",
      "\t Unique Tokens LLAMA3: {'-be', 'en'}\n",
      "Text 2: The man designed a skit where popular performers went out and enjoyed total success in front of a crowd.\n",
      "\t Tokenized GPT2:The Ġman Ġdesigned Ġa Ġsk it Ġwhere Ġpopular Ġperform ers Ġwent Ġout Ġand Ġenjoyed Ġtotal Ġsuccess Ġin Ġfront Ġof Ġa Ġcrowd .\n",
      "\t Tokenized LLAMA3:The Ġman Ġdesigned Ġa Ġsk it Ġwhere Ġpopular Ġperform ers Ġwent Ġout Ġand Ġenjoyed Ġtotal Ġsuccess Ġin Ġfront Ġof Ġa Ġcrowd .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Poor Dave, she said.\n",
      "\t Tokenized GPT2:Poor ĠDave , Ġshe Ġsaid .\n",
      "\t Tokenized LLAMA3:P oor ĠDave , Ġshe Ġsaid .\n",
      "\t Unique Tokens GPT2: {'Poor'}\n",
      "\t Unique Tokens LLAMA3: {'P', 'oor'}\n",
      "Text 2: She felt bad for Dave.\n",
      "\t Tokenized GPT2:She Ġfelt Ġbad Ġfor ĠDave .\n",
      "\t Tokenized LLAMA3:She Ġfelt Ġbad Ġfor ĠDave .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: with little back packs of their own and you know things like that\n",
      "\t Tokenized GPT2:with Ġlittle Ġback Ġpacks Ġof Ġtheir Ġown Ġand Ġyou Ġknow Ġthings Ġlike Ġthat\n",
      "\t Tokenized LLAMA3:with Ġlittle Ġback Ġpacks Ġof Ġtheir Ġown Ġand Ġyou Ġknow Ġthings Ġlike Ġthat\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I'm not sure they're old enough to have back packs.\n",
      "\t Tokenized GPT2:I 'm Ġnot Ġsure Ġthey 're Ġold Ġenough Ġto Ġhave Ġback Ġpacks .\n",
      "\t Tokenized LLAMA3:I 'm Ġnot Ġsure Ġthey 're Ġold Ġenough Ġto Ġhave Ġback Ġpacks .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: In the other sight he saw Adrin's hands cocking back a pair of dragon-hammered pistols.\n",
      "\t Tokenized GPT2:In Ġthe Ġother Ġsight Ġhe Ġsaw ĠAd rin 's Ġhands Ġcock ing Ġback Ġa Ġpair Ġof Ġdragon - ham mered Ġpistol s .\n",
      "\t Tokenized LLAMA3:In Ġthe Ġother Ġsight Ġhe Ġsaw ĠAd rin 's Ġhands Ġcock ing Ġback Ġa Ġpair Ġof Ġdragon -h am mered Ġpist ols .\n",
      "\t Unique Tokens GPT2: {'s', '-', 'Ġpistol', 'ham'}\n",
      "\t Unique Tokens LLAMA3: {'Ġpist', '-h', 'am', 'ols'}\n",
      "Text 2: Adrin fired his machine gun as he watched.\n",
      "\t Tokenized GPT2:Ad rin Ġfired Ġhis Ġmachine Ġgun Ġas Ġhe Ġwatched .\n",
      "\t Tokenized LLAMA3:Ad rin Ġfired Ġhis Ġmachine Ġgun Ġas Ġhe Ġwatched .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Conspiracy theorists  MasterCard is investing in a chip that can store electronic cash, your medical history, and keys to your home and office.\n",
      "\t Tokenized GPT2:Cons pir acy Ġtheor ists Ġ ĠMaster Card Ġis Ġinvesting Ġin Ġa Ġchip Ġthat Ġcan Ġstore Ġelectronic Ġcash , Ġyour Ġmedical Ġhistory , Ġand Ġkeys Ġto Ġyour Ġhome Ġand Ġoffice .\n",
      "\t Tokenized LLAMA3:Cons pir acy Ġthe or ists Ġ ĠMaster Card Ġis Ġinvesting Ġin Ġa Ġchip Ġthat Ġcan Ġstore Ġelectronic Ġcash , Ġyour Ġmedical Ġhistory , Ġand Ġkeys Ġto Ġyour Ġhome Ġand Ġoffice .\n",
      "\t Unique Tokens GPT2: {'Ġtheor'}\n",
      "\t Unique Tokens LLAMA3: {'Ġthe', 'or'}\n",
      "Text 2: Conspiracy theorists believe Mastercard is working on a chip to store all your personal data.\n",
      "\t Tokenized GPT2:Cons pir acy Ġtheor ists Ġbelieve ĠMaster card Ġis Ġworking Ġon Ġa Ġchip Ġto Ġstore Ġall Ġyour Ġpersonal Ġdata .\n",
      "\t Tokenized LLAMA3:Cons pir acy Ġthe or ists Ġbelieve ĠMaster card Ġis Ġworking Ġon Ġa Ġchip Ġto Ġstore Ġall Ġyour Ġpersonal Ġdata .\n",
      "\t Unique Tokens GPT2: {'Ġtheor'}\n",
      "\t Unique Tokens LLAMA3: {'Ġthe', 'or'}\n",
      "==contradiction==\n",
      "Text 1: This call to play fortuneteller is not easily refused.\n",
      "\t Tokenized GPT2:This Ġcall Ġto Ġplay Ġfort un et eller Ġis Ġnot Ġeasily Ġrefused .\n",
      "\t Tokenized LLAMA3:This Ġcall Ġto Ġplay Ġfort un et eller Ġis Ġnot Ġeasily Ġrefused .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It's easily refused the call to play fortuneteller.\n",
      "\t Tokenized GPT2:It 's Ġeasily Ġrefused Ġthe Ġcall Ġto Ġplay Ġfort un et eller .\n",
      "\t Tokenized LLAMA3:It 's Ġeasily Ġrefused Ġthe Ġcall Ġto Ġplay Ġfort un et eller .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: After the high emotion of de Gaulle's march down the Champs-Elys??es, the business of post-war reconstruction, though boosted by the generous aid of the Americans' Mar?­shall Plan, proved arduous, and the wartime alliance of de Gaulle's conservatives and the Communist Party soon broke down.\n",
      "\t Tokenized GPT2:After Ġthe Ġhigh Ġemotion Ġof Ġde ĠG aul le 's Ġmarch Ġdown Ġthe ĠCh amps - E ly s ?? es , Ġthe Ġbusiness Ġof Ġpost - war Ġreconstruction , Ġthough Ġboost ed Ġby Ġthe Ġgenerous Ġaid Ġof Ġthe ĠAmericans ' ĠMar ? ÂŃ sh all ĠPlan , Ġproved Ġar du ous , Ġand Ġthe Ġw art ime Ġalliance Ġof Ġde ĠG aul le 's Ġconservatives Ġand Ġthe ĠCommun ist ĠParty Ġsoon Ġbroke Ġdown .\n",
      "\t Tokenized LLAMA3:After Ġthe Ġhigh Ġemotion Ġof Ġde ĠG aul le 's Ġmarch Ġdown Ġthe ĠCh amps -E ly s ?? es , Ġthe Ġbusiness Ġof Ġpost -war Ġreconstruction , Ġthough Ġboost ed Ġby Ġthe Ġgenerous Ġaid Ġof Ġthe ĠAmericans ' ĠMar ? ÂŃ sh all ĠPlan , Ġproved Ġar du ous , Ġand Ġthe Ġw art ime Ġalliance Ġof Ġde ĠG aul le 's Ġconservatives Ġand Ġthe ĠCommun ist ĠParty Ġsoon Ġbroke Ġdown .\n",
      "\t Unique Tokens GPT2: {'-', 'E', 'war'}\n",
      "\t Unique Tokens LLAMA3: {'-E', '-war'}\n",
      "Text 2: Though the Marshall Plan was designed to help other countries, it failed to fulfill its purpose with the Communists.\n",
      "\t Tokenized GPT2:Though Ġthe ĠMarshall ĠPlan Ġwas Ġdesigned Ġto Ġhelp Ġother Ġcountries , Ġit Ġfailed Ġto Ġfulfill Ġits Ġpurpose Ġwith Ġthe ĠCommun ists .\n",
      "\t Tokenized LLAMA3:Though Ġthe ĠMarshall ĠPlan Ġwas Ġdesigned Ġto Ġhelp Ġother Ġcountries , Ġit Ġfailed Ġto Ġfulfill Ġits Ġpurpose Ġwith Ġthe ĠCommun ists .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: The main gate of the churchyard leads out to Greyfriars Place, and across the street you will find an excellent view of one of Scotland's newest museums.\n",
      "\t Tokenized GPT2:The Ġmain Ġgate Ġof Ġthe Ġchurch yard Ġleads Ġout Ġto ĠGrey f ri ars ĠPlace , Ġand Ġacross Ġthe Ġstreet Ġyou Ġwill Ġfind Ġan Ġexcellent Ġview Ġof Ġone Ġof ĠScotland 's Ġnewest Ġmuse ums .\n",
      "\t Tokenized LLAMA3:The Ġmain Ġgate Ġof Ġthe Ġchurch yard Ġleads Ġout Ġto ĠGrey f ri ars ĠPlace , Ġand Ġacross Ġthe Ġstreet Ġyou Ġwill Ġfind Ġan Ġexcellent Ġview Ġof Ġone Ġof ĠScotland 's Ġnewest Ġmuse ums .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Near the church you can see Greyfriars Place and a new museum. \n",
      "\t Tokenized GPT2:N ear Ġthe Ġchurch Ġyou Ġcan Ġsee ĠGrey f ri ars ĠPlace Ġand Ġa Ġnew Ġmuseum . Ġ\n",
      "\t Tokenized LLAMA3:N ear Ġthe Ġchurch Ġyou Ġcan Ġsee ĠGrey f ri ars ĠPlace Ġand Ġa Ġnew Ġmuseum . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: To see the desert at its best, go out at dawn and at sunset.\n",
      "\t Tokenized GPT2:To Ġsee Ġthe Ġdesert Ġat Ġits Ġbest , Ġgo Ġout Ġat Ġdawn Ġand Ġat Ġsunset .\n",
      "\t Tokenized LLAMA3:To Ġsee Ġthe Ġdesert Ġat Ġits Ġbest , Ġgo Ġout Ġat Ġdawn Ġand Ġat Ġsunset .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Go at noon to see the desert for the best view.\n",
      "\t Tokenized GPT2:Go Ġat Ġnoon Ġto Ġsee Ġthe Ġdesert Ġfor Ġthe Ġbest Ġview .\n",
      "\t Tokenized LLAMA3:Go Ġat Ġnoon Ġto Ġsee Ġthe Ġdesert Ġfor Ġthe Ġbest Ġview .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: This was the site of the Bateau-Lavoir studio, an unprepossessing glass-roofed loft reconstructed since a 1970 fire.\n",
      "\t Tokenized GPT2:This Ġwas Ġthe Ġsite Ġof Ġthe ĠB ate au - L av oir Ġstudio , Ġan Ġun pre poss ess ing Ġglass - ro of ed Ġlo ft Ġreconstruct ed Ġsince Ġa Ġ1970 Ġfire .\n",
      "\t Tokenized LLAMA3:This Ġwas Ġthe Ġsite Ġof Ġthe ĠB ate au -L av oir Ġstudio , Ġan Ġun pre poss ess ing Ġglass -ro of ed Ġlo ft Ġreconstruct ed Ġsince Ġa Ġ 197 0 Ġfire .\n",
      "\t Unique Tokens GPT2: {'Ġ1970', 'ro', 'L', '-'}\n",
      "\t Unique Tokens LLAMA3: {'-ro', '-L', 'Ġ', '197', '0'}\n",
      "Text 2: The glass roof was shattered in 1990 as a result of having debris fall on top of it.\n",
      "\t Tokenized GPT2:The Ġglass Ġroof Ġwas Ġshattered Ġin Ġ1990 Ġas Ġa Ġresult Ġof Ġhaving Ġdebris Ġfall Ġon Ġtop Ġof Ġit .\n",
      "\t Tokenized LLAMA3:The Ġglass Ġroof Ġwas Ġshattered Ġin Ġ 199 0 Ġas Ġa Ġresult Ġof Ġhaving Ġdebris Ġfall Ġon Ġtop Ġof Ġit .\n",
      "\t Unique Tokens GPT2: {'Ġ1990'}\n",
      "\t Unique Tokens LLAMA3: {'199', '0', 'Ġ'}\n",
      "==contradiction==\n",
      "Text 1: He dismounted and Ca'daan saw he was smaller than the rest.\n",
      "\t Tokenized GPT2:He Ġdis mount ed Ġand ĠCa 'd aan Ġsaw Ġhe Ġwas Ġsmaller Ġthan Ġthe Ġrest .\n",
      "\t Tokenized LLAMA3:He Ġdis mount ed Ġand ĠCa 'd aan Ġsaw Ġhe Ġwas Ġsmaller Ġthan Ġthe Ġrest .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He was very tall.\n",
      "\t Tokenized GPT2:He Ġwas Ġvery Ġtall .\n",
      "\t Tokenized LLAMA3:He Ġwas Ġvery Ġtall .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Tommy had a healthy and vigorous appetite.\n",
      "\t Tokenized GPT2:Tom my Ġhad Ġa Ġhealthy Ġand Ġvig orous Ġappetite .\n",
      "\t Tokenized LLAMA3:Tom my Ġhad Ġa Ġhealthy Ġand Ġvig orous Ġappetite .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Tommy hadn't eaten all day.\n",
      "\t Tokenized GPT2:Tom my Ġhadn 't Ġeaten Ġall Ġday .\n",
      "\t Tokenized LLAMA3:Tom my Ġhadn 't Ġeaten Ġall Ġday .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: so you um-hum so you think it comes down to education or or something like that\n",
      "\t Tokenized GPT2:so Ġyou Ġum - hum Ġso Ġyou Ġthink Ġit Ġcomes Ġdown Ġto Ġeducation Ġor Ġor Ġsomething Ġlike Ġthat\n",
      "\t Tokenized LLAMA3:so Ġyou Ġum -h um Ġso Ġyou Ġthink Ġit Ġcomes Ġdown Ġto Ġeducation Ġor Ġor Ġsomething Ġlike Ġthat\n",
      "\t Unique Tokens GPT2: {'-', 'hum'}\n",
      "\t Unique Tokens LLAMA3: {'um', '-h'}\n",
      "Text 2: IT all boils down to how much education you have. \n",
      "\t Tokenized GPT2:IT Ġall Ġbo ils Ġdown Ġto Ġhow Ġmuch Ġeducation Ġyou Ġhave . Ġ\n",
      "\t Tokenized LLAMA3:IT Ġall Ġbo ils Ġdown Ġto Ġhow Ġmuch Ġeducation Ġyou Ġhave . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: As Russell points out, some 400,000 legal aid cases go unassisted each year.\n",
      "\t Tokenized GPT2:As ĠRussell Ġpoints Ġout , Ġsome Ġ400 , 000 Ġlegal Ġaid Ġcases Ġgo Ġun ass isted Ġeach Ġyear .\n",
      "\t Tokenized LLAMA3:As ĠRussell Ġpoints Ġout , Ġsome Ġ 400 , 000 Ġlegal Ġaid Ġcases Ġgo Ġun ass isted Ġeach Ġyear .\n",
      "\t Unique Tokens GPT2: {'Ġ400'}\n",
      "\t Unique Tokens LLAMA3: {'400', 'Ġ'}\n",
      "Text 2: Zero legal aid cases go unassisted each year.\n",
      "\t Tokenized GPT2:Zero Ġlegal Ġaid Ġcases Ġgo Ġun ass isted Ġeach Ġyear .\n",
      "\t Tokenized LLAMA3:Zero Ġlegal Ġaid Ġcases Ġgo Ġun ass isted Ġeach Ġyear .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: We need to be sure of our going.\" But Tuppence, for once, seemed tongue-tied.\n",
      "\t Tokenized GPT2:We Ġneed Ġto Ġbe Ġsure Ġof Ġour Ġgoing .\" ĠBut ĠT upp ence , Ġfor Ġonce , Ġseemed Ġtongue - t ied .\n",
      "\t Tokenized LLAMA3:We Ġneed Ġto Ġbe Ġsure Ġof Ġour Ġgoing .\" ĠBut ĠT upp ence , Ġfor Ġonce , Ġseemed Ġtongue -t ied .\n",
      "\t Unique Tokens GPT2: {'-', 't'}\n",
      "\t Unique Tokens LLAMA3: {'-t'}\n",
      "Text 2: Tuppence was shocked.\n",
      "\t Tokenized GPT2:T upp ence Ġwas Ġshocked .\n",
      "\t Tokenized LLAMA3:T upp ence Ġwas Ġshocked .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The strychnine had been found in a drawer in the prisoner's room. \n",
      "\t Tokenized GPT2:The Ġst ry chn ine Ġhad Ġbeen Ġfound Ġin Ġa Ġdrawer Ġin Ġthe Ġprisoner 's Ġroom . Ġ\n",
      "\t Tokenized LLAMA3:The Ġst ry chn ine Ġhad Ġbeen Ġfound Ġin Ġa Ġdrawer Ġin Ġthe Ġprisoner 's Ġroom . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: They found the strychnine under the prisoner's bed. \n",
      "\t Tokenized GPT2:They Ġfound Ġthe Ġst ry chn ine Ġunder Ġthe Ġprisoner 's Ġbed . Ġ\n",
      "\t Tokenized LLAMA3:They Ġfound Ġthe Ġst ry chn ine Ġunder Ġthe Ġprisoner 's Ġbed . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: and uh you know it's like they they consider that but it would be the same way here you know it's like if if you had to do it you know you have a big sign i'm sorry i don't get paid you know\n",
      "\t Tokenized GPT2:and Ġuh Ġyou Ġknow Ġit 's Ġlike Ġthey Ġthey Ġconsider Ġthat Ġbut Ġit Ġwould Ġbe Ġthe Ġsame Ġway Ġhere Ġyou Ġknow Ġit 's Ġlike Ġif Ġif Ġyou Ġhad Ġto Ġdo Ġit Ġyou Ġknow Ġyou Ġhave Ġa Ġbig Ġsign Ġi 'm Ġsorry Ġi Ġdon 't Ġget Ġpaid Ġyou Ġknow\n",
      "\t Tokenized LLAMA3:and Ġuh Ġyou Ġknow Ġit 's Ġlike Ġthey Ġthey Ġconsider Ġthat Ġbut Ġit Ġwould Ġbe Ġthe Ġsame Ġway Ġhere Ġyou Ġknow Ġit 's Ġlike Ġif Ġif Ġyou Ġhad Ġto Ġdo Ġit Ġyou Ġknow Ġyou Ġhave Ġa Ġbig Ġsign Ġi 'm Ġsorry Ġi Ġdon 't Ġget Ġpaid Ġyou Ġknow\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: If I were somewhere else I would be getting paid. \n",
      "\t Tokenized GPT2:If ĠI Ġwere Ġsomewhere Ġelse ĠI Ġwould Ġbe Ġgetting Ġpaid . Ġ\n",
      "\t Tokenized LLAMA3:If ĠI Ġwere Ġsomewhere Ġelse ĠI Ġwould Ġbe Ġgetting Ġpaid . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: well the floor was uneven you know\n",
      "\t Tokenized GPT2:well Ġthe Ġfloor Ġwas Ġuneven Ġyou Ġknow\n",
      "\t Tokenized LLAMA3:well Ġthe Ġfloor Ġwas Ġuneven Ġyou Ġknow\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: well, you're aware that the floor wasn't even\n",
      "\t Tokenized GPT2:well , Ġyou 're Ġaware Ġthat Ġthe Ġfloor Ġwasn 't Ġeven\n",
      "\t Tokenized LLAMA3:well , Ġyou 're Ġaware Ġthat Ġthe Ġfloor Ġwasn 't Ġeven\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Never trust a Sather, Bork said softly.\n",
      "\t Tokenized GPT2:Never Ġtrust Ġa ĠS ather , ĠB ork Ġsaid Ġsoftly .\n",
      "\t Tokenized LLAMA3:Never Ġtrust Ġa ĠS ather , ĠB ork Ġsaid Ġsoftly .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Borker said to never trust a Sather.\n",
      "\t Tokenized GPT2:B ork er Ġsaid Ġto Ġnever Ġtrust Ġa ĠS ather .\n",
      "\t Tokenized LLAMA3:B ork er Ġsaid Ġto Ġnever Ġtrust Ġa ĠS ather .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Like Arabs and Jews, Diamond warns, Koreans and Japanese are joined by blood yet locked in traditional enmity.\n",
      "\t Tokenized GPT2:Like ĠAr abs Ġand ĠJews , ĠDiamond Ġwar ns , ĠKore ans Ġand ĠJapanese Ġare Ġjoined Ġby Ġblood Ġyet Ġlocked Ġin Ġtraditional Ġen m ity .\n",
      "\t Tokenized LLAMA3:Like ĠAr abs Ġand ĠJews , ĠDiamond Ġwar ns , ĠKore ans Ġand ĠJapanese Ġare Ġjoined Ġby Ġblood Ġyet Ġlocked Ġin Ġtraditional Ġen m ity .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Koreans and Japanese have tension between them because of a war long ago.\n",
      "\t Tokenized GPT2:K ore ans Ġand ĠJapanese Ġhave Ġtension Ġbetween Ġthem Ġbecause Ġof Ġa Ġwar Ġlong Ġago .\n",
      "\t Tokenized LLAMA3:K ore ans Ġand ĠJapanese Ġhave Ġtension Ġbetween Ġthem Ġbecause Ġof Ġa Ġwar Ġlong Ġago .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: South Carolina has no referendum right, so the Supreme Court canceled the vote and upheld the ban.\n",
      "\t Tokenized GPT2:South ĠCarolina Ġhas Ġno Ġreferendum Ġright , Ġso Ġthe ĠSupreme ĠCourt Ġcanceled Ġthe Ġvote Ġand Ġup held Ġthe Ġban .\n",
      "\t Tokenized LLAMA3:South ĠCarolina Ġhas Ġno Ġreferendum Ġright , Ġso Ġthe ĠSupreme ĠCourt Ġcanceled Ġthe Ġvote Ġand Ġup held Ġthe Ġban .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: South Carolina has a referendum right, so the Supreme Court was powerless over the state.\n",
      "\t Tokenized GPT2:South ĠCarolina Ġhas Ġa Ġreferendum Ġright , Ġso Ġthe ĠSupreme ĠCourt Ġwas Ġpower less Ġover Ġthe Ġstate .\n",
      "\t Tokenized LLAMA3:South ĠCarolina Ġhas Ġa Ġreferendum Ġright , Ġso Ġthe ĠSupreme ĠCourt Ġwas Ġpower less Ġover Ġthe Ġstate .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Arafat is also ailing and has no clear successor.\n",
      "\t Tokenized GPT2:A ra fat Ġis Ġalso Ġa iling Ġand Ġhas Ġno Ġclear Ġsuccessor .\n",
      "\t Tokenized LLAMA3:A raf at Ġis Ġalso Ġa iling Ġand Ġhas Ġno Ġclear Ġsuccessor .\n",
      "\t Unique Tokens GPT2: {'ra', 'fat'}\n",
      "\t Unique Tokens LLAMA3: {'at', 'raf'}\n",
      "Text 2: Arafat is in bad health and does not have a person chosen to take his place.\n",
      "\t Tokenized GPT2:A ra fat Ġis Ġin Ġbad Ġhealth Ġand Ġdoes Ġnot Ġhave Ġa Ġperson Ġchosen Ġto Ġtake Ġhis Ġplace .\n",
      "\t Tokenized LLAMA3:A raf at Ġis Ġin Ġbad Ġhealth Ġand Ġdoes Ġnot Ġhave Ġa Ġperson Ġchosen Ġto Ġtake Ġhis Ġplace .\n",
      "\t Unique Tokens GPT2: {'ra', 'fat'}\n",
      "\t Unique Tokens LLAMA3: {'at', 'raf'}\n",
      "==entailment==\n",
      "Text 1: but uh i've always enjoyed uh the train and you know fooling with it and all\n",
      "\t Tokenized GPT2:but Ġuh Ġi 've Ġalways Ġenjoyed Ġuh Ġthe Ġtrain Ġand Ġyou Ġknow Ġfool ing Ġwith Ġit Ġand Ġall\n",
      "\t Tokenized LLAMA3:but Ġuh Ġi 've Ġalways Ġenjoyed Ġuh Ġthe Ġtrain Ġand Ġyou Ġknow Ġfool ing Ġwith Ġit Ġand Ġall\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I have always had a love for trains.\n",
      "\t Tokenized GPT2:I Ġhave Ġalways Ġhad Ġa Ġlove Ġfor Ġtrains .\n",
      "\t Tokenized LLAMA3:I Ġhave Ġalways Ġhad Ġa Ġlove Ġfor Ġtrains .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: and they're fairly close to the water aren't they i mean they're right on the late\n",
      "\t Tokenized GPT2:and Ġthey 're Ġfairly Ġclose Ġto Ġthe Ġwater Ġaren 't Ġthey Ġi Ġmean Ġthey 're Ġright Ġon Ġthe Ġlate\n",
      "\t Tokenized LLAMA3:and Ġthey 're Ġfairly Ġclose Ġto Ġthe Ġwater Ġaren 't Ġthey Ġi Ġmean Ġthey 're Ġright Ġon Ġthe Ġlate\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: They're a distance from the water aren't they.\n",
      "\t Tokenized GPT2:They 're Ġa Ġdistance Ġfrom Ġthe Ġwater Ġaren 't Ġthey .\n",
      "\t Tokenized LLAMA3:They 're Ġa Ġdistance Ġfrom Ġthe Ġwater Ġaren 't Ġthey .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Bien! he said at last. \n",
      "\t Tokenized GPT2:B ien ! Ġhe Ġsaid Ġat Ġlast . Ġ\n",
      "\t Tokenized LLAMA3:B ien ! Ġhe Ġsaid Ġat Ġlast . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He wasted no time speaking. \n",
      "\t Tokenized GPT2:He Ġwasted Ġno Ġtime Ġspeaking . Ġ\n",
      "\t Tokenized LLAMA3:He Ġwasted Ġno Ġtime Ġspeaking . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Lie back, and DON'T THINK.\n",
      "\t Tokenized GPT2:L ie Ġback , Ġand ĠDON ' T ĠTHINK .\n",
      "\t Tokenized LLAMA3:L ie Ġback , Ġand ĠDON 'T ĠTH INK .\n",
      "\t Unique Tokens GPT2: {'ĠTHINK', 'T', \"'\"}\n",
      "\t Unique Tokens LLAMA3: {\"'T\", 'ĠTH', 'INK'}\n",
      "Text 2: Stand up and start thinking.\n",
      "\t Tokenized GPT2:Stand Ġup Ġand Ġstart Ġthinking .\n",
      "\t Tokenized LLAMA3:Stand Ġup Ġand Ġstart Ġthinking .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: It vibrated under his hand.\n",
      "\t Tokenized GPT2:It Ġvibr ated Ġunder Ġhis Ġhand .\n",
      "\t Tokenized LLAMA3:It Ġvibr ated Ġunder Ġhis Ġhand .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It hummed quietly in his hand.\n",
      "\t Tokenized GPT2:It Ġhummed Ġquietly Ġin Ġhis Ġhand .\n",
      "\t Tokenized LLAMA3:It Ġhummed Ġquietly Ġin Ġhis Ġhand .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: This whole unsavory episode brings back memories of skits with Monty Python ! One of my favorite lines was, You are guilty of six--no, seven--charges of heresy.\n",
      "\t Tokenized GPT2:This Ġwhole Ġuns av ory Ġepisode Ġbrings Ġback Ġmemories Ġof Ġsk its Ġwith ĠMon ty ĠPython Ġ! ĠOne Ġof Ġmy Ġfavorite Ġlines Ġwas , ĠYou Ġare Ġguilty Ġof Ġsix -- no , Ġseven -- char ges Ġof Ġhe res y .\n",
      "\t Tokenized LLAMA3:This Ġwhole Ġuns av ory Ġepisode Ġbrings Ġback Ġmemories Ġof Ġsk its Ġwith ĠMon ty ĠPython Ġ! ĠOne Ġof Ġmy Ġfavorite Ġlines Ġwas , ĠYou Ġare Ġguilty Ġof Ġsix -- no , Ġseven -- char ges Ġof Ġhe res y .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: This episode reminds me of skits with Monthy Python.\n",
      "\t Tokenized GPT2:This Ġepisode Ġreminds Ġme Ġof Ġsk its Ġwith ĠMon thy ĠPython .\n",
      "\t Tokenized LLAMA3:This Ġepisode Ġreminds Ġme Ġof Ġsk its Ġwith ĠMon thy ĠPython .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: In the short term, U.S. consumers will benefit from cheap imports (as will U.S. multinationals that use parts made in East Asian factories).\n",
      "\t Tokenized GPT2:In Ġthe Ġshort Ġterm , ĠU . S . Ġconsumers Ġwill Ġbenefit Ġfrom Ġcheap Ġimports Ġ( as Ġwill ĠU . S . Ġmult ination als Ġthat Ġuse Ġparts Ġmade Ġin ĠEast ĠAsian Ġfactories ).\n",
      "\t Tokenized LLAMA3:In Ġthe Ġshort Ġterm , ĠU .S . Ġconsumers Ġwill Ġbenefit Ġfrom Ġcheap Ġimports Ġ( as Ġwill ĠU .S . Ġmult ination als Ġthat Ġuse Ġparts Ġmade Ġin ĠEast ĠAsian Ġfactories ).\n",
      "\t Unique Tokens GPT2: {'S'}\n",
      "\t Unique Tokens LLAMA3: {'.S'}\n",
      "Text 2: U.S. consumers and factories in East Asia benefit from imports.\n",
      "\t Tokenized GPT2:U . S . Ġconsumers Ġand Ġfactories Ġin ĠEast ĠAsia Ġbenefit Ġfrom Ġimports .\n",
      "\t Tokenized LLAMA3:U .S . Ġconsumers Ġand Ġfactories Ġin ĠEast ĠAsia Ġbenefit Ġfrom Ġimports .\n",
      "\t Unique Tokens GPT2: {'S'}\n",
      "\t Unique Tokens LLAMA3: {'.S'}\n",
      "==entailment==\n",
      "Text 1: I noticed that there was a long branch running out from the tree in the right direction.\n",
      "\t Tokenized GPT2:I Ġnoticed Ġthat Ġthere Ġwas Ġa Ġlong Ġbranch Ġrunning Ġout Ġfrom Ġthe Ġtree Ġin Ġthe Ġright Ġdirection .\n",
      "\t Tokenized LLAMA3:I Ġnoticed Ġthat Ġthere Ġwas Ġa Ġlong Ġbranch Ġrunning Ġout Ġfrom Ġthe Ġtree Ġin Ġthe Ġright Ġdirection .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: There was a rather lengthy branch that was pointing in the right direction.  \n",
      "\t Tokenized GPT2:There Ġwas Ġa Ġrather Ġlengthy Ġbranch Ġthat Ġwas Ġpointing Ġin Ġthe Ġright Ġdirection . ĠĠ\n",
      "\t Tokenized LLAMA3:There Ġwas Ġa Ġrather Ġlengthy Ġbranch Ġthat Ġwas Ġpointing Ġin Ġthe Ġright Ġdirection . ĠĠ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Figure 1:  Delivery Points to Stops\n",
      "\t Tokenized GPT2:Figure Ġ1 : Ġ ĠDel ivery ĠPoints Ġto ĠSt ops\n",
      "\t Tokenized LLAMA3:Figure Ġ 1 : Ġ ĠDel ivery ĠPoint s Ġto ĠSt ops\n",
      "\t Unique Tokens GPT2: {'ĠPoints', 'Ġ1'}\n",
      "\t Unique Tokens LLAMA3: {'s', 'ĠPoint', '1'}\n",
      "Text 2: The third figure covers delivery points to stops\n",
      "\t Tokenized GPT2:The Ġthird Ġfigure Ġcovers Ġdelivery Ġpoints Ġto Ġstops\n",
      "\t Tokenized LLAMA3:The Ġthird Ġfigure Ġcovers Ġdelivery Ġpoints Ġto Ġstops\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: So unlike people who are fortunate enough to be able to afford attorneys and can go to another lawyer, our clients are simply lost in the legal system if they cannot get access to it from us.\n",
      "\t Tokenized GPT2:So Ġunlike Ġpeople Ġwho Ġare Ġfortunate Ġenough Ġto Ġbe Ġable Ġto Ġafford Ġattorneys Ġand Ġcan Ġgo Ġto Ġanother Ġlawyer , Ġour Ġclients Ġare Ġsimply Ġlost Ġin Ġthe Ġlegal Ġsystem Ġif Ġthey Ġcannot Ġget Ġaccess Ġto Ġit Ġfrom Ġus .\n",
      "\t Tokenized LLAMA3:So Ġunlike Ġpeople Ġwho Ġare Ġfortunate Ġenough Ġto Ġbe Ġable Ġto Ġafford Ġattorneys Ġand Ġcan Ġgo Ġto Ġanother Ġlawyer , Ġour Ġclients Ġare Ġsimply Ġlost Ġin Ġthe Ġlegal Ġsystem Ġif Ġthey Ġcannot Ġget Ġaccess Ġto Ġit Ġfrom Ġus .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Our clients can afford attorneys and bouncing between lawyers.\n",
      "\t Tokenized GPT2:Our Ġclients Ġcan Ġafford Ġattorneys Ġand Ġbouncing Ġbetween Ġlawyers .\n",
      "\t Tokenized LLAMA3:Our Ġclients Ġcan Ġafford Ġattorneys Ġand Ġbouncing Ġbetween Ġlawyers .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: uh i really i miss college i had a good time\n",
      "\t Tokenized GPT2:uh Ġi Ġreally Ġi Ġmiss Ġcollege Ġi Ġhad Ġa Ġgood Ġtime\n",
      "\t Tokenized LLAMA3:uh Ġi Ġreally Ġi Ġmiss Ġcollege Ġi Ġhad Ġa Ġgood Ġtime\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I enjoyed my time in university. \n",
      "\t Tokenized GPT2:I Ġenjoyed Ġmy Ġtime Ġin Ġuniversity . Ġ\n",
      "\t Tokenized LLAMA3:I Ġenjoyed Ġmy Ġtime Ġin Ġuniversity . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: The baker was not jolly.\n",
      "\t Tokenized GPT2:The Ġb aker Ġwas Ġnot Ġj olly .\n",
      "\t Tokenized LLAMA3:The Ġb aker Ġwas Ġnot Ġj olly .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The baker wasn't happy.\n",
      "\t Tokenized GPT2:The Ġb aker Ġwasn 't Ġhappy .\n",
      "\t Tokenized LLAMA3:The Ġb aker Ġwasn 't Ġhappy .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Poor Dave, she said.\n",
      "\t Tokenized GPT2:Poor ĠDave , Ġshe Ġsaid .\n",
      "\t Tokenized LLAMA3:P oor ĠDave , Ġshe Ġsaid .\n",
      "\t Unique Tokens GPT2: {'Poor'}\n",
      "\t Unique Tokens LLAMA3: {'P', 'oor'}\n",
      "Text 2: She was happy for Dave.\n",
      "\t Tokenized GPT2:She Ġwas Ġhappy Ġfor ĠDave .\n",
      "\t Tokenized LLAMA3:She Ġwas Ġhappy Ġfor ĠDave .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: yeah well i'm a hot weather person i'm i can take the heat but i don't like the cold\n",
      "\t Tokenized GPT2:yeah Ġwell Ġi 'm Ġa Ġhot Ġweather Ġperson Ġi 'm Ġi Ġcan Ġtake Ġthe Ġheat Ġbut Ġi Ġdon 't Ġlike Ġthe Ġcold\n",
      "\t Tokenized LLAMA3:yeah Ġwell Ġi 'm Ġa Ġhot Ġweather Ġperson Ġi 'm Ġi Ġcan Ġtake Ġthe Ġheat Ġbut Ġi Ġdon 't Ġlike Ġthe Ġcold\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I do not like warm weather at all.  \n",
      "\t Tokenized GPT2:I Ġdo Ġnot Ġlike Ġwarm Ġweather Ġat Ġall . ĠĠ\n",
      "\t Tokenized LLAMA3:I Ġdo Ġnot Ġlike Ġwarm Ġweather Ġat Ġall . ĠĠ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: In the vaults of the Bank.\n",
      "\t Tokenized GPT2:In Ġthe Ġvault s Ġof Ġthe ĠBank .\n",
      "\t Tokenized LLAMA3:In Ġthe Ġvault s Ġof Ġthe ĠBank .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: In the cash register at the bank.\n",
      "\t Tokenized GPT2:In Ġthe Ġcash Ġregister Ġat Ġthe Ġbank .\n",
      "\t Tokenized LLAMA3:In Ġthe Ġcash Ġregister Ġat Ġthe Ġbank .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: There followed the Balkan Wars, in which Turkey lost western Thrace and Macedonia, then World War I, into which Turkey entered on Germany's side.\n",
      "\t Tokenized GPT2:There Ġfollowed Ġthe ĠB alk an ĠWars , Ġin Ġwhich ĠTurkey Ġlost Ġwestern ĠThr ace Ġand ĠM aced onia , Ġthen ĠWorld ĠWar ĠI , Ġinto Ġwhich ĠTurkey Ġentered Ġon ĠGermany 's Ġside .\n",
      "\t Tokenized LLAMA3:There Ġfollowed Ġthe ĠB alk an ĠWars , Ġin Ġwhich ĠTurkey Ġlost Ġwestern ĠThr ace Ġand ĠM aced onia , Ġthen ĠWorld ĠWar ĠI , Ġinto Ġwhich ĠTurkey Ġentered Ġon ĠGermany 's Ġside .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Turkey entered the first world war fighting against Germany.\n",
      "\t Tokenized GPT2:Tur key Ġentered Ġthe Ġfirst Ġworld Ġwar Ġfighting Ġagainst ĠGermany .\n",
      "\t Tokenized LLAMA3:Tur key Ġentered Ġthe Ġfirst Ġworld Ġwar Ġfighting Ġagainst ĠGermany .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The spear missed Vrenna by only a hand-span.\n",
      "\t Tokenized GPT2:The Ġspear Ġmissed ĠV ren na Ġby Ġonly Ġa Ġhand - span .\n",
      "\t Tokenized LLAMA3:The Ġspear Ġmissed ĠV ren na Ġby Ġonly Ġa Ġhand -s pan .\n",
      "\t Unique Tokens GPT2: {'-', 'span'}\n",
      "\t Unique Tokens LLAMA3: {'-s', 'pan'}\n",
      "Text 2: The spear smacked the man in the face. \n",
      "\t Tokenized GPT2:The Ġspear Ġsm acked Ġthe Ġman Ġin Ġthe Ġface . Ġ\n",
      "\t Tokenized LLAMA3:The Ġspear Ġsm acked Ġthe Ġman Ġin Ġthe Ġface . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: There never will be.\n",
      "\t Tokenized GPT2:There Ġnever Ġwill Ġbe .\n",
      "\t Tokenized LLAMA3:There Ġnever Ġwill Ġbe .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It should happen soon.\n",
      "\t Tokenized GPT2:It Ġshould Ġhappen Ġsoon .\n",
      "\t Tokenized LLAMA3:It Ġshould Ġhappen Ġsoon .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: yeah maybe the maybe they'll bring their good schools with them  you know if the industry comes\n",
      "\t Tokenized GPT2:yeah Ġmaybe Ġthe Ġmaybe Ġthey 'll Ġbring Ġtheir Ġgood Ġschools Ġwith Ġthem Ġ Ġyou Ġknow Ġif Ġthe Ġindustry Ġcomes\n",
      "\t Tokenized LLAMA3:yeah Ġmaybe Ġthe Ġmaybe Ġthey 'll Ġbring Ġtheir Ġgood Ġschools Ġwith Ġthem Ġ Ġyou Ġknow Ġif Ġthe Ġindustry Ġcomes\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: If the industry comes, it will foster the creation of better schools.\n",
      "\t Tokenized GPT2:If Ġthe Ġindustry Ġcomes , Ġit Ġwill Ġfoster Ġthe Ġcreation Ġof Ġbetter Ġschools .\n",
      "\t Tokenized LLAMA3:If Ġthe Ġindustry Ġcomes , Ġit Ġwill Ġfoster Ġthe Ġcreation Ġof Ġbetter Ġschools .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: He was a pilot, not a platoon leader.\n",
      "\t Tokenized GPT2:He Ġwas Ġa Ġpilot , Ġnot Ġa Ġplat oon Ġleader .\n",
      "\t Tokenized LLAMA3:He Ġwas Ġa Ġpilot , Ġnot Ġa Ġplat oon Ġleader .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He was no platoon leader, but a lowly pilot.\n",
      "\t Tokenized GPT2:He Ġwas Ġno Ġplat oon Ġleader , Ġbut Ġa Ġlow ly Ġpilot .\n",
      "\t Tokenized LLAMA3:He Ġwas Ġno Ġplat oon Ġleader , Ġbut Ġa Ġlow ly Ġpilot .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Despite their 17th-century origins, these gardens avoid the rigid geometry of the Tuileries and Ver?­sailles.\n",
      "\t Tokenized GPT2:Despite Ġtheir Ġ17 th - century Ġorigins , Ġthese Ġgardens Ġavoid Ġthe Ġrigid Ġgeometry Ġof Ġthe ĠTu iler ies Ġand ĠVer ? ÂŃ sa illes .\n",
      "\t Tokenized LLAMA3:Despite Ġtheir Ġ 17 th -century Ġorigins , Ġthese Ġgardens Ġavoid Ġthe Ġrigid Ġgeometry Ġof Ġthe ĠTu iler ies Ġand ĠVer ? ÂŃ sa illes .\n",
      "\t Unique Tokens GPT2: {'-', 'century', 'Ġ17'}\n",
      "\t Unique Tokens LLAMA3: {'-century', 'Ġ', '17'}\n",
      "Text 2: The gardens are not shaped like the Tuileries or Versailles.\n",
      "\t Tokenized GPT2:The Ġgardens Ġare Ġnot Ġshaped Ġlike Ġthe ĠTu iler ies Ġor ĠVers a illes .\n",
      "\t Tokenized LLAMA3:The Ġgardens Ġare Ġnot Ġshaped Ġlike Ġthe ĠTu iler ies Ġor ĠVers a illes .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: He writes that it's the first time he's added such a track.\n",
      "\t Tokenized GPT2:He Ġwrites Ġthat Ġit 's Ġthe Ġfirst Ġtime Ġhe 's Ġadded Ġsuch Ġa Ġtrack .\n",
      "\t Tokenized LLAMA3:He Ġwrites Ġthat Ġit 's Ġthe Ġfirst Ġtime Ġhe 's Ġadded Ġsuch Ġa Ġtrack .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He creates tracks like this all the time.\n",
      "\t Tokenized GPT2:He Ġcreates Ġtracks Ġlike Ġthis Ġall Ġthe Ġtime .\n",
      "\t Tokenized LLAMA3:He Ġcreates Ġtracks Ġlike Ġthis Ġall Ġthe Ġtime .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: which they probably Mexican people don't even know what a taco salad is but i think it's now it's moving up too because uh just a change you know just something different\n",
      "\t Tokenized GPT2:which Ġthey Ġprobably ĠMexican Ġpeople Ġdon 't Ġeven Ġknow Ġwhat Ġa Ġt aco Ġsalad Ġis Ġbut Ġi Ġthink Ġit 's Ġnow Ġit 's Ġmoving Ġup Ġtoo Ġbecause Ġuh Ġjust Ġa Ġchange Ġyou Ġknow Ġjust Ġsomething Ġdifferent\n",
      "\t Tokenized LLAMA3:which Ġthey Ġprobably ĠMexican Ġpeople Ġdon 't Ġeven Ġknow Ġwhat Ġa Ġt aco Ġsalad Ġis Ġbut Ġi Ġthink Ġit 's Ġnow Ġit 's Ġmoving Ġup Ġtoo Ġbecause Ġuh Ġjust Ġa Ġchange Ġyou Ġknow Ġjust Ġsomething Ġdifferent\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Taco salad isn't a traditional Mexican dish, but it's becoming one because of cultural movement.\n",
      "\t Tokenized GPT2:T aco Ġsalad Ġisn 't Ġa Ġtraditional ĠMexican Ġdish , Ġbut Ġit 's Ġbecoming Ġone Ġbecause Ġof Ġcultural Ġmovement .\n",
      "\t Tokenized LLAMA3:T aco Ġsalad Ġisn 't Ġa Ġtraditional ĠMexican Ġdish , Ġbut Ġit 's Ġbecoming Ġone Ġbecause Ġof Ġcultural Ġmovement .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Sir James's presence in Manchester was not accidental.\n",
      "\t Tokenized GPT2:Sir ĠJames 's Ġpresence Ġin ĠManchester Ġwas Ġnot Ġaccidental .\n",
      "\t Tokenized LLAMA3:Sir ĠJames 's Ġpresence Ġin ĠManchester Ġwas Ġnot Ġaccident al .\n",
      "\t Unique Tokens GPT2: {'Ġaccidental'}\n",
      "\t Unique Tokens LLAMA3: {'al', 'Ġaccident'}\n",
      "Text 2: Sir James was present in Manchester on purpose.\n",
      "\t Tokenized GPT2:Sir ĠJames Ġwas Ġpresent Ġin ĠManchester Ġon Ġpurpose .\n",
      "\t Tokenized LLAMA3:Sir ĠJames Ġwas Ġpresent Ġin ĠManchester Ġon Ġpurpose .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Reportedly the biggest payment made in such a case, it is hardly a nick in Texaco's annual revenue of more than $30 billion.\n",
      "\t Tokenized GPT2:Rep orted ly Ġthe Ġbiggest Ġpayment Ġmade Ġin Ġsuch Ġa Ġcase , Ġit Ġis Ġhardly Ġa Ġnick Ġin ĠTex aco 's Ġannual Ġrevenue Ġof Ġmore Ġthan Ġ$ 30 Ġbillion .\n",
      "\t Tokenized LLAMA3:Rep orted ly Ġthe Ġbiggest Ġpayment Ġmade Ġin Ġsuch Ġa Ġcase , Ġit Ġis Ġhardly Ġa Ġnick Ġin ĠTex aco 's Ġannual Ġrevenue Ġof Ġmore Ġthan Ġ$ 30 Ġbillion .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The biggest payment bankrupted the company.\n",
      "\t Tokenized GPT2:The Ġbiggest Ġpayment Ġbankrupt ed Ġthe Ġcompany .\n",
      "\t Tokenized LLAMA3:The Ġbiggest Ġpayment Ġbankrupt ed Ġthe Ġcompany .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: A spark of annoyance lit Lincoln's eyes; the smallest hint of Natalia's Russian fire.\n",
      "\t Tokenized GPT2:A Ġspark Ġof Ġannoyance Ġlit ĠLincoln 's Ġeyes ; Ġthe Ġsmallest Ġhint Ġof ĠNatal ia 's ĠRussian Ġfire .\n",
      "\t Tokenized LLAMA3:A Ġspark Ġof Ġannoyance Ġlit ĠLincoln 's Ġeyes ; Ġthe Ġsmallest Ġhint Ġof ĠNatal ia 's ĠRussian Ġfire .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Lincoln wanted to kill Natalia with his bare hands in that precise moment.\n",
      "\t Tokenized GPT2:Lin coln Ġwanted Ġto Ġkill ĠNatal ia Ġwith Ġhis Ġbare Ġhands Ġin Ġthat Ġprecise Ġmoment .\n",
      "\t Tokenized LLAMA3:Lin coln Ġwanted Ġto Ġkill ĠNatal ia Ġwith Ġhis Ġbare Ġhands Ġin Ġthat Ġprecise Ġmoment .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Perhaps a further password would be required, or, at any rate, some proof of identity.\n",
      "\t Tokenized GPT2:Perhaps Ġa Ġfurther Ġpassword Ġwould Ġbe Ġrequired , Ġor , Ġat Ġany Ġrate , Ġsome Ġproof Ġof Ġidentity .\n",
      "\t Tokenized LLAMA3:Perhaps Ġa Ġfurther Ġpassword Ġwould Ġbe Ġrequired , Ġor , Ġat Ġany Ġrate , Ġsome Ġproof Ġof Ġidentity .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Passwords are unnecessary as they waste additional time.\n",
      "\t Tokenized GPT2:Pass words Ġare Ġunnecessary Ġas Ġthey Ġwaste Ġadditional Ġtime .\n",
      "\t Tokenized LLAMA3:Password s Ġare Ġunnecessary Ġas Ġthey Ġwaste Ġadditional Ġtime .\n",
      "\t Unique Tokens GPT2: {'words', 'Pass'}\n",
      "\t Unique Tokens LLAMA3: {'s', 'Password'}\n",
      "==entailment==\n",
      "Text 1: H-2A agricultural workers are required to maintain a foreign residence which they have no intention of abandoning.\n",
      "\t Tokenized GPT2:H - 2 A Ġagricultural Ġworkers Ġare Ġrequired Ġto Ġmaintain Ġa Ġforeign Ġresidence Ġwhich Ġthey Ġhave Ġno Ġintention Ġof Ġabandon ing .\n",
      "\t Tokenized LLAMA3:H - 2 A Ġagricultural Ġworkers Ġare Ġrequired Ġto Ġmaintain Ġa Ġforeign Ġresidence Ġwhich Ġthey Ġhave Ġno Ġintention Ġof Ġabandon ing .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Permanent foreign residence is required for some types of agricultural work visas.\n",
      "\t Tokenized GPT2:P erman ent Ġforeign Ġresidence Ġis Ġrequired Ġfor Ġsome Ġtypes Ġof Ġagricultural Ġwork Ġvis as .\n",
      "\t Tokenized LLAMA3:P erman ent Ġforeign Ġresidence Ġis Ġrequired Ġfor Ġsome Ġtypes Ġof Ġagricultural Ġwork Ġvis as .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Angry consumers would complain about cheapo car care.\n",
      "\t Tokenized GPT2:Ang ry Ġconsumers Ġwould Ġcomplain Ġabout Ġcheap o Ġcar Ġcare .\n",
      "\t Tokenized LLAMA3:Ang ry Ġconsumers Ġwould Ġcomplain Ġabout Ġcheap o Ġcar Ġcare .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Cheapo car care is a complaint angry consumers have.\n",
      "\t Tokenized GPT2:Che ap o Ġcar Ġcare Ġis Ġa Ġcomplaint Ġangry Ġconsumers Ġhave .\n",
      "\t Tokenized LLAMA3:Che ap o Ġcar Ġcare Ġis Ġa Ġcomplaint Ġangry Ġconsumers Ġhave .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: While AILA has joined the ACLU and other organizations in a Freedom of Information Act request to find out who is being detained where and why, Mohammed notes that the reasons for the immigrants' detention were not immediately clear and sometimes had dire consequences.\n",
      "\t Tokenized GPT2:While ĠA IL A Ġhas Ġjoined Ġthe ĠACL U Ġand Ġother Ġorganizations Ġin Ġa ĠFreedom Ġof ĠInformation ĠAct Ġrequest Ġto Ġfind Ġout Ġwho Ġis Ġbeing Ġdet ained Ġwhere Ġand Ġwhy , ĠMoh ammed Ġnotes Ġthat Ġthe Ġreasons Ġfor Ġthe Ġimmigrants ' Ġdetention Ġwere Ġnot Ġimmediately Ġclear Ġand Ġsometimes Ġhad Ġdire Ġconsequences .\n",
      "\t Tokenized LLAMA3:While ĠA IL A Ġhas Ġjoined Ġthe ĠACL U Ġand Ġother Ġorganizations Ġin Ġa ĠFreedom Ġof ĠInformation ĠAct Ġrequest Ġto Ġfind Ġout Ġwho Ġis Ġbeing Ġdet ained Ġwhere Ġand Ġwhy , ĠMoh ammed Ġnotes Ġthat Ġthe Ġreasons Ġfor Ġthe Ġimmigrants ' Ġdetention Ġwere Ġnot Ġimmediately Ġclear Ġand Ġsometimes Ġhad Ġdire Ġconsequences .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The EPA joined the ACLU in requesting the information.\n",
      "\t Tokenized GPT2:The ĠE PA Ġjoined Ġthe ĠACL U Ġin Ġrequesting Ġthe Ġinformation .\n",
      "\t Tokenized LLAMA3:The ĠE PA Ġjoined Ġthe ĠACL U Ġin Ġrequesting Ġthe Ġinformation .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: HCFA published a Notice of Proposed Rulemaking on March 28, 1997 (62 Fed.\n",
      "\t Tokenized GPT2:HC FA Ġpublished Ġa ĠNotice Ġof ĠPro posed ĠRule making Ġon ĠMarch Ġ28 , Ġ1997 Ġ( 62 ĠFed .\n",
      "\t Tokenized LLAMA3:HC FA Ġpublished Ġa ĠNotice Ġof ĠPro posed ĠRule making Ġon ĠMarch Ġ 28 , Ġ 199 7 Ġ( 62 ĠFed .\n",
      "\t Unique Tokens GPT2: {'Ġ1997', 'Ġ28'}\n",
      "\t Unique Tokens LLAMA3: {'199', '7', '28', 'Ġ'}\n",
      "Text 2: HCFA decided to keep it a secret when they proposed rules.\n",
      "\t Tokenized GPT2:HC FA Ġdecided Ġto Ġkeep Ġit Ġa Ġsecret Ġwhen Ġthey Ġproposed Ġrules .\n",
      "\t Tokenized LLAMA3:HC FA Ġdecided Ġto Ġkeep Ġit Ġa Ġsecret Ġwhen Ġthey Ġproposed Ġrules .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: evaluation questions.\n",
      "\t Tokenized GPT2:e valuation Ġquestions .\n",
      "\t Tokenized LLAMA3:e valuation Ġquestions .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: There are evaluation questions on the topic.\n",
      "\t Tokenized GPT2:There Ġare Ġevaluation Ġquestions Ġon Ġthe Ġtopic .\n",
      "\t Tokenized LLAMA3:There Ġare Ġevaluation Ġquestions Ġon Ġthe Ġtopic .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: oh yeah IBM uh i mean uh a lot of people use human factors folks but IBM is what i'm looking at right now\n",
      "\t Tokenized GPT2:oh Ġyeah ĠIBM Ġuh Ġi Ġmean Ġuh Ġa Ġlot Ġof Ġpeople Ġuse Ġhuman Ġfactors Ġfolks Ġbut ĠIBM Ġis Ġwhat Ġi 'm Ġlooking Ġat Ġright Ġnow\n",
      "\t Tokenized LLAMA3:oh Ġyeah ĠIBM Ġuh Ġi Ġmean Ġuh Ġa Ġlot Ġof Ġpeople Ġuse Ġhuman Ġfactors Ġfolks Ġbut ĠIBM Ġis Ġwhat Ġi 'm Ġlooking Ġat Ġright Ġnow\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I'm looking at IBM right now but I've looked at tons of other things.\n",
      "\t Tokenized GPT2:I 'm Ġlooking Ġat ĠIBM Ġright Ġnow Ġbut ĠI 've Ġlooked Ġat Ġtons Ġof Ġother Ġthings .\n",
      "\t Tokenized LLAMA3:I 'm Ġlooking Ġat ĠIBM Ġright Ġnow Ġbut ĠI 've Ġlooked Ġat Ġtons Ġof Ġother Ġthings .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Pro-Microsoft analysts spin this as a heroic sacrifice, removing the lightning rod whose seemingly disingenuous testimony has ostensibly driven the DOJ to the verge of demanding the company's breakup.\n",
      "\t Tokenized GPT2:Pro - Microsoft Ġanalysts Ġspin Ġthis Ġas Ġa Ġheroic Ġsacrifice , Ġremoving Ġthe Ġlightning Ġrod Ġwhose Ġseemingly Ġdis ing enu ous Ġtestimony Ġhas Ġo st ens ibly Ġdriven Ġthe ĠDO J Ġto Ġthe Ġverge Ġof Ġdemanding Ġthe Ġcompany 's Ġbreak up .\n",
      "\t Tokenized LLAMA3:Pro -M icrosoft Ġanalysts Ġspin Ġthis Ġas Ġa Ġheroic Ġsacrifice , Ġremoving Ġthe Ġlightning Ġrod Ġwhose Ġseemingly Ġdis ing enu ous Ġtestimony Ġhas Ġo st ens ibly Ġdriven Ġthe ĠDO J Ġto Ġthe Ġverge Ġof Ġdemanding Ġthe Ġcompany 's Ġbreak up .\n",
      "\t Unique Tokens GPT2: {'-', 'Microsoft'}\n",
      "\t Unique Tokens LLAMA3: {'-M', 'icrosoft'}\n",
      "Text 2: Pro-Microsoft analysts say that was a sacrifice for the company, risking their future.\n",
      "\t Tokenized GPT2:Pro - Microsoft Ġanalysts Ġsay Ġthat Ġwas Ġa Ġsacrifice Ġfor Ġthe Ġcompany , Ġrisk ing Ġtheir Ġfuture .\n",
      "\t Tokenized LLAMA3:Pro -M icrosoft Ġanalysts Ġsay Ġthat Ġwas Ġa Ġsacrifice Ġfor Ġthe Ġcompany , Ġrisk ing Ġtheir Ġfuture .\n",
      "\t Unique Tokens GPT2: {'-', 'Microsoft'}\n",
      "\t Unique Tokens LLAMA3: {'-M', 'icrosoft'}\n",
      "==entailment==\n",
      "Text 1: someone else noticed it and i said well i guess that's true and it was somewhat melodio us in other words it wasn't just you know it was really funny\n",
      "\t Tokenized GPT2:someone Ġelse Ġnoticed Ġit Ġand Ġi Ġsaid Ġwell Ġi Ġguess Ġthat 's Ġtrue Ġand Ġit Ġwas Ġsomewhat Ġmel od io Ġus Ġin Ġother Ġwords Ġit Ġwasn 't Ġjust Ġyou Ġknow Ġit Ġwas Ġreally Ġfunny\n",
      "\t Tokenized LLAMA3:someone Ġelse Ġnoticed Ġit Ġand Ġi Ġsaid Ġwell Ġi Ġguess Ġthat 's Ġtrue Ġand Ġit Ġwas Ġsomewhat Ġmel od io Ġus Ġin Ġother Ġwords Ġit Ġwasn 't Ġjust Ġyou Ġknow Ġit Ġwas Ġreally Ġfunny\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Someone else paid attention to it and it was really funny. \n",
      "\t Tokenized GPT2:Someone Ġelse Ġpaid Ġattention Ġto Ġit Ġand Ġit Ġwas Ġreally Ġfunny . Ġ\n",
      "\t Tokenized LLAMA3:Someone Ġelse Ġpaid Ġattention Ġto Ġit Ġand Ġit Ġwas Ġreally Ġfunny . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The mansions have been downgraded to consulates since the capital was transferred to Ankara in 1923, and modern shops and restaurants have sprung up.\n",
      "\t Tokenized GPT2:The Ġmans ions Ġhave Ġbeen Ġdown grad ed Ġto Ġcons ulates Ġsince Ġthe Ġcapital Ġwas Ġtransferred Ġto ĠAn k ara Ġin Ġ19 23 , Ġand Ġmodern Ġshops Ġand Ġrestaurants Ġhave Ġspr ung Ġup .\n",
      "\t Tokenized LLAMA3:The Ġmans ions Ġhave Ġbeen Ġdown grad ed Ġto Ġcons ulates Ġsince Ġthe Ġcapital Ġwas Ġtransferred Ġto ĠAn k ara Ġin Ġ 192 3 , Ġand Ġmodern Ġshops Ġand Ġrestaurants Ġhave Ġspr ung Ġup .\n",
      "\t Unique Tokens GPT2: {'Ġ19', '23'}\n",
      "\t Unique Tokens LLAMA3: {'3', '192', 'Ġ'}\n",
      "Text 2: The city of Ankara has always been the capital of the nation.\n",
      "\t Tokenized GPT2:The Ġcity Ġof ĠAn k ara Ġhas Ġalways Ġbeen Ġthe Ġcapital Ġof Ġthe Ġnation .\n",
      "\t Tokenized LLAMA3:The Ġcity Ġof ĠAn k ara Ġhas Ġalways Ġbeen Ġthe Ġcapital Ġof Ġthe Ġnation .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Still, it would be interesting to know. 109 Poirot looked at me very earnestly, and again shook his head. \n",
      "\t Tokenized GPT2:Still , Ġit Ġwould Ġbe Ġinteresting Ġto Ġknow . Ġ109 ĠP oir ot Ġlooked Ġat Ġme Ġvery Ġearnest ly , Ġand Ġagain Ġshook Ġhis Ġhead . Ġ\n",
      "\t Tokenized LLAMA3:Still , Ġit Ġwould Ġbe Ġinteresting Ġto Ġknow . Ġ 109 ĠP oir ot Ġlooked Ġat Ġme Ġvery Ġearnest ly , Ġand Ġagain Ġshook Ġhis Ġhead . Ġ\n",
      "\t Unique Tokens GPT2: {'Ġ109'}\n",
      "\t Unique Tokens LLAMA3: {'109'}\n",
      "Text 2: Poirot was disappointed with me.\n",
      "\t Tokenized GPT2:P oir ot Ġwas Ġdisappointed Ġwith Ġme .\n",
      "\t Tokenized LLAMA3:P oir ot Ġwas Ġdisappointed Ġwith Ġme .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Also downtown is the Flower Market, on Wall and 8th streets; fresh-cut flowers and a variety of plants can be had for bargain prices, but the best selections are found before dawn.\n",
      "\t Tokenized GPT2:Also Ġdowntown Ġis Ġthe ĠFl ower ĠMarket , Ġon ĠWall Ġand Ġ8 th Ġstreets ; Ġfresh - cut Ġflowers Ġand Ġa Ġvariety Ġof Ġplants Ġcan Ġbe Ġhad Ġfor Ġbargain Ġprices , Ġbut Ġthe Ġbest Ġselections Ġare Ġfound Ġbefore Ġdawn .\n",
      "\t Tokenized LLAMA3:Also Ġdowntown Ġis Ġthe ĠFl ower ĠMarket , Ġon ĠWall Ġand Ġ 8 th Ġstreets ; Ġfresh -cut Ġflowers Ġand Ġa Ġvariety Ġof Ġplants Ġcan Ġbe Ġhad Ġfor Ġbargain Ġprices , Ġbut Ġthe Ġbest Ġse lections Ġare Ġfound Ġbefore Ġdawn .\n",
      "\t Unique Tokens GPT2: {'-', 'Ġselections', 'cut', 'Ġ8'}\n",
      "\t Unique Tokens LLAMA3: {'-cut', 'lections', 'Ġ', '8', 'Ġse'}\n",
      "Text 2: Fresh-cut flowers available at the market range from cheap to expensive.\n",
      "\t Tokenized GPT2:F resh - cut Ġflowers Ġavailable Ġat Ġthe Ġmarket Ġrange Ġfrom Ġcheap Ġto Ġexpensive .\n",
      "\t Tokenized LLAMA3:F resh -cut Ġflowers Ġavailable Ġat Ġthe Ġmarket Ġrange Ġfrom Ġcheap Ġto Ġexpensive .\n",
      "\t Unique Tokens GPT2: {'-', 'cut'}\n",
      "\t Unique Tokens LLAMA3: {'-cut'}\n",
      "==contradiction==\n",
      "Text 1: you know like CODA comes out of your out of your pay and the credit union comes out of your pay so we don't have to do anything there and the rest of it as far as my salary goes i just have it automatically deposited in into our bank\n",
      "\t Tokenized GPT2:you Ġknow Ġlike ĠC OD A Ġcomes Ġout Ġof Ġyour Ġout Ġof Ġyour Ġpay Ġand Ġthe Ġcredit Ġunion Ġcomes Ġout Ġof Ġyour Ġpay Ġso Ġwe Ġdon 't Ġhave Ġto Ġdo Ġanything Ġthere Ġand Ġthe Ġrest Ġof Ġit Ġas Ġfar Ġas Ġmy Ġsalary Ġgoes Ġi Ġjust Ġhave Ġit Ġautomatically Ġdeposited Ġin Ġinto Ġour Ġbank\n",
      "\t Tokenized LLAMA3:you Ġknow Ġlike ĠC OD A Ġcomes Ġout Ġof Ġyour Ġout Ġof Ġyour Ġpay Ġand Ġthe Ġcredit Ġunion Ġcomes Ġout Ġof Ġyour Ġpay Ġso Ġwe Ġdon 't Ġhave Ġto Ġdo Ġanything Ġthere Ġand Ġthe Ġrest Ġof Ġit Ġas Ġfar Ġas Ġmy Ġsalary Ġgoes Ġi Ġjust Ġhave Ġit Ġautomatically Ġdeposited Ġin Ġinto Ġour Ġbank\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: After CODA and credit union, nothing is left of my salary.\n",
      "\t Tokenized GPT2:After ĠC OD A Ġand Ġcredit Ġunion , Ġnothing Ġis Ġleft Ġof Ġmy Ġsalary .\n",
      "\t Tokenized LLAMA3:After ĠC OD A Ġand Ġcredit Ġunion , Ġnothing Ġis Ġleft Ġof Ġmy Ġsalary .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The spot does leave the viewer wondering about the rest of the story, and what tale the condom could tell.\n",
      "\t Tokenized GPT2:The Ġspot Ġdoes Ġleave Ġthe Ġviewer Ġwondering Ġabout Ġthe Ġrest Ġof Ġthe Ġstory , Ġand Ġwhat Ġtale Ġthe Ġcond om Ġcould Ġtell .\n",
      "\t Tokenized LLAMA3:The Ġspot Ġdoes Ġleave Ġthe Ġviewer Ġwondering Ġabout Ġthe Ġrest Ġof Ġthe Ġstory , Ġand Ġwhat Ġtale Ġthe Ġcond om Ġcould Ġtell .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The spot resolves the storyline neatly for viewers, especially regarding the condom.\n",
      "\t Tokenized GPT2:The Ġspot Ġres olves Ġthe Ġstoryline Ġneatly Ġfor Ġviewers , Ġespecially Ġregarding Ġthe Ġcond om .\n",
      "\t Tokenized LLAMA3:The Ġspot Ġres olves Ġthe Ġstoryline Ġneatly Ġfor Ġviewers , Ġespecially Ġregarding Ġthe Ġcond om .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: and uh you know once you start up at the top and try to get those dollars on down to the hands that need them you know there's a lot of places the money stops and disappears along the way\n",
      "\t Tokenized GPT2:and Ġuh Ġyou Ġknow Ġonce Ġyou Ġstart Ġup Ġat Ġthe Ġtop Ġand Ġtry Ġto Ġget Ġthose Ġdollars Ġon Ġdown Ġto Ġthe Ġhands Ġthat Ġneed Ġthem Ġyou Ġknow Ġthere 's Ġa Ġlot Ġof Ġplaces Ġthe Ġmoney Ġstops Ġand Ġdisappears Ġalong Ġthe Ġway\n",
      "\t Tokenized LLAMA3:and Ġuh Ġyou Ġknow Ġonce Ġyou Ġstart Ġup Ġat Ġthe Ġtop Ġand Ġtry Ġto Ġget Ġthose Ġdollars Ġon Ġdown Ġto Ġthe Ġhands Ġthat Ġneed Ġthem Ġyou Ġknow Ġthere 's Ġa Ġlot Ġof Ġplaces Ġthe Ġmoney Ġstops Ġand Ġdisappears Ġalong Ġthe Ġway\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: All the money always gets into the hands of those who need it.\n",
      "\t Tokenized GPT2:All Ġthe Ġmoney Ġalways Ġgets Ġinto Ġthe Ġhands Ġof Ġthose Ġwho Ġneed Ġit .\n",
      "\t Tokenized LLAMA3:All Ġthe Ġmoney Ġalways Ġgets Ġinto Ġthe Ġhands Ġof Ġthose Ġwho Ġneed Ġit .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: yeah i've i wish they'd split that bowling season up into uh three seasons\n",
      "\t Tokenized GPT2:yeah Ġi 've Ġi Ġwish Ġthey 'd Ġsplit Ġthat Ġbow ling Ġseason Ġup Ġinto Ġuh Ġthree Ġseasons\n",
      "\t Tokenized LLAMA3:yeah Ġi 've Ġi Ġwish Ġthey 'd Ġsplit Ġthat Ġbow ling Ġseason Ġup Ġinto Ġuh Ġthree Ġseasons\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I am happy with the bowling season schedule as is.\n",
      "\t Tokenized GPT2:I Ġam Ġhappy Ġwith Ġthe Ġbow ling Ġseason Ġschedule Ġas Ġis .\n",
      "\t Tokenized LLAMA3:I Ġam Ġhappy Ġwith Ġthe Ġbow ling Ġseason Ġschedule Ġas Ġis .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Some travelers add Molokai and Lanai to their itineraries.\n",
      "\t Tokenized GPT2:Some Ġtravelers Ġadd ĠMol ok ai Ġand ĠLan ai Ġto Ġtheir Ġit iner aries .\n",
      "\t Tokenized LLAMA3:Some Ġtravel ers Ġadd ĠMol ok ai Ġand ĠLan ai Ġto Ġtheir Ġit iner aries .\n",
      "\t Unique Tokens GPT2: {'Ġtravelers'}\n",
      "\t Unique Tokens LLAMA3: {'Ġtravel', 'ers'}\n",
      "Text 2: Several tourists decide to plan for traveling to Molokai and Lanai.\n",
      "\t Tokenized GPT2:Several Ġtourists Ġdecide Ġto Ġplan Ġfor Ġtraveling Ġto ĠMol ok ai Ġand ĠLan ai .\n",
      "\t Tokenized LLAMA3:Several Ġtourists Ġdecide Ġto Ġplan Ġfor Ġtraveling Ġto ĠMol ok ai Ġand ĠLan ai .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: TIG funds support the Technology Evaluation Project, an initiative of the Legal Aid Society of Cincinnati.\n",
      "\t Tokenized GPT2:T IG Ġfunds Ġsupport Ġthe ĠTechnology ĠE valuation ĠProject , Ġan Ġinitiative Ġof Ġthe ĠLe gal ĠA id ĠSociety Ġof ĠC incinnati .\n",
      "\t Tokenized LLAMA3:T IG Ġfunds Ġsupport Ġthe ĠTechnology ĠE valuation ĠProject , Ġan Ġinitiative Ġof Ġthe ĠLe gal ĠA id ĠSociety Ġof ĠC incinnati .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: TIG funds are used to support the Technology Evolution project, a legal aid society in Cincinnati. \n",
      "\t Tokenized GPT2:T IG Ġfunds Ġare Ġused Ġto Ġsupport Ġthe ĠTechnology ĠEvolution Ġproject , Ġa Ġlegal Ġaid Ġsociety Ġin ĠC incinnati . Ġ\n",
      "\t Tokenized LLAMA3:T IG Ġfunds Ġare Ġused Ġto Ġsupport Ġthe ĠTechnology ĠEvolution Ġproject , Ġa Ġlegal Ġaid Ġsociety Ġin ĠC incinnati . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: oh i enjoyed it i mean it was just more for my money\n",
      "\t Tokenized GPT2:oh Ġi Ġenjoyed Ġit Ġi Ġmean Ġit Ġwas Ġjust Ġmore Ġfor Ġmy Ġmoney\n",
      "\t Tokenized LLAMA3:oh Ġi Ġenjoyed Ġit Ġi Ġmean Ġit Ġwas Ġjust Ġmore Ġfor Ġmy Ġmoney\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It was worth the money for the time.\n",
      "\t Tokenized GPT2:It Ġwas Ġworth Ġthe Ġmoney Ġfor Ġthe Ġtime .\n",
      "\t Tokenized LLAMA3:It Ġwas Ġworth Ġthe Ġmoney Ġfor Ġthe Ġtime .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: His mother died when he was young, and he was adopted by the Brodkeys.\n",
      "\t Tokenized GPT2:His Ġmother Ġdied Ġwhen Ġhe Ġwas Ġyoung , Ġand Ġhe Ġwas Ġadopted Ġby Ġthe ĠBro d keys .\n",
      "\t Tokenized LLAMA3:His Ġmother Ġdied Ġwhen Ġhe Ġwas Ġyoung , Ġand Ġhe Ġwas Ġadopted Ġby Ġthe ĠBro d keys .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He changed his name to Brodkey when he was adopted.\n",
      "\t Tokenized GPT2:He Ġchanged Ġhis Ġname Ġto ĠBro d key Ġwhen Ġhe Ġwas Ġadopted .\n",
      "\t Tokenized LLAMA3:He Ġchanged Ġhis Ġname Ġto ĠBro d key Ġwhen Ġhe Ġwas Ġadopted .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: 4 million homes watch the evening news on CBS, ABC, and NBC.\n",
      "\t Tokenized GPT2:4 Ġmillion Ġhomes Ġwatch Ġthe Ġevening Ġnews Ġon ĠCBS , ĠABC , Ġand ĠNBC .\n",
      "\t Tokenized LLAMA3:4 Ġmillion Ġhomes Ġwatch Ġthe Ġevening Ġnews Ġon ĠCBS , ĠABC , Ġand ĠNBC .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: CBS, ABC and NBC are the leaders in news. \n",
      "\t Tokenized GPT2:C BS , ĠABC Ġand ĠNBC Ġare Ġthe Ġleaders Ġin Ġnews . Ġ\n",
      "\t Tokenized LLAMA3:C BS , ĠABC Ġand ĠNBC Ġare Ġthe Ġleaders Ġin Ġnews . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: And he claimed she earned $11,000 a month - or $132,000 a year - from a home quilting business she had owned for 22 years.\n",
      "\t Tokenized GPT2:And Ġhe Ġclaimed Ġshe Ġearned Ġ$ 11 , 000 Ġa Ġmonth Ġ- Ġor Ġ$ 132 , 000 Ġa Ġyear Ġ- Ġfrom Ġa Ġhome Ġqu il ting Ġbusiness Ġshe Ġhad Ġowned Ġfor Ġ22 Ġyears .\n",
      "\t Tokenized LLAMA3:And Ġhe Ġclaimed Ġshe Ġearned Ġ$ 11 , 000 Ġa Ġmonth Ġ- Ġor Ġ$ 132 , 000 Ġa Ġyear Ġ- Ġfrom Ġa Ġhome Ġqu il ting Ġbusiness Ġshe Ġhad Ġowned Ġfor Ġ 22 Ġyears .\n",
      "\t Unique Tokens GPT2: {'Ġ22'}\n",
      "\t Unique Tokens LLAMA3: {'22', 'Ġ'}\n",
      "Text 2: He said she had a business that she started 10 years ago.\n",
      "\t Tokenized GPT2:He Ġsaid Ġshe Ġhad Ġa Ġbusiness Ġthat Ġshe Ġstarted Ġ10 Ġyears Ġago .\n",
      "\t Tokenized LLAMA3:He Ġsaid Ġshe Ġhad Ġa Ġbusiness Ġthat Ġshe Ġstarted Ġ 10 Ġyears Ġago .\n",
      "\t Unique Tokens GPT2: {'Ġ10'}\n",
      "\t Unique Tokens LLAMA3: {'10', 'Ġ'}\n",
      "==neutral==\n",
      "Text 1: yeah well Rochester's like right on the shores isn't it\n",
      "\t Tokenized GPT2:yeah Ġwell ĠRoche ster 's Ġlike Ġright Ġon Ġthe Ġsh ores Ġisn 't Ġit\n",
      "\t Tokenized LLAMA3:yeah Ġwell ĠR oche ster 's Ġlike Ġright Ġon Ġthe Ġsh ores Ġisn 't Ġit\n",
      "\t Unique Tokens GPT2: {'ĠRoche'}\n",
      "\t Unique Tokens LLAMA3: {'oche', 'ĠR'}\n",
      "Text 2: Rochester is right on the shores of the great lakes.\n",
      "\t Tokenized GPT2:R oche ster Ġis Ġright Ġon Ġthe Ġsh ores Ġof Ġthe Ġgreat Ġlakes .\n",
      "\t Tokenized LLAMA3:R oche ster Ġis Ġright Ġon Ġthe Ġsh ores Ġof Ġthe Ġgreat Ġl akes .\n",
      "\t Unique Tokens GPT2: {'Ġlakes'}\n",
      "\t Unique Tokens LLAMA3: {'akes', 'Ġl'}\n",
      "==entailment==\n",
      "Text 1: yeah that's probably a  a little bit under what it is for this time of year i i think i haven't seen the weather the news the weather on the news in the evening lately but i think the average high would be it should be about seventy\n",
      "\t Tokenized GPT2:yeah Ġthat 's Ġprobably Ġa Ġ Ġa Ġlittle Ġbit Ġunder Ġwhat Ġit Ġis Ġfor Ġthis Ġtime Ġof Ġyear Ġi Ġi Ġthink Ġi Ġhaven 't Ġseen Ġthe Ġweather Ġthe Ġnews Ġthe Ġweather Ġon Ġthe Ġnews Ġin Ġthe Ġevening Ġlately Ġbut Ġi Ġthink Ġthe Ġaverage Ġhigh Ġwould Ġbe Ġit Ġshould Ġbe Ġabout Ġsevent y\n",
      "\t Tokenized LLAMA3:yeah Ġthat 's Ġprobably Ġa Ġ Ġa Ġlittle Ġbit Ġunder Ġwhat Ġit Ġis Ġfor Ġthis Ġtime Ġof Ġyear Ġi Ġi Ġthink Ġi Ġhaven 't Ġseen Ġthe Ġweather Ġthe Ġnews Ġthe Ġweather Ġon Ġthe Ġnews Ġin Ġthe Ġevening Ġlately Ġbut Ġi Ġthink Ġthe Ġaverage Ġhigh Ġwould Ġbe Ġit Ġshould Ġbe Ġabout Ġsevent y\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I have not viewed the weather lately on the evening news.\n",
      "\t Tokenized GPT2:I Ġhave Ġnot Ġviewed Ġthe Ġweather Ġlately Ġon Ġthe Ġevening Ġnews .\n",
      "\t Tokenized LLAMA3:I Ġhave Ġnot Ġviewed Ġthe Ġweather Ġlately Ġon Ġthe Ġevening Ġnews .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: uh wasn't that Jane Eyre no he wrote Jane Eyre too\n",
      "\t Tokenized GPT2:uh Ġwasn 't Ġthat ĠJane ĠE y re Ġno Ġhe Ġwrote ĠJane ĠE y re Ġtoo\n",
      "\t Tokenized LLAMA3:uh Ġwasn 't Ġthat ĠJane ĠE y re Ġno Ġhe Ġwrote ĠJane ĠE y re Ġtoo\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He did not write Jane Eyre or any other book.\n",
      "\t Tokenized GPT2:He Ġdid Ġnot Ġwrite ĠJane ĠE y re Ġor Ġany Ġother Ġbook .\n",
      "\t Tokenized LLAMA3:He Ġdid Ġnot Ġwrite ĠJane ĠE y re Ġor Ġany Ġother Ġbook .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Hardly catering to locals, Universal Citys Cityalk attempts to snag tourist dollars with its extensive collection of retail wonders, including magic shops, toy stores, sports shops, and a host of science fiction memorabilia.\n",
      "\t Tokenized GPT2:Hard ly Ġcater ing Ġto Ġlocals , ĠUniversal ĠCity s ĠCity alk Ġattempts Ġto Ġsn ag Ġtourist Ġdollars Ġwith Ġits Ġextensive Ġcollection Ġof Ġretail Ġwonders , Ġincluding Ġmagic Ġshops , Ġtoy Ġstores , Ġsports Ġshops , Ġand Ġa Ġhost Ġof Ġscience Ġfiction Ġmemor abil ia .\n",
      "\t Tokenized LLAMA3:Hard ly Ġcater ing Ġto Ġlocals , ĠUniversal ĠCity s ĠCity alk Ġattempts Ġto Ġsn ag Ġtourist Ġdollars Ġwith Ġits Ġextensive Ġcollection Ġof Ġretail Ġwonders , Ġincluding Ġmagic Ġshops , Ġtoy Ġstores , Ġsports Ġshops , Ġand Ġa Ġhost Ġof Ġscience Ġfiction Ġmemor abil ia .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Locals aren't the primary target of the many magic shops, toy stores, and sports shops.\n",
      "\t Tokenized GPT2:Loc als Ġaren 't Ġthe Ġprimary Ġtarget Ġof Ġthe Ġmany Ġmagic Ġshops , Ġtoy Ġstores , Ġand Ġsports Ġshops .\n",
      "\t Tokenized LLAMA3:Loc als Ġaren 't Ġthe Ġprimary Ġtarget Ġof Ġthe Ġmany Ġmagic Ġshops , Ġtoy Ġstores , Ġand Ġsports Ġshops .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Normally, these discussions are kept secret.\n",
      "\t Tokenized GPT2:Norm ally , Ġthese Ġdiscussions Ġare Ġkept Ġsecret .\n",
      "\t Tokenized LLAMA3:Norm ally , Ġthese Ġdiscussions Ġare Ġkept Ġsecret .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: In usual circumstances, what is said is not to be shared..\n",
      "\t Tokenized GPT2:In Ġusual Ġcircumstances , Ġwhat Ġis Ġsaid Ġis Ġnot Ġto Ġbe Ġshared ..\n",
      "\t Tokenized LLAMA3:In Ġusual Ġcircumstances , Ġwhat Ġis Ġsaid Ġis Ġnot Ġto Ġbe Ġshared ..\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Participate in the postaward audit for assessing thedegree of success of the acquisition.\n",
      "\t Tokenized GPT2:Part icip ate Ġin Ġthe Ġpost a ward Ġaudit Ġfor Ġassessing Ġthe degree Ġof Ġsuccess Ġof Ġthe Ġacquisition .\n",
      "\t Tokenized LLAMA3:Part icip ate Ġin Ġthe Ġpost a ward Ġaudit Ġfor Ġassessing Ġthe de gree Ġof Ġsuccess Ġof Ġthe Ġacquisition .\n",
      "\t Unique Tokens GPT2: {'degree'}\n",
      "\t Unique Tokens LLAMA3: {'de', 'gree'}\n",
      "Text 2: An audit after the award has been presented.\n",
      "\t Tokenized GPT2:An Ġaudit Ġafter Ġthe Ġaward Ġhas Ġbeen Ġpresented .\n",
      "\t Tokenized LLAMA3:An Ġaudit Ġafter Ġthe Ġaward Ġhas Ġbeen Ġpresented .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: He fled in his car when cops arrived and led them on a chase that ended in the massive crash.\n",
      "\t Tokenized GPT2:He Ġfled Ġin Ġhis Ġcar Ġwhen Ġcops Ġarrived Ġand Ġled Ġthem Ġon Ġa Ġchase Ġthat Ġended Ġin Ġthe Ġmassive Ġcrash .\n",
      "\t Tokenized LLAMA3:He Ġfled Ġin Ġhis Ġcar Ġwhen Ġcops Ġarrived Ġand Ġled Ġthem Ġon Ġa Ġchase Ġthat Ġended Ġin Ġthe Ġmassive Ġcrash .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The cops set off in pursuit until a traffic accident happened.\n",
      "\t Tokenized GPT2:The Ġcops Ġset Ġoff Ġin Ġpursuit Ġuntil Ġa Ġtraffic Ġaccident Ġhappened .\n",
      "\t Tokenized LLAMA3:The Ġcops Ġset Ġoff Ġin Ġpursuit Ġuntil Ġa Ġtraffic Ġaccident Ġhappened .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The National Football League semifinals are set.\n",
      "\t Tokenized GPT2:The ĠNational ĠFootball ĠLeague Ġsem if inals Ġare Ġset .\n",
      "\t Tokenized LLAMA3:The ĠNational ĠFootball ĠLeague Ġsem if inals Ġare Ġset .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: They were unable to disclose when the dates would be set.\n",
      "\t Tokenized GPT2:They Ġwere Ġunable Ġto Ġdisclose Ġwhen Ġthe Ġdates Ġwould Ġbe Ġset .\n",
      "\t Tokenized LLAMA3:They Ġwere Ġunable Ġto Ġdisclose Ġwhen Ġthe Ġdates Ġwould Ġbe Ġset .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1:  Most menu prices include taxes and a service charge, but it's customary to leave a tip if you were served satisfactorily.\n",
      "\t Tokenized GPT2:ĠMost Ġmenu Ġprices Ġinclude Ġtaxes Ġand Ġa Ġservice Ġcharge , Ġbut Ġit 's Ġcustom ary Ġto Ġleave Ġa Ġtip Ġif Ġyou Ġwere Ġserved Ġsatisf actor ily .\n",
      "\t Tokenized LLAMA3:ĠMost Ġmenu Ġprices Ġinclude Ġtaxes Ġand Ġa Ġservice Ġcharge , Ġbut Ġit 's Ġcustom ary Ġto Ġleave Ġa Ġtip Ġif Ġyou Ġwere Ġserved Ġsatisf actor ily .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Tips are not accepted at most restaurants, as there is already a sales tax.\n",
      "\t Tokenized GPT2:T ips Ġare Ġnot Ġaccepted Ġat Ġmost Ġrestaurants , Ġas Ġthere Ġis Ġalready Ġa Ġsales Ġtax .\n",
      "\t Tokenized LLAMA3:T ips Ġare Ġnot Ġaccepted Ġat Ġmost Ġrestaurants , Ġas Ġthere Ġis Ġalready Ġa Ġsales Ġtax .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: But they also don't seem to mind when the tranquillity of a Zen temple rock garden is shattered by recorded announcements blaring from loudspeakers parroting the information already contained in the leaflets provided at the ticket office; when heavy-metal pop music loudly emanates from the radio of \n",
      "\t Tokenized GPT2:But Ġthey Ġalso Ġdon 't Ġseem Ġto Ġmind Ġwhen Ġthe Ġtran qu ill ity Ġof Ġa ĠZen Ġtemple Ġrock Ġgarden Ġis Ġshattered Ġby Ġrecorded Ġannouncements Ġbl aring Ġfrom Ġloud spe akers Ġpar r oting Ġthe Ġinformation Ġalready Ġcontained Ġin Ġthe Ġleaf lets Ġprovided Ġat Ġthe Ġticket Ġoffice ; Ġwhen Ġheavy - met al Ġpop Ġmusic Ġloudly Ġem an ates Ġfrom Ġthe Ġradio Ġof Ġ\n",
      "\t Tokenized LLAMA3:But Ġthey Ġalso Ġdon 't Ġseem Ġto Ġmind Ġwhen Ġthe Ġtran qu ill ity Ġof Ġa ĠZen Ġtemple Ġrock Ġgarden Ġis Ġshattered Ġby Ġrecorded Ġannounce ments Ġbl aring Ġfrom Ġloud spe akers Ġpar r oting Ġthe Ġinformation Ġalready Ġcontained Ġin Ġthe Ġleaf lets Ġprovided Ġat Ġthe Ġticket Ġoffice ; Ġwhen Ġheavy -m etal Ġpop Ġmusic Ġloudly Ġem an ates Ġfrom Ġthe Ġradio Ġof Ġ\n",
      "\t Unique Tokens GPT2: {'al', 'Ġannouncements', 'met'}\n",
      "\t Unique Tokens LLAMA3: {'etal', 'Ġannounce', 'ments', '-m'}\n",
      "Text 2: A Zen temple rock garden is a zen place.\n",
      "\t Tokenized GPT2:A ĠZen Ġtemple Ġrock Ġgarden Ġis Ġa Ġz en Ġplace .\n",
      "\t Tokenized LLAMA3:A ĠZen Ġtemple Ġrock Ġgarden Ġis Ġa Ġz en Ġplace .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: She will step down from the court in December 2002.\n",
      "\t Tokenized GPT2:She Ġwill Ġstep Ġdown Ġfrom Ġthe Ġcourt Ġin ĠDecember Ġ2002 .\n",
      "\t Tokenized LLAMA3:She Ġwill Ġstep Ġdown Ġfrom Ġthe Ġcourt Ġin ĠDecember Ġ 200 2 .\n",
      "\t Unique Tokens GPT2: {'Ġ2002'}\n",
      "\t Unique Tokens LLAMA3: {'2', 'Ġ', '200'}\n",
      "Text 2: She's going to step down from the court in the winter of 2020.\n",
      "\t Tokenized GPT2:She 's Ġgoing Ġto Ġstep Ġdown Ġfrom Ġthe Ġcourt Ġin Ġthe Ġwinter Ġof Ġ2020 .\n",
      "\t Tokenized LLAMA3:She 's Ġgoing Ġto Ġstep Ġdown Ġfrom Ġthe Ġcourt Ġin Ġthe Ġwinter Ġof Ġ 202 0 .\n",
      "\t Unique Tokens GPT2: {'Ġ2020'}\n",
      "\t Unique Tokens LLAMA3: {'0', '202', 'Ġ'}\n",
      "==contradiction==\n",
      "Text 1: I took to him at once.\n",
      "\t Tokenized GPT2:I Ġtook Ġto Ġhim Ġat Ġonce .\n",
      "\t Tokenized LLAMA3:I Ġtook Ġto Ġhim Ġat Ġonce .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I was immediately repulsed by him, and still feel the same way about him. \n",
      "\t Tokenized GPT2:I Ġwas Ġimmediately Ġrep uls ed Ġby Ġhim , Ġand Ġstill Ġfeel Ġthe Ġsame Ġway Ġabout Ġhim . Ġ\n",
      "\t Tokenized LLAMA3:I Ġwas Ġimmediately Ġrep uls ed Ġby Ġhim , Ġand Ġstill Ġfeel Ġthe Ġsame Ġway Ġabout Ġhim . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Act Accounting the Great Management Reform Act\n",
      "\t Tokenized GPT2:Act ĠAccount ing Ġthe ĠGreat ĠManagement ĠRe form ĠAct\n",
      "\t Tokenized LLAMA3:Act ĠAccount ing Ġthe ĠGreat ĠManagement ĠRe form ĠAct\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Act accounting great management reform act \n",
      "\t Tokenized GPT2:Act Ġaccounting Ġgreat Ġmanagement Ġreform Ġact Ġ\n",
      "\t Tokenized LLAMA3:Act Ġaccounting Ġgreat Ġmanagement Ġreform Ġact Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: The importer pays duties that are required by law\n",
      "\t Tokenized GPT2:The Ġim porter Ġpays Ġduties Ġthat Ġare Ġrequired Ġby Ġlaw\n",
      "\t Tokenized LLAMA3:The Ġim porter Ġpays Ġduties Ġthat Ġare Ġrequired Ġby Ġlaw\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Importer pays taxed that law requires\n",
      "\t Tokenized GPT2:Im porter Ġpays Ġtax ed Ġthat Ġlaw Ġrequires\n",
      "\t Tokenized LLAMA3:Im porter Ġpays Ġtax ed Ġthat Ġlaw Ġrequires\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: The spear missed Vrenna by only a hand-span.\n",
      "\t Tokenized GPT2:The Ġspear Ġmissed ĠV ren na Ġby Ġonly Ġa Ġhand - span .\n",
      "\t Tokenized LLAMA3:The Ġspear Ġmissed ĠV ren na Ġby Ġonly Ġa Ġhand -s pan .\n",
      "\t Unique Tokens GPT2: {'-', 'span'}\n",
      "\t Unique Tokens LLAMA3: {'-s', 'pan'}\n",
      "Text 2: It was a short distance from the person to the weapon.\n",
      "\t Tokenized GPT2:It Ġwas Ġa Ġshort Ġdistance Ġfrom Ġthe Ġperson Ġto Ġthe Ġweapon .\n",
      "\t Tokenized LLAMA3:It Ġwas Ġa Ġshort Ġdistance Ġfrom Ġthe Ġperson Ġto Ġthe Ġweapon .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The strychnine had been found in a drawer in the prisoner's room. \n",
      "\t Tokenized GPT2:The Ġst ry chn ine Ġhad Ġbeen Ġfound Ġin Ġa Ġdrawer Ġin Ġthe Ġprisoner 's Ġroom . Ġ\n",
      "\t Tokenized LLAMA3:The Ġst ry chn ine Ġhad Ġbeen Ġfound Ġin Ġa Ġdrawer Ġin Ġthe Ġprisoner 's Ġroom . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The strychnine that was in the drawer was powdered. \n",
      "\t Tokenized GPT2:The Ġst ry chn ine Ġthat Ġwas Ġin Ġthe Ġdrawer Ġwas Ġpow dered . Ġ\n",
      "\t Tokenized LLAMA3:The Ġst ry chn ine Ġthat Ġwas Ġin Ġthe Ġdrawer Ġwas Ġpow dered . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Whether you drink beer or alcohol or not, a trip to Dublin isn't complete without a visit to some of its pubs don't miss this experience.\n",
      "\t Tokenized GPT2:Whether Ġyou Ġdrink Ġbeer Ġor Ġalcohol Ġor Ġnot , Ġa Ġtrip Ġto ĠDublin Ġisn 't Ġcomplete Ġwithout Ġa Ġvisit Ġto Ġsome Ġof Ġits Ġpub s Ġdon 't Ġmiss Ġthis Ġexperience .\n",
      "\t Tokenized LLAMA3:Whether Ġyou Ġdrink Ġbeer Ġor Ġalcohol Ġor Ġnot , Ġa Ġtrip Ġto ĠDublin Ġisn 't Ġcomplete Ġwithout Ġa Ġvisit Ġto Ġsome Ġof Ġits Ġpub s Ġdon 't Ġmiss Ġthis Ġexperience .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Dublin's pubs are beautiful and evocative, worth a trip even if you don't drink\n",
      "\t Tokenized GPT2:D ublin 's Ġpub s Ġare Ġbeautiful Ġand Ġev oc ative , Ġworth Ġa Ġtrip Ġeven Ġif Ġyou Ġdon 't Ġdrink\n",
      "\t Tokenized LLAMA3:D ublin 's Ġpub s Ġare Ġbeautiful Ġand Ġev oc ative , Ġworth Ġa Ġtrip Ġeven Ġif Ġyou Ġdon 't Ġdrink\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: While documenting the basis for judgments can be more difficult than documenting nonjudgmental information, overall the chain of evidence or audit trail techniques should not pose any greater difficulty for GAO evaluators than our documentation procedures for other evaluation methods.\n",
      "\t Tokenized GPT2:While Ġdocument ing Ġthe Ġbasis Ġfor Ġjud gments Ġcan Ġbe Ġmore Ġdifficult Ġthan Ġdocument ing Ġnon jud gment al Ġinformation , Ġoverall Ġthe Ġchain Ġof Ġevidence Ġor Ġaudit Ġtrail Ġtechniques Ġshould Ġnot Ġpose Ġany Ġgreater Ġdifficulty Ġfor ĠGA O Ġevalu ators Ġthan Ġour Ġdocumentation Ġprocedures Ġfor Ġother Ġevaluation Ġmethods .\n",
      "\t Tokenized LLAMA3:While Ġdocument ing Ġthe Ġbasis Ġfor Ġjud gments Ġcan Ġbe Ġmore Ġdifficult Ġthan Ġdocument ing Ġnon jud gment al Ġinformation , Ġoverall Ġthe Ġchain Ġof Ġevidence Ġor Ġaudit Ġtrail Ġtechniques Ġshould Ġnot Ġpose Ġany Ġgreater Ġdifficulty Ġfor ĠGA O Ġevalu ators Ġthan Ġour Ġdocumentation Ġprocedures Ġfor Ġother Ġevaluation Ġmethods .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: GAO evaluators are trained to analyze and document the chain of evidence.\n",
      "\t Tokenized GPT2:GA O Ġevalu ators Ġare Ġtrained Ġto Ġanalyze Ġand Ġdocument Ġthe Ġchain Ġof Ġevidence .\n",
      "\t Tokenized LLAMA3:GA O Ġevalu ators Ġare Ġtrained Ġto Ġanalyze Ġand Ġdocument Ġthe Ġchain Ġof Ġevidence .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: um-hum um-hum yeah well uh i can see you know it's it's it's it's kind of funny because we it seems like we loan money you know we money with strings attached and if the government changes and the country that we loan the money to um i can see why the might have a different attitude towards paying i\n",
      "\t Tokenized GPT2:um - hum Ġum - hum Ġyeah Ġwell Ġuh Ġi Ġcan Ġsee Ġyou Ġknow Ġit 's Ġit 's Ġit 's Ġit 's Ġkind Ġof Ġfunny Ġbecause Ġwe Ġit Ġseems Ġlike Ġwe Ġloan Ġmoney Ġyou Ġknow Ġwe Ġmoney Ġwith Ġstrings Ġattached Ġand Ġif Ġthe Ġgovernment Ġchanges Ġand Ġthe Ġcountry Ġthat Ġwe Ġloan Ġthe Ġmoney Ġto Ġum Ġi Ġcan Ġsee Ġwhy Ġthe Ġmight Ġhave Ġa Ġdifferent Ġattitude Ġtowards Ġpaying Ġi\n",
      "\t Tokenized LLAMA3:um -h um Ġum -h um Ġyeah Ġwell Ġuh Ġi Ġcan Ġsee Ġyou Ġknow Ġit 's Ġit 's Ġit 's Ġit 's Ġkind Ġof Ġfunny Ġbecause Ġwe Ġit Ġseems Ġlike Ġwe Ġloan Ġmoney Ġyou Ġknow Ġwe Ġmoney Ġwith Ġstrings Ġattached Ġand Ġif Ġthe Ġgovernment Ġchanges Ġand Ġthe Ġcountry Ġthat Ġwe Ġloan Ġthe Ġmoney Ġto Ġum Ġi Ġcan Ġsee Ġwhy Ġthe Ġmight Ġhave Ġa Ġdifferent Ġattitude Ġtowards Ġpaying Ġi\n",
      "\t Unique Tokens GPT2: {'-', 'hum'}\n",
      "\t Unique Tokens LLAMA3: {'-h'}\n",
      "Text 2: We loan a lot of money with strings attached and I feel bad about it.\n",
      "\t Tokenized GPT2:We Ġloan Ġa Ġlot Ġof Ġmoney Ġwith Ġstrings Ġattached Ġand ĠI Ġfeel Ġbad Ġabout Ġit .\n",
      "\t Tokenized LLAMA3:We Ġloan Ġa Ġlot Ġof Ġmoney Ġwith Ġstrings Ġattached Ġand ĠI Ġfeel Ġbad Ġabout Ġit .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: At the end of the Wars of Spanish, Austrian, and Polish Succession, the Austrians had taken over northern Italy from the Spanish.\n",
      "\t Tokenized GPT2:At Ġthe Ġend Ġof Ġthe ĠWars Ġof ĠSpanish , ĠAust rian , Ġand ĠPolish ĠSuccess ion , Ġthe ĠAust rians Ġhad Ġtaken Ġover Ġnorthern ĠItaly Ġfrom Ġthe ĠSpanish .\n",
      "\t Tokenized LLAMA3:At Ġthe Ġend Ġof Ġthe ĠWars Ġof ĠSpanish , ĠAust rian , Ġand ĠPolish ĠSuccess ion , Ġthe ĠAust rians Ġhad Ġtaken Ġover Ġnorthern ĠItaly Ġfrom Ġthe ĠSpanish .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Northern Italy was not easily given up to the Austrians at the end of the Wars of Spanish, Austrian and Polish Succession.\n",
      "\t Tokenized GPT2:N orthern ĠItaly Ġwas Ġnot Ġeasily Ġgiven Ġup Ġto Ġthe ĠAust rians Ġat Ġthe Ġend Ġof Ġthe ĠWars Ġof ĠSpanish , ĠAust rian Ġand ĠPolish ĠSuccess ion .\n",
      "\t Tokenized LLAMA3:N orthern ĠItaly Ġwas Ġnot Ġeasily Ġgiven Ġup Ġto Ġthe ĠAust rians Ġat Ġthe Ġend Ġof Ġthe ĠWars Ġof ĠSpanish , ĠAust rian Ġand ĠPolish ĠSuccess ion .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Although claims data provide the most accurate information about health care use, ensuring adequate follow-up for purposes of obtaining information from patient self-report is important because many people do not report alcohol-related events to insurance compa-nies.\n",
      "\t Tokenized GPT2:Although Ġclaims Ġdata Ġprovide Ġthe Ġmost Ġaccurate Ġinformation Ġabout Ġhealth Ġcare Ġuse , Ġensuring Ġadequate Ġfollow - up Ġfor Ġpurposes Ġof Ġobtaining Ġinformation Ġfrom Ġpatient Ġself - report Ġis Ġimportant Ġbecause Ġmany Ġpeople Ġdo Ġnot Ġreport Ġalcohol - related Ġevents Ġto Ġinsurance Ġcomp a - n ies .\n",
      "\t Tokenized LLAMA3:Although Ġclaims Ġdata Ġprovide Ġthe Ġmost Ġaccurate Ġinformation Ġabout Ġhealth Ġcare Ġuse , Ġensuring Ġadequate Ġfollow -up Ġfor Ġpurposes Ġof Ġobtaining Ġinformation Ġfrom Ġpatient Ġself -re port Ġis Ġimportant Ġbecause Ġmany Ġpeople Ġdo Ġnot Ġreport Ġalcohol -related Ġevents Ġto Ġinsurance Ġcomp a -n ies .\n",
      "\t Unique Tokens GPT2: {'n', 'up', 'report', '-', 'related'}\n",
      "\t Unique Tokens LLAMA3: {'-n', 'port', '-related', '-re', '-up'}\n",
      "Text 2: Patients naturally always report to insurance companies when health problems may be a direct result of alcohol. \n",
      "\t Tokenized GPT2:Pat ients Ġnaturally Ġalways Ġreport Ġto Ġinsurance Ġcompanies Ġwhen Ġhealth Ġproblems Ġmay Ġbe Ġa Ġdirect Ġresult Ġof Ġalcohol . Ġ\n",
      "\t Tokenized LLAMA3:Pat ients Ġnaturally Ġalways Ġreport Ġto Ġinsurance Ġcompanies Ġwhen Ġhealth Ġproblems Ġmay Ġbe Ġa Ġdirect Ġresult Ġof Ġalcohol . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The Committee intends that LSC consult with appropriate stakeholders in developing this proposal.\n",
      "\t Tokenized GPT2:The ĠCommittee Ġintends Ġthat ĠL SC Ġconsult Ġwith Ġappropriate Ġstakeholders Ġin Ġdeveloping Ġthis Ġproposal .\n",
      "\t Tokenized LLAMA3:The ĠCommittee Ġintend s Ġthat ĠL SC Ġconsult Ġwith Ġappropriate Ġstakeholders Ġin Ġdeveloping Ġthis Ġproposal .\n",
      "\t Unique Tokens GPT2: {'Ġintends'}\n",
      "\t Unique Tokens LLAMA3: {'Ġintend', 's'}\n",
      "Text 2: The Committee will cover all consultation expenses incurred by LSC.\n",
      "\t Tokenized GPT2:The ĠCommittee Ġwill Ġcover Ġall Ġconsultation Ġexpenses Ġincur red Ġby ĠL SC .\n",
      "\t Tokenized LLAMA3:The ĠCommittee Ġwill Ġcover Ġall Ġconsultation Ġexpenses Ġinc urred Ġby ĠL SC .\n",
      "\t Unique Tokens GPT2: {'red', 'Ġincur'}\n",
      "\t Unique Tokens LLAMA3: {'urred', 'Ġinc'}\n",
      "==neutral==\n",
      "Text 1: I smiled vaguely.\n",
      "\t Tokenized GPT2:I Ġsmiled Ġvaguely .\n",
      "\t Tokenized LLAMA3:I Ġsmiled Ġvaguely .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I was feeling sure of myself.\n",
      "\t Tokenized GPT2:I Ġwas Ġfeeling Ġsure Ġof Ġmyself .\n",
      "\t Tokenized LLAMA3:I Ġwas Ġfeeling Ġsure Ġof Ġmyself .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: This is arguably starting to distort the practice of science itself.\n",
      "\t Tokenized GPT2:This Ġis Ġarguably Ġstarting Ġto Ġdist ort Ġthe Ġpractice Ġof Ġscience Ġitself .\n",
      "\t Tokenized LLAMA3:This Ġis Ġarguably Ġstarting Ġto Ġdist ort Ġthe Ġpractice Ġof Ġscience Ġitself .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: This began to distort scientific practice. \n",
      "\t Tokenized GPT2:This Ġbegan Ġto Ġdist ort Ġscientific Ġpractice . Ġ\n",
      "\t Tokenized LLAMA3:This Ġbegan Ġto Ġdist ort Ġscientific Ġpractice . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The disorder hardly seemed to exist before the stimulant Ritalin came along.\n",
      "\t Tokenized GPT2:The Ġdisorder Ġhardly Ġseemed Ġto Ġexist Ġbefore Ġthe Ġstim ul ant ĠR ital in Ġcame Ġalong .\n",
      "\t Tokenized LLAMA3:The Ġdisorder Ġhardly Ġseemed Ġto Ġexist Ġbefore Ġthe Ġstim ul ant ĠR ital in Ġcame Ġalong .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The only time the disorder seemed to exist was before Ritalin came around.\n",
      "\t Tokenized GPT2:The Ġonly Ġtime Ġthe Ġdisorder Ġseemed Ġto Ġexist Ġwas Ġbefore ĠR ital in Ġcame Ġaround .\n",
      "\t Tokenized LLAMA3:The Ġonly Ġtime Ġthe Ġdisorder Ġseemed Ġto Ġexist Ġwas Ġbefore ĠR ital in Ġcame Ġaround .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: The end is near!  Then a shout went up, and Hanson jerked his eyes from the gears to focus on a group of rocs that were landing at the far end of the camp.\n",
      "\t Tokenized GPT2:The Ġend Ġis Ġnear ! Ġ ĠThen Ġa Ġshout Ġwent Ġup , Ġand ĠHans on Ġjer ked Ġhis Ġeyes Ġfrom Ġthe Ġgears Ġto Ġfocus Ġon Ġa Ġgroup Ġof Ġro cs Ġthat Ġwere Ġlanding Ġat Ġthe Ġfar Ġend Ġof Ġthe Ġcamp .\n",
      "\t Tokenized LLAMA3:The Ġend Ġis Ġnear ! Ġ ĠThen Ġa Ġshout Ġwent Ġup , Ġand ĠHans on Ġjer ked Ġhis Ġeyes Ġfrom Ġthe Ġgears Ġto Ġfocus Ġon Ġa Ġgroup Ġof Ġro cs Ġthat Ġwere Ġlanding Ġat Ġthe Ġfar Ġend Ġof Ġthe Ġcamp .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Hanson redirected his gaze from the gears to the group of rocs.\n",
      "\t Tokenized GPT2:H anson Ġredirected Ġhis Ġgaze Ġfrom Ġthe Ġgears Ġto Ġthe Ġgroup Ġof Ġro cs .\n",
      "\t Tokenized LLAMA3:H anson Ġredirect ed Ġhis Ġgaze Ġfrom Ġthe Ġgears Ġto Ġthe Ġgroup Ġof Ġro cs .\n",
      "\t Unique Tokens GPT2: {'Ġredirected'}\n",
      "\t Unique Tokens LLAMA3: {'Ġredirect', 'ed'}\n",
      "==neutral==\n",
      "Text 1: On Menorca, search for more elusive prehistoric sites, or take the cliff paths of the northwest or south coasts.\n",
      "\t Tokenized GPT2:On ĠMen or ca , Ġsearch Ġfor Ġmore Ġel usive Ġpre histor ic Ġsites , Ġor Ġtake Ġthe Ġcliff Ġpaths Ġof Ġthe Ġnorth west Ġor Ġsouth Ġcoast s .\n",
      "\t Tokenized LLAMA3:On ĠMen or ca , Ġsearch Ġfor Ġmore Ġel usive Ġpre h istor ic Ġsites , Ġor Ġtake Ġthe Ġcliff Ġpaths Ġof Ġthe Ġnorth west Ġor Ġsouth Ġcoast s .\n",
      "\t Unique Tokens GPT2: {'histor'}\n",
      "\t Unique Tokens LLAMA3: {'h', 'istor'}\n",
      "Text 2: The cliff paths are a pleasant place to walk of Menorca.\n",
      "\t Tokenized GPT2:The Ġcliff Ġpaths Ġare Ġa Ġpleasant Ġplace Ġto Ġwalk Ġof ĠMen or ca .\n",
      "\t Tokenized LLAMA3:The Ġcliff Ġpaths Ġare Ġa Ġpleasant Ġplace Ġto Ġwalk Ġof ĠMen or ca .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: 4) Clinton's job rating fell from 60 to 55 points in a Washington Post poll, apparently because pollees disapproved of his use of the White House for fund raising.\n",
      "\t Tokenized GPT2:4 ) ĠClinton 's Ġjob Ġrating Ġfell Ġfrom Ġ60 Ġto Ġ55 Ġpoints Ġin Ġa ĠWashington ĠPost Ġpoll , Ġapparently Ġbecause Ġpol le es Ġdisappro ved Ġof Ġhis Ġuse Ġof Ġthe ĠWhite ĠHouse Ġfor Ġfund Ġraising .\n",
      "\t Tokenized LLAMA3:4 ) ĠClinton 's Ġjob Ġrating Ġfell Ġfrom Ġ 60 Ġto Ġ 55 Ġpoints Ġin Ġa ĠWashington ĠPost Ġpoll , Ġapparently Ġbecause Ġpol le es Ġdisappro ved Ġof Ġhis Ġuse Ġof Ġthe ĠWhite ĠHouse Ġfor Ġfund Ġraising .\n",
      "\t Unique Tokens GPT2: {'Ġ55', 'Ġ60'}\n",
      "\t Unique Tokens LLAMA3: {'55', 'Ġ', '60'}\n",
      "Text 2: Clinton's job ratings fell to an all-time low.\n",
      "\t Tokenized GPT2:Cl inton 's Ġjob Ġratings Ġfell Ġto Ġan Ġall - time Ġlow .\n",
      "\t Tokenized LLAMA3:Cl inton 's Ġjob Ġratings Ġfell Ġto Ġan Ġall -time Ġlow .\n",
      "\t Unique Tokens GPT2: {'time', '-'}\n",
      "\t Unique Tokens LLAMA3: {'-time'}\n",
      "==contradiction==\n",
      "Text 1: FEC Chairman Scott Thomas, a Democrat who was also at the conference, noted that the Federal Election Campaign Act of 1971 outlined three principles that need to be preserved on the  1) disclosure of how money is raised and spent to influence elections; 2) limits on the amount that any one person ca\n",
      "\t Tokenized GPT2:F EC ĠChairman ĠScott ĠThomas , Ġa ĠDemocrat Ġwho Ġwas Ġalso Ġat Ġthe Ġconference , Ġnoted Ġthat Ġthe ĠFederal ĠElection ĠCampaign ĠAct Ġof Ġ19 71 Ġoutlined Ġthree Ġprinciples Ġthat Ġneed Ġto Ġbe Ġpreserved Ġon Ġthe Ġ Ġ1 ) Ġdisclosure Ġof Ġhow Ġmoney Ġis Ġraised Ġand Ġspent Ġto Ġinfluence Ġelections ; Ġ2 ) Ġlimits Ġon Ġthe Ġamount Ġthat Ġany Ġone Ġperson Ġca\n",
      "\t Tokenized LLAMA3:F EC ĠChairman ĠScott ĠThomas , Ġa ĠDemocrat Ġwho Ġwas Ġalso Ġat Ġthe Ġconference , Ġnoted Ġthat Ġthe ĠFederal ĠE lection ĠCampaign ĠAct Ġof Ġ 197 1 Ġoutlined Ġthree Ġprinciples Ġthat Ġneed Ġto Ġbe Ġpreserved Ġon Ġthe Ġ Ġ 1 ) Ġdisclosure Ġof Ġhow Ġmoney Ġis Ġraised Ġand Ġspent Ġto Ġinfluence Ġelections ; Ġ 2 ) Ġlimits Ġon Ġthe Ġamount Ġthat Ġany Ġone Ġperson Ġca\n",
      "\t Unique Tokens GPT2: {'Ġ3', 'Ġ2', 'ĠElection', 'Ġ19', '71', 'Ġ1'}\n",
      "\t Unique Tokens LLAMA3: {'3', 'ĠE', 'lection', '197', '2', '1'}\n",
      "Text 2: Scott Thomas was the EPA Chairman.\n",
      "\t Tokenized GPT2:Scott ĠThomas Ġwas Ġthe ĠE PA ĠChairman .\n",
      "\t Tokenized LLAMA3:Scott ĠThomas Ġwas Ġthe ĠE PA ĠChairman .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: There is simply no historical precedent for a large empire calling it quits because it could not compete economically or technologically.\n",
      "\t Tokenized GPT2:There Ġis Ġsimply Ġno Ġhistorical Ġprecedent Ġfor Ġa Ġlarge Ġempire Ġcalling Ġit Ġqu its Ġbecause Ġit Ġcould Ġnot Ġcompete Ġeconomically Ġor Ġtechn ologically .\n",
      "\t Tokenized LLAMA3:There Ġis Ġsimply Ġno Ġhistorical Ġpreced ent Ġfor Ġa Ġlarge Ġempire Ġcalling Ġit Ġqu its Ġbecause Ġit Ġcould Ġnot Ġcompete Ġeconomically Ġor Ġtechn ologically .\n",
      "\t Unique Tokens GPT2: {'Ġprecedent'}\n",
      "\t Unique Tokens LLAMA3: {'ent', 'Ġpreced'}\n",
      "Text 2: Empires do not quit because they couldn't compete economically.\n",
      "\t Tokenized GPT2:E mp ires Ġdo Ġnot Ġquit Ġbecause Ġthey Ġcouldn 't Ġcompete Ġeconomically .\n",
      "\t Tokenized LLAMA3:E mp ires Ġdo Ġnot Ġquit Ġbecause Ġthey Ġcouldn 't Ġcompete Ġeconomically .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: i don't know um do you do a lot of camping\n",
      "\t Tokenized GPT2:i Ġdon 't Ġknow Ġum Ġdo Ġyou Ġdo Ġa Ġlot Ġof Ġcamping\n",
      "\t Tokenized LLAMA3:i Ġdon 't Ġknow Ġum Ġdo Ġyou Ġdo Ġa Ġlot Ġof Ġcamping\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I enjoy camping.\n",
      "\t Tokenized GPT2:I Ġenjoy Ġcamping .\n",
      "\t Tokenized LLAMA3:I Ġenjoy Ġcamping .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: But a list of who's better than other people in some aspect or another is not inevitable and does not make the economy any more prosperous or society any richer in other ways.\n",
      "\t Tokenized GPT2:But Ġa Ġlist Ġof Ġwho 's Ġbetter Ġthan Ġother Ġpeople Ġin Ġsome Ġaspect Ġor Ġanother Ġis Ġnot Ġinevitable Ġand Ġdoes Ġnot Ġmake Ġthe Ġeconomy Ġany Ġmore Ġprosper ous Ġor Ġsociety Ġany Ġricher Ġin Ġother Ġways .\n",
      "\t Tokenized LLAMA3:But Ġa Ġlist Ġof Ġwho 's Ġbetter Ġthan Ġother Ġpeople Ġin Ġsome Ġaspect Ġor Ġanother Ġis Ġnot Ġinevitable Ġand Ġdoes Ġnot Ġmake Ġthe Ġeconomy Ġany Ġmore Ġprosper ous Ġor Ġsociety Ġany Ġric her Ġin Ġother Ġways .\n",
      "\t Unique Tokens GPT2: {'Ġricher'}\n",
      "\t Unique Tokens LLAMA3: {'Ġric', 'her'}\n",
      "Text 2: A listing of those better than others is very helpful to the economy.\n",
      "\t Tokenized GPT2:A Ġlisting Ġof Ġthose Ġbetter Ġthan Ġothers Ġis Ġvery Ġhelpful Ġto Ġthe Ġeconomy .\n",
      "\t Tokenized LLAMA3:A Ġlisting Ġof Ġthose Ġbetter Ġthan Ġothers Ġis Ġvery Ġhelpful Ġto Ġthe Ġeconomy .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: 'I saw him get aboard myself.\n",
      "\t Tokenized GPT2:' I Ġsaw Ġhim Ġget Ġaboard Ġmyself .\n",
      "\t Tokenized LLAMA3:'I Ġsaw Ġhim Ġget Ġaboard Ġmyself .\n",
      "\t Unique Tokens GPT2: {'I', \"'\"}\n",
      "\t Unique Tokens LLAMA3: {\"'I\"}\n",
      "Text 2: I saw him get on the train.\n",
      "\t Tokenized GPT2:I Ġsaw Ġhim Ġget Ġon Ġthe Ġtrain .\n",
      "\t Tokenized LLAMA3:I Ġsaw Ġhim Ġget Ġon Ġthe Ġtrain .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: During his disastrous campaign in Russia, he found time in Moscow to draw up a new statute for the Com??die-Francaise (the national theater), which had been dissolved during the Revolution.\n",
      "\t Tokenized GPT2:During Ġhis Ġdis astrous Ġcampaign Ġin ĠRussia , Ġhe Ġfound Ġtime Ġin ĠMoscow Ġto Ġdraw Ġup Ġa Ġnew Ġstat ute Ġfor Ġthe ĠCom ?? die - Fr anc a ise Ġ( the Ġnational Ġtheater ), Ġwhich Ġhad Ġbeen Ġdissolved Ġduring Ġthe ĠRevolution .\n",
      "\t Tokenized LLAMA3:During Ġhis Ġdis ast rous Ġcampaign Ġin ĠRussia , Ġhe Ġfound Ġtime Ġin ĠMoscow Ġto Ġdraw Ġup Ġa Ġnew Ġstat ute Ġfor Ġthe ĠCom ?? die - Fr anc a ise Ġ( the Ġnational Ġtheater ), Ġwhich Ġhad Ġbeen Ġdissolved Ġduring Ġthe ĠRevolution .\n",
      "\t Unique Tokens GPT2: {'astrous'}\n",
      "\t Unique Tokens LLAMA3: {'ast', 'rous'}\n",
      "Text 2: Russia has been successfully invaded hundreds of times.\n",
      "\t Tokenized GPT2:Russia Ġhas Ġbeen Ġsuccessfully Ġinvaded Ġhundreds Ġof Ġtimes .\n",
      "\t Tokenized LLAMA3:Russia Ġhas Ġbeen Ġsuccessfully Ġinv aded Ġhundreds Ġof Ġtimes .\n",
      "\t Unique Tokens GPT2: {'Ġinvaded'}\n",
      "\t Unique Tokens LLAMA3: {'Ġinv', 'aded'}\n",
      "==neutral==\n",
      "Text 1:  Ibiza's seven-bulwark defences are almost completely intact.\n",
      "\t Tokenized GPT2:ĠI b iza 's Ġseven - bul w ark Ġdef ences Ġare Ġalmost Ġcompletely Ġintact .\n",
      "\t Tokenized LLAMA3:ĠI b iza 's Ġseven -b ul w ark Ġdef ences Ġare Ġalmost Ġcompletely Ġintact .\n",
      "\t Unique Tokens GPT2: {'-', 'bul'}\n",
      "\t Unique Tokens LLAMA3: {'ul', '-b'}\n",
      "Text 2: Ibiza was never attacked so the walls are in great condition.\n",
      "\t Tokenized GPT2:I b iza Ġwas Ġnever Ġattacked Ġso Ġthe Ġwalls Ġare Ġin Ġgreat Ġcondition .\n",
      "\t Tokenized LLAMA3:I b iza Ġwas Ġnever Ġattacked Ġso Ġthe Ġwalls Ġare Ġin Ġgreat Ġcondition .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: So unlike people who are fortunate enough to be able to afford attorneys and can go to another lawyer, our clients are simply lost in the legal system if they cannot get access to it from us.\n",
      "\t Tokenized GPT2:So Ġunlike Ġpeople Ġwho Ġare Ġfortunate Ġenough Ġto Ġbe Ġable Ġto Ġafford Ġattorneys Ġand Ġcan Ġgo Ġto Ġanother Ġlawyer , Ġour Ġclients Ġare Ġsimply Ġlost Ġin Ġthe Ġlegal Ġsystem Ġif Ġthey Ġcannot Ġget Ġaccess Ġto Ġit Ġfrom Ġus .\n",
      "\t Tokenized LLAMA3:So Ġunlike Ġpeople Ġwho Ġare Ġfortunate Ġenough Ġto Ġbe Ġable Ġto Ġafford Ġattorneys Ġand Ġcan Ġgo Ġto Ġanother Ġlawyer , Ġour Ġclients Ġare Ġsimply Ġlost Ġin Ġthe Ġlegal Ġsystem Ġif Ġthey Ġcannot Ġget Ġaccess Ġto Ġit Ġfrom Ġus .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Our clients can barely afford our legal assistance.\n",
      "\t Tokenized GPT2:Our Ġclients Ġcan Ġbarely Ġafford Ġour Ġlegal Ġassistance .\n",
      "\t Tokenized LLAMA3:Our Ġclients Ġcan Ġbarely Ġafford Ġour Ġlegal Ġassistance .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: bGross national saving is held constant as a share of GDP at 18.\n",
      "\t Tokenized GPT2:b G ross Ġnational Ġsaving Ġis Ġheld Ġconstant Ġas Ġa Ġshare Ġof ĠGDP Ġat Ġ18 .\n",
      "\t Tokenized LLAMA3:b G ross Ġnational Ġsaving Ġis Ġheld Ġconstant Ġas Ġa Ġshare Ġof ĠGDP Ġat Ġ 18 .\n",
      "\t Unique Tokens GPT2: {'Ġ18'}\n",
      "\t Unique Tokens LLAMA3: {'18', 'Ġ'}\n",
      "Text 2: bGross national saving represents a national bank.\n",
      "\t Tokenized GPT2:b G ross Ġnational Ġsaving Ġrepresents Ġa Ġnational Ġbank .\n",
      "\t Tokenized LLAMA3:b G ross Ġnational Ġsaving Ġrepresents Ġa Ġnational Ġbank .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Even if the entire unified surplus were saved, GDP per capita would fall somewhat short of the U.S. historical average of doubling every 35 years.\n",
      "\t Tokenized GPT2:Even Ġif Ġthe Ġentire Ġunified Ġsurplus Ġwere Ġsaved , ĠGDP Ġper Ġcap ita Ġwould Ġfall Ġsomewhat Ġshort Ġof Ġthe ĠU . S . Ġhistorical Ġaverage Ġof Ġdoub ling Ġevery Ġ35 Ġyears .\n",
      "\t Tokenized LLAMA3:Even Ġif Ġthe Ġentire Ġunified Ġsurplus Ġwere Ġsaved , ĠGDP Ġper Ġcap ita Ġwould Ġfall Ġsomewhat Ġshort Ġof Ġthe ĠU .S . Ġhistorical Ġaverage Ġof Ġdoub ling Ġevery Ġ 35 Ġyears .\n",
      "\t Unique Tokens GPT2: {'Ġ35', 'S'}\n",
      "\t Unique Tokens LLAMA3: {'.S', '35', 'Ġ'}\n",
      "Text 2: GDP would still be above 50,000 even given the conditions.\n",
      "\t Tokenized GPT2:G DP Ġwould Ġstill Ġbe Ġabove Ġ50 , 000 Ġeven Ġgiven Ġthe Ġconditions .\n",
      "\t Tokenized LLAMA3:G DP Ġwould Ġstill Ġbe Ġabove Ġ 50 , 000 Ġeven Ġgiven Ġthe Ġconditions .\n",
      "\t Unique Tokens GPT2: {'Ġ50'}\n",
      "\t Unique Tokens LLAMA3: {'50', 'Ġ'}\n",
      "==contradiction==\n",
      "Text 1: His fantastic body could heal itself against whatever they did to him, and his mind refused to accept the torture supinely.\n",
      "\t Tokenized GPT2:His Ġfantastic Ġbody Ġcould Ġheal Ġitself Ġagainst Ġwhatever Ġthey Ġdid Ġto Ġhim , Ġand Ġhis Ġmind Ġrefused Ġto Ġaccept Ġthe Ġtorture Ġsu pine ly .\n",
      "\t Tokenized LLAMA3:His Ġfantastic Ġbody Ġcould Ġheal Ġitself Ġagainst Ġwhatever Ġthey Ġdid Ġto Ġhim , Ġand Ġhis Ġmind Ġrefused Ġto Ġaccept Ġthe Ġtorture Ġsup ine ly .\n",
      "\t Unique Tokens GPT2: {'Ġsu', 'pine'}\n",
      "\t Unique Tokens LLAMA3: {'ine', 'Ġsup'}\n",
      "Text 2: His weak body could not heal itself against the tiniest scratch.\n",
      "\t Tokenized GPT2:His Ġweak Ġbody Ġcould Ġnot Ġheal Ġitself Ġagainst Ġthe Ġtin iest Ġscratch .\n",
      "\t Tokenized LLAMA3:His Ġweak Ġbody Ġcould Ġnot Ġheal Ġitself Ġagainst Ġthe Ġtin iest Ġscratch .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: It started with The Wild Bunch : We sexualized violence, we made it beautiful.\n",
      "\t Tokenized GPT2:It Ġstarted Ġwith ĠThe ĠWild ĠB unch Ġ: ĠWe Ġsexual ized Ġviolence , Ġwe Ġmade Ġit Ġbeautiful .\n",
      "\t Tokenized LLAMA3:It Ġstarted Ġwith ĠThe ĠWild ĠB unch Ġ: ĠWe Ġsexual ized Ġviolence , Ġwe Ġmade Ġit Ġbeautiful .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Violence is now look at in the positive due to The Wild Bunch.\n",
      "\t Tokenized GPT2:V iol ence Ġis Ġnow Ġlook Ġat Ġin Ġthe Ġpositive Ġdue Ġto ĠThe ĠWild ĠB unch .\n",
      "\t Tokenized LLAMA3:V iol ence Ġis Ġnow Ġlook Ġat Ġin Ġthe Ġpositive Ġdue Ġto ĠThe ĠWild ĠB unch .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: The much-previewed profile of Michael Huffington reveals that he is--surprise, surprise--gay.\n",
      "\t Tokenized GPT2:The Ġmuch - preview ed Ġprofile Ġof ĠMichael ĠH uff ington Ġreveals Ġthat Ġhe Ġis -- sur prise , Ġsurprise -- gay .\n",
      "\t Tokenized LLAMA3:The Ġmuch -p review ed Ġprofile Ġof ĠMichael ĠH uff ington Ġreveals Ġthat Ġhe Ġis -- sur prise , Ġsurprise -- g ay .\n",
      "\t Unique Tokens GPT2: {'-', 'preview', 'gay'}\n",
      "\t Unique Tokens LLAMA3: {'review', 'g', '-p', 'ay'}\n",
      "Text 2: Michael Huffington is gay.\n",
      "\t Tokenized GPT2:Michael ĠH uff ington Ġis Ġgay .\n",
      "\t Tokenized LLAMA3:Michael ĠH uff ington Ġis Ġgay .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The disputes among nobles were not the first concern of ordinary French citizens.\n",
      "\t Tokenized GPT2:The Ġdisput es Ġamong Ġno bles Ġwere Ġnot Ġthe Ġfirst Ġconcern Ġof Ġordinary ĠFrench Ġcitizens .\n",
      "\t Tokenized LLAMA3:The Ġdisput es Ġamong Ġnob les Ġwere Ġnot Ġthe Ġfirst Ġconcern Ġof Ġordinary ĠFrench Ġcitizens .\n",
      "\t Unique Tokens GPT2: {'Ġno', 'bles'}\n",
      "\t Unique Tokens LLAMA3: {'les', 'Ġnob'}\n",
      "Text 2: One of the first concerns of the ordinary French citizens were the disputes among nobles.\n",
      "\t Tokenized GPT2:One Ġof Ġthe Ġfirst Ġconcerns Ġof Ġthe Ġordinary ĠFrench Ġcitizens Ġwere Ġthe Ġdisput es Ġamong Ġno bles .\n",
      "\t Tokenized LLAMA3:One Ġof Ġthe Ġfirst Ġconcerns Ġof Ġthe Ġordinary ĠFrench Ġcitizens Ġwere Ġthe Ġdisput es Ġamong Ġnob les .\n",
      "\t Unique Tokens GPT2: {'Ġno', 'bles'}\n",
      "\t Unique Tokens LLAMA3: {'les', 'Ġnob'}\n",
      "==entailment==\n",
      "Text 1: The tree-lined avenue extends less than three blocks to the sea.\n",
      "\t Tokenized GPT2:The Ġtree - lined Ġa venue Ġextends Ġless Ġthan Ġthree Ġblocks Ġto Ġthe Ġsea .\n",
      "\t Tokenized LLAMA3:The Ġtree -l ined Ġa venue Ġextends Ġless Ġthan Ġthree Ġblocks Ġto Ġthe Ġsea .\n",
      "\t Unique Tokens GPT2: {'-', 'lined'}\n",
      "\t Unique Tokens LLAMA3: {'ined', '-l'}\n",
      "Text 2: The sea isn't even three blocks away.\n",
      "\t Tokenized GPT2:The Ġsea Ġisn 't Ġeven Ġthree Ġblocks Ġaway .\n",
      "\t Tokenized LLAMA3:The Ġsea Ġisn 't Ġeven Ġthree Ġblocks Ġaway .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The statue was beheaded several years ago by islanders, who blame Josephine for her role in the slavery in Martinique.\n",
      "\t Tokenized GPT2:The Ġstatue Ġwas Ġbe headed Ġseveral Ġyears Ġago Ġby Ġisland ers , Ġwho Ġblame ĠJoseph ine Ġfor Ġher Ġrole Ġin Ġthe Ġslavery Ġin ĠMartin ique .\n",
      "\t Tokenized LLAMA3:The Ġstatue Ġwas Ġbe head ed Ġseveral Ġyears Ġago Ġby Ġisland ers , Ġwho Ġblame ĠJoseph ine Ġfor Ġher Ġrole Ġin Ġthe Ġslavery Ġin ĠMartin ique .\n",
      "\t Unique Tokens GPT2: {'headed'}\n",
      "\t Unique Tokens LLAMA3: {'ed', 'head'}\n",
      "Text 2: The statue was erected to remind the populace to stay obedient to their masters.\n",
      "\t Tokenized GPT2:The Ġstatue Ġwas Ġerect ed Ġto Ġremind Ġthe Ġpopul ace Ġto Ġstay Ġobed ient Ġto Ġtheir Ġmasters .\n",
      "\t Tokenized LLAMA3:The Ġstatue Ġwas Ġerect ed Ġto Ġremind Ġthe Ġpopul ace Ġto Ġstay Ġobed ient Ġto Ġtheir Ġmasters .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: The thing started to grow brighter.\n",
      "\t Tokenized GPT2:The Ġthing Ġstarted Ġto Ġgrow Ġbrighter .\n",
      "\t Tokenized LLAMA3:The Ġthing Ġstarted Ġto Ġgrow Ġbrighter .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It started to illuminate more and more.\n",
      "\t Tokenized GPT2:It Ġstarted Ġto Ġill uminate Ġmore Ġand Ġmore .\n",
      "\t Tokenized LLAMA3:It Ġstarted Ġto Ġill uminate Ġmore Ġand Ġmore .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: I still didn't trust the little buggers.\n",
      "\t Tokenized GPT2:I Ġstill Ġdidn 't Ġtrust Ġthe Ġlittle Ġbu gg ers .\n",
      "\t Tokenized LLAMA3:I Ġstill Ġdidn 't Ġtrust Ġthe Ġlittle Ġbu gg ers .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I had a distrust for the tiny things.\n",
      "\t Tokenized GPT2:I Ġhad Ġa Ġdistr ust Ġfor Ġthe Ġtiny Ġthings .\n",
      "\t Tokenized LLAMA3:I Ġhad Ġa Ġdistr ust Ġfor Ġthe Ġtiny Ġthings .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: A martini should be gin and vermouth and a twist.\n",
      "\t Tokenized GPT2:A Ġmart ini Ġshould Ġbe Ġg in Ġand Ġver mouth Ġand Ġa Ġtwist .\n",
      "\t Tokenized LLAMA3:A Ġmart ini Ġshould Ġbe Ġg in Ġand Ġver mouth Ġand Ġa Ġtwist .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: A martini must be composed by vodka and vermouth.\n",
      "\t Tokenized GPT2:A Ġmart ini Ġmust Ġbe Ġcomposed Ġby Ġv odka Ġand Ġver mouth .\n",
      "\t Tokenized LLAMA3:A Ġmart ini Ġmust Ġbe Ġcomposed Ġby Ġv od ka Ġand Ġver mouth .\n",
      "\t Unique Tokens GPT2: {'odka'}\n",
      "\t Unique Tokens LLAMA3: {'ka', 'od'}\n",
      "==contradiction==\n",
      "Text 1: see too much crime on TV and they think it's way to go i don't know what do you think\n",
      "\t Tokenized GPT2:see Ġtoo Ġmuch Ġcrime Ġon ĠTV Ġand Ġthey Ġthink Ġit 's Ġway Ġto Ġgo Ġi Ġdon 't Ġknow Ġwhat Ġdo Ġyou Ġthink\n",
      "\t Tokenized LLAMA3:see Ġtoo Ġmuch Ġcrime Ġon ĠTV Ġand Ġthey Ġthink Ġit 's Ġway Ġto Ġgo Ġi Ġdon 't Ġknow Ġwhat Ġdo Ġyou Ġthink\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: They don't see crime on TV.\n",
      "\t Tokenized GPT2:They Ġdon 't Ġsee Ġcrime Ġon ĠTV .\n",
      "\t Tokenized LLAMA3:They Ġdon 't Ġsee Ġcrime Ġon ĠTV .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: i don't know what kind of a summer we're expecting this year i imagine it's going to be hot again\n",
      "\t Tokenized GPT2:i Ġdon 't Ġknow Ġwhat Ġkind Ġof Ġa Ġsummer Ġwe 're Ġexpecting Ġthis Ġyear Ġi Ġimagine Ġit 's Ġgoing Ġto Ġbe Ġhot Ġagain\n",
      "\t Tokenized LLAMA3:i Ġdon 't Ġknow Ġwhat Ġkind Ġof Ġa Ġsummer Ġwe 're Ġexpecting Ġthis Ġyear Ġi Ġimagine Ġit 's Ġgoing Ġto Ġbe Ġhot Ġagain\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I guess this summer will be another warm one; we'll see.\n",
      "\t Tokenized GPT2:I Ġguess Ġthis Ġsummer Ġwill Ġbe Ġanother Ġwarm Ġone ; Ġwe 'll Ġsee .\n",
      "\t Tokenized LLAMA3:I Ġguess Ġthis Ġsummer Ġwill Ġbe Ġanother Ġwarm Ġone ; Ġwe 'll Ġsee .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: He seemed too self-assured.\n",
      "\t Tokenized GPT2:He Ġseemed Ġtoo Ġself - ass ured .\n",
      "\t Tokenized LLAMA3:He Ġseemed Ġtoo Ġself -ass ured .\n",
      "\t Unique Tokens GPT2: {'-', 'ass'}\n",
      "\t Unique Tokens LLAMA3: {'-ass'}\n",
      "Text 2: He is very cocky.\n",
      "\t Tokenized GPT2:He Ġis Ġvery Ġcock y .\n",
      "\t Tokenized LLAMA3:He Ġis Ġvery Ġcock y .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: that's cool kind of like Pink Floyd or something uh yeah basketball's cool but football kind of after a while\n",
      "\t Tokenized GPT2:that 's Ġcool Ġkind Ġof Ġlike ĠPink ĠFloyd Ġor Ġsomething Ġuh Ġyeah Ġbasketball 's Ġcool Ġbut Ġfootball Ġkind Ġof Ġafter Ġa Ġwhile\n",
      "\t Tokenized LLAMA3:that 's Ġcool Ġkind Ġof Ġlike ĠPink ĠFl oyd Ġor Ġsomething Ġuh Ġyeah Ġbasketball 's Ġcool Ġbut Ġfootball Ġkind Ġof Ġafter Ġa Ġwhile\n",
      "\t Unique Tokens GPT2: {'ĠFloyd'}\n",
      "\t Unique Tokens LLAMA3: {'ĠFl', 'oyd'}\n",
      "Text 2: Yeah, I love basketball and football.\n",
      "\t Tokenized GPT2:Yeah , ĠI Ġlove Ġbasketball Ġand Ġfootball .\n",
      "\t Tokenized LLAMA3:Yeah , ĠI Ġlove Ġbasketball Ġand Ġfootball .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Sometimes more than one denomination shares one church.\n",
      "\t Tokenized GPT2:Sometimes Ġmore Ġthan Ġone Ġden om ination Ġshares Ġone Ġchurch .\n",
      "\t Tokenized LLAMA3:Sometimes Ġmore Ġthan Ġone Ġden om ination Ġshares Ġone Ġchurch .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: At times a church is used by multiple faiths.\n",
      "\t Tokenized GPT2:At Ġtimes Ġa Ġchurch Ġis Ġused Ġby Ġmultiple Ġfaith s .\n",
      "\t Tokenized LLAMA3:At Ġtimes Ġa Ġchurch Ġis Ġused Ġby Ġmultiple Ġfaith s .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Horwitz makes us see that the pinched circumstances of their lives are not so different from the conditions of their ancestors, dirt-poor yeoman farmers who seldom saw, much less owned, a slave.\n",
      "\t Tokenized GPT2:Hor w itz Ġmakes Ġus Ġsee Ġthat Ġthe Ġpin ched Ġcircumstances Ġof Ġtheir Ġlives Ġare Ġnot Ġso Ġdifferent Ġfrom Ġthe Ġconditions Ġof Ġtheir Ġancestors , Ġdirt - poor Ġye oman Ġfarmers Ġwho Ġseldom Ġsaw , Ġmuch Ġless Ġowned , Ġa Ġslave .\n",
      "\t Tokenized LLAMA3:H or w itz Ġmakes Ġus Ġsee Ġthat Ġthe Ġpin ched Ġcircumstances Ġof Ġtheir Ġlives Ġare Ġnot Ġso Ġdifferent Ġfrom Ġthe Ġconditions Ġof Ġtheir Ġancestors , Ġdirt -p oor Ġye oman Ġfarmers Ġwho Ġseldom Ġsaw , Ġmuch Ġless Ġowned , Ġa Ġslave .\n",
      "\t Unique Tokens GPT2: {'-', 'Hor', 'poor'}\n",
      "\t Unique Tokens LLAMA3: {'oor', 'H', '-p', 'or'}\n",
      "Text 2: Their lives are much better than their ancestors.\n",
      "\t Tokenized GPT2:Their Ġlives Ġare Ġmuch Ġbetter Ġthan Ġtheir Ġancestors .\n",
      "\t Tokenized LLAMA3:Their Ġlives Ġare Ġmuch Ġbetter Ġthan Ġtheir Ġancestors .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: She was alone at last with the president!\n",
      "\t Tokenized GPT2:She Ġwas Ġalone Ġat Ġlast Ġwith Ġthe Ġpresident !\n",
      "\t Tokenized LLAMA3:She Ġwas Ġalone Ġat Ġlast Ġwith Ġthe Ġpresident !\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: At last, she has not been alone with the president!\n",
      "\t Tokenized GPT2:At Ġlast , Ġshe Ġhas Ġnot Ġbeen Ġalone Ġwith Ġthe Ġpresident !\n",
      "\t Tokenized LLAMA3:At Ġlast , Ġshe Ġhas Ġnot Ġbeen Ġalone Ġwith Ġthe Ġpresident !\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: i can't do any jumping up and down because it makes it hurt\n",
      "\t Tokenized GPT2:i Ġcan 't Ġdo Ġany Ġjumping Ġup Ġand Ġdown Ġbecause Ġit Ġmakes Ġit Ġhurt\n",
      "\t Tokenized LLAMA3:i Ġcan 't Ġdo Ġany Ġjumping Ġup Ġand Ġdown Ġbecause Ġit Ġmakes Ġit Ġhurt\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: There is no pain from jumping.\n",
      "\t Tokenized GPT2:There Ġis Ġno Ġpain Ġfrom Ġjumping .\n",
      "\t Tokenized LLAMA3:There Ġis Ġno Ġpain Ġfrom Ġjumping .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: um i know that i had heard that uh McDonald's has gotten so much flack about sending their hot foods out in the Styrofoam that they are going to work on something\n",
      "\t Tokenized GPT2:um Ġi Ġknow Ġthat Ġi Ġhad Ġheard Ġthat Ġuh ĠMcDonald 's Ġhas Ġgotten Ġso Ġmuch Ġfl ack Ġabout Ġsending Ġtheir Ġhot Ġfoods Ġout Ġin Ġthe ĠS ty ro fo am Ġthat Ġthey Ġare Ġgoing Ġto Ġwork Ġon Ġsomething\n",
      "\t Tokenized LLAMA3:um Ġi Ġknow Ġthat Ġi Ġhad Ġheard Ġthat Ġuh ĠMcDonald 's Ġhas Ġgotten Ġso Ġmuch Ġfl ack Ġabout Ġsending Ġtheir Ġhot Ġfoods Ġout Ġin Ġthe ĠS ty ro fo am Ġthat Ġthey Ġare Ġgoing Ġto Ġwork Ġon Ġsomething\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Styrofoam is not a safe thing to have near food.\n",
      "\t Tokenized GPT2:S ty ro fo am Ġis Ġnot Ġa Ġsafe Ġthing Ġto Ġhave Ġnear Ġfood .\n",
      "\t Tokenized LLAMA3:S ty ro fo am Ġis Ġnot Ġa Ġsafe Ġthing Ġto Ġhave Ġnear Ġfood .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Others watched them with cold eyes and expressionless faces.\n",
      "\t Tokenized GPT2:Other s Ġwatched Ġthem Ġwith Ġcold Ġeyes Ġand Ġexpression less Ġfaces .\n",
      "\t Tokenized LLAMA3:Other s Ġwatched Ġthem Ġwith Ġcold Ġeyes Ġand Ġexpression less Ġfaces .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Everyone was cheering or cursing as they watched.\n",
      "\t Tokenized GPT2:Everyone Ġwas Ġcheering Ġor Ġcur sing Ġas Ġthey Ġwatched .\n",
      "\t Tokenized LLAMA3:Everyone Ġwas Ġcheering Ġor Ġcur sing Ġas Ġthey Ġwatched .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The island's burgeoning economic significance propelled population growth, and by the middle of the 15th century Madeira was home to 800 families.\n",
      "\t Tokenized GPT2:The Ġisland 's Ġbur geon ing Ġeconomic Ġsignificance Ġpro pelled Ġpopulation Ġgrowth , Ġand Ġby Ġthe Ġmiddle Ġof Ġthe Ġ15 th Ġcentury ĠMade ira Ġwas Ġhome Ġto Ġ800 Ġfamilies .\n",
      "\t Tokenized LLAMA3:The Ġisland 's Ġbur geon ing Ġeconomic Ġsignificance Ġpro pelled Ġpopulation Ġgrowth , Ġand Ġby Ġthe Ġmiddle Ġof Ġthe Ġ 15 th Ġcentury ĠMade ira Ġwas Ġhome Ġto Ġ 800 Ġfamilies .\n",
      "\t Unique Tokens GPT2: {'Ġ800', 'Ġ15'}\n",
      "\t Unique Tokens LLAMA3: {'800', '15', 'Ġ'}\n",
      "Text 2: Madeira proved to be uninhabitable.\n",
      "\t Tokenized GPT2:M ade ira Ġproved Ġto Ġbe Ġun in hab itable .\n",
      "\t Tokenized LLAMA3:M ade ira Ġproved Ġto Ġbe Ġun in hab itable .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: The anthropologist Napoleon Chagnon has shown that Yanomamo men who have killed other men have more wives and more offspring than average guys.\n",
      "\t Tokenized GPT2:The Ġanthrop ologist ĠNapoleon ĠCh agn on Ġhas Ġshown Ġthat ĠYan om amo Ġmen Ġwho Ġhave Ġkilled Ġother Ġmen Ġhave Ġmore Ġwives Ġand Ġmore Ġoffspring Ġthan Ġaverage Ġguys .\n",
      "\t Tokenized LLAMA3:The Ġanthrop ologist ĠNap oleon ĠCh agn on Ġhas Ġshown Ġthat ĠYan om amo Ġmen Ġwho Ġhave Ġkilled Ġother Ġmen Ġhave Ġmore Ġwives Ġand Ġmore Ġoffspring Ġthan Ġaverage Ġguys .\n",
      "\t Unique Tokens GPT2: {'ĠNapoleon'}\n",
      "\t Unique Tokens LLAMA3: {'oleon', 'ĠNap'}\n",
      "Text 2: Yanomamo men who kill other men have better chances at getting more wives.\n",
      "\t Tokenized GPT2:Y an om amo Ġmen Ġwho Ġkill Ġother Ġmen Ġhave Ġbetter Ġchances Ġat Ġgetting Ġmore Ġwives .\n",
      "\t Tokenized LLAMA3:Y an om amo Ġmen Ġwho Ġkill Ġother Ġmen Ġhave Ġbetter Ġchances Ġat Ġgetting Ġmore Ġwives .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: i mean that's a real attractive option if you have the the technology for it all it was was you know i mean she just used a phone modem and she was like she was sitting in the office\n",
      "\t Tokenized GPT2:i Ġmean Ġthat 's Ġa Ġreal Ġattractive Ġoption Ġif Ġyou Ġhave Ġthe Ġthe Ġtechnology Ġfor Ġit Ġall Ġit Ġwas Ġwas Ġyou Ġknow Ġi Ġmean Ġshe Ġjust Ġused Ġa Ġphone Ġmodem Ġand Ġshe Ġwas Ġlike Ġshe Ġwas Ġsitting Ġin Ġthe Ġoffice\n",
      "\t Tokenized LLAMA3:i Ġmean Ġthat 's Ġa Ġreal Ġattractive Ġoption Ġif Ġyou Ġhave Ġthe Ġthe Ġtechnology Ġfor Ġit Ġall Ġit Ġwas Ġwas Ġyou Ġknow Ġi Ġmean Ġshe Ġjust Ġused Ġa Ġphone Ġmodem Ġand Ġshe Ġwas Ġlike Ġshe Ġwas Ġsitting Ġin Ġthe Ġoffice\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: She used a phone modem but it was very different than if she were in the office. \n",
      "\t Tokenized GPT2:She Ġused Ġa Ġphone Ġmodem Ġbut Ġit Ġwas Ġvery Ġdifferent Ġthan Ġif Ġshe Ġwere Ġin Ġthe Ġoffice . Ġ\n",
      "\t Tokenized LLAMA3:She Ġused Ġa Ġphone Ġmodem Ġbut Ġit Ġwas Ġvery Ġdifferent Ġthan Ġif Ġshe Ġwere Ġin Ġthe Ġoffice . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Not only must capital goods be replaced as they depreciate, but new generations of workers must be comparably\n",
      "\t Tokenized GPT2:Not Ġonly Ġmust Ġcapital Ġgoods Ġbe Ġreplaced Ġas Ġthey Ġdep reci ate , Ġbut Ġnew Ġgenerations Ġof Ġworkers Ġmust Ġbe Ġcompar ably\n",
      "\t Tokenized LLAMA3:Not Ġonly Ġmust Ġcapital Ġgoods Ġbe Ġreplaced Ġas Ġthey Ġdep reci ate , Ġbut Ġnew Ġgenerations Ġof Ġworkers Ġmust Ġbe Ġcompar ably\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Capital goods are able to last for eternity.\n",
      "\t Tokenized GPT2:Cap ital Ġgoods Ġare Ġable Ġto Ġlast Ġfor Ġeternity .\n",
      "\t Tokenized LLAMA3:Cap ital Ġgoods Ġare Ġable Ġto Ġlast Ġfor Ġeternity .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: This provides insight into the important Japanese concept of katachi (form), the rough equivalent of  It isn't what you do; it's the way that you do it.  \n",
      "\t Tokenized GPT2:This Ġprovides Ġinsight Ġinto Ġthe Ġimportant ĠJapanese Ġconcept Ġof Ġk at achi Ġ( form ), Ġthe Ġrough Ġequivalent Ġof Ġ ĠIt Ġisn 't Ġwhat Ġyou Ġdo ; Ġit 's Ġthe Ġway Ġthat Ġyou Ġdo Ġit . ĠĠ\n",
      "\t Tokenized LLAMA3:This Ġprovides Ġinsight Ġinto Ġthe Ġimportant ĠJapanese Ġconcept Ġof Ġk at achi Ġ( form ), Ġthe Ġrough Ġequivalent Ġof Ġ ĠIt Ġisn 't Ġwhat Ġyou Ġdo ; Ġit 's Ġthe Ġway Ġthat Ġyou Ġdo Ġit . ĠĠ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: All Japanese people abide by the concept of katachi.\n",
      "\t Tokenized GPT2:All ĠJapanese Ġpeople Ġab ide Ġby Ġthe Ġconcept Ġof Ġk at achi .\n",
      "\t Tokenized LLAMA3:All ĠJapanese Ġpeople Ġab ide Ġby Ġthe Ġconcept Ġof Ġk at achi .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Outside, set in manicured gardens, are the remains of the Abbey of Holyrood.\n",
      "\t Tokenized GPT2:Out side , Ġset Ġin Ġman ic ured Ġgardens , Ġare Ġthe Ġremains Ġof Ġthe ĠAb bey Ġof ĠHoly ro od .\n",
      "\t Tokenized LLAMA3:Out side , Ġset Ġin Ġman ic ured Ġgardens , Ġare Ġthe Ġremains Ġof Ġthe ĠAb bey Ġof ĠHoly ro od .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The gardens containing the remains of the Abbey of Holyrood are in disarray and not well-kept.\n",
      "\t Tokenized GPT2:The Ġgardens Ġcontaining Ġthe Ġremains Ġof Ġthe ĠAb bey Ġof ĠHoly ro od Ġare Ġin Ġdis array Ġand Ġnot Ġwell - ke pt .\n",
      "\t Tokenized LLAMA3:The Ġgardens Ġcontaining Ġthe Ġremains Ġof Ġthe ĠAb bey Ġof ĠHoly ro od Ġare Ġin Ġdis array Ġand Ġnot Ġwell - ke pt .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: It cannot be outlawed.\n",
      "\t Tokenized GPT2:It Ġcannot Ġbe Ġout law ed .\n",
      "\t Tokenized LLAMA3:It Ġcannot Ġbe Ġout law ed .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Abortion cannot be outlawed.\n",
      "\t Tokenized GPT2:Ab ortion Ġcannot Ġbe Ġout law ed .\n",
      "\t Tokenized LLAMA3:Ab ortion Ġcannot Ġbe Ġout law ed .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1:  The leaves of the papyrus were dried and used by Ancient Egyptians as a form of paper.\n",
      "\t Tokenized GPT2:ĠThe Ġleaves Ġof Ġthe Ġpap yrus Ġwere Ġdried Ġand Ġused Ġby ĠAncient ĠEgypt ians Ġas Ġa Ġform Ġof Ġpaper .\n",
      "\t Tokenized LLAMA3:ĠThe Ġleaves Ġof Ġthe Ġpap yrus Ġwere Ġdried Ġand Ġused Ġby ĠAn cient ĠEgypt ians Ġas Ġa Ġform Ġof Ġpaper .\n",
      "\t Unique Tokens GPT2: {'ĠAncient'}\n",
      "\t Unique Tokens LLAMA3: {'cient', 'ĠAn'}\n",
      "Text 2: Papyrus paper was only used by wealthy Egyptians because it was so expensive.\n",
      "\t Tokenized GPT2:P ap yrus Ġpaper Ġwas Ġonly Ġused Ġby Ġwealthy ĠEgypt ians Ġbecause Ġit Ġwas Ġso Ġexpensive .\n",
      "\t Tokenized LLAMA3:P ap yrus Ġpaper Ġwas Ġonly Ġused Ġby Ġwealthy ĠEgypt ians Ġbecause Ġit Ġwas Ġso Ġexpensive .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: One opportunist who stayed was Octavius Decatur Gass.\n",
      "\t Tokenized GPT2:One Ġopport un ist Ġwho Ġstayed Ġwas ĠOct av ius ĠDec atur ĠG ass .\n",
      "\t Tokenized LLAMA3:One Ġopport un ist Ġwho Ġstayed Ġwas ĠOct av ius ĠDec atur ĠG ass .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Octavius described himself as an opportunist often. \n",
      "\t Tokenized GPT2:Oct av ius Ġdescribed Ġhimself Ġas Ġan Ġopport un ist Ġoften . Ġ\n",
      "\t Tokenized LLAMA3:Oct av ius Ġdescribed Ġhimself Ġas Ġan Ġopport un ist Ġoften . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Many users commented on the effectiveness of the new technology in promoting closer relationships among providers.\n",
      "\t Tokenized GPT2:Many Ġusers Ġcommented Ġon Ġthe Ġeffectiveness Ġof Ġthe Ġnew Ġtechnology Ġin Ġpromoting Ġcloser Ġrelationships Ġamong Ġproviders .\n",
      "\t Tokenized LLAMA3:Many Ġusers Ġcommented Ġon Ġthe Ġeffectiveness Ġof Ġthe Ġnew Ġtechnology Ġin Ġpromoting Ġcloser Ġrelationships Ġamong Ġproviders .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: They were disappointed to hear the consumer complaints.\n",
      "\t Tokenized GPT2:They Ġwere Ġdisappointed Ġto Ġhear Ġthe Ġconsumer Ġcomplaints .\n",
      "\t Tokenized LLAMA3:They Ġwere Ġdisappointed Ġto Ġhear Ġthe Ġconsumer Ġcomplaints .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Recent SAB deliberations on mortality and morbidity valuation approaches suggest that some adjustments to unit values are appropriate to reflect economic theory (EPA-SAB-EEAC-00-013, 2000).\n",
      "\t Tokenized GPT2:Recent ĠS AB Ġdeliber ations Ġon Ġmortality Ġand Ġmorbid ity Ġvaluation Ġapproaches Ġsuggest Ġthat Ġsome Ġadjustments Ġto Ġunit Ġvalues Ġare Ġappropriate Ġto Ġreflect Ġeconomic Ġtheory Ġ( E PA - S AB - EE AC - 00 - 013 , Ġ2000 ).\n",
      "\t Tokenized LLAMA3:Rec ent ĠS AB Ġdeliber ations Ġon Ġmortality Ġand Ġmorbid ity Ġval uation Ġapproaches Ġsuggest Ġthat Ġsome Ġadjustments Ġto Ġunit Ġvalues Ġare Ġappropriate Ġto Ġreflect Ġeconomic Ġtheory Ġ( E PA -S AB - EE AC - 00 - 013 , Ġ 200 0 ).\n",
      "\t Unique Tokens GPT2: {'Ġvaluation', 'S', 'Recent', 'Ġ2000'}\n",
      "\t Unique Tokens LLAMA3: {'ent', 'Ġ', 'Ġval', '-S', '0', 'Rec', 'uation', '200'}\n",
      "Text 2: Economic theory is the deciding factor when it comes to valuation.\n",
      "\t Tokenized GPT2:E conomic Ġtheory Ġis Ġthe Ġdeciding Ġfactor Ġwhen Ġit Ġcomes Ġto Ġvaluation .\n",
      "\t Tokenized LLAMA3:E conomic Ġtheory Ġis Ġthe Ġdeciding Ġfactor Ġwhen Ġit Ġcomes Ġto Ġval uation .\n",
      "\t Unique Tokens GPT2: {'Ġvaluation'}\n",
      "\t Unique Tokens LLAMA3: {'uation', 'Ġval'}\n",
      "==entailment==\n",
      "Text 1: Monday's Question (No.\n",
      "\t Tokenized GPT2:Monday 's ĠQuestion Ġ( No .\n",
      "\t Tokenized LLAMA3:Monday 's ĠQuestion Ġ( No .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: There was a question on Monday.\n",
      "\t Tokenized GPT2:There Ġwas Ġa Ġquestion Ġon ĠMonday .\n",
      "\t Tokenized LLAMA3:There Ġwas Ġa Ġquestion Ġon ĠMonday .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: yeah so it's easy to do i'm actually interested in getting one of those kind of my wife has been talking about this in the past couple of years one of those kind of campers that pop-up so it's about uh maybe eight foot square and but only about two feet tall and when you get to where you're going it\n",
      "\t Tokenized GPT2:yeah Ġso Ġit 's Ġeasy Ġto Ġdo Ġi 'm Ġactually Ġinterested Ġin Ġgetting Ġone Ġof Ġthose Ġkind Ġof Ġmy Ġwife Ġhas Ġbeen Ġtalking Ġabout Ġthis Ġin Ġthe Ġpast Ġcouple Ġof Ġyears Ġone Ġof Ġthose Ġkind Ġof Ġcamp ers Ġthat Ġpop - up Ġso Ġit 's Ġabout Ġuh Ġmaybe Ġeight Ġfoot Ġsquare Ġand Ġbut Ġonly Ġabout Ġtwo Ġfeet Ġtall Ġand Ġwhen Ġyou Ġget Ġto Ġwhere Ġyou 're Ġgoing Ġit\n",
      "\t Tokenized LLAMA3:yeah Ġso Ġit 's Ġeasy Ġto Ġdo Ġi 'm Ġactually Ġinterested Ġin Ġgetting Ġone Ġof Ġthose Ġkind Ġof Ġmy Ġwife Ġhas Ġbeen Ġtalking Ġabout Ġthis Ġin Ġthe Ġpast Ġcouple Ġof Ġyears Ġone Ġof Ġthose Ġkind Ġof Ġcamp ers Ġthat Ġpop -up Ġso Ġit 's Ġabout Ġuh Ġmaybe Ġeight Ġfoot Ġsquare Ġand Ġbut Ġonly Ġabout Ġtwo Ġfeet Ġtall Ġand Ġwhen Ġyou Ġget Ġto Ġwhere Ġyou 're Ġgoing Ġit\n",
      "\t Unique Tokens GPT2: {'-', 'up'}\n",
      "\t Unique Tokens LLAMA3: {'-up'}\n",
      "Text 2: I don't want a camper like that.\n",
      "\t Tokenized GPT2:I Ġdon 't Ġwant Ġa Ġcam per Ġlike Ġthat .\n",
      "\t Tokenized LLAMA3:I Ġdon 't Ġwant Ġa Ġcam per Ġlike Ġthat .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Larger boats for up to 20 people, plus crew, offer organized gourmet cruises.\n",
      "\t Tokenized GPT2:L ar ger Ġboats Ġfor Ġup Ġto Ġ20 Ġpeople , Ġplus Ġcrew , Ġoffer Ġorganized Ġg our met Ġcru ises .\n",
      "\t Tokenized LLAMA3:L ar ger Ġboats Ġfor Ġup Ġto Ġ 20 Ġpeople , Ġplus Ġcrew , Ġoffer Ġorganized Ġg our met Ġcru ises .\n",
      "\t Unique Tokens GPT2: {'Ġ20'}\n",
      "\t Unique Tokens LLAMA3: {'20', 'Ġ'}\n",
      "Text 2: Larger boats that can fit up to 20 people (not including the crew), have gourmet cruises.\n",
      "\t Tokenized GPT2:L ar ger Ġboats Ġthat Ġcan Ġfit Ġup Ġto Ġ20 Ġpeople Ġ( not Ġincluding Ġthe Ġcrew ), Ġhave Ġg our met Ġcru ises .\n",
      "\t Tokenized LLAMA3:L ar ger Ġboats Ġthat Ġcan Ġfit Ġup Ġto Ġ 20 Ġpeople Ġ( not Ġincluding Ġthe Ġcrew ), Ġhave Ġg our met Ġcru ises .\n",
      "\t Unique Tokens GPT2: {'Ġ20'}\n",
      "\t Unique Tokens LLAMA3: {'20', 'Ġ'}\n",
      "==neutral==\n",
      "Text 1: Don't forget to take a change of clothing and a towel.\n",
      "\t Tokenized GPT2:Don 't Ġforget Ġto Ġtake Ġa Ġchange Ġof Ġclothing Ġand Ġa Ġtowel .\n",
      "\t Tokenized LLAMA3:Don 't Ġforget Ġto Ġtake Ġa Ġchange Ġof Ġclothing Ġand Ġa Ġtowel .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: You should buy new clothes and other stuff.\n",
      "\t Tokenized GPT2:You Ġshould Ġbuy Ġnew Ġclothes Ġand Ġother Ġstuff .\n",
      "\t Tokenized LLAMA3:You Ġshould Ġbuy Ġnew Ġclothes Ġand Ġother Ġstuff .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: farmworkers conducted by the U.S.\n",
      "\t Tokenized GPT2:f arm workers Ġconducted Ġby Ġthe ĠU . S .\n",
      "\t Tokenized LLAMA3:f arm work ers Ġconducted Ġby Ġthe ĠU .S .\n",
      "\t Unique Tokens GPT2: {'S', 'workers'}\n",
      "\t Unique Tokens LLAMA3: {'work', '.S', 'ers'}\n",
      "Text 2: Some farm laborers were sampled.\n",
      "\t Tokenized GPT2:Some Ġfarm Ġlab ore rs Ġwere Ġsampled .\n",
      "\t Tokenized LLAMA3:Some Ġfarm Ġlab ore rs Ġwere Ġsampl ed .\n",
      "\t Unique Tokens GPT2: {'Ġsampled'}\n",
      "\t Unique Tokens LLAMA3: {'ed', 'Ġsampl'}\n",
      "==neutral==\n",
      "Text 1: there and they uh they in fact they had this was in uh the late twenties and they in fact used some of the equipment that had been left over and uh he turned them down it it's interesting that that most people don't realize how small the canal is have you ever been there\n",
      "\t Tokenized GPT2:there Ġand Ġthey Ġuh Ġthey Ġin Ġfact Ġthey Ġhad Ġthis Ġwas Ġin Ġuh Ġthe Ġlate Ġtwent ies Ġand Ġthey Ġin Ġfact Ġused Ġsome Ġof Ġthe Ġequipment Ġthat Ġhad Ġbeen Ġleft Ġover Ġand Ġuh Ġhe Ġturned Ġthem Ġdown Ġit Ġit 's Ġinteresting Ġthat Ġthat Ġmost Ġpeople Ġdon 't Ġrealize Ġhow Ġsmall Ġthe Ġcanal Ġis Ġhave Ġyou Ġever Ġbeen Ġthere\n",
      "\t Tokenized LLAMA3:there Ġand Ġthey Ġuh Ġthey Ġin Ġfact Ġthey Ġhad Ġthis Ġwas Ġin Ġuh Ġthe Ġlate Ġtwent ies Ġand Ġthey Ġin Ġfact Ġused Ġsome Ġof Ġthe Ġequipment Ġthat Ġhad Ġbeen Ġleft Ġover Ġand Ġuh Ġhe Ġturned Ġthem Ġdown Ġit Ġit 's Ġinteresting Ġthat Ġthat Ġmost Ġpeople Ġdon 't Ġrealize Ġhow Ġsmall Ġthe Ġcanal Ġis Ġhave Ġyou Ġever Ġbeen Ġthere\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: This was in 1928\n",
      "\t Tokenized GPT2:This Ġwas Ġin Ġ19 28\n",
      "\t Tokenized LLAMA3:This Ġwas Ġin Ġ 192 8\n",
      "\t Unique Tokens GPT2: {'28', 'Ġ19'}\n",
      "\t Unique Tokens LLAMA3: {'192', 'Ġ', '8'}\n",
      "==neutral==\n",
      "Text 1: Carmel Man, a relation of the Neanderthal family, lived here 600,000 years ago.\n",
      "\t Tokenized GPT2:C arm el ĠMan , Ġa Ġrelation Ġof Ġthe ĠNe ander thal Ġfamily , Ġlived Ġhere Ġ600 , 000 Ġyears Ġago .\n",
      "\t Tokenized LLAMA3:C arm el ĠMan , Ġa Ġrelation Ġof Ġthe ĠNe ander thal Ġfamily , Ġlived Ġhere Ġ 600 , 000 Ġyears Ġago .\n",
      "\t Unique Tokens GPT2: {'Ġ600'}\n",
      "\t Unique Tokens LLAMA3: {'600', 'Ġ'}\n",
      "Text 2: Carmel Man is still relatively well-preserved today, which is how we were able to estimate his age.\n",
      "\t Tokenized GPT2:C arm el ĠMan Ġis Ġstill Ġrelatively Ġwell - pres erved Ġtoday , Ġwhich Ġis Ġhow Ġwe Ġwere Ġable Ġto Ġestimate Ġhis Ġage .\n",
      "\t Tokenized LLAMA3:C arm el ĠMan Ġis Ġstill Ġrelatively Ġwell -pres erved Ġtoday , Ġwhich Ġis Ġhow Ġwe Ġwere Ġable Ġto Ġestimate Ġhis Ġage .\n",
      "\t Unique Tokens GPT2: {'-', 'pres'}\n",
      "\t Unique Tokens LLAMA3: {'-pres'}\n",
      "==neutral==\n",
      "Text 1: The WP says that the Paula Jones trial judge has had an interesting prior run-in with Bill Clinton.\n",
      "\t Tokenized GPT2:The ĠWP Ġsays Ġthat Ġthe ĠPaul a ĠJones Ġtrial Ġjudge Ġhas Ġhad Ġan Ġinteresting Ġprior Ġrun - in Ġwith ĠBill ĠClinton .\n",
      "\t Tokenized LLAMA3:The ĠWP Ġsays Ġthat Ġthe ĠPaul a ĠJones Ġtrial Ġjudge Ġhas Ġhad Ġan Ġinteresting Ġprior Ġrun -in Ġwith ĠBill ĠClinton .\n",
      "\t Unique Tokens GPT2: {'-', 'in'}\n",
      "\t Unique Tokens LLAMA3: {'-in'}\n",
      "Text 2: The man did not know he was a judge.\n",
      "\t Tokenized GPT2:The Ġman Ġdid Ġnot Ġknow Ġhe Ġwas Ġa Ġjudge .\n",
      "\t Tokenized LLAMA3:The Ġman Ġdid Ġnot Ġknow Ġhe Ġwas Ġa Ġjudge .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Also, other sorbent-based approaches in development may prove in time to be preferable to ACI, making the use of ACI only a conservative assumption.\n",
      "\t Tokenized GPT2:Also , Ġother Ġsor b ent - based Ġapproaches Ġin Ġdevelopment Ġmay Ġprove Ġin Ġtime Ġto Ġbe Ġpreferable Ġto ĠAC I , Ġmaking Ġthe Ġuse Ġof ĠAC I Ġonly Ġa Ġconservative Ġassumption .\n",
      "\t Tokenized LLAMA3:Also , Ġother Ġsor b ent -based Ġapproaches Ġin Ġdevelopment Ġmay Ġprove Ġin Ġtime Ġto Ġbe Ġprefer able Ġto ĠA CI , Ġmaking Ġthe Ġuse Ġof ĠA CI Ġonly Ġa Ġconservative Ġassumption .\n",
      "\t Unique Tokens GPT2: {'ĠAC', 'Ġpreferable', 'based', '-', 'I'}\n",
      "\t Unique Tokens LLAMA3: {'-based', 'CI', 'able', 'ĠA', 'Ġprefer'}\n",
      "Text 2: Sorbent-based approaches in development may be preferable to ACl.\n",
      "\t Tokenized GPT2:S orb ent - based Ġapproaches Ġin Ġdevelopment Ġmay Ġbe Ġpreferable Ġto ĠA Cl .\n",
      "\t Tokenized LLAMA3:S or b ent -based Ġapproaches Ġin Ġdevelopment Ġmay Ġbe Ġprefer able Ġto ĠA Cl .\n",
      "\t Unique Tokens GPT2: {'Ġpreferable', 'based', 'orb', '-'}\n",
      "\t Unique Tokens LLAMA3: {'-based', 'or', 'able', 'b', 'Ġprefer'}\n",
      "==entailment==\n",
      "Text 1: oh that might be kind of interesting is it\n",
      "\t Tokenized GPT2:oh Ġthat Ġmight Ġbe Ġkind Ġof Ġinteresting Ġis Ġit\n",
      "\t Tokenized LLAMA3:oh Ġthat Ġmight Ġbe Ġkind Ġof Ġinteresting Ġis Ġit\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: That sounds kinda interesting to me.\n",
      "\t Tokenized GPT2:That Ġsounds Ġkinda Ġinteresting Ġto Ġme .\n",
      "\t Tokenized LLAMA3:That Ġsounds Ġkinda Ġinteresting Ġto Ġme .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: They were so sure of themselves that they took it for granted he had made a mistake.\"\n",
      "\t Tokenized GPT2:They Ġwere Ġso Ġsure Ġof Ġthemselves Ġthat Ġthey Ġtook Ġit Ġfor Ġgranted Ġhe Ġhad Ġmade Ġa Ġmistake .\"\n",
      "\t Tokenized LLAMA3:They Ġwere Ġso Ġsure Ġof Ġthemselves Ġthat Ġthey Ġtook Ġit Ġfor Ġgranted Ġhe Ġhad Ġmade Ġa Ġmistake .\"\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He made a mistake but concealed it so it wasn't obvious.\n",
      "\t Tokenized GPT2:He Ġmade Ġa Ġmistake Ġbut Ġconcealed Ġit Ġso Ġit Ġwasn 't Ġobvious .\n",
      "\t Tokenized LLAMA3:He Ġmade Ġa Ġmistake Ġbut Ġconce aled Ġit Ġso Ġit Ġwasn 't Ġobvious .\n",
      "\t Unique Tokens GPT2: {'Ġconcealed'}\n",
      "\t Unique Tokens LLAMA3: {'aled', 'Ġconce'}\n",
      "==contradiction==\n",
      "Text 1: hi Mary have you gone visiting uh any new restaurants lately\n",
      "\t Tokenized GPT2:hi ĠMary Ġhave Ġyou Ġgone Ġvisiting Ġuh Ġany Ġnew Ġrestaurants Ġlately\n",
      "\t Tokenized LLAMA3:hi ĠMary Ġhave Ġyou Ġgone Ġvisiting Ġuh Ġany Ġnew Ġrestaurants Ġlately\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Hi Mary, thanks for going to the restaurant with me yesterday, it was fun.\n",
      "\t Tokenized GPT2:Hi ĠMary , Ġthanks Ġfor Ġgoing Ġto Ġthe Ġrestaurant Ġwith Ġme Ġyesterday , Ġit Ġwas Ġfun .\n",
      "\t Tokenized LLAMA3:Hi ĠMary , Ġthanks Ġfor Ġgoing Ġto Ġthe Ġrestaurant Ġwith Ġme Ġyesterday , Ġit Ġwas Ġfun .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Politically, it's anti-democratic, replacing congressional and executive branch decision-making.\n",
      "\t Tokenized GPT2:Pol it ically , Ġit 's Ġanti - dem ocratic , Ġreplacing Ġcongressional Ġand Ġexecutive Ġbranch Ġdecision - making .\n",
      "\t Tokenized LLAMA3:Pol it ically , Ġit 's Ġanti -dem ocratic , Ġreplacing Ġcongressional Ġand Ġexecutive Ġbranch Ġdecision -making .\n",
      "\t Unique Tokens GPT2: {'making', 'dem', '-'}\n",
      "\t Unique Tokens LLAMA3: {'-making', '-dem'}\n",
      "Text 2: It's anti-democratic and takes the decision-making away from the executive branch in DC.\n",
      "\t Tokenized GPT2:It 's Ġanti - dem ocratic Ġand Ġtakes Ġthe Ġdecision - making Ġaway Ġfrom Ġthe Ġexecutive Ġbranch Ġin ĠDC .\n",
      "\t Tokenized LLAMA3:It 's Ġanti -dem ocratic Ġand Ġtakes Ġthe Ġdecision -making Ġaway Ġfrom Ġthe Ġexecutive Ġbranch Ġin ĠDC .\n",
      "\t Unique Tokens GPT2: {'making', 'dem', '-'}\n",
      "\t Unique Tokens LLAMA3: {'-making', '-dem'}\n",
      "==neutral==\n",
      "Text 1: Hey, no problem, a fine policy.\n",
      "\t Tokenized GPT2:Hey , Ġno Ġproblem , Ġa Ġfine Ġpolicy .\n",
      "\t Tokenized LLAMA3:Hey , Ġno Ġproblem , Ġa Ġfine Ġpolicy .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: No trouble, the best policy.\n",
      "\t Tokenized GPT2:No Ġtrouble , Ġthe Ġbest Ġpolicy .\n",
      "\t Tokenized LLAMA3:No Ġtrouble , Ġthe Ġbest Ġpolicy .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: South Along the Caribbean\n",
      "\t Tokenized GPT2:South ĠAlong Ġthe ĠCaribbean\n",
      "\t Tokenized LLAMA3:South ĠAlong Ġthe ĠCaribbean\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Opposite of North along the Caribbean.\n",
      "\t Tokenized GPT2:O pp os ite Ġof ĠNorth Ġalong Ġthe ĠCaribbean .\n",
      "\t Tokenized LLAMA3:O pp os ite Ġof ĠNorth Ġalong Ġthe ĠCaribbean .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: On the northwestern Alpine frontier, a new state had appeared on the scene, destined to lead the movement to a united Italy.\n",
      "\t Tokenized GPT2:On Ġthe Ġnorth western ĠAl pine Ġfront ier , Ġa Ġnew Ġstate Ġhad Ġappeared Ġon Ġthe Ġscene , Ġdestined Ġto Ġlead Ġthe Ġmovement Ġto Ġa Ġunited ĠItaly .\n",
      "\t Tokenized LLAMA3:On Ġthe Ġnorth western ĠAl pine Ġfront ier , Ġa Ġnew Ġstate Ġhad Ġappeared Ġon Ġthe Ġscene , Ġdestined Ġto Ġlead Ġthe Ġmovement Ġto Ġa Ġunited ĠItaly .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The alpine frontier was separated from Italy by glaciers.\n",
      "\t Tokenized GPT2:The Ġal pine Ġfront ier Ġwas Ġseparated Ġfrom ĠItaly Ġby Ġgl ac iers .\n",
      "\t Tokenized LLAMA3:The Ġal pine Ġfront ier Ġwas Ġseparated Ġfrom ĠItaly Ġby Ġgl ac iers .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: And she came to you?\n",
      "\t Tokenized GPT2:And Ġshe Ġcame Ġto Ġyou ?\n",
      "\t Tokenized LLAMA3:And Ġshe Ġcame Ġto Ġyou ?\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The person asked if the woman came to him.\n",
      "\t Tokenized GPT2:The Ġperson Ġasked Ġif Ġthe Ġwoman Ġcame Ġto Ġhim .\n",
      "\t Tokenized LLAMA3:The Ġperson Ġasked Ġif Ġthe Ġwoman Ġcame Ġto Ġhim .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The river-beds are mostly too shallow for anything but flat-bottomed boats.\n",
      "\t Tokenized GPT2:The Ġriver - bed s Ġare Ġmostly Ġtoo Ġshallow Ġfor Ġanything Ġbut Ġflat - bottom ed Ġboats .\n",
      "\t Tokenized LLAMA3:The Ġriver -b eds Ġare Ġmostly Ġtoo Ġshallow Ġfor Ġanything Ġbut Ġflat -bottom ed Ġboats .\n",
      "\t Unique Tokens GPT2: {'bed', '-', 'bottom', 's'}\n",
      "\t Unique Tokens LLAMA3: {'-b', 'eds', '-bottom'}\n",
      "Text 2: Boats that are not flat-bottomed are illegal on the river.\n",
      "\t Tokenized GPT2:Bo ats Ġthat Ġare Ġnot Ġflat - bottom ed Ġare Ġillegal Ġon Ġthe Ġriver .\n",
      "\t Tokenized LLAMA3:Bo ats Ġthat Ġare Ġnot Ġflat -bottom ed Ġare Ġillegal Ġon Ġthe Ġriver .\n",
      "\t Unique Tokens GPT2: {'-', 'bottom'}\n",
      "\t Unique Tokens LLAMA3: {'-bottom'}\n",
      "==neutral==\n",
      "Text 1: Cases in Comparative\n",
      "\t Tokenized GPT2:C ases Ġin ĠCompar ative\n",
      "\t Tokenized LLAMA3:C ases Ġin ĠCompar ative\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Cases can be part of a legal matter.\n",
      "\t Tokenized GPT2:C ases Ġcan Ġbe Ġpart Ġof Ġa Ġlegal Ġmatter .\n",
      "\t Tokenized LLAMA3:C ases Ġcan Ġbe Ġpart Ġof Ġa Ġlegal Ġmatter .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The credibility of the United States working with its European partners in NATO is on the line.\n",
      "\t Tokenized GPT2:The Ġcredibility Ġof Ġthe ĠUnited ĠStates Ġworking Ġwith Ġits ĠEuropean Ġpartners Ġin ĠNATO Ġis Ġon Ġthe Ġline .\n",
      "\t Tokenized LLAMA3:The Ġcredibility Ġof Ġthe ĠUnited ĠStates Ġworking Ġwith Ġits ĠEuropean Ġpartners Ġin ĠNATO Ġis Ġon Ġthe Ġline .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The United States currently enjoys high favorability among its European allies who trust it completely.\n",
      "\t Tokenized GPT2:The ĠUnited ĠStates Ġcurrently Ġenjoys Ġhigh Ġfavor ability Ġamong Ġits ĠEuropean Ġallies Ġwho Ġtrust Ġit Ġcompletely .\n",
      "\t Tokenized LLAMA3:The ĠUnited ĠStates Ġcurrently Ġenjoys Ġhigh Ġfavor ability Ġamong Ġits ĠEuropean Ġallies Ġwho Ġtrust Ġit Ġcompletely .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Perhaps we should prepare a militia.\n",
      "\t Tokenized GPT2:Perhaps Ġwe Ġshould Ġprepare Ġa Ġmilit ia .\n",
      "\t Tokenized LLAMA3:Perhaps Ġwe Ġshould Ġprepare Ġa Ġmilit ia .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Maybe it would be a good idea if we prepared a militia.\n",
      "\t Tokenized GPT2:Maybe Ġit Ġwould Ġbe Ġa Ġgood Ġidea Ġif Ġwe Ġprepared Ġa Ġmilit ia .\n",
      "\t Tokenized LLAMA3:Maybe Ġit Ġwould Ġbe Ġa Ġgood Ġidea Ġif Ġwe Ġprepared Ġa Ġmilit ia .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: In this respect, bringing Steve Jobs back to save Apple is like bringing Gen.\n",
      "\t Tokenized GPT2:In Ġthis Ġrespect , Ġbringing ĠSteve ĠJobs Ġback Ġto Ġsave ĠApple Ġis Ġlike Ġbringing ĠGen .\n",
      "\t Tokenized LLAMA3:In Ġthis Ġrespect , Ġbringing ĠSteve ĠJobs Ġback Ġto Ġsave ĠApple Ġis Ġlike Ġbringing ĠGen .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Steve Jobs unretired in 2002.\n",
      "\t Tokenized GPT2:Steve ĠJobs Ġun ret ired Ġin Ġ2002 .\n",
      "\t Tokenized LLAMA3:Steve ĠJobs Ġun ret ired Ġin Ġ 200 2 .\n",
      "\t Unique Tokens GPT2: {'Ġ2002'}\n",
      "\t Unique Tokens LLAMA3: {'2', 'Ġ', '200'}\n",
      "==neutral==\n",
      "Text 1: Smart men make good thieves, as long as they're desperate.\n",
      "\t Tokenized GPT2:Smart Ġmen Ġmake Ġgood Ġth ieves , Ġas Ġlong Ġas Ġthey 're Ġdesperate .\n",
      "\t Tokenized LLAMA3:Sm art Ġmen Ġmake Ġgood Ġth ieves , Ġas Ġlong Ġas Ġthey 're Ġdesperate .\n",
      "\t Unique Tokens GPT2: {'Smart'}\n",
      "\t Unique Tokens LLAMA3: {'Sm', 'art'}\n",
      "Text 2: Most thieves are desperate.\n",
      "\t Tokenized GPT2:Most Ġth ieves Ġare Ġdesperate .\n",
      "\t Tokenized LLAMA3:Most Ġth ieves Ġare Ġdesperate .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Other pundits beam their opinions at us as through a time warp, from the hazy days of past administrations.\n",
      "\t Tokenized GPT2:Other Ġp und its Ġbeam Ġtheir Ġopinions Ġat Ġus Ġas Ġthrough Ġa Ġtime Ġwar p , Ġfrom Ġthe Ġha zy Ġdays Ġof Ġpast Ġadministr ations .\n",
      "\t Tokenized LLAMA3:Other Ġp und its Ġbeam Ġtheir Ġopinions Ġat Ġus Ġas Ġthrough Ġa Ġtime Ġwar p , Ġfrom Ġthe Ġha zy Ġdays Ġof Ġpast Ġadministr ations .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Other experts give their opinions through time warp from past administrations.\n",
      "\t Tokenized GPT2:Other Ġexperts Ġgive Ġtheir Ġopinions Ġthrough Ġtime Ġwar p Ġfrom Ġpast Ġadministr ations .\n",
      "\t Tokenized LLAMA3:Other Ġexperts Ġgive Ġtheir Ġopinions Ġthrough Ġtime Ġwar p Ġfrom Ġpast Ġadministr ations .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: uh it's in Georgia it's yeah it's right outside of Macon and and it's just a i like the way that i like the way that idea of the south is\n",
      "\t Tokenized GPT2:uh Ġit 's Ġin ĠGeorgia Ġit 's Ġyeah Ġit 's Ġright Ġoutside Ġof ĠMac on Ġand Ġand Ġit 's Ġjust Ġa Ġi Ġlike Ġthe Ġway Ġthat Ġi Ġlike Ġthe Ġway Ġthat Ġidea Ġof Ġthe Ġsouth Ġis\n",
      "\t Tokenized LLAMA3:uh Ġit 's Ġin ĠGeorgia Ġit 's Ġyeah Ġit 's Ġright Ġoutside Ġof ĠMac on Ġand Ġand Ġit 's Ġjust Ġa Ġi Ġlike Ġthe Ġway Ġthat Ġi Ġlike Ġthe Ġway Ġthat Ġidea Ġof Ġthe Ġsouth Ġis\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It's in Georgia but it's a very long distance from Macon.\n",
      "\t Tokenized GPT2:It 's Ġin ĠGeorgia Ġbut Ġit 's Ġa Ġvery Ġlong Ġdistance Ġfrom ĠMac on .\n",
      "\t Tokenized LLAMA3:It 's Ġin ĠGeorgia Ġbut Ġit 's Ġa Ġvery Ġlong Ġdistance Ġfrom ĠMac on .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Coast Guard rules establishing bridgeopening schedules).\n",
      "\t Tokenized GPT2:Co ast ĠGuard Ġrules Ġestablishing Ġbridge opening Ġschedules ).\n",
      "\t Tokenized LLAMA3:Co ast ĠGuard Ġrules Ġestablishing Ġbridge open ing Ġschedules ).\n",
      "\t Unique Tokens GPT2: {'opening'}\n",
      "\t Unique Tokens LLAMA3: {'open', 'ing'}\n",
      "Text 2: The Coast Guard is in charge of opening bridges.\n",
      "\t Tokenized GPT2:The ĠCoast ĠGuard Ġis Ġin Ġcharge Ġof Ġopening Ġbridges .\n",
      "\t Tokenized LLAMA3:The ĠCoast ĠGuard Ġis Ġin Ġcharge Ġof Ġopening Ġbridges .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: did oh they're they are everywhere they\n",
      "\t Tokenized GPT2:did Ġoh Ġthey 're Ġthey Ġare Ġeverywhere Ġthey\n",
      "\t Tokenized LLAMA3:did Ġoh Ġthey 're Ġthey Ġare Ġeverywhere Ġthey\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: They are all over.\n",
      "\t Tokenized GPT2:They Ġare Ġall Ġover .\n",
      "\t Tokenized LLAMA3:They Ġare Ġall Ġover .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: In the other sight he saw Adrin's hands cocking back a pair of dragon-hammered pistols.\n",
      "\t Tokenized GPT2:In Ġthe Ġother Ġsight Ġhe Ġsaw ĠAd rin 's Ġhands Ġcock ing Ġback Ġa Ġpair Ġof Ġdragon - ham mered Ġpistol s .\n",
      "\t Tokenized LLAMA3:In Ġthe Ġother Ġsight Ġhe Ġsaw ĠAd rin 's Ġhands Ġcock ing Ġback Ġa Ġpair Ġof Ġdragon -h am mered Ġpist ols .\n",
      "\t Unique Tokens GPT2: {'s', '-', 'Ġpistol', 'ham'}\n",
      "\t Unique Tokens LLAMA3: {'Ġpist', '-h', 'am', 'ols'}\n",
      "Text 2: Adrin was seen cocking a pistol in each hand out of the corner of his eye.\n",
      "\t Tokenized GPT2:Ad rin Ġwas Ġseen Ġcock ing Ġa Ġpistol Ġin Ġeach Ġhand Ġout Ġof Ġthe Ġcorner Ġof Ġhis Ġeye .\n",
      "\t Tokenized LLAMA3:Ad rin Ġwas Ġseen Ġcock ing Ġa Ġpistol Ġin Ġeach Ġhand Ġout Ġof Ġthe Ġcorner Ġof Ġhis Ġeye .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: In an effort to more thoroughly explore this topic, we expanded our discussions beyond the eight organizations that were the primary subjects of our study by requesting the Computer Security Institute to informally poll its most active members on this subject.\n",
      "\t Tokenized GPT2:In Ġan Ġeffort Ġto Ġmore Ġthoroughly Ġexplore Ġthis Ġtopic , Ġwe Ġexpanded Ġour Ġdiscussions Ġbeyond Ġthe Ġeight Ġorganizations Ġthat Ġwere Ġthe Ġprimary Ġsubjects Ġof Ġour Ġstudy Ġby Ġrequesting Ġthe ĠComputer ĠSecurity ĠInstitute Ġto Ġinform ally Ġpoll Ġits Ġmost Ġactive Ġmembers Ġon Ġthis Ġsubject .\n",
      "\t Tokenized LLAMA3:In Ġan Ġeffort Ġto Ġmore Ġthoroughly Ġexplore Ġthis Ġtopic , Ġwe Ġexpanded Ġour Ġdiscussions Ġbeyond Ġthe Ġeight Ġorganizations Ġthat Ġwere Ġthe Ġprimary Ġsubjects Ġof Ġour Ġstudy Ġby Ġrequesting Ġthe ĠComputer ĠSecurity ĠInstitute Ġto Ġinform ally Ġpoll Ġits Ġmost Ġactive Ġmembers Ġon Ġthis Ġsubject .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: We are discussing this topic with more than just the original organizations.  \n",
      "\t Tokenized GPT2:We Ġare Ġdiscussing Ġthis Ġtopic Ġwith Ġmore Ġthan Ġjust Ġthe Ġoriginal Ġorganizations . ĠĠ\n",
      "\t Tokenized LLAMA3:We Ġare Ġdiscussing Ġthis Ġtopic Ġwith Ġmore Ġthan Ġjust Ġthe Ġoriginal Ġorganizations . ĠĠ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: After four years, Clinton has learned how to avoid looking unpresidential.\n",
      "\t Tokenized GPT2:After Ġfour Ġyears , ĠClinton Ġhas Ġlearned Ġhow Ġto Ġavoid Ġlooking Ġun pres ident ial .\n",
      "\t Tokenized LLAMA3:After Ġfour Ġyears , ĠClinton Ġhas Ġlearned Ġhow Ġto Ġavoid Ġlooking Ġun pres ident ial .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Four years later, Clinton understands how to look presidential.\n",
      "\t Tokenized GPT2:Four Ġyears Ġlater , ĠClinton Ġunderstands Ġhow Ġto Ġlook Ġpresidential .\n",
      "\t Tokenized LLAMA3:Four Ġyears Ġlater , ĠClinton Ġunderstands Ġhow Ġto Ġlook Ġpresidential .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: During the Crimean War (1854 56) she set up a hospital in the huge Selimiye Barracks (Selimiye Kelase).\n",
      "\t Tokenized GPT2:During Ġthe ĠCrime an ĠWar Ġ( 18 54 Ġ56 ) Ġshe Ġset Ġup Ġa Ġhospital Ġin Ġthe Ġhuge ĠSel imi ye ĠBarr acks Ġ( Sel imi ye ĠKel ase ).\n",
      "\t Tokenized LLAMA3:During Ġthe ĠCrime an ĠWar Ġ( 185 4 Ġ 56 ) Ġshe Ġset Ġup Ġa Ġhospital Ġin Ġthe Ġhuge ĠSel imi ye ĠBarr acks Ġ( S el imi ye ĠKel ase ).\n",
      "\t Unique Tokens GPT2: {'54', 'Sel', '18', 'Ġ56'}\n",
      "\t Unique Tokens LLAMA3: {'S', '185', 'Ġ', '56', 'el', '4'}\n",
      "Text 2: The hospital set up in the Selimiye Barracks is no longer standing.\n",
      "\t Tokenized GPT2:The Ġhospital Ġset Ġup Ġin Ġthe ĠSel imi ye ĠBarr acks Ġis Ġno Ġlonger Ġstanding .\n",
      "\t Tokenized LLAMA3:The Ġhospital Ġset Ġup Ġin Ġthe ĠSel imi ye ĠBarr acks Ġis Ġno Ġlonger Ġstanding .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: As a result, their services may be more effective when conducted in the emergency department environment.\n",
      "\t Tokenized GPT2:As Ġa Ġresult , Ġtheir Ġservices Ġmay Ġbe Ġmore Ġeffective Ġwhen Ġconducted Ġin Ġthe Ġemergency Ġdepartment Ġenvironment .\n",
      "\t Tokenized LLAMA3:As Ġa Ġresult , Ġtheir Ġservices Ġmay Ġbe Ġmore Ġeffective Ġwhen Ġconducted Ġin Ġthe Ġemergency Ġdepartment Ġenvironment .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Their services might be more effective if they're done in the ED.\n",
      "\t Tokenized GPT2:Their Ġservices Ġmight Ġbe Ġmore Ġeffective Ġif Ġthey 're Ġdone Ġin Ġthe ĠED .\n",
      "\t Tokenized LLAMA3:Their Ġservices Ġmight Ġbe Ġmore Ġeffective Ġif Ġthey 're Ġdone Ġin Ġthe ĠED .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: i know that you know the further we go from Adam the worse the food is for you but God still somehow makes us all be able to still live i think it's a miracle we're all still alive after so many generations well the last couple of processed foods you know i mean but i don't know i like to i like to \n",
      "\t Tokenized GPT2:i Ġknow Ġthat Ġyou Ġknow Ġthe Ġfurther Ġwe Ġgo Ġfrom ĠAdam Ġthe Ġworse Ġthe Ġfood Ġis Ġfor Ġyou Ġbut ĠGod Ġstill Ġsomehow Ġmakes Ġus Ġall Ġbe Ġable Ġto Ġstill Ġlive Ġi Ġthink Ġit 's Ġa Ġmiracle Ġwe 're Ġall Ġstill Ġalive Ġafter Ġso Ġmany Ġgenerations Ġwell Ġthe Ġlast Ġcouple Ġof Ġprocessed Ġfoods Ġyou Ġknow Ġi Ġmean Ġbut Ġi Ġdon 't Ġknow Ġi Ġlike Ġto Ġi Ġlike Ġto Ġ\n",
      "\t Tokenized LLAMA3:i Ġknow Ġthat Ġyou Ġknow Ġthe Ġfurther Ġwe Ġgo Ġfrom ĠAdam Ġthe Ġworse Ġthe Ġfood Ġis Ġfor Ġyou Ġbut ĠGod Ġstill Ġsomehow Ġmakes Ġus Ġall Ġbe Ġable Ġto Ġstill Ġlive Ġi Ġthink Ġit 's Ġa Ġmiracle Ġwe 're Ġall Ġstill Ġalive Ġafter Ġso Ġmany Ġgenerations Ġwell Ġthe Ġlast Ġcouple Ġof Ġprocessed Ġfoods Ġyou Ġknow Ġi Ġmean Ġbut Ġi Ġdon 't Ġknow Ġi Ġlike Ġto Ġi Ġlike Ġto Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It is miraculous God still provides for us to this day.\n",
      "\t Tokenized GPT2:It Ġis Ġmir ac ulous ĠGod Ġstill Ġprovides Ġfor Ġus Ġto Ġthis Ġday .\n",
      "\t Tokenized LLAMA3:It Ġis Ġmir ac ulous ĠGod Ġstill Ġprovides Ġfor Ġus Ġto Ġthis Ġday .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: As a counterweight to the Singapore Chinese, he would bring in the North Borneo states of Sabah and Sarawak, granting them special privileges for their indigenous populations and funds for the development of their backward economies.\n",
      "\t Tokenized GPT2:As Ġa Ġcounter weight Ġto Ġthe ĠSingapore ĠChinese , Ġhe Ġwould Ġbring Ġin Ġthe ĠNorth ĠB orne o Ġstates Ġof ĠSab ah Ġand ĠSar aw ak , Ġgrant ing Ġthem Ġspecial Ġprivileges Ġfor Ġtheir Ġindigenous Ġpopulations Ġand Ġfunds Ġfor Ġthe Ġdevelopment Ġof Ġtheir Ġbackward Ġeconomies .\n",
      "\t Tokenized LLAMA3:As Ġa Ġcounter weight Ġto Ġthe ĠSingapore ĠChinese , Ġhe Ġwould Ġbring Ġin Ġthe ĠNorth ĠB orne o Ġstates Ġof ĠSab ah Ġand ĠSar aw ak , Ġgrant ing Ġthem Ġspecial Ġprivileges Ġfor Ġtheir Ġindigenous Ġpopulations Ġand Ġfunds Ġfor Ġthe Ġdevelopment Ġof Ġtheir Ġbackward Ġeconomies .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The states of Sabah and Sarawak committed mass genocide of their indigenous populations.\n",
      "\t Tokenized GPT2:The Ġstates Ġof ĠSab ah Ġand ĠSar aw ak Ġcommitted Ġmass Ġgenocide Ġof Ġtheir Ġindigenous Ġpopulations .\n",
      "\t Tokenized LLAMA3:The Ġstates Ġof ĠSab ah Ġand ĠSar aw ak Ġcommitted Ġmass Ġgen ocide Ġof Ġtheir Ġindigenous Ġpopulations .\n",
      "\t Unique Tokens GPT2: {'Ġgenocide'}\n",
      "\t Unique Tokens LLAMA3: {'Ġgen', 'ocide'}\n",
      "==entailment==\n",
      "Text 1: Tell me, how did those scribbled words on the envelope help you to discover that a will was made yesterday afternoon?\" Poirot smiled. \n",
      "\t Tokenized GPT2:Tell Ġme , Ġhow Ġdid Ġthose Ġscri bb led Ġwords Ġon Ġthe Ġenvelope Ġhelp Ġyou Ġto Ġdiscover Ġthat Ġa Ġwill Ġwas Ġmade Ġyesterday Ġafternoon ?\" ĠP oir ot Ġsmiled . Ġ\n",
      "\t Tokenized LLAMA3:Tell Ġme , Ġhow Ġdid Ġthose Ġscri bb led Ġwords Ġon Ġthe Ġenvelope Ġhelp Ġyou Ġto Ġdiscover Ġthat Ġa Ġwill Ġwas Ġmade Ġyesterday Ġafternoon ?\" ĠP oir ot Ġsmiled . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: How did you work out from that text that there was a new will?\n",
      "\t Tokenized GPT2:How Ġdid Ġyou Ġwork Ġout Ġfrom Ġthat Ġtext Ġthat Ġthere Ġwas Ġa Ġnew Ġwill ?\n",
      "\t Tokenized LLAMA3:How Ġdid Ġyou Ġwork Ġout Ġfrom Ġthat Ġtext Ġthat Ġthere Ġwas Ġa Ġnew Ġwill ?\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: A detailed English explanation of the plot is always provided, and wireless recorded commentary units are sometimes available.\n",
      "\t Tokenized GPT2:A Ġdetailed ĠEnglish Ġexplanation Ġof Ġthe Ġplot Ġis Ġalways Ġprovided , Ġand Ġwireless Ġrecorded Ġcommentary Ġunits Ġare Ġsometimes Ġavailable .\n",
      "\t Tokenized LLAMA3:A Ġdetailed ĠEnglish Ġexplanation Ġof Ġthe Ġplot Ġis Ġalways Ġprovided , Ġand Ġwireless Ġrecorded Ġcommentary Ġunits Ġare Ġsometimes Ġavailable .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: A detailed plot, written in English, is always available and an audio commentary is sometimes available.\n",
      "\t Tokenized GPT2:A Ġdetailed Ġplot , Ġwritten Ġin ĠEnglish , Ġis Ġalways Ġavailable Ġand Ġan Ġaudio Ġcommentary Ġis Ġsometimes Ġavailable .\n",
      "\t Tokenized LLAMA3:A Ġdetailed Ġplot , Ġwritten Ġin ĠEnglish , Ġis Ġalways Ġavailable Ġand Ġan Ġaudio Ġcommentary Ġis Ġsometimes Ġavailable .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Or to judge by the  Failing to nurse at night can lead to painful engorgement or even breast infection.\n",
      "\t Tokenized GPT2:Or Ġto Ġjudge Ġby Ġthe Ġ ĠF ailing Ġto Ġnurse Ġat Ġnight Ġcan Ġlead Ġto Ġpainful Ġeng org ement Ġor Ġeven Ġbreast Ġinfection .\n",
      "\t Tokenized LLAMA3:Or Ġto Ġjudge Ġby Ġthe Ġ ĠF ailing Ġto Ġnurse Ġat Ġnight Ġcan Ġlead Ġto Ġpainful Ġeng org ement Ġor Ġeven Ġbreast Ġinfection .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Mothers can go many hours without nursing.\n",
      "\t Tokenized GPT2:M others Ġcan Ġgo Ġmany Ġhours Ġwithout Ġnursing .\n",
      "\t Tokenized LLAMA3:M others Ġcan Ġgo Ġmany Ġhours Ġwithout Ġnursing .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Your man wouldn't have remained conscious after the first blow.\n",
      "\t Tokenized GPT2:Your Ġman Ġwouldn 't Ġhave Ġremained Ġconscious Ġafter Ġthe Ġfirst Ġblow .\n",
      "\t Tokenized LLAMA3:Your Ġman Ġwouldn 't Ġhave Ġremained Ġconscious Ġafter Ġthe Ġfirst Ġblow .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Your man stayed conscious even after the first blow.\n",
      "\t Tokenized GPT2:Your Ġman Ġstayed Ġconscious Ġeven Ġafter Ġthe Ġfirst Ġblow .\n",
      "\t Tokenized LLAMA3:Your Ġman Ġstayed Ġconscious Ġeven Ġafter Ġthe Ġfirst Ġblow .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: With most plants needing to install control equipment to meet these requirements, it is likely that this approach would lead to installation of controls that become obsolete and stranded capital investments as additional requirements are promulgated.\n",
      "\t Tokenized GPT2:With Ġmost Ġplants Ġneeding Ġto Ġinstall Ġcontrol Ġequipment Ġto Ġmeet Ġthese Ġrequirements , Ġit Ġis Ġlikely Ġthat Ġthis Ġapproach Ġwould Ġlead Ġto Ġinstallation Ġof Ġcontrols Ġthat Ġbecome Ġobsolete Ġand Ġstr anded Ġcapital Ġinvestments Ġas Ġadditional Ġrequirements Ġare Ġprom ul g ated .\n",
      "\t Tokenized LLAMA3:With Ġmost Ġplants Ġneeding Ġto Ġinstall Ġcontrol Ġequipment Ġto Ġmeet Ġthese Ġrequirements , Ġit Ġis Ġlikely Ġthat Ġthis Ġapproach Ġwould Ġlead Ġto Ġinstallation Ġof Ġcontrols Ġthat Ġbecome Ġobsolete Ġand Ġstr anded Ġcapital Ġinvestments Ġas Ġadditional Ġrequirements Ġare Ġprom ul g ated .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Most plants are already up to code.\n",
      "\t Tokenized GPT2:Most Ġplants Ġare Ġalready Ġup Ġto Ġcode .\n",
      "\t Tokenized LLAMA3:Most Ġplants Ġare Ġalready Ġup Ġto Ġcode .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Do you want to see historic sights and tour museums and art galleries?\n",
      "\t Tokenized GPT2:Do Ġyou Ġwant Ġto Ġsee Ġhistoric Ġsights Ġand Ġtour Ġmuse ums Ġand Ġart Ġgall eries ?\n",
      "\t Tokenized LLAMA3:Do Ġyou Ġwant Ġto Ġsee Ġhistoric Ġsights Ġand Ġtour Ġmuse ums Ġand Ġart Ġgall eries ?\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Since you love learning new things, would you like to visit some historic places, museums, and art galleries?\n",
      "\t Tokenized GPT2:Since Ġyou Ġlove Ġlearning Ġnew Ġthings , Ġwould Ġyou Ġlike Ġto Ġvisit Ġsome Ġhistoric Ġplaces , Ġmuse ums , Ġand Ġart Ġgall eries ?\n",
      "\t Tokenized LLAMA3:Since Ġyou Ġlove Ġlearning Ġnew Ġthings , Ġwould Ġyou Ġlike Ġto Ġvisit Ġsome Ġhistoric Ġplaces , Ġmuse ums , Ġand Ġart Ġgall eries ?\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Miller claimed the First Amendment (right to freedom of speech and association) rather than taking the Fifth (right against self-incrimination).\n",
      "\t Tokenized GPT2:M iller Ġclaimed Ġthe ĠFirst ĠAmendment Ġ( right Ġto Ġfreedom Ġof Ġspeech Ġand Ġassociation ) Ġrather Ġthan Ġtaking Ġthe ĠFifth Ġ( right Ġagainst Ġself - inc rimination ).\n",
      "\t Tokenized LLAMA3:M iller Ġclaimed Ġthe ĠFirst ĠAmendment Ġ( right Ġto Ġfreedom Ġof Ġspeech Ġand Ġassociation ) Ġrather Ġthan Ġtaking Ġthe ĠFifth Ġ( right Ġagainst Ġself -in cr imination ).\n",
      "\t Unique Tokens GPT2: {'-', 'inc', 'rimination'}\n",
      "\t Unique Tokens LLAMA3: {'-in', 'cr', 'imination'}\n",
      "Text 2: The man did not plead the Fifth.\n",
      "\t Tokenized GPT2:The Ġman Ġdid Ġnot Ġple ad Ġthe ĠFifth .\n",
      "\t Tokenized LLAMA3:The Ġman Ġdid Ġnot Ġple ad Ġthe ĠFifth .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The primary screen must be integrated into the standard intake procedure of the emergency setting and must be the responsibility of the staff to administer to all patients.\n",
      "\t Tokenized GPT2:The Ġprimary Ġscreen Ġmust Ġbe Ġintegrated Ġinto Ġthe Ġstandard Ġintake Ġprocedure Ġof Ġthe Ġemergency Ġsetting Ġand Ġmust Ġbe Ġthe Ġresponsibility Ġof Ġthe Ġstaff Ġto Ġadmin ister Ġto Ġall Ġpatients .\n",
      "\t Tokenized LLAMA3:The Ġprimary Ġscreen Ġmust Ġbe Ġintegrated Ġinto Ġthe Ġstandard Ġintake Ġprocedure Ġof Ġthe Ġemergency Ġsetting Ġand Ġmust Ġbe Ġthe Ġresponsibility Ġof Ġthe Ġstaff Ġto Ġadmin ister Ġto Ġall Ġpatients .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Integration of primary screens will prevent patients from leaving early.\n",
      "\t Tokenized GPT2:Integr ation Ġof Ġprimary Ġscreens Ġwill Ġprevent Ġpatients Ġfrom Ġleaving Ġearly .\n",
      "\t Tokenized LLAMA3:Integr ation Ġof Ġprimary Ġscreens Ġwill Ġprevent Ġpatients Ġfrom Ġleaving Ġearly .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: General Motors, for instance, lost $460 million to strikes in 1997, but investors treated the costs as a kind of extraordinary charge and valued the company as if the losses had never happened.\n",
      "\t Tokenized GPT2:General ĠMot ors , Ġfor Ġinstance , Ġlost Ġ$ 4 60 Ġmillion Ġto Ġstrikes Ġin Ġ1997 , Ġbut Ġinvestors Ġtreated Ġthe Ġcosts Ġas Ġa Ġkind Ġof Ġextraordinary Ġcharge Ġand Ġvalued Ġthe Ġcompany Ġas Ġif Ġthe Ġlosses Ġhad Ġnever Ġhappened .\n",
      "\t Tokenized LLAMA3:General ĠMot ors , Ġfor Ġinstance , Ġlost Ġ$ 460 Ġmillion Ġto Ġstrikes Ġin Ġ 199 7 , Ġbut Ġinvestors Ġtreated Ġthe Ġcosts Ġas Ġa Ġkind Ġof Ġextraordinary Ġcharge Ġand Ġvalued Ġthe Ġcompany Ġas Ġif Ġthe Ġlosses Ġhad Ġnever Ġhappened .\n",
      "\t Unique Tokens GPT2: {'4', 'Ġ1997', '60'}\n",
      "\t Unique Tokens LLAMA3: {'460', '7', '199', 'Ġ'}\n",
      "Text 2: GM lost a lot of money in labor disputes but was victorious in the end.\n",
      "\t Tokenized GPT2:GM Ġlost Ġa Ġlot Ġof Ġmoney Ġin Ġlabor Ġdisput es Ġbut Ġwas Ġvict orious Ġin Ġthe Ġend .\n",
      "\t Tokenized LLAMA3:GM Ġlost Ġa Ġlot Ġof Ġmoney Ġin Ġlabor Ġdisput es Ġbut Ġwas Ġvict orious Ġin Ġthe Ġend .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: the only problem is it's not large enough it only holds about i think they squeezed when Ryan struck out his five thousandth player they they squeezed about forty thousand people in there\n",
      "\t Tokenized GPT2:the Ġonly Ġproblem Ġis Ġit 's Ġnot Ġlarge Ġenough Ġit Ġonly Ġholds Ġabout Ġi Ġthink Ġthey Ġsqueezed Ġwhen ĠRyan Ġstruck Ġout Ġhis Ġfive Ġthousand th Ġplayer Ġthey Ġthey Ġsqueezed Ġabout Ġforty Ġthousand Ġpeople Ġin Ġthere\n",
      "\t Tokenized LLAMA3:the Ġonly Ġproblem Ġis Ġit 's Ġnot Ġlarge Ġenough Ġit Ġonly Ġholds Ġabout Ġi Ġthink Ġthey Ġsqueezed Ġwhen ĠRyan Ġstruck Ġout Ġhis Ġfive Ġthousand th Ġplayer Ġthey Ġthey Ġsqueezed Ġabout Ġforty Ġthousand Ġpeople Ġin Ġthere\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It doesn't hold many people so it was standing room only.\n",
      "\t Tokenized GPT2:It Ġdoesn 't Ġhold Ġmany Ġpeople Ġso Ġit Ġwas Ġstanding Ġroom Ġonly .\n",
      "\t Tokenized LLAMA3:It Ġdoesn 't Ġhold Ġmany Ġpeople Ġso Ġit Ġwas Ġstanding Ġroom Ġonly .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Paris and its immediate surroundings are a magnet for tourists, students, businessmen, artists, inventors ' in short, everyone except perhaps the farmer and fisherman, who may well come to the city to protest government policies.\n",
      "\t Tokenized GPT2:Par is Ġand Ġits Ġimmediate Ġsurroundings Ġare Ġa Ġmagnet Ġfor Ġtourists , Ġstudents , Ġbusiness men , Ġartists , Ġinvent ors Ġ' Ġin Ġshort , Ġeveryone Ġexcept Ġperhaps Ġthe Ġfarmer Ġand Ġfisher man , Ġwho Ġmay Ġwell Ġcome Ġto Ġthe Ġcity Ġto Ġprotest Ġgovernment Ġpolicies .\n",
      "\t Tokenized LLAMA3:Par is Ġand Ġits Ġimmediate Ġsurroundings Ġare Ġa Ġmagnet Ġfor Ġtourists , Ġstudents , Ġbusiness men , Ġartists , Ġinvent ors Ġ' Ġin Ġshort , Ġeveryone Ġexcept Ġperhaps Ġthe Ġfarmer Ġand Ġfisher man , Ġwho Ġmay Ġwell Ġcome Ġto Ġthe Ġcity Ġto Ġprotest Ġgovernment Ġpolicies .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Businessmen spend more time in Paris and its immediate surroundings than inventors do.\n",
      "\t Tokenized GPT2:Business men Ġspend Ġmore Ġtime Ġin ĠParis Ġand Ġits Ġimmediate Ġsurroundings Ġthan Ġinvent ors Ġdo .\n",
      "\t Tokenized LLAMA3:Business men Ġspend Ġmore Ġtime Ġin ĠParis Ġand Ġits Ġimmediate Ġsurroundings Ġthan Ġinvent ors Ġdo .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Then he turned to Tommy.\n",
      "\t Tokenized GPT2:Then Ġhe Ġturned Ġto ĠTommy .\n",
      "\t Tokenized LLAMA3:Then Ġhe Ġturned Ġto ĠTommy .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: He talked to Tommy.\n",
      "\t Tokenized GPT2:He Ġtalked Ġto ĠTommy .\n",
      "\t Tokenized LLAMA3:He Ġtalked Ġto ĠTommy .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The seven grants flow from a new Nonprofit Capacity Building program at the foundation, part of a trend among philanthropists to give money to help organizations grow stronger, rather than to the program services they provide.\n",
      "\t Tokenized GPT2:The Ġseven Ġgrants Ġflow Ġfrom Ġa Ġnew ĠNon profit ĠCap acity ĠBuilding Ġprogram Ġat Ġthe Ġfoundation , Ġpart Ġof Ġa Ġtrend Ġamong Ġphil anthrop ists Ġto Ġgive Ġmoney Ġto Ġhelp Ġorganizations Ġgrow Ġstronger , Ġrather Ġthan Ġto Ġthe Ġprogram Ġservices Ġthey Ġprovide .\n",
      "\t Tokenized LLAMA3:The Ġseven Ġgrants Ġflow Ġfrom Ġa Ġnew ĠNon pro fit ĠCap acity ĠBuilding Ġprogram Ġat Ġthe Ġfoundation , Ġpart Ġof Ġa Ġtrend Ġamong Ġphil an throp ists Ġto Ġgive Ġmoney Ġto Ġhelp Ġorganizations Ġgrow Ġstronger , Ġrather Ġthan Ġto Ġthe Ġprogram Ġservices Ġthey Ġprovide .\n",
      "\t Unique Tokens GPT2: {'profit', 'anthrop'}\n",
      "\t Unique Tokens LLAMA3: {'throp', 'pro', 'fit', 'an'}\n",
      "Text 2: The grants flow from a Nonprofit Capacity Building program at the foundation, exemplifying a trend among philanthropists to give money to grow organizations and then to take them over.\n",
      "\t Tokenized GPT2:The Ġgrants Ġflow Ġfrom Ġa ĠNon profit ĠCap acity ĠBuilding Ġprogram Ġat Ġthe Ġfoundation , Ġexempl ifying Ġa Ġtrend Ġamong Ġphil anthrop ists Ġto Ġgive Ġmoney Ġto Ġgrow Ġorganizations Ġand Ġthen Ġto Ġtake Ġthem Ġover .\n",
      "\t Tokenized LLAMA3:The Ġgrants Ġflow Ġfrom Ġa ĠNon pro fit ĠCap acity ĠBuilding Ġprogram Ġat Ġthe Ġfoundation , Ġexempl ifying Ġa Ġtrend Ġamong Ġphil an throp ists Ġto Ġgive Ġmoney Ġto Ġgrow Ġorganizations Ġand Ġthen Ġto Ġtake Ġthem Ġover .\n",
      "\t Unique Tokens GPT2: {'profit', 'anthrop'}\n",
      "\t Unique Tokens LLAMA3: {'throp', 'pro', 'fit', 'an'}\n",
      "==contradiction==\n",
      "Text 1: Yet, in the mouths of the white townsfolk of Salisbury, N.C., it sounds convincing.\n",
      "\t Tokenized GPT2:Yet , Ġin Ġthe Ġmouths Ġof Ġthe Ġwhite Ġtowns folk Ġof ĠSal is bury , ĠN . C ., Ġit Ġsounds Ġconvincing .\n",
      "\t Tokenized LLAMA3:Yet , Ġin Ġthe Ġmouths Ġof Ġthe Ġwhite Ġtowns fol k Ġof ĠSal is bury , ĠN .C ., Ġit Ġsounds Ġconvincing .\n",
      "\t Unique Tokens GPT2: {'folk', 'C'}\n",
      "\t Unique Tokens LLAMA3: {'fol', '.C', 'k'}\n",
      "Text 2: White people in Salisbury, N.C. don't believe it. \n",
      "\t Tokenized GPT2:White Ġpeople Ġin ĠSal is bury , ĠN . C . Ġdon 't Ġbelieve Ġit . Ġ\n",
      "\t Tokenized LLAMA3:White Ġpeople Ġin ĠSal is bury , ĠN .C . Ġdon 't Ġbelieve Ġit . Ġ\n",
      "\t Unique Tokens GPT2: {'C'}\n",
      "\t Unique Tokens LLAMA3: {'.C'}\n",
      "==contradiction==\n",
      "Text 1: As the road climbs toward the entrance, you'll pass fields full of Santorini's famed tomatoes growing on the steep slopes.\n",
      "\t Tokenized GPT2:As Ġthe Ġroad Ġclim bs Ġtoward Ġthe Ġentrance , Ġyou 'll Ġpass Ġfields Ġfull Ġof ĠSant or ini 's Ġfam ed Ġtomatoes Ġgrowing Ġon Ġthe Ġsteep Ġsl opes .\n",
      "\t Tokenized LLAMA3:As Ġthe Ġroad Ġclim bs Ġtoward Ġthe Ġentrance , Ġyou 'll Ġpass Ġfields Ġfull Ġof ĠSant or ini 's Ġfam ed Ġtomatoes Ġgrowing Ġon Ġthe Ġsteep Ġsl opes .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The climate in Santorini is not conducive to tomato farming.\n",
      "\t Tokenized GPT2:The Ġclimate Ġin ĠSant or ini Ġis Ġnot Ġcon duc ive Ġto Ġtomato Ġfarming .\n",
      "\t Tokenized LLAMA3:The Ġclimate Ġin ĠSant or ini Ġis Ġnot Ġcon duc ive Ġto Ġtomato Ġfarming .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: for the direct sunlight and stuff right but uh but i i haven't really found it too bad we've lived in our house about uh oh thirteen years i suppose and and really really only painted once and you know it was new when we bought it and we painted one time since then but you know it's probably going t\n",
      "\t Tokenized GPT2:for Ġthe Ġdirect Ġsunlight Ġand Ġstuff Ġright Ġbut Ġuh Ġbut Ġi Ġi Ġhaven 't Ġreally Ġfound Ġit Ġtoo Ġbad Ġwe 've Ġlived Ġin Ġour Ġhouse Ġabout Ġuh Ġoh Ġthirteen Ġyears Ġi Ġsuppose Ġand Ġand Ġreally Ġreally Ġonly Ġpainted Ġonce Ġand Ġyou Ġknow Ġit Ġwas Ġnew Ġwhen Ġwe Ġbought Ġit Ġand Ġwe Ġpainted Ġone Ġtime Ġsince Ġthen Ġbut Ġyou Ġknow Ġit 's Ġprobably Ġgoing Ġt\n",
      "\t Tokenized LLAMA3:for Ġthe Ġdirect Ġsunlight Ġand Ġstuff Ġright Ġbut Ġuh Ġbut Ġi Ġi Ġhaven 't Ġreally Ġfound Ġit Ġtoo Ġbad Ġwe 've Ġlived Ġin Ġour Ġhouse Ġabout Ġuh Ġoh Ġth irteen Ġyears Ġi Ġsuppose Ġand Ġand Ġreally Ġreally Ġonly Ġpainted Ġonce Ġand Ġyou Ġknow Ġit Ġwas Ġnew Ġwhen Ġwe Ġbought Ġit Ġand Ġwe Ġpainted Ġone Ġtime Ġsince Ġthen Ġbut Ġyou Ġknow Ġit 's Ġprobably Ġgoing Ġt\n",
      "\t Unique Tokens GPT2: {'Ġthirteen'}\n",
      "\t Unique Tokens LLAMA3: {'irteen', 'Ġth'}\n",
      "Text 2: I only had to paint the house once.\n",
      "\t Tokenized GPT2:I Ġonly Ġhad Ġto Ġpaint Ġthe Ġhouse Ġonce .\n",
      "\t Tokenized LLAMA3:I Ġonly Ġhad Ġto Ġpaint Ġthe Ġhouse Ġonce .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: She leaned back in her chair.\n",
      "\t Tokenized GPT2:She Ġleaned Ġback Ġin Ġher Ġchair .\n",
      "\t Tokenized LLAMA3:She Ġleaned Ġback Ġin Ġher Ġchair .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: She was sitting on a chair. \n",
      "\t Tokenized GPT2:She Ġwas Ġsitting Ġon Ġa Ġchair . Ġ\n",
      "\t Tokenized LLAMA3:She Ġwas Ġsitting Ġon Ġa Ġchair . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Tommy realized perfectly that in his own wits lay the only chance of escape, and behind his casual manner he was racking his brains furiously.\n",
      "\t Tokenized GPT2:Tom my Ġrealized Ġperfectly Ġthat Ġin Ġhis Ġown Ġw its Ġlay Ġthe Ġonly Ġchance Ġof Ġescape , Ġand Ġbehind Ġhis Ġcasual Ġmanner Ġhe Ġwas Ġr acking Ġhis Ġbrains Ġfur iously .\n",
      "\t Tokenized LLAMA3:Tom my Ġrealized Ġperfectly Ġthat Ġin Ġhis Ġown Ġw its Ġlay Ġthe Ġonly Ġchance Ġof Ġescape , Ġand Ġbehind Ġhis Ġcasual Ġmanner Ġhe Ġwas Ġr acking Ġhis Ġbrains Ġfur iously .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Tommy was keeping a calm demeanor, though his mind was racing with thoughts of escaping.\n",
      "\t Tokenized GPT2:Tom my Ġwas Ġkeeping Ġa Ġcalm Ġdem eanor , Ġthough Ġhis Ġmind Ġwas Ġracing Ġwith Ġthoughts Ġof Ġescaping .\n",
      "\t Tokenized LLAMA3:Tom my Ġwas Ġkeeping Ġa Ġcalm Ġdem eanor , Ġthough Ġhis Ġmind Ġwas Ġracing Ġwith Ġthoughts Ġof Ġescaping .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1:  The Romans never really infiltrated Ibiza, and even after the defeat of Hannibal in 202 b.c. during the Second Punic War their influence was restrained.\n",
      "\t Tokenized GPT2:ĠThe ĠRomans Ġnever Ġreally Ġinfiltr ated ĠI b iza , Ġand Ġeven Ġafter Ġthe Ġdefeat Ġof ĠHannibal Ġin Ġ20 2 Ġb . c . Ġduring Ġthe ĠSecond ĠPun ic ĠWar Ġtheir Ġinfluence Ġwas Ġrestr ained .\n",
      "\t Tokenized LLAMA3:ĠThe ĠRom ans Ġnever Ġreally Ġinfiltr ated ĠI b iza , Ġand Ġeven Ġafter Ġthe Ġdefeat Ġof ĠHannibal Ġin Ġ 202 Ġb .c . Ġduring Ġthe ĠSecond ĠPun ic ĠWar Ġtheir Ġinfluence Ġwas Ġrestr ained .\n",
      "\t Unique Tokens GPT2: {'ĠRomans', 'c', 'Ġ20', '2'}\n",
      "\t Unique Tokens LLAMA3: {'Ġ', 'ĠRom', 'ans', '.c', '202'}\n",
      "Text 2: The Romans infiltrated Ibiza.\n",
      "\t Tokenized GPT2:The ĠRomans Ġinfiltr ated ĠI b iza .\n",
      "\t Tokenized LLAMA3:The ĠRom ans Ġinfiltr ated ĠI b iza .\n",
      "\t Unique Tokens GPT2: {'ĠRomans'}\n",
      "\t Unique Tokens LLAMA3: {'ans', 'ĠRom'}\n",
      "==entailment==\n",
      "Text 1: Such a knowledgebased process enables decision makers to be reasonably certain about critical facets of the product under development when they need this knowledge.\n",
      "\t Tokenized GPT2:Such Ġa Ġknowledge based Ġprocess Ġenables Ġdecision Ġmakers Ġto Ġbe Ġreasonably Ġcertain Ġabout Ġcritical Ġfac ets Ġof Ġthe Ġproduct Ġunder Ġdevelopment Ġwhen Ġthey Ġneed Ġthis Ġknowledge .\n",
      "\t Tokenized LLAMA3:Such Ġa Ġknowledge based Ġprocess Ġenables Ġdecision Ġmakers Ġto Ġbe Ġreasonably Ġcertain Ġabout Ġcritical Ġfac ets Ġof Ġthe Ġproduct Ġunder Ġdevelopment Ġwhen Ġthey Ġneed Ġthis Ġknowledge .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The knowledge base will give them information needed for the product to be developed.\n",
      "\t Tokenized GPT2:The Ġknowledge Ġbase Ġwill Ġgive Ġthem Ġinformation Ġneeded Ġfor Ġthe Ġproduct Ġto Ġbe Ġdeveloped .\n",
      "\t Tokenized LLAMA3:The Ġknowledge Ġbase Ġwill Ġgive Ġthem Ġinformation Ġneeded Ġfor Ġthe Ġproduct Ġto Ġbe Ġdeveloped .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The Drawing Room was partially destroyed by fire in 1941, and its furnishings are faithful reproductions; the huge (repaired) Ming punch bowl is striking.\n",
      "\t Tokenized GPT2:The ĠDraw ing ĠRoom Ġwas Ġpartially Ġdestroyed Ġby Ġfire Ġin Ġ19 41 , Ġand Ġits Ġfurn ishing s Ġare Ġfaithful Ġreprodu ctions ; Ġthe Ġhuge Ġ( rep aired ) ĠMing Ġpunch Ġbowl Ġis Ġstriking .\n",
      "\t Tokenized LLAMA3:The ĠDraw ing ĠRoom Ġwas Ġpartially Ġdestroyed Ġby Ġfire Ġin Ġ 194 1 , Ġand Ġits Ġfurn ishing s Ġare Ġfaithful Ġreprodu ctions ; Ġthe Ġhuge Ġ( rep aired ) ĠM ing Ġpunch Ġbowl Ġis Ġstriking .\n",
      "\t Unique Tokens GPT2: {'41', 'ĠMing', 'Ġ19'}\n",
      "\t Unique Tokens LLAMA3: {'ĠM', '194', '1', 'Ġ'}\n",
      "Text 2: The 1941 fire spared the Drawing Room.\n",
      "\t Tokenized GPT2:The Ġ19 41 Ġfire Ġsp ared Ġthe ĠDraw ing ĠRoom .\n",
      "\t Tokenized LLAMA3:The Ġ 194 1 Ġfire Ġsp ared Ġthe ĠDraw ing ĠRoom .\n",
      "\t Unique Tokens GPT2: {'41', 'Ġ19'}\n",
      "\t Unique Tokens LLAMA3: {'1', '194', 'Ġ'}\n",
      "==contradiction==\n",
      "Text 1: Total electricity expenditures increase by about 15% to 30% depending on the year and the scenario (see Table 3, below, and the tables in Appendix 5.2 for more detail on the changing pattern of expenditures).\n",
      "\t Tokenized GPT2:Total Ġelectricity Ġexpend it ures Ġincrease Ġby Ġabout Ġ15 % Ġto Ġ30 % Ġdepending Ġon Ġthe Ġyear Ġand Ġthe Ġscenario Ġ( see ĠTable Ġ3 , Ġbelow , Ġand Ġthe Ġtables Ġin ĠAppend ix Ġ5 . 2 Ġfor Ġmore Ġdetail Ġon Ġthe Ġchanging Ġpattern Ġof Ġexpend it ures ).\n",
      "\t Tokenized LLAMA3:Total Ġelectricity Ġexpend it ures Ġincrease Ġby Ġabout Ġ 15 % Ġto Ġ 30 % Ġdepending Ġon Ġthe Ġyear Ġand Ġthe Ġscenario Ġ( see ĠTable Ġ 3 , Ġbelow , Ġand Ġthe Ġtables Ġin ĠAppend ix Ġ 5 . 2 Ġfor Ġmore Ġdetail Ġon Ġthe Ġchanging Ġpattern Ġof Ġexpend it ures ).\n",
      "\t Unique Tokens GPT2: {'Ġ3', 'Ġ5', 'Ġ30', 'Ġ15'}\n",
      "\t Unique Tokens LLAMA3: {'3', 'Ġ', '15', '30', '5'}\n",
      "Text 2: They were sad to see the costs go up 80% in a year.\n",
      "\t Tokenized GPT2:They Ġwere Ġsad Ġto Ġsee Ġthe Ġcosts Ġgo Ġup Ġ80 % Ġin Ġa Ġyear .\n",
      "\t Tokenized LLAMA3:They Ġwere Ġsad Ġto Ġsee Ġthe Ġcosts Ġgo Ġup Ġ 80 % Ġin Ġa Ġyear .\n",
      "\t Unique Tokens GPT2: {'Ġ80'}\n",
      "\t Unique Tokens LLAMA3: {'80', 'Ġ'}\n",
      "==entailment==\n",
      "Text 1: Or else it was administered in the brandy you gave her.\n",
      "\t Tokenized GPT2:Or Ġelse Ġit Ġwas Ġadministered Ġin Ġthe Ġbrand y Ġyou Ġgave Ġher .\n",
      "\t Tokenized LLAMA3:Or Ġelse Ġit Ġwas Ġadministered Ġin Ġthe Ġbrand y Ġyou Ġgave Ġher .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Otherwise it was put in the alcohol you gave her.\n",
      "\t Tokenized GPT2:Otherwise Ġit Ġwas Ġput Ġin Ġthe Ġalcohol Ġyou Ġgave Ġher .\n",
      "\t Tokenized LLAMA3:Otherwise Ġit Ġwas Ġput Ġin Ġthe Ġalcohol Ġyou Ġgave Ġher .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: My article does not say or imply that real earnings growth only reflects retentions and that dividend growth must be zero or that all valuation techniques are out the window for firms that don't pay dividends.\n",
      "\t Tokenized GPT2:My Ġarticle Ġdoes Ġnot Ġsay Ġor Ġimply Ġthat Ġreal Ġearnings Ġgrowth Ġonly Ġreflects Ġret ent ions Ġand Ġthat Ġdividend Ġgrowth Ġmust Ġbe Ġzero Ġor Ġthat Ġall Ġvaluation Ġtechniques Ġare Ġout Ġthe Ġwindow Ġfor Ġfirms Ġthat Ġdon 't Ġpay Ġdividend s .\n",
      "\t Tokenized LLAMA3:My Ġarticle Ġdoes Ġnot Ġsay Ġor Ġimply Ġthat Ġreal Ġearnings Ġgrowth Ġonly Ġreflects Ġret ent ions Ġand Ġthat Ġdividend Ġgrowth Ġmust Ġbe Ġzero Ġor Ġthat Ġall Ġval uation Ġtechniques Ġare Ġout Ġthe Ġwindow Ġfor Ġfirms Ġthat Ġdon 't Ġpay Ġdividend s .\n",
      "\t Unique Tokens GPT2: {'Ġvaluation'}\n",
      "\t Unique Tokens LLAMA3: {'uation', 'Ġval'}\n",
      "Text 2: My article simply implies that real earnings growth reflects only retentions and that dividend growth must be zero or that valuation techniques are unused for firms which don't pay dividends.\n",
      "\t Tokenized GPT2:My Ġarticle Ġsimply Ġimplies Ġthat Ġreal Ġearnings Ġgrowth Ġreflects Ġonly Ġret ent ions Ġand Ġthat Ġdividend Ġgrowth Ġmust Ġbe Ġzero Ġor Ġthat Ġvaluation Ġtechniques Ġare Ġunused Ġfor Ġfirms Ġwhich Ġdon 't Ġpay Ġdividend s .\n",
      "\t Tokenized LLAMA3:My Ġarticle Ġsimply Ġimplies Ġthat Ġreal Ġearnings Ġgrowth Ġreflects Ġonly Ġret ent ions Ġand Ġthat Ġdividend Ġgrowth Ġmust Ġbe Ġzero Ġor Ġthat Ġval uation Ġtechniques Ġare Ġunused Ġfor Ġfirms Ġwhich Ġdon 't Ġpay Ġdividend s .\n",
      "\t Unique Tokens GPT2: {'Ġvaluation'}\n",
      "\t Unique Tokens LLAMA3: {'uation', 'Ġval'}\n",
      "==neutral==\n",
      "Text 1: and the like a guy does it and he has his own pigs\n",
      "\t Tokenized GPT2:and Ġthe Ġlike Ġa Ġguy Ġdoes Ġit Ġand Ġhe Ġhas Ġhis Ġown Ġpigs\n",
      "\t Tokenized LLAMA3:and Ġthe Ġlike Ġa Ġguy Ġdoes Ġit Ġand Ġhe Ġhas Ġhis Ġown Ġpigs\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The guy is a pig farmer in Iowa.\n",
      "\t Tokenized GPT2:The Ġguy Ġis Ġa Ġpig Ġfarmer Ġin ĠIowa .\n",
      "\t Tokenized LLAMA3:The Ġguy Ġis Ġa Ġpig Ġfarmer Ġin ĠIowa .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Then Shuman claims that Linux provides no graphical user interface.\n",
      "\t Tokenized GPT2:Then ĠSh uman Ġclaims Ġthat ĠLinux Ġprovides Ġno Ġgraphical Ġuser Ġinterface .\n",
      "\t Tokenized LLAMA3:Then ĠSh uman Ġclaims Ġthat ĠLinux Ġprovides Ġno Ġgraphical Ġuser Ġinterface .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: They knew what they were talking about.\n",
      "\t Tokenized GPT2:They Ġknew Ġwhat Ġthey Ġwere Ġtalking Ġabout .\n",
      "\t Tokenized LLAMA3:They Ġknew Ġwhat Ġthey Ġwere Ġtalking Ġabout .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: The universal credibility problem with polling is that wordsmithing and mathematics don't mix, and never will.\n",
      "\t Tokenized GPT2:The Ġuniversal Ġcredibility Ġproblem Ġwith Ġpolling Ġis Ġthat Ġwords mith ing Ġand Ġmathematics Ġdon 't Ġmix , Ġand Ġnever Ġwill .\n",
      "\t Tokenized LLAMA3:The Ġuniversal Ġcredibility Ġproblem Ġwith Ġpolling Ġis Ġthat Ġwords mith ing Ġand Ġmathematics Ġdon 't Ġmix , Ġand Ġnever Ġwill .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Mathematics is the most important aspect of polling.\n",
      "\t Tokenized GPT2:Mat hemat ics Ġis Ġthe Ġmost Ġimportant Ġaspect Ġof Ġpolling .\n",
      "\t Tokenized LLAMA3:Mat hematics Ġis Ġthe Ġmost Ġimportant Ġaspect Ġof Ġpolling .\n",
      "\t Unique Tokens GPT2: {'hemat', 'ics'}\n",
      "\t Unique Tokens LLAMA3: {'hematics'}\n",
      "==contradiction==\n",
      "Text 1: you don't think it's a deterrent\n",
      "\t Tokenized GPT2:you Ġdon 't Ġthink Ġit 's Ġa Ġdeter rent\n",
      "\t Tokenized LLAMA3:you Ġdon 't Ġthink Ġit 's Ġa Ġdeter rent\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: You have absolutely no doubt whatsoever that it will not be a deterrent\n",
      "\t Tokenized GPT2:You Ġhave Ġabsolutely Ġno Ġdoubt Ġwhatsoever Ġthat Ġit Ġwill Ġnot Ġbe Ġa Ġdeter rent\n",
      "\t Tokenized LLAMA3:You Ġhave Ġabsolutely Ġno Ġdoubt Ġwhatsoever Ġthat Ġit Ġwill Ġnot Ġbe Ġa Ġdeter rent\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: If you missed the two top stories in yesterday's USAT --the government's first post-deregulation attempt to preserve competitiveness among airlines and the emergence of a drug that can prevent breast cancer--they are on the NYT 's front today.\n",
      "\t Tokenized GPT2:If Ġyou Ġmissed Ġthe Ġtwo Ġtop Ġstories Ġin Ġyesterday 's ĠUS AT Ġ-- the Ġgovernment 's Ġfirst Ġpost - d ere g ulation Ġattempt Ġto Ġpreserve Ġcompet it iveness Ġamong Ġairlines Ġand Ġthe Ġemergence Ġof Ġa Ġdrug Ġthat Ġcan Ġprevent Ġbreast Ġcancer -- they Ġare Ġon Ġthe ĠNYT Ġ' s Ġfront Ġtoday .\n",
      "\t Tokenized LLAMA3:If Ġyou Ġmissed Ġthe Ġtwo Ġtop Ġstories Ġin Ġyesterday 's ĠUS AT Ġ-- the Ġgovernment 's Ġfirst Ġpost -d ere g ulation Ġattempt Ġto Ġpreserve Ġcompet it iveness Ġamong Ġa irlines Ġand Ġthe Ġemergence Ġof Ġa Ġdrug Ġthat Ġcan Ġprevent Ġbreast Ġcancer -- they Ġare Ġon Ġthe ĠNYT Ġ' s Ġfront Ġtoday .\n",
      "\t Unique Tokens GPT2: {'-', 'Ġairlines', 'd'}\n",
      "\t Unique Tokens LLAMA3: {'-d', 'irlines'}\n",
      "Text 2: These two story are causing an uproar among wide audiences.\n",
      "\t Tokenized GPT2:These Ġtwo Ġstory Ġare Ġcausing Ġan Ġup ro ar Ġamong Ġwide Ġaudiences .\n",
      "\t Tokenized LLAMA3:These Ġtwo Ġstory Ġare Ġcausing Ġan Ġup ro ar Ġamong Ġwide Ġaudiences .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: He felt the off-hand dagger's weight in the small of his back.\n",
      "\t Tokenized GPT2:He Ġfelt Ġthe Ġoff - hand Ġd agger 's Ġweight Ġin Ġthe Ġsmall Ġof Ġhis Ġback .\n",
      "\t Tokenized LLAMA3:He Ġfelt Ġthe Ġoff -hand Ġd agger 's Ġweight Ġin Ġthe Ġsmall Ġof Ġhis Ġback .\n",
      "\t Unique Tokens GPT2: {'-', 'hand'}\n",
      "\t Unique Tokens LLAMA3: {'-hand'}\n",
      "Text 2: The knife was on his back.\n",
      "\t Tokenized GPT2:The Ġknife Ġwas Ġon Ġhis Ġback .\n",
      "\t Tokenized LLAMA3:The Ġknife Ġwas Ġon Ġhis Ġback .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: so well i think we've taken up at least five minutes\n",
      "\t Tokenized GPT2:so Ġwell Ġi Ġthink Ġwe 've Ġtaken Ġup Ġat Ġleast Ġfive Ġminutes\n",
      "\t Tokenized LLAMA3:so Ġwell Ġi Ġthink Ġwe 've Ġtaken Ġup Ġat Ġleast Ġfive Ġminutes\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I have taken up the last 5 minutes.\n",
      "\t Tokenized GPT2:I Ġhave Ġtaken Ġup Ġthe Ġlast Ġ5 Ġminutes .\n",
      "\t Tokenized LLAMA3:I Ġhave Ġtaken Ġup Ġthe Ġlast Ġ 5 Ġminutes .\n",
      "\t Unique Tokens GPT2: {'Ġ5'}\n",
      "\t Unique Tokens LLAMA3: {'5', 'Ġ'}\n",
      "==contradiction==\n",
      "Text 1: The tree-lined avenue extends less than three blocks to the sea.\n",
      "\t Tokenized GPT2:The Ġtree - lined Ġa venue Ġextends Ġless Ġthan Ġthree Ġblocks Ġto Ġthe Ġsea .\n",
      "\t Tokenized LLAMA3:The Ġtree -l ined Ġa venue Ġextends Ġless Ġthan Ġthree Ġblocks Ġto Ġthe Ġsea .\n",
      "\t Unique Tokens GPT2: {'-', 'lined'}\n",
      "\t Unique Tokens LLAMA3: {'ined', '-l'}\n",
      "Text 2: You must travel two miles via the avenue to the sea.\n",
      "\t Tokenized GPT2:You Ġmust Ġtravel Ġtwo Ġmiles Ġvia Ġthe Ġa venue Ġto Ġthe Ġsea .\n",
      "\t Tokenized LLAMA3:You Ġmust Ġtravel Ġtwo Ġmiles Ġvia Ġthe Ġa venue Ġto Ġthe Ġsea .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Growth &amp\n",
      "\t Tokenized GPT2:G row th Ġ& amp\n",
      "\t Tokenized LLAMA3:G row th Ġ& amp\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Shrinking.\n",
      "\t Tokenized GPT2:Sh r inking .\n",
      "\t Tokenized LLAMA3:Sh r inking .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Many Greeks in Asia Minor were forced to leave their homes and brought an influence of eastern cadences with them.\n",
      "\t Tokenized GPT2:Many ĠGree ks Ġin ĠAsia ĠMin or Ġwere Ġforced Ġto Ġleave Ġtheir Ġhomes Ġand Ġbrought Ġan Ġinfluence Ġof Ġeastern Ġcad ences Ġwith Ġthem .\n",
      "\t Tokenized LLAMA3:Many ĠGree ks Ġin ĠAsia ĠMin or Ġwere Ġforced Ġto Ġleave Ġtheir Ġhomes Ġand Ġbrought Ġan Ġinfluence Ġof Ġeastern Ġcad ences Ġwith Ġthem .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The greens in Asia minor were able to stay in their homes. \n",
      "\t Tokenized GPT2:The Ġg reens Ġin ĠAsia Ġminor Ġwere Ġable Ġto Ġstay Ġin Ġtheir Ġhomes . Ġ\n",
      "\t Tokenized LLAMA3:The Ġg reens Ġin ĠAsia Ġminor Ġwere Ġable Ġto Ġstay Ġin Ġtheir Ġhomes . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: The author began with a set of hunches or hypotheses about what can go wrong in agency management, and what would be evidence supporting-or contradicting-these hypotheses.\n",
      "\t Tokenized GPT2:The Ġauthor Ġbegan Ġwith Ġa Ġset Ġof Ġhun ches Ġor Ġhypot heses Ġabout Ġwhat Ġcan Ġgo Ġwrong Ġin Ġagency Ġmanagement , Ġand Ġwhat Ġwould Ġbe Ġevidence Ġsupporting - or Ġcontrad icting - these Ġhypot heses .\n",
      "\t Tokenized LLAMA3:The Ġauthor Ġbegan Ġwith Ġa Ġset Ġof Ġhun ches Ġor Ġhypot heses Ġabout Ġwhat Ġcan Ġgo Ġwrong Ġin Ġagency Ġmanagement , Ġand Ġwhat Ġwould Ġbe Ġevidence Ġsupporting -or Ġcontrad icting -the se Ġhypot heses .\n",
      "\t Unique Tokens GPT2: {'-', 'these', 'or'}\n",
      "\t Unique Tokens LLAMA3: {'se', '-the', '-or'}\n",
      "Text 2: The author began with a set of theories about the ways in which agency management can go right.\n",
      "\t Tokenized GPT2:The Ġauthor Ġbegan Ġwith Ġa Ġset Ġof Ġtheories Ġabout Ġthe Ġways Ġin Ġwhich Ġagency Ġmanagement Ġcan Ġgo Ġright .\n",
      "\t Tokenized LLAMA3:The Ġauthor Ġbegan Ġwith Ġa Ġset Ġof Ġtheories Ġabout Ġthe Ġways Ġin Ġwhich Ġagency Ġmanagement Ġcan Ġgo Ġright .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: It was going to be a hot day. \n",
      "\t Tokenized GPT2:It Ġwas Ġgoing Ġto Ġbe Ġa Ġhot Ġday . Ġ\n",
      "\t Tokenized LLAMA3:It Ġwas Ġgoing Ġto Ġbe Ġa Ġhot Ġday . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: It was already hot and was going to get hotter.\n",
      "\t Tokenized GPT2:It Ġwas Ġalready Ġhot Ġand Ġwas Ġgoing Ġto Ġget Ġhot ter .\n",
      "\t Tokenized LLAMA3:It Ġwas Ġalready Ġhot Ġand Ġwas Ġgoing Ġto Ġget Ġhot ter .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: In the stock market, however, the damage can get much worse.\n",
      "\t Tokenized GPT2:In Ġthe Ġstock Ġmarket , Ġhowever , Ġthe Ġdamage Ġcan Ġget Ġmuch Ġworse .\n",
      "\t Tokenized LLAMA3:In Ġthe Ġstock Ġmarket , Ġhowever , Ġthe Ġdamage Ġcan Ġget Ġmuch Ġworse .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The damage to the stock market can get much worse when prices increase. \n",
      "\t Tokenized GPT2:The Ġdamage Ġto Ġthe Ġstock Ġmarket Ġcan Ġget Ġmuch Ġworse Ġwhen Ġprices Ġincrease . Ġ\n",
      "\t Tokenized LLAMA3:The Ġdamage Ġto Ġthe Ġstock Ġmarket Ġcan Ġget Ġmuch Ġworse Ġwhen Ġprices Ġincrease . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: i mean that's a real attractive option if you have the the technology for it all it was was you know i mean she just used a phone modem and she was like she was sitting in the office\n",
      "\t Tokenized GPT2:i Ġmean Ġthat 's Ġa Ġreal Ġattractive Ġoption Ġif Ġyou Ġhave Ġthe Ġthe Ġtechnology Ġfor Ġit Ġall Ġit Ġwas Ġwas Ġyou Ġknow Ġi Ġmean Ġshe Ġjust Ġused Ġa Ġphone Ġmodem Ġand Ġshe Ġwas Ġlike Ġshe Ġwas Ġsitting Ġin Ġthe Ġoffice\n",
      "\t Tokenized LLAMA3:i Ġmean Ġthat 's Ġa Ġreal Ġattractive Ġoption Ġif Ġyou Ġhave Ġthe Ġthe Ġtechnology Ġfor Ġit Ġall Ġit Ġwas Ġwas Ġyou Ġknow Ġi Ġmean Ġshe Ġjust Ġused Ġa Ġphone Ġmodem Ġand Ġshe Ġwas Ġlike Ġshe Ġwas Ġsitting Ġin Ġthe Ġoffice\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The phone modem was easy to set up and use. \n",
      "\t Tokenized GPT2:The Ġphone Ġmodem Ġwas Ġeasy Ġto Ġset Ġup Ġand Ġuse . Ġ\n",
      "\t Tokenized LLAMA3:The Ġphone Ġmodem Ġwas Ġeasy Ġto Ġset Ġup Ġand Ġuse . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Small boats tie up here with batches of crayfish, fresh fish, and eel, and housewives clamor for the fishermen to weigh their choices on rudimentary scales.\n",
      "\t Tokenized GPT2:Small Ġboats Ġtie Ġup Ġhere Ġwith Ġbat ches Ġof Ġcr ay fish , Ġfresh Ġfish , Ġand Ġe el , Ġand Ġhouse w ives Ġclam or Ġfor Ġthe Ġfisher men Ġto Ġweigh Ġtheir Ġchoices Ġon Ġrud iment ary Ġscales .\n",
      "\t Tokenized LLAMA3:Small Ġboats Ġtie Ġup Ġhere Ġwith Ġbat ches Ġof Ġcr ay fish , Ġfresh Ġfish , Ġand Ġe el , Ġand Ġhouse w ives Ġclam or Ġfor Ġthe Ġfisher men Ġto Ġweigh Ġtheir Ġchoices Ġon Ġr ud iment ary Ġscales .\n",
      "\t Unique Tokens GPT2: {'Ġrud'}\n",
      "\t Unique Tokens LLAMA3: {'Ġr', 'ud'}\n",
      "Text 2: The fish are weighed using high tech digital scales.\n",
      "\t Tokenized GPT2:The Ġfish Ġare Ġweighed Ġusing Ġhigh Ġtech Ġdigital Ġscales .\n",
      "\t Tokenized LLAMA3:The Ġfish Ġare Ġweighed Ġusing Ġhigh Ġtech Ġdigital Ġscales .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: and once we came here it was like gosh i just miss that because it really is exciting to be around people of different\n",
      "\t Tokenized GPT2:and Ġonce Ġwe Ġcame Ġhere Ġit Ġwas Ġlike Ġgosh Ġi Ġjust Ġmiss Ġthat Ġbecause Ġit Ġreally Ġis Ġexciting Ġto Ġbe Ġaround Ġpeople Ġof Ġdifferent\n",
      "\t Tokenized LLAMA3:and Ġonce Ġwe Ġcame Ġhere Ġit Ġwas Ġlike Ġgosh Ġi Ġjust Ġmiss Ġthat Ġbecause Ġit Ġreally Ġis Ġexciting Ġto Ġbe Ġaround Ġpeople Ġof Ġdifferent\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: This place does not surprise me anymore.  \n",
      "\t Tokenized GPT2:This Ġplace Ġdoes Ġnot Ġsurprise Ġme Ġanymore . ĠĠ\n",
      "\t Tokenized LLAMA3:This Ġplace Ġdoes Ġnot Ġsurprise Ġme Ġanymore . ĠĠ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Too bad it chose to use McIntyre instead.\n",
      "\t Tokenized GPT2:Too Ġbad Ġit Ġchose Ġto Ġuse ĠMc Int y re Ġinstead .\n",
      "\t Tokenized LLAMA3:Too Ġbad Ġit Ġchose Ġto Ġuse ĠMc Int y re Ġinstead .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: McIntyre was not picked to be used.\n",
      "\t Tokenized GPT2:Mc Int y re Ġwas Ġnot Ġpicked Ġto Ġbe Ġused .\n",
      "\t Tokenized LLAMA3:Mc Int y re Ġwas Ġnot Ġpicked Ġto Ġbe Ġused .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: right just get you away from the everyday things that are going on we when the children were smaller we used to go to uh Delaware along the ocean ocean most every year and that was fun we stayed mostly in state parks and uh we really enjoyed that\n",
      "\t Tokenized GPT2:right Ġjust Ġget Ġyou Ġaway Ġfrom Ġthe Ġeveryday Ġthings Ġthat Ġare Ġgoing Ġon Ġwe Ġwhen Ġthe Ġchildren Ġwere Ġsmaller Ġwe Ġused Ġto Ġgo Ġto Ġuh ĠDel aware Ġalong Ġthe Ġocean Ġocean Ġmost Ġevery Ġyear Ġand Ġthat Ġwas Ġfun Ġwe Ġstayed Ġmostly Ġin Ġstate Ġparks Ġand Ġuh Ġwe Ġreally Ġenjoyed Ġthat\n",
      "\t Tokenized LLAMA3:right Ġjust Ġget Ġyou Ġaway Ġfrom Ġthe Ġeveryday Ġthings Ġthat Ġare Ġgoing Ġon Ġwe Ġwhen Ġthe Ġchildren Ġwere Ġsmaller Ġwe Ġused Ġto Ġgo Ġto Ġuh ĠDel aware Ġalong Ġthe Ġocean Ġocean Ġmost Ġevery Ġyear Ġand Ġthat Ġwas Ġfun Ġwe Ġstayed Ġmostly Ġin Ġstate Ġparks Ġand Ġuh Ġwe Ġreally Ġenjoyed Ġthat\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The kids used to prefer the ocean over state parks.\n",
      "\t Tokenized GPT2:The Ġkids Ġused Ġto Ġprefer Ġthe Ġocean Ġover Ġstate Ġparks .\n",
      "\t Tokenized LLAMA3:The Ġkids Ġused Ġto Ġprefer Ġthe Ġocean Ġover Ġstate Ġparks .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Robust  came in third among words and phrases submitted (220 citations in the CR ), and unlike the previous two, it seems to be a genuinely new cliche; at any rate, Chatterbox hadn't previously been aware of its overuse.\n",
      "\t Tokenized GPT2:Rob ust Ġ Ġcame Ġin Ġthird Ġamong Ġwords Ġand Ġphrases Ġsubmitted Ġ( 220 Ġcit ations Ġin Ġthe ĠCR Ġ), Ġand Ġunlike Ġthe Ġprevious Ġtwo , Ġit Ġseems Ġto Ġbe Ġa Ġgenuinely Ġnew Ġcliche ; Ġat Ġany Ġrate , ĠChat ter box Ġhadn 't Ġpreviously Ġbeen Ġaware Ġof Ġits Ġover use .\n",
      "\t Tokenized LLAMA3:Rob ust Ġ Ġcame Ġin Ġthird Ġamong Ġwords Ġand Ġphrases Ġsubmitted Ġ( 220 Ġcit ations Ġin Ġthe ĠCR Ġ), Ġand Ġunlike Ġthe Ġprevious Ġtwo , Ġit Ġseems Ġto Ġbe Ġa Ġgenuinely Ġnew Ġcl iche ; Ġat Ġany Ġrate , ĠChat ter box Ġhadn 't Ġpreviously Ġbeen Ġaware Ġof Ġits Ġover use .\n",
      "\t Unique Tokens GPT2: {'Ġcliche'}\n",
      "\t Unique Tokens LLAMA3: {'iche', 'Ġcl'}\n",
      "Text 2: Chatterbox was surprised by all of the phrases and words that were submitted.\n",
      "\t Tokenized GPT2:Chat ter box Ġwas Ġsurprised Ġby Ġall Ġof Ġthe Ġphrases Ġand Ġwords Ġthat Ġwere Ġsubmitted .\n",
      "\t Tokenized LLAMA3:Chat ter box Ġwas Ġsurprised Ġby Ġall Ġof Ġthe Ġphrases Ġand Ġwords Ġthat Ġwere Ġsubmitted .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Some predict the jokes will wear thin soon, while others call it definitively depraved (Tom Shales, the Washington Post ). (Download a clip from South Park here.)\n",
      "\t Tokenized GPT2:Some Ġpredict Ġthe Ġjokes Ġwill Ġwear Ġthin Ġsoon , Ġwhile Ġothers Ġcall Ġit Ġdefinitive ly Ġde pr aved Ġ( Tom ĠSh ales , Ġthe ĠWashington ĠPost Ġ). Ġ( Download Ġa Ġclip Ġfrom ĠSouth ĠPark Ġhere .)\n",
      "\t Tokenized LLAMA3:Some Ġpredict Ġthe Ġjokes Ġwill Ġwear Ġthin Ġsoon , Ġwhile Ġothers Ġcall Ġit Ġdefinitive ly Ġde pr aved Ġ( Tom ĠSh ales , Ġthe ĠWashington ĠPost Ġ). Ġ( Download Ġa Ġclip Ġfrom ĠSouth ĠPark Ġhere .)\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Everyone thinks the jokes will always be funny.\n",
      "\t Tokenized GPT2:Everyone Ġthinks Ġthe Ġjokes Ġwill Ġalways Ġbe Ġfunny .\n",
      "\t Tokenized LLAMA3:Everyone Ġthinks Ġthe Ġjokes Ġwill Ġalways Ġbe Ġfunny .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: There are a number of expensive jewelry and other duty-free shops, all with goods priced in US dollars (duty-free goods must always be paid for in foreign currency).\n",
      "\t Tokenized GPT2:There Ġare Ġa Ġnumber Ġof Ġexpensive Ġjewelry Ġand Ġother Ġduty - free Ġshops , Ġall Ġwith Ġgoods Ġpriced Ġin ĠUS Ġdollars Ġ( d uty - free Ġgoods Ġmust Ġalways Ġbe Ġpaid Ġfor Ġin Ġforeign Ġcurrency ).\n",
      "\t Tokenized LLAMA3:There Ġare Ġa Ġnumber Ġof Ġexpensive Ġjewelry Ġand Ġother Ġduty -free Ġshops , Ġall Ġwith Ġgoods Ġpriced Ġin ĠUS Ġdollars Ġ( d uty -free Ġgoods Ġmust Ġalways Ġbe Ġpaid Ġfor Ġin Ġforeign Ġcurrency ).\n",
      "\t Unique Tokens GPT2: {'-', 'free'}\n",
      "\t Unique Tokens LLAMA3: {'-free'}\n",
      "Text 2: Be sure to bring currencies other than the US dollar when buying goods from the duty-free shops.\n",
      "\t Tokenized GPT2:Be Ġsure Ġto Ġbring Ġcur rencies Ġother Ġthan Ġthe ĠUS Ġdollar Ġwhen Ġbuying Ġgoods Ġfrom Ġthe Ġduty - free Ġshops .\n",
      "\t Tokenized LLAMA3:Be Ġsure Ġto Ġbring Ġcur rencies Ġother Ġthan Ġthe ĠUS Ġdollar Ġwhen Ġbuying Ġgoods Ġfrom Ġthe Ġduty -free Ġshops .\n",
      "\t Unique Tokens GPT2: {'-', 'free'}\n",
      "\t Unique Tokens LLAMA3: {'-free'}\n",
      "==entailment==\n",
      "Text 1: Some rooms have balconies.\n",
      "\t Tokenized GPT2:Some Ġrooms Ġhave Ġbalcon ies .\n",
      "\t Tokenized LLAMA3:Some Ġrooms Ġhave Ġbalcon ies .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Some rooms have balconies off of them.\n",
      "\t Tokenized GPT2:Some Ġrooms Ġhave Ġbalcon ies Ġoff Ġof Ġthem .\n",
      "\t Tokenized LLAMA3:Some Ġrooms Ġhave Ġbalcon ies Ġoff Ġof Ġthem .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: But you would not trust me.\"\n",
      "\t Tokenized GPT2:But Ġyou Ġwould Ġnot Ġtrust Ġme .\"\n",
      "\t Tokenized LLAMA3:But Ġyou Ġwould Ġnot Ġtrust Ġme .\"\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: You trust me implicitly. \n",
      "\t Tokenized GPT2:You Ġtrust Ġme Ġimplicitly . Ġ\n",
      "\t Tokenized LLAMA3:You Ġtrust Ġme Ġimplicitly . Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: Second, reducing the rate of HIV transmission is in any event not the only social goal worth  If it were, we'd outlaw sex entirely.\n",
      "\t Tokenized GPT2:Second , Ġreducing Ġthe Ġrate Ġof ĠHIV Ġtransmission Ġis Ġin Ġany Ġevent Ġnot Ġthe Ġonly Ġsocial Ġgoal Ġworth Ġ ĠIf Ġit Ġwere , Ġwe 'd Ġout law Ġsex Ġentirely .\n",
      "\t Tokenized LLAMA3:Second , Ġreducing Ġthe Ġrate Ġof ĠHIV Ġtransmission Ġis Ġin Ġany Ġevent Ġnot Ġthe Ġonly Ġsocial Ġgoal Ġworth Ġ ĠIf Ġit Ġwere , Ġwe 'd Ġout law Ġsex Ġentirely .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Reducing the transmission of HIV is just as important as reducing drug abuse.\n",
      "\t Tokenized GPT2:Red ucing Ġthe Ġtransmission Ġof ĠHIV Ġis Ġjust Ġas Ġimportant Ġas Ġreducing Ġdrug Ġabuse .\n",
      "\t Tokenized LLAMA3:Red uc ing Ġthe Ġtransmission Ġof ĠHIV Ġis Ġjust Ġas Ġimportant Ġas Ġreducing Ġdrug Ġabuse .\n",
      "\t Unique Tokens GPT2: {'ucing'}\n",
      "\t Unique Tokens LLAMA3: {'uc', 'ing'}\n",
      "==contradiction==\n",
      "Text 1: um-hum with the ice yeah\n",
      "\t Tokenized GPT2:um - hum Ġwith Ġthe Ġice Ġyeah\n",
      "\t Tokenized LLAMA3:um -h um Ġwith Ġthe Ġice Ġyeah\n",
      "\t Unique Tokens GPT2: {'-', 'hum'}\n",
      "\t Unique Tokens LLAMA3: {'-h'}\n",
      "Text 2: With the sunshine and heat wave yes.\n",
      "\t Tokenized GPT2:With Ġthe Ġsunshine Ġand Ġheat Ġwave Ġyes .\n",
      "\t Tokenized LLAMA3:With Ġthe Ġsunshine Ġand Ġheat Ġwave Ġyes .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Since there is no airport on the island, all visitors must arrive at the port, Skala, where most of the hotels are located and all commercial activity is carried out.\n",
      "\t Tokenized GPT2:Since Ġthere Ġis Ġno Ġairport Ġon Ġthe Ġisland , Ġall Ġvisitors Ġmust Ġarrive Ġat Ġthe Ġport , ĠSk ala , Ġwhere Ġmost Ġof Ġthe Ġhotels Ġare Ġlocated Ġand Ġall Ġcommercial Ġactivity Ġis Ġcarried Ġout .\n",
      "\t Tokenized LLAMA3:Since Ġthere Ġis Ġno Ġairport Ġon Ġthe Ġisland , Ġall Ġvisitors Ġmust Ġarrive Ġat Ġthe Ġport , ĠSk ala , Ġwhere Ġmost Ġof Ġthe Ġhotels Ġare Ġlocated Ġand Ġall Ġcommercial Ġactivity Ġis Ġcarried Ġout .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: The best way to get onto the island is by plane.\n",
      "\t Tokenized GPT2:The Ġbest Ġway Ġto Ġget Ġonto Ġthe Ġisland Ġis Ġby Ġplane .\n",
      "\t Tokenized LLAMA3:The Ġbest Ġway Ġto Ġget Ġonto Ġthe Ġisland Ġis Ġby Ġplane .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: yeah well i was surprised at the the way they drafted last year they didn't really didn't go for the uh big offensive lineman or the defensive lineman they're going for the skilled positions so quarterbacks they really\n",
      "\t Tokenized GPT2:yeah Ġwell Ġi Ġwas Ġsurprised Ġat Ġthe Ġthe Ġway Ġthey Ġdrafted Ġlast Ġyear Ġthey Ġdidn 't Ġreally Ġdidn 't Ġgo Ġfor Ġthe Ġuh Ġbig Ġoffensive Ġl inem an Ġor Ġthe Ġdefensive Ġl inem an Ġthey 're Ġgoing Ġfor Ġthe Ġskilled Ġpositions Ġso Ġquarter backs Ġthey Ġreally\n",
      "\t Tokenized LLAMA3:yeah Ġwell Ġi Ġwas Ġsurprised Ġat Ġthe Ġthe Ġway Ġthey Ġdrafted Ġlast Ġyear Ġthey Ġdidn 't Ġreally Ġdidn 't Ġgo Ġfor Ġthe Ġuh Ġbig Ġoffensive Ġl inem an Ġor Ġthe Ġdefensive Ġl inem an Ġthey 're Ġgoing Ġfor Ġthe Ġskilled Ġpositions Ġso Ġquarter backs Ġthey Ġreally\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I thought they should focus more on big linemen.\n",
      "\t Tokenized GPT2:I Ġthought Ġthey Ġshould Ġfocus Ġmore Ġon Ġbig Ġl inem en .\n",
      "\t Tokenized LLAMA3:I Ġthought Ġthey Ġshould Ġfocus Ġmore Ġon Ġbig Ġl inem en .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: If you have any questions regarding this report, please call me at (202) 512-4841.\n",
      "\t Tokenized GPT2:If Ġyou Ġhave Ġany Ġquestions Ġregarding Ġthis Ġreport , Ġplease Ġcall Ġme Ġat Ġ( 202 ) Ġ512 - 48 41 .\n",
      "\t Tokenized LLAMA3:If Ġyou Ġhave Ġany Ġquestions Ġregarding Ġthis Ġreport , Ġplease Ġcall Ġme Ġat Ġ( 202 ) Ġ 512 - 484 1 .\n",
      "\t Unique Tokens GPT2: {'Ġ512', '48', '41'}\n",
      "\t Unique Tokens LLAMA3: {'484', '1', '512', 'Ġ'}\n",
      "Text 2: My phone number is  (202) 512-4841.\n",
      "\t Tokenized GPT2:My Ġphone Ġnumber Ġis Ġ Ġ( 202 ) Ġ512 - 48 41 .\n",
      "\t Tokenized LLAMA3:My Ġphone Ġnumber Ġis Ġ Ġ( 202 ) Ġ 512 - 484 1 .\n",
      "\t Unique Tokens GPT2: {'Ġ512', '48', '41'}\n",
      "\t Unique Tokens LLAMA3: {'484', '512', '1'}\n",
      "==entailment==\n",
      "Text 1: Finish it, someone yelled.\n",
      "\t Tokenized GPT2:Finish Ġit , Ġsomeone Ġyelled .\n",
      "\t Tokenized LLAMA3:Fin ish Ġit , Ġsomeone Ġyelled .\n",
      "\t Unique Tokens GPT2: {'Finish'}\n",
      "\t Unique Tokens LLAMA3: {'Fin', 'ish'}\n",
      "Text 2: Someone yelled to finish it.\n",
      "\t Tokenized GPT2:Someone Ġyelled Ġto Ġfinish Ġit .\n",
      "\t Tokenized LLAMA3:Someone Ġyelled Ġto Ġfinish Ġit .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: do you really romance\n",
      "\t Tokenized GPT2:do Ġyou Ġreally Ġromance\n",
      "\t Tokenized LLAMA3:do Ġyou Ġreally Ġromance\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Do you really have an affair?\n",
      "\t Tokenized GPT2:Do Ġyou Ġreally Ġhave Ġan Ġaffair ?\n",
      "\t Tokenized LLAMA3:Do Ġyou Ġreally Ġhave Ġan Ġaffair ?\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: i can believe i can believe that\n",
      "\t Tokenized GPT2:i Ġcan Ġbelieve Ġi Ġcan Ġbelieve Ġthat\n",
      "\t Tokenized LLAMA3:i Ġcan Ġbelieve Ġi Ġcan Ġbelieve Ġthat\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: That's something that's believable.\n",
      "\t Tokenized GPT2:That 's Ġsomething Ġthat 's Ġbelievable .\n",
      "\t Tokenized LLAMA3:That 's Ġsomething Ġthat 's Ġbelievable .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: Extremely limited exceptions to the authority are established in 31 U.S.C.\n",
      "\t Tokenized GPT2:Ext remely Ġlimited Ġexceptions Ġto Ġthe Ġauthority Ġare Ġestablished Ġin Ġ31 ĠU . S . C .\n",
      "\t Tokenized LLAMA3:Ext remely Ġlimited Ġexceptions Ġto Ġthe Ġauthority Ġare Ġestablished Ġin Ġ 31 ĠU .S .C .\n",
      "\t Unique Tokens GPT2: {'S', 'Ġ31', 'C'}\n",
      "\t Unique Tokens LLAMA3: {'.S', '.C', '31', 'Ġ'}\n",
      "Text 2: There are only a selected few exceptions.\n",
      "\t Tokenized GPT2:There Ġare Ġonly Ġa Ġselected Ġfew Ġexceptions .\n",
      "\t Tokenized LLAMA3:There Ġare Ġonly Ġa Ġselected Ġfew Ġexceptions .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: He seemed too self-assured.\n",
      "\t Tokenized GPT2:He Ġseemed Ġtoo Ġself - ass ured .\n",
      "\t Tokenized LLAMA3:He Ġseemed Ġtoo Ġself -ass ured .\n",
      "\t Unique Tokens GPT2: {'-', 'ass'}\n",
      "\t Unique Tokens LLAMA3: {'-ass'}\n",
      "Text 2: He is insecure.\n",
      "\t Tokenized GPT2:He Ġis Ġinsecure .\n",
      "\t Tokenized LLAMA3:He Ġis Ġinsecure .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==entailment==\n",
      "Text 1: No one would ever think of sentiment in connection with you.\n",
      "\t Tokenized GPT2:No Ġone Ġwould Ġever Ġthink Ġof Ġsentiment Ġin Ġconnection Ġwith Ġyou .\n",
      "\t Tokenized LLAMA3:No Ġone Ġwould Ġever Ġthink Ġof Ġsentiment Ġin Ġconnection Ġwith Ġyou .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: Sentiment when connecting with you is beyond everyone's expectations.\n",
      "\t Tokenized GPT2:Sent iment Ġwhen Ġconnecting Ġwith Ġyou Ġis Ġbeyond Ġeveryone 's Ġexpectations .\n",
      "\t Tokenized LLAMA3:S ent iment Ġwhen Ġconnecting Ġwith Ġyou Ġis Ġbeyond Ġeveryone 's Ġexpectations .\n",
      "\t Unique Tokens GPT2: {'Sent'}\n",
      "\t Unique Tokens LLAMA3: {'S', 'ent'}\n",
      "==contradiction==\n",
      "Text 1: It's conceivable that some of these allegations are true, and there's no harm in checking them out, as long as the decedent's family agrees to participate.\n",
      "\t Tokenized GPT2:It 's Ġconce ivable Ġthat Ġsome Ġof Ġthese Ġallegations Ġare Ġtrue , Ġand Ġthere 's Ġno Ġharm Ġin Ġchecking Ġthem Ġout , Ġas Ġlong Ġas Ġthe Ġde ced ent 's Ġfamily Ġagrees Ġto Ġparticipate .\n",
      "\t Tokenized LLAMA3:It 's Ġconce ivable Ġthat Ġsome Ġof Ġthese Ġallegations Ġare Ġtrue , Ġand Ġthere 's Ġno Ġharm Ġin Ġchecking Ġthem Ġout , Ġas Ġlong Ġas Ġthe Ġde ced ent 's Ġfamily Ġagrees Ġto Ġparticipate .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: None  of the allegations are true.\n",
      "\t Tokenized GPT2:None Ġ Ġof Ġthe Ġallegations Ġare Ġtrue .\n",
      "\t Tokenized LLAMA3:None Ġ Ġof Ġthe Ġallegations Ġare Ġtrue .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: and the wind started blowing and it was one of my earlier trips to be really out in the middle of\n",
      "\t Tokenized GPT2:and Ġthe Ġwind Ġstarted Ġblowing Ġand Ġit Ġwas Ġone Ġof Ġmy Ġearlier Ġtrips Ġto Ġbe Ġreally Ġout Ġin Ġthe Ġmiddle Ġof\n",
      "\t Tokenized LLAMA3:and Ġthe Ġwind Ġstarted Ġblowing Ġand Ġit Ġwas Ġone Ġof Ġmy Ġearlier Ġtrips Ġto Ġbe Ġreally Ġout Ġin Ġthe Ġmiddle Ġof\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I was glad that the wind was calm.\n",
      "\t Tokenized GPT2:I Ġwas Ġglad Ġthat Ġthe Ġwind Ġwas Ġcalm .\n",
      "\t Tokenized LLAMA3:I Ġwas Ġglad Ġthat Ġthe Ġwind Ġwas Ġcalm .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==contradiction==\n",
      "Text 1: Where would he be today without American commercial know-how?\n",
      "\t Tokenized GPT2:Where Ġwould Ġhe Ġbe Ġtoday Ġwithout ĠAmerican Ġcommercial Ġknow - how ?\n",
      "\t Tokenized LLAMA3:Where Ġwould Ġhe Ġbe Ġtoday Ġwithout ĠAmerican Ġcommercial Ġknow -h ow ?\n",
      "\t Unique Tokens GPT2: {'-', 'how'}\n",
      "\t Unique Tokens LLAMA3: {'ow', '-h'}\n",
      "Text 2: Americans lack important commercial know-how.\n",
      "\t Tokenized GPT2:Americans Ġlack Ġimportant Ġcommercial Ġknow - how .\n",
      "\t Tokenized LLAMA3:Americ ans Ġlack Ġimportant Ġcommercial Ġknow -h ow .\n",
      "\t Unique Tokens GPT2: {'Americans', 'how', '-'}\n",
      "\t Unique Tokens LLAMA3: {'Americ', 'ans', 'ow', '-h'}\n",
      "==neutral==\n",
      "Text 1: Larger ski resorts are 90 minutes away.\n",
      "\t Tokenized GPT2:L ar ger Ġski Ġres orts Ġare Ġ90 Ġminutes Ġaway .\n",
      "\t Tokenized LLAMA3:L ar ger Ġski Ġres orts Ġare Ġ 90 Ġminutes Ġaway .\n",
      "\t Unique Tokens GPT2: {'Ġ90'}\n",
      "\t Unique Tokens LLAMA3: {'90', 'Ġ'}\n",
      "Text 2: The largest resort is actually 100 minutes away.\n",
      "\t Tokenized GPT2:The Ġlargest Ġresort Ġis Ġactually Ġ100 Ġminutes Ġaway .\n",
      "\t Tokenized LLAMA3:The Ġlargest Ġresort Ġis Ġactually Ġ 100 Ġminutes Ġaway .\n",
      "\t Unique Tokens GPT2: {'Ġ100'}\n",
      "\t Unique Tokens LLAMA3: {'100', 'Ġ'}\n",
      "==neutral==\n",
      "Text 1: 'These are human lives.\n",
      "\t Tokenized GPT2:' These Ġare Ġhuman Ġlives .\n",
      "\t Tokenized LLAMA3:'T he se Ġare Ġhuman Ġlives .\n",
      "\t Unique Tokens GPT2: {'These', \"'\"}\n",
      "\t Unique Tokens LLAMA3: {\"'T\", 'se', 'he'}\n",
      "Text 2: Human lives are at stake \n",
      "\t Tokenized GPT2:Human Ġlives Ġare Ġat Ġstake Ġ\n",
      "\t Tokenized LLAMA3:Human Ġlives Ġare Ġat Ġstake Ġ\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "==neutral==\n",
      "Text 1: so uh i hope you like your office\n",
      "\t Tokenized GPT2:so Ġuh Ġi Ġhope Ġyou Ġlike Ġyour Ġoffice\n",
      "\t Tokenized LLAMA3:so Ġuh Ġi Ġhope Ġyou Ġlike Ġyour Ġoffice\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n",
      "Text 2: I hope you like your new office.\n",
      "\t Tokenized GPT2:I Ġhope Ġyou Ġlike Ġyour Ġnew Ġoffice .\n",
      "\t Tokenized LLAMA3:I Ġhope Ġyou Ġlike Ġyour Ġnew Ġoffice .\n",
      "\t Unique Tokens GPT2: set()\n",
      "\t Unique Tokens LLAMA3: set()\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "first_tok = gpt2_tok\n",
    "first_name = \"GPT2\"\n",
    "second_tok = llama3_tok\n",
    "second_name = \"LLAMA3\"\n",
    "for index, row in df_mnli_gpt2_correct_llama3_wrong.iterrows():\n",
    "    if row[\"predictions\"] == 0:\n",
    "        print(\"==entailment==\")\n",
    "    elif row[\"predictions\"] == 1:\n",
    "        print(\"==neutral==\")\n",
    "    elif row[\"predictions\"] == 2:\n",
    "        print(\"==contradiction==\")\n",
    "    print(f\"Text 1: {row['premise'][:300]}\")\n",
    "    print(f\"\\t Tokenized {first_name}:{' '.join(first_tok.tokenize(row['premise'][:300]))}\")\n",
    "    print(f\"\\t Tokenized {second_name}:{' '.join(second_tok.tokenize(row['premise'][:300]))}\")\n",
    "    print(f\"\\t Unique Tokens {first_name}: {set(first_tok.tokenize(row['premise'])) - set(second_tok.tokenize(row['premise']))}\")\n",
    "    print(f\"\\t Unique Tokens {second_name}: {set(second_tok.tokenize(row['premise'])) - set(first_tok.tokenize(row['premise']))}\")\n",
    "    print(f\"Text 2: {row['hypothesis'][:300]}\")   \n",
    "    print(f\"\\t Tokenized {first_name}:{' '.join(first_tok.tokenize(row['hypothesis'][:300]))}\")\n",
    "    print(f\"\\t Tokenized {second_name}:{' '.join(second_tok.tokenize(row['hypothesis'][:300]))}\")\n",
    "    print(f\"\\t Unique Tokens {first_name}: {set(first_tok.tokenize(row['hypothesis'])) - set(second_tok.tokenize(row['hypothesis']))}\")\n",
    "    print(f\"\\t Unique Tokens {second_name}: {set(second_tok.tokenize(row['hypothesis'])) - set(first_tok.tokenize(row['hypothesis']))}\")\n",
    "    i += 1\n",
    "    \n",
    "    if 20 > i > 50: break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T17:08:35.263847Z",
     "start_time": "2025-02-06T17:08:34.983967Z"
    }
   },
   "id": "b1f8e021331e341e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c5a98a66521e40c9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
